/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
2025-09-01 21:44:08,063   INFO  **********************Start logging**********************
2025-09-01 21:44:08,063   INFO  CUDA_VISIBLE_DEVICES=ALL
2025-09-01 21:44:08,063   INFO  Training in distributed mode : total_batch_size: 32
2025-09-01 21:44:08,064   INFO  cfg_file         cfgs/sparse_models/sparse_former_light.yaml
2025-09-01 21:44:08,064   INFO  batch_size       4
2025-09-01 21:44:08,064   INFO  epochs           20
2025-09-01 21:44:08,064   INFO  workers          12
2025-09-01 21:44:08,064   INFO  extra_tag        default
2025-09-01 21:44:08,064   INFO  ckpt             None
2025-09-01 21:44:08,064   INFO  pretrained_model None
2025-09-01 21:44:08,064   INFO  launcher         pytorch
2025-09-01 21:44:08,064   INFO  tcp_port         18888
2025-09-01 21:44:08,064   INFO  sync_bn          True
2025-09-01 21:44:08,064   INFO  fix_random_seed  False
2025-09-01 21:44:08,064   INFO  ckpt_save_interval 1
2025-09-01 21:44:08,064   INFO  local_rank       0
2025-09-01 21:44:08,064   INFO  max_ckpt_save_num 30
2025-09-01 21:44:08,064   INFO  merge_all_iters_to_one_epoch False
2025-09-01 21:44:08,064   INFO  set_cfgs         None
2025-09-01 21:44:08,064   INFO  max_waiting_mins 0
2025-09-01 21:44:08,064   INFO  start_epoch      0
2025-09-01 21:44:08,064   INFO  num_epochs_to_eval 0
2025-09-01 21:44:08,064   INFO  save_to_file     False
2025-09-01 21:44:08,064   INFO  use_tqdm_to_record False
2025-09-01 21:44:08,064   INFO  logger_iter_interval 50
2025-09-01 21:44:08,065   INFO  ckpt_save_time_interval 300
2025-09-01 21:44:08,065   INFO  wo_gpu_stat      True
2025-09-01 21:44:08,065   INFO  use_amp          False
2025-09-01 21:44:08,065   INFO  eval_map         False
2025-09-01 21:44:08,065   INFO  dataset          nuscenes
2025-09-01 21:44:08,065   INFO  root_dir         /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2
2025-09-01 21:44:08,065   INFO  output_dir       /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/sparseformer/all/voxelnext_en_3layer_1lc_zhx_de_1layer_v3_32p_.0diff_.1lr_.1wd
2025-09-01 21:44:08,065   INFO  cfg.ROOT_DIR: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2
2025-09-01 21:44:08,065   INFO  cfg.LOCAL_RANK: 0
2025-09-01 21:44:08,065   INFO  cfg.CLASS_NAMES: ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']
2025-09-01 21:44:08,065   INFO  ----------- DATA_CONFIG -----------
2025-09-01 21:44:08,065   INFO  cfg.DATA_CONFIG.DATASET: NuScenesDataset
2025-09-01 21:44:08,065   INFO  cfg.DATA_CONFIG.DATA_PATH: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/data/nuscenes
2025-09-01 21:44:08,065   INFO  cfg.DATA_CONFIG.VERSION: v1.0-trainval
2025-09-01 21:44:08,065   INFO  cfg.DATA_CONFIG.MAX_SWEEPS: 10
2025-09-01 21:44:08,065   INFO  cfg.DATA_CONFIG.PRED_VELOCITY: True
2025-09-01 21:44:08,065   INFO  cfg.DATA_CONFIG.SET_NAN_VELOCITY_TO_ZEROS: True
2025-09-01 21:44:08,065   INFO  cfg.DATA_CONFIG.FILTER_MIN_POINTS_IN_GT: 1
2025-09-01 21:44:08,065   INFO  ----------- DATA_SPLIT -----------
2025-09-01 21:44:08,065   INFO  cfg.DATA_CONFIG.DATA_SPLIT.train: train
2025-09-01 21:44:08,065   INFO  cfg.DATA_CONFIG.DATA_SPLIT.test: val
2025-09-01 21:44:08,065   INFO  ----------- INFO_PATH -----------
2025-09-01 21:44:08,065   INFO  cfg.DATA_CONFIG.INFO_PATH.train: ['nuscenes_infos_10sweeps_train.pkl']
2025-09-01 21:44:08,066   INFO  cfg.DATA_CONFIG.INFO_PATH.test: ['nuscenes_infos_10sweeps_val.pkl']
2025-09-01 21:44:08,066   INFO  cfg.DATA_CONFIG.POINT_CLOUD_RANGE: [-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]
2025-09-01 21:44:08,066   INFO  cfg.DATA_CONFIG.BALANCED_RESAMPLING: True
2025-09-01 21:44:08,066   INFO  ----------- DATA_AUGMENTOR -----------
2025-09-01 21:44:08,066   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.DISABLE_AUG_LIST: ['placeholder']
2025-09-01 21:44:08,066   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.AUG_CONFIG_LIST: [{'NAME': 'gt_sampling', 'DB_INFO_PATH': ['nuscenes_dbinfos_10sweeps_withvelo.pkl'], 'USE_SHARED_MEMORY': True, 'DB_DATA_PATH': ['nuscenes_10sweeps_withvelo_lidar.npy'], 'PREPARE': {'filter_by_min_points': ['car:5', 'truck:5', 'construction_vehicle:5', 'bus:5', 'trailer:5', 'barrier:5', 'motorcycle:5', 'bicycle:5', 'pedestrian:5', 'traffic_cone:5']}, 'SAMPLE_GROUPS': ['car:2', 'truck:3', 'construction_vehicle:7', 'bus:4', 'trailer:6', 'barrier:2', 'motorcycle:6', 'bicycle:6', 'pedestrian:2', 'traffic_cone:2'], 'NUM_POINT_FEATURES': 5, 'DATABASE_WITH_FAKELIDAR': False, 'REMOVE_EXTRA_WIDTH': [0.0, 0.0, 0.0], 'LIMIT_WHOLE_SCENE': True}, {'NAME': 'random_world_flip', 'ALONG_AXIS_LIST': ['x', 'y']}, {'NAME': 'random_world_rotation', 'WORLD_ROT_ANGLE': [-0.78539816, 0.78539816]}, {'NAME': 'random_world_scaling', 'WORLD_SCALE_RANGE': [0.9, 1.1]}, {'NAME': 'random_world_translation', 'NOISE_TRANSLATE_STD': [0.5, 0.5, 0.5]}]
2025-09-01 21:44:08,067   INFO  ----------- POINT_FEATURE_ENCODING -----------
2025-09-01 21:44:08,067   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.encoding_type: absolute_coordinates_encoding
2025-09-01 21:44:08,067   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.used_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-09-01 21:44:08,067   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.src_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-09-01 21:44:08,067   INFO  cfg.DATA_CONFIG.DATA_PROCESSOR: [{'NAME': 'mask_points_and_boxes_outside_range', 'REMOVE_OUTSIDE_BOXES': True, 'MASK_Z': True}, {'NAME': 'shuffle_points', 'SHUFFLE_ENABLED': {'train': True, 'test': True}}, {'NAME': 'transform_points_to_voxels_placeholder', 'VOXEL_SIZE': [0.075, 0.075, 0.2]}]
2025-09-01 21:44:08,067   INFO  cfg.DATA_CONFIG._BASE_CONFIG_: cfgs/dataset_configs/nuscenes_dataset.yaml
2025-09-01 21:44:08,067   INFO  ----------- MODEL -----------
2025-09-01 21:44:08,067   INFO  cfg.MODEL.NAME: TransFusion
2025-09-01 21:44:08,067   INFO  ----------- VFE -----------
2025-09-01 21:44:08,067   INFO  cfg.MODEL.VFE.NAME: DynamicVoxelVFE
2025-09-01 21:44:08,067   INFO  cfg.MODEL.VFE.USE_NORM: True
2025-09-01 21:44:08,067   INFO  cfg.MODEL.VFE.WITH_DISTANCE: False
2025-09-01 21:44:08,067   INFO  cfg.MODEL.VFE.USE_ABSLOTE_XYZ: True
2025-09-01 21:44:08,067   INFO  cfg.MODEL.VFE.NUM_FILTERS: [64, 64]
2025-09-01 21:44:08,067   INFO  ----------- BACKBONE_3D -----------
2025-09-01 21:44:08,067   INFO  cfg.MODEL.BACKBONE_3D.NAME: VoxelResBackBone8xVoxelNeXt
2025-09-01 21:44:08,067   INFO  cfg.MODEL.BACKBONE_3D.CHANNELS: [64, 64, 64, 128, 128]
2025-09-01 21:44:08,068   INFO  ----------- DENSE_HEAD -----------
2025-09-01 21:44:08,068   INFO  cfg.MODEL.DENSE_HEAD.NAME: SparseFormerHead
2025-09-01 21:44:08,068   INFO  ----------- SPENCODER -----------
2025-09-01 21:44:08,068   INFO  cfg.MODEL.DENSE_HEAD.SPENCODER.NUM_LAYERS: 3
2025-09-01 21:44:08,068   INFO  cfg.MODEL.DENSE_HEAD.SPENCODER.ORDERS: [['z', 'z-trans'], ['hilbert', 'hilbert-trans'], ['x', 'y']]
2025-09-01 21:44:08,068   INFO  ----------- SMSA -----------
2025-09-01 21:44:08,068   INFO  cfg.MODEL.DENSE_HEAD.SPENCODER.SMSA.EMBED_DIM: 128
2025-09-01 21:44:08,068   INFO  cfg.MODEL.DENSE_HEAD.SPENCODER.SMSA.DEPTH: 8
2025-09-01 21:44:08,068   INFO  cfg.MODEL.DENSE_HEAD.SPENCODER.SMSA.NUM_LEVELS: 3
2025-09-01 21:44:08,068   INFO  cfg.MODEL.DENSE_HEAD.SPENCODER.SMSA.WINDOW_SHAPE: [9, 9]
2025-09-01 21:44:08,068   INFO  cfg.MODEL.DENSE_HEAD.SPENCODER.SMSA.DROP_PATH: 0.2
2025-09-01 21:44:08,068   INFO  cfg.MODEL.DENSE_HEAD.SPENCODER.SMSA.SPATIAL_ENHANCE: True
2025-09-01 21:44:08,068   INFO  cfg.MODEL.DENSE_HEAD.SPENCODER.SMSA.FUSED_ADD_NORM: True
2025-09-01 21:44:08,068   INFO  cfg.MODEL.DENSE_HEAD.SPENCODER.SMSA.RMS_NORM: True
2025-09-01 21:44:08,068   INFO  cfg.MODEL.DENSE_HEAD.SPENCODER.SMSA.NORM_EPSILON: 1e-05
2025-09-01 21:44:08,068   INFO  cfg.MODEL.DENSE_HEAD.SPENCODER.SMSA.RESIDUAL_IN_FP32: True
2025-09-01 21:44:08,068   INFO  cfg.MODEL.DENSE_HEAD.SPENCODER.SMSA.DIFF_COEF: 0.0
2025-09-01 21:44:08,068   INFO  cfg.MODEL.DENSE_HEAD.SPENCODER.SMSA.DIFF_KERNEL: [7, 5, 3]
2025-09-01 21:44:08,068   INFO  ----------- SPDECODER -----------
2025-09-01 21:44:08,068   INFO  cfg.MODEL.DENSE_HEAD.SPDECODER.NUM_LAYERS: 1
2025-09-01 21:44:08,068   INFO  cfg.MODEL.DENSE_HEAD.SPDECODER.NUM_LEVELS: 3
2025-09-01 21:44:08,068   INFO  cfg.MODEL.DENSE_HEAD.SPDECODER.WINDOW_SHAPE: [[9, 9], [5, 5], [3, 3]]
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.SPDECODER.NUM_POINTS: [32, 32, 32]
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.SPDECODER.DEPTH: [8, 7, 6]
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.SPDECODER.FFN_DIM: 256
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.SPDECODER.DROPOUT: 0.1
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.SPDECODER.NUM_HEADS: 8
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.SPDECODER.FUSION_HEADS: 4
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.SPDECODER.ORDERS: [['x', 'y']]
2025-09-01 21:44:08,069   INFO  ----------- SDCA -----------
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.SPDECODER.SDCA.EMBED_DIM: 128
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.SPDECODER.SDCA.NUM_HEADS: 8
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.SPDECODER.SDCA.DROPOUT: 0.1
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.CLASS_AGNOSTIC: False
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.USE_BIAS_BEFORE_NORM: False
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.USE_TENSOR_MASK: True
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.USE_DENSE_HEATMAP: True
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.INPUT_FEATURES: 128
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.NUM_PROPOSALS: 200
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.HIDDEN_CHANNEL: 128
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.NUM_CLASSES: 10
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.NUM_HEADS: 8
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.NMS_KERNEL_SIZE: 3
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.FFN_CHANNEL: 256
2025-09-01 21:44:08,069   INFO  cfg.MODEL.DENSE_HEAD.DROPOUT: 0.1
2025-09-01 21:44:08,070   INFO  cfg.MODEL.DENSE_HEAD.BN_MOMENTUM: 0.1
2025-09-01 21:44:08,070   INFO  cfg.MODEL.DENSE_HEAD.ACTIVATION: relu
2025-09-01 21:44:08,070   INFO  cfg.MODEL.DENSE_HEAD.NUM_HM_CONV: 2
2025-09-01 21:44:08,070   INFO  ----------- SEPARATE_HEAD_CFG -----------
2025-09-01 21:44:08,070   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_ORDER: ['center', 'height', 'dim', 'rot', 'vel']
2025-09-01 21:44:08,070   INFO  ----------- HEAD_DICT -----------
2025-09-01 21:44:08,070   INFO  ----------- center -----------
2025-09-01 21:44:08,070   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.out_channels: 2
2025-09-01 21:44:08,070   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.num_conv: 2
2025-09-01 21:44:08,070   INFO  ----------- height -----------
2025-09-01 21:44:08,070   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.height.out_channels: 1
2025-09-01 21:44:08,070   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.height.num_conv: 2
2025-09-01 21:44:08,070   INFO  ----------- dim -----------
2025-09-01 21:44:08,070   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.out_channels: 3
2025-09-01 21:44:08,070   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.num_conv: 2
2025-09-01 21:44:08,070   INFO  ----------- rot -----------
2025-09-01 21:44:08,070   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.out_channels: 2
2025-09-01 21:44:08,070   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.num_conv: 2
2025-09-01 21:44:08,070   INFO  ----------- vel -----------
2025-09-01 21:44:08,070   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.out_channels: 2
2025-09-01 21:44:08,070   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.num_conv: 2
2025-09-01 21:44:08,070   INFO  ----------- TARGET_ASSIGNER_CONFIG -----------
2025-09-01 21:44:08,071   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.DATASET: nuScenes
2025-09-01 21:44:08,071   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.FEATURE_MAP_STRIDE: 8
2025-09-01 21:44:08,071   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.GAUSSIAN_OVERLAP: 0.1
2025-09-01 21:44:08,071   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.MIN_RADIUS: 2
2025-09-01 21:44:08,071   INFO  ----------- HUNGARIAN_ASSIGNER -----------
2025-09-01 21:44:08,071   INFO  ----------- cls_cost -----------
2025-09-01 21:44:08,071   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.gamma: 2.0
2025-09-01 21:44:08,071   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.alpha: 0.25
2025-09-01 21:44:08,071   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.weight: 0.15
2025-09-01 21:44:08,071   INFO  ----------- reg_cost -----------
2025-09-01 21:44:08,071   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.reg_cost.weight: 0.25
2025-09-01 21:44:08,071   INFO  ----------- iou_cost -----------
2025-09-01 21:44:08,071   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.iou_cost.weight: 0.25
2025-09-01 21:44:08,071   INFO  ----------- LOSS_CONFIG -----------
2025-09-01 21:44:08,071   INFO  ----------- LOSS_WEIGHTS -----------
2025-09-01 21:44:08,071   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.cls_weight: 1.0
2025-09-01 21:44:08,071   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.bbox_weight: 0.25
2025-09-01 21:44:08,071   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.hm_weight: 1.0
2025-09-01 21:44:08,071   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.code_weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2]
2025-09-01 21:44:08,071   INFO  ----------- LOSS_CLS -----------
2025-09-01 21:44:08,071   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.use_sigmoid: True
2025-09-01 21:44:08,071   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.gamma: 2.0
2025-09-01 21:44:08,072   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.alpha: 0.25
2025-09-01 21:44:08,072   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_IOU: False
2025-09-01 21:44:08,072   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_IOU_REG: False
2025-09-01 21:44:08,072   INFO  ----------- POST_PROCESSING -----------
2025-09-01 21:44:08,072   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.SCORE_THRESH: 0.0
2025-09-01 21:44:08,072   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.POST_CENTER_RANGE: [-61.2, -61.2, -10.0, 61.2, 61.2, 10.0]
2025-09-01 21:44:08,072   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.USE_IOU_TO_RECTIFY_SCORE: False
2025-09-01 21:44:08,072   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.IOU_RECTIFIER: [0.5]
2025-09-01 21:44:08,072   INFO  ----------- NMS_CONFIG -----------
2025-09-01 21:44:08,072   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_TYPE: nms_gpu
2025-09-01 21:44:08,072   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_THRESH: 0.2
2025-09-01 21:44:08,072   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_PRE_MAXSIZE: 1000
2025-09-01 21:44:08,072   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_POST_MAXSIZE: 100
2025-09-01 21:44:08,072   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.SCORE_THRES: 0.0
2025-09-01 21:44:08,072   INFO  ----------- POST_PROCESSING -----------
2025-09-01 21:44:08,072   INFO  cfg.MODEL.POST_PROCESSING.RECALL_THRESH_LIST: [0.3, 0.5, 0.7]
2025-09-01 21:44:08,072   INFO  cfg.MODEL.POST_PROCESSING.SCORE_THRESH: 0.1
2025-09-01 21:44:08,072   INFO  cfg.MODEL.POST_PROCESSING.OUTPUT_RAW_SCORE: False
2025-09-01 21:44:08,072   INFO  cfg.MODEL.POST_PROCESSING.EVAL_METRIC: kitti
2025-09-01 21:44:08,072   INFO  ----------- OPTIMIZATION -----------
2025-09-01 21:44:08,072   INFO  cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU: 4
2025-09-01 21:44:08,072   INFO  cfg.OPTIMIZATION.NUM_EPOCHS: 20
2025-09-01 21:44:08,073   INFO  cfg.OPTIMIZATION.OPTIMIZER: adam_onecycle
2025-09-01 21:44:08,073   INFO  cfg.OPTIMIZATION.LR: 0.001
2025-09-01 21:44:08,073   INFO  cfg.OPTIMIZATION.WEIGHT_DECAY: 0.01
2025-09-01 21:44:08,073   INFO  cfg.OPTIMIZATION.MOMENTUM: 0.9
2025-09-01 21:44:08,073   INFO  cfg.OPTIMIZATION.MOMS: [0.95, 0.85]
2025-09-01 21:44:08,073   INFO  cfg.OPTIMIZATION.PCT_START: 0.4
2025-09-01 21:44:08,073   INFO  cfg.OPTIMIZATION.DIV_FACTOR: 10
2025-09-01 21:44:08,073   INFO  cfg.OPTIMIZATION.DECAY_STEP_LIST: [35, 45]
2025-09-01 21:44:08,073   INFO  cfg.OPTIMIZATION.LR_DECAY: 0.1
2025-09-01 21:44:08,073   INFO  cfg.OPTIMIZATION.LR_CLIP: 1e-07
2025-09-01 21:44:08,073   INFO  cfg.OPTIMIZATION.LR_WARMUP: False
2025-09-01 21:44:08,073   INFO  cfg.OPTIMIZATION.WARMUP_EPOCH: 1
2025-09-01 21:44:08,073   INFO  cfg.OPTIMIZATION.GRAD_NORM_CLIP: 35
2025-09-01 21:44:08,073   INFO  cfg.OPTIMIZATION.LOSS_SCALE_FP16: 4.0
2025-09-01 21:44:08,073   INFO  ----------- HOOK -----------
2025-09-01 21:44:08,073   INFO  ----------- DisableAugmentationHook -----------
2025-09-01 21:44:08,073   INFO  cfg.HOOK.DisableAugmentationHook.DISABLE_AUG_LIST: ['gt_sampling']
2025-09-01 21:44:08,073   INFO  cfg.HOOK.DisableAugmentationHook.NUM_LAST_EPOCHS: 4
2025-09-01 21:44:08,073   INFO  cfg.TAG: sparse_former_light
2025-09-01 21:44:08,073   INFO  cfg.EXP_GROUP_PATH: sparse_models
2025-09-01 21:44:08,073   INFO  cfg.OUTPUT_DIR: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/sparseformer/all/voxelnext_en_3layer_1lc_zhx_de_1layer_v3_32p_.0diff_.1lr_.1wd
2025-09-01 21:44:08,082   INFO  ----------- Create dataloader & network & optimizer -----------
2025-09-01 21:44:13,319   INFO  Database filter by min points car: 339949 => 294532
2025-09-01 21:44:13,331   INFO  Database filter by min points truck: 65262 => 60344
2025-09-01 21:44:13,333   INFO  Database filter by min points construction_vehicle: 11050 => 10589
2025-09-01 21:44:13,334   INFO  Database filter by min points bus: 12286 => 11619
2025-09-01 21:44:13,337   INFO  Database filter by min points trailer: 19202 => 17934
2025-09-01 21:44:13,350   INFO  Database filter by min points barrier: 107507 => 101993
2025-09-01 21:44:13,352   INFO  Database filter by min points motorcycle: 8846 => 8055
2025-09-01 21:44:13,353   INFO  Database filter by min points bicycle: 8185 => 7531
2025-09-01 21:44:13,371   INFO  Database filter by min points pedestrian: 161928 => 148520
2025-09-01 21:44:13,379   INFO  Database filter by min points traffic_cone: 62964 => 55504
2025-09-01 21:44:13,380   INFO  Loading GT database to shared memory
eflops107:23:23 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:23:23 [0] NCCL INFO Bootstrap : Using bond0:10.16.9.235<0>
eflops107:23:23 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops107:23:23 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops107:23:23 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.18.5+cuda11.8
eflops107:27:27 [4] NCCL INFO cudaDriverVersion 12050
eflops107:25:25 [2] NCCL INFO cudaDriverVersion 12050
eflops107:27:27 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:29:29 [6] NCCL INFO cudaDriverVersion 12050
eflops107:26:26 [3] NCCL INFO cudaDriverVersion 12050
eflops107:25:25 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:30:30 [7] NCCL INFO cudaDriverVersion 12050
eflops107:26:26 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:29:29 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:30:30 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:24:24 [1] NCCL INFO cudaDriverVersion 12050
eflops107:28:28 [5] NCCL INFO cudaDriverVersion 12050
eflops107:24:24 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:28:28 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:27:27 [4] NCCL INFO Bootstrap : Using bond0:10.16.9.235<0>
eflops107:25:25 [2] NCCL INFO Bootstrap : Using bond0:10.16.9.235<0>
eflops107:30:30 [7] NCCL INFO Bootstrap : Using bond0:10.16.9.235<0>
eflops107:27:27 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops107:27:27 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops107:25:25 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops107:25:25 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops107:30:30 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops107:30:30 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops107:24:24 [1] NCCL INFO Bootstrap : Using bond0:10.16.9.235<0>
eflops107:24:24 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops107:24:24 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops107:26:26 [3] NCCL INFO Bootstrap : Using bond0:10.16.9.235<0>
eflops107:29:29 [6] NCCL INFO Bootstrap : Using bond0:10.16.9.235<0>
eflops107:28:28 [5] NCCL INFO Bootstrap : Using bond0:10.16.9.235<0>
eflops107:26:26 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops107:26:26 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops107:29:29 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops107:29:29 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops107:28:28 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops107:28:28 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops107:23:99 [0] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops107:23:99 [0] NCCL INFO P2P plugin IBext
eflops107:23:99 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops107:23:99 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops107:23:99 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops107:23:99 [0] NCCL INFO NET/IB : No device found.
eflops107:23:99 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops107:23:99 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:23:99 [0] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.235<0>
eflops107:23:99 [0] NCCL INFO Using network Socket
eflops107:27:102 [4] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops107:27:102 [4] NCCL INFO P2P plugin IBext
eflops107:27:102 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:25:101 [2] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops107:25:101 [2] NCCL INFO P2P plugin IBext
eflops107:25:101 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops107:27:102 [4] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops107:27:102 [4] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops107:27:102 [4] NCCL INFO NET/IB : No device found.
eflops107:27:102 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops107:27:102 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:27:102 [4] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.235<0>
eflops107:27:102 [4] NCCL INFO Using network Socket

eflops107:25:101 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops107:25:101 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops107:25:101 [2] NCCL INFO NET/IB : No device found.
eflops107:28:106 [5] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops107:28:106 [5] NCCL INFO P2P plugin IBext
eflops107:25:101 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops107:25:101 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:28:106 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:25:101 [2] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.235<0>
eflops107:25:101 [2] NCCL INFO Using network Socket

eflops107:28:106 [5] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops107:28:106 [5] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops107:28:106 [5] NCCL INFO NET/IB : No device found.
eflops107:28:106 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops107:28:106 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:28:106 [5] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.235<0>
eflops107:28:106 [5] NCCL INFO Using network Socket
eflops107:30:100 [7] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops107:30:100 [7] NCCL INFO P2P plugin IBext
eflops107:30:100 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops107:30:100 [7] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops107:30:100 [7] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops107:30:100 [7] NCCL INFO NET/IB : No device found.
eflops107:30:100 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops107:30:100 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:30:100 [7] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.235<0>
eflops107:30:100 [7] NCCL INFO Using network Socket
eflops107:24:103 [1] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops107:24:103 [1] NCCL INFO P2P plugin IBext
eflops107:24:103 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops107:24:103 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops107:24:103 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops107:24:103 [1] NCCL INFO NET/IB : No device found.
eflops107:24:103 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops107:24:103 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:26:105 [3] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops107:26:105 [3] NCCL INFO P2P plugin IBext
eflops107:26:105 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:24:103 [1] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.235<0>
eflops107:24:103 [1] NCCL INFO Using network Socket
eflops107:29:104 [6] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops107:29:104 [6] NCCL INFO P2P plugin IBext
eflops107:29:104 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops107:26:105 [3] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops107:26:105 [3] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops107:26:105 [3] NCCL INFO NET/IB : No device found.
eflops107:26:105 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops107:26:105 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:26:105 [3] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.235<0>
eflops107:26:105 [3] NCCL INFO Using network Socket

eflops107:29:104 [6] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops107:29:104 [6] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops107:29:104 [6] NCCL INFO NET/IB : No device found.
eflops107:29:104 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops107:29:104 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops107:29:104 [6] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.235<0>
eflops107:29:104 [6] NCCL INFO Using network Socket
eflops107:25:101 [2] NCCL INFO comm 0x11728cf0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 53000 commId 0x3ef44be7e5667a69 - Init START
eflops107:24:103 [1] NCCL INFO comm 0x124e62c0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 50000 commId 0x3ef44be7e5667a69 - Init START
eflops107:23:99 [0] NCCL INFO comm 0x123e08e0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 4f000 commId 0x3ef44be7e5667a69 - Init START
eflops107:30:100 [7] NCCL INFO comm 0x1191d730 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a4000 commId 0x3ef44be7e5667a69 - Init START
eflops107:29:104 [6] NCCL INFO comm 0x123424a0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a0000 commId 0x3ef44be7e5667a69 - Init START
eflops107:28:106 [5] NCCL INFO comm 0x11e4aac0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 9d000 commId 0x3ef44be7e5667a69 - Init START
eflops107:27:102 [4] NCCL INFO comm 0x10b676e0 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9c000 commId 0x3ef44be7e5667a69 - Init START
eflops107:26:105 [3] NCCL INFO comm 0x114e21f0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 57000 commId 0x3ef44be7e5667a69 - Init START
eflops107:24:103 [1] NCCL INFO Setting affinity for GPU 1 to 0fffff,ff000000,0fffffff
eflops107:30:100 [7] NCCL INFO Setting affinity for GPU 7 to ffff,fff00000,00ffffff,f0000000
eflops107:23:99 [0] NCCL INFO Setting affinity for GPU 0 to 0fffff,ff000000,0fffffff
eflops107:25:101 [2] NCCL INFO Setting affinity for GPU 2 to 0fffff,ff000000,0fffffff
eflops107:27:102 [4] NCCL INFO Setting affinity for GPU 4 to ffff,fff00000,00ffffff,f0000000
eflops107:29:104 [6] NCCL INFO Setting affinity for GPU 6 to ffff,fff00000,00ffffff,f0000000
eflops107:26:105 [3] NCCL INFO Setting affinity for GPU 3 to 0fffff,ff000000,0fffffff
eflops107:28:106 [5] NCCL INFO Setting affinity for GPU 5 to ffff,fff00000,00ffffff,f0000000
eflops107:23:99 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
eflops107:23:99 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
eflops107:23:99 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
eflops107:23:99 [0] NCCL INFO P2P Chunksize set to 131072
eflops107:30:100 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
eflops107:30:100 [7] NCCL INFO P2P Chunksize set to 131072
eflops107:29:104 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5
eflops107:29:104 [6] NCCL INFO P2P Chunksize set to 131072
eflops107:28:106 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
eflops107:28:106 [5] NCCL INFO P2P Chunksize set to 131072
eflops107:27:102 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
eflops107:27:102 [4] NCCL INFO P2P Chunksize set to 131072
eflops107:26:105 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
eflops107:26:105 [3] NCCL INFO P2P Chunksize set to 131072
eflops107:25:101 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
eflops107:25:101 [2] NCCL INFO P2P Chunksize set to 131072
eflops107:24:103 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
eflops107:24:103 [1] NCCL INFO P2P Chunksize set to 131072
eflops107:25:101 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
eflops107:29:104 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC
eflops107:28:106 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC
eflops107:30:100 [7] NCCL INFO Channel 00 : 7[7] -> 0[0] via SHM/direct/direct
eflops107:24:103 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
eflops107:29:104 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC
eflops107:25:101 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
eflops107:26:105 [3] NCCL INFO Channel 00 : 3[3] -> 4[4] via SHM/direct/direct
eflops107:30:100 [7] NCCL INFO Channel 01 : 7[7] -> 0[0] via SHM/direct/direct
eflops107:28:106 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC
eflops107:24:103 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
eflops107:26:105 [3] NCCL INFO Channel 01 : 3[3] -> 4[4] via SHM/direct/direct
eflops107:23:99 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
eflops107:27:102 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
eflops107:23:99 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
eflops107:27:102 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
eflops107:29:104 [6] NCCL INFO Connected all rings
eflops107:25:101 [2] NCCL INFO Connected all rings
eflops107:29:104 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC
eflops107:23:99 [0] NCCL INFO Connected all rings
eflops107:25:101 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
eflops107:25:101 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
eflops107:24:103 [1] NCCL INFO Connected all rings
eflops107:28:106 [5] NCCL INFO Connected all rings
eflops107:30:100 [7] NCCL INFO Connected all rings
eflops107:30:100 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC
eflops107:27:102 [4] NCCL INFO Connected all rings
eflops107:29:104 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC
eflops107:30:100 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC
eflops107:28:106 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
eflops107:28:106 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
eflops107:24:103 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
eflops107:27:102 [4] NCCL INFO Channel 00 : 4[4] -> 3[3] via SHM/direct/direct
eflops107:30:100 [7] NCCL INFO Connected all trees
eflops107:30:100 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops107:30:100 [7] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops107:27:102 [4] NCCL INFO Channel 01 : 4[4] -> 3[3] via SHM/direct/direct
eflops107:24:103 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
eflops107:26:105 [3] NCCL INFO Connected all rings
eflops107:29:104 [6] NCCL INFO Connected all trees
eflops107:29:104 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops107:29:104 [6] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops107:28:106 [5] NCCL INFO Connected all trees
eflops107:23:99 [0] NCCL INFO Connected all trees
eflops107:28:106 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops107:28:106 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops107:23:99 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops107:23:99 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops107:24:103 [1] NCCL INFO Connected all trees
eflops107:24:103 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops107:24:103 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops107:26:105 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
eflops107:26:105 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
eflops107:25:101 [2] NCCL INFO Connected all trees
eflops107:25:101 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops107:25:101 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops107:26:105 [3] NCCL INFO Connected all trees
eflops107:26:105 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops107:26:105 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops107:27:102 [4] NCCL INFO Connected all trees
eflops107:27:102 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops107:27:102 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops107:23:99 [0] NCCL INFO comm 0x123e08e0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 4f000 commId 0x3ef44be7e5667a69 - Init COMPLETE
eflops107:27:102 [4] NCCL INFO comm 0x10b676e0 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9c000 commId 0x3ef44be7e5667a69 - Init COMPLETE
eflops107:30:100 [7] NCCL INFO comm 0x1191d730 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a4000 commId 0x3ef44be7e5667a69 - Init COMPLETE
eflops107:26:105 [3] NCCL INFO comm 0x114e21f0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 57000 commId 0x3ef44be7e5667a69 - Init COMPLETE
eflops107:24:103 [1] NCCL INFO comm 0x124e62c0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 50000 commId 0x3ef44be7e5667a69 - Init COMPLETE
eflops107:29:104 [6] NCCL INFO comm 0x123424a0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a0000 commId 0x3ef44be7e5667a69 - Init COMPLETE
eflops107:28:106 [5] NCCL INFO comm 0x11e4aac0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 9d000 commId 0x3ef44be7e5667a69 - Init COMPLETE
eflops107:25:101 [2] NCCL INFO comm 0x11728cf0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 53000 commId 0x3ef44be7e5667a69 - Init COMPLETE
2025-09-01 21:44:26,779   INFO  GT database has been saved to shared memory
2025-09-01 21:44:27,054   INFO  Loading NuScenes dataset
2025-09-01 21:44:28,822   INFO  Total samples for NuScenes dataset: 28130
2025-09-01 21:44:29,089   INFO  Total samples after balanced resampling: 123580
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2025-09-01 21:44:30,869   INFO  ----------- Model TransFusion created, param count: 13998174 -----------
2025-09-01 21:44:30,869   INFO  DistributedDataParallel(
  (module): TransFusion(
    (vfe): DynamicVoxelVFE(
      (pfn_layers): ModuleList(
        (0): PFNLayerV2(
          (linear): Linear(in_features=11, out_features=32, bias=False)
          (norm): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
        )
        (1): PFNLayerV2(
          (linear): Linear(in_features=64, out_features=64, bias=False)
          (norm): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
        )
      )
    )
    (backbone_3d): VoxelResBackBone8xVoxelNeXt(
      (conv_input): SparseSequential(
        (0): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
      (conv1): SparseSequential(
        (0): SparseBasicBlock(
          (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (conv2): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (2): SparseBasicBlock(
          (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (conv3): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (2): SparseBasicBlock(
          (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (conv4): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (2): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (conv5): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (2): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (conv6): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (2): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
    )
    (map_to_bev_module): None
    (pfe): None
    (backbone_2d): None
    (dense_head): SparseFormerHead(
      (loss_cls): SigmoidFocalClassificationLoss()
      (loss_bbox): L1Loss()
      (loss_heatmap): GaussianFocalLoss()
      (heatmap_head): SparseSequential(
        (0): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): SubMConv2d(128, 10, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
      )
      (shared_conv): SparseSequential(
        (0): SparseConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (4): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
      )
      (class_encoding): Conv1d(10, 128, kernel_size=(1,), stride=(1,))
      (prediction_head): SeparateHead(
        (center): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (height): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
        (dim): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 3, kernel_size=(1,), stride=(1,))
        )
        (rot): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (vel): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (heatmap): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 10, kernel_size=(1,), stride=(1,))
        )
      )
      (encoder): SPEncoder(
        (blocks): ModuleList(
          (0): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): ZOrderSerialization()
            )
          )
          (1): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): HilbertSerialization()
            )
          )
          (2): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): FlattenWindowsSerialization()
            )
          )
        )
        (embeddings): ModuleList(
          (0-2): 3 x MSSubConvEmbeddingLearned(
            (stem): ModuleList(
              (0-2): 3 x SparseBasicBlock2D(
                (conv1): SubMConv2d(2, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
              )
            )
          )
        )
      )
      (decoder): SPDecoder(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (cross_attn): ModuleList(
          (0): ModuleList(
            (0-2): 3 x SDCA(
              (input_layer): SerializationLayer(
                (serialization): FlattenWindowsSerialization()
              )
              (blocks): ModuleList(
                (0-1): 2 x DeformableAttention(
                  (sampling_offsets): Linear(in_features=128, out_features=256, bias=True)
                  (sampling_weights): Linear(in_features=128, out_features=256, bias=True)
                  (value_proj): Linear(in_features=128, out_features=128, bias=True)
                  (output_proj): Linear(in_features=128, out_features=128, bias=True)
                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (fusions): ModuleList(
          (0): LevelFusion(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
        )
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_posembed): ConvEmbeddingLearned(
          (stem): Sequential(
            (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
        )
        (cross_posembed): MSSubConvEmbeddingLearned(
          (stem): ModuleList(
            (0-2): 3 x SparseBasicBlock2D(
              (conv1): SubMConv2d(2, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
              (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu): ReLU()
              (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            )
          )
        )
      )
    )
    (point_head): None
    (roi_head): None
  )
)
epochs:   0%|          | 0/20 [00:00<?, ?it/s]epochs:   0%|          | 0/20 [00:00<?, ?it/s]epochs:   0%|          | 0/20 [00:00<?, ?it/s]epochs:   0%|          | 0/20 [00:00<?, ?it/s]epochs:   0%|          | 0/20 [00:00<?, ?it/s]epochs:   0%|          | 0/20 [00:00<?, ?it/s]epochs:   0%|          | 0/20 [00:00<?, ?it/s]2025-09-01 21:44:30,875   INFO  **********************Start training sparse_models/sparse_former_light(default)**********************
epochs:   0%|          | 0/20 [00:00<?, ?it/s]
train:   0%|          | 0/3862 [00:00<?, ?it/s][A/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
2025-09-01 21:44:57,924   INFO  Train:    1/20 (  5%) [   0/3862 (  0%)]  Loss: 863.3 (863.)  LR: 1.000e-04  Grad: 35.0000  max=3.4425(module.dense_head.heatmap_head.3.bias)  min: -0.4958(module.dense_head.heatmap_head.1.bias)  NaN: False  loss_hm=856.2700, loss_cls=0.6084, loss_bbox=6.4049, matched_ious=0.0018, d_time=4.74(4.74), f_time=20.37(20.37), b_time=25.11(25.11)  Time cost: 00:25/26:49:40 [00:26/536:33:21]  Acc_iter 1           Data time: 4.74(4.74)  Forward time: 20.37(20.37)  Batch time: 25.11(25.11)
2025-09-01 21:46:06,880   INFO  Train:    1/20 (  5%) [  49/3862 (  1%)]  Loss: 256.9 (419.)  LR: 1.000e-04  Grad: 35.0000  max=4.2526(module.dense_head.heatmap_head.3.bias)  min: -0.7712(module.dense_head.heatmap_head.1.bias)  NaN: False  loss_hm=401.2943, loss_cls=0.4277, loss_bbox=7.8319, matched_ious=0.0031, d_time=0.01(0.10), f_time=1.26(1.78), b_time=1.28(1.88)  Time cost: 01:33/1:59:25 [01:35/40:17:39]  Acc_iter 50          Data time: 0.01(0.10)  Forward time: 1.26(1.78)  Batch time: 1.28(1.88)
2025-09-01 21:47:15,806   INFO  Train:    1/20 (  5%) [  99/3862 (  3%)]  Loss: 154.8 (314.)  LR: 1.000e-04  Grad: 35.0000  max=3.9506(module.dense_head.heatmap_head.3.bias)  min: -0.6929(module.dense_head.heatmap_head.1.bias)  NaN: False  loss_hm=202.6838, loss_cls=0.3717, loss_bbox=7.2066, matched_ious=0.0054, d_time=0.01(0.05), f_time=1.45(1.58), b_time=1.46(1.63)  Time cost: 02:42/1:42:09 [02:44/34:54:18]  Acc_iter 100         Data time: 0.01(0.05)  Forward time: 1.45(1.58)  Batch time: 1.46(1.63)
2025-09-01 21:48:25,959   INFO  Train:    1/20 (  5%) [ 149/3862 (  4%)]  Loss: 95.11 (250.)  LR: 1.001e-04  Grad: 35.0000  max=3.7779(module.dense_head.heatmap_head.3.bias)  min: -0.6520(module.dense_head.heatmap_head.1.bias)  NaN: False  loss_hm=114.7644, loss_cls=0.3457, loss_bbox=6.7224, matched_ious=0.0062, d_time=0.00(0.04), f_time=1.28(1.52), b_time=1.28(1.55)  Time cost: 03:53/1:36:08 [03:55/33:16:13]  Acc_iter 150         Data time: 0.00(0.04)  Forward time: 1.28(1.52)  Batch time: 1.28(1.55)
2025-09-01 21:49:38,528   INFO  Train:    1/20 (  5%) [ 199/3862 (  5%)]  Loss: 72.41 (207.)  LR: 1.001e-04  Grad: 35.0000  max=3.8268(module.dense_head.heatmap_head.3.bias)  min: -0.6529(module.dense_head.heatmap_head.1.weight)  NaN: False  loss_hm=69.7009, loss_cls=0.3279, loss_bbox=6.8202, matched_ious=0.0080, d_time=0.00(0.03), f_time=1.35(1.50), b_time=1.36(1.53)  Time cost: 05:05/1:33:17 [05:07/32:42:05]  Acc_iter 200         Data time: 0.00(0.03)  Forward time: 1.35(1.50)  Batch time: 1.36(1.53)
2025-09-01 21:50:48,177   INFO  Train:    1/20 (  5%) [ 249/3862 (  6%)]  Loss: 29.23 (175.)  LR: 1.001e-04  Grad: 35.0000  max=3.6421(module.dense_head.heatmap_head.3.bias)  min: -0.7167(module.dense_head.heatmap_head.1.weight)  NaN: False  loss_hm=38.4153, loss_cls=0.3254, loss_bbox=6.7407, matched_ious=0.0117, d_time=0.00(0.03), f_time=1.41(1.48), b_time=1.41(1.50)  Time cost: 06:15/1:30:22 [06:17/32:05:56]  Acc_iter 250         Data time: 0.00(0.03)  Forward time: 1.41(1.48)  Batch time: 1.41(1.50)
2025-09-01 21:51:57,927   INFO  Train:    1/20 (  5%) [ 299/3862 (  8%)]  Loss: 22.37 (150.)  LR: 1.002e-04  Grad: 35.0000  max=3.2981(module.dense_head.heatmap_head.3.bias)  min: -0.8578(module.dense_head.encoder.blocks.0.forward_blocks.0.mixer.in_proj.weight)  NaN: False  loss_hm=20.1602, loss_cls=0.3232, loss_bbox=6.5244, matched_ious=0.0107, d_time=0.00(0.02), f_time=1.40(1.46), b_time=1.41(1.48)  Time cost: 07:25/1:28:05 [07:26/31:42:13]  Acc_iter 300         Data time: 0.00(0.02)  Forward time: 1.40(1.46)  Batch time: 1.41(1.48)
2025-09-01 21:53:07,117   INFO  Train:    1/20 (  5%) [ 349/3862 (  9%)]  Loss: 13.29 (131.)  LR: 1.003e-04  Grad: 27.7861  max=2.7793(module.dense_head.heatmap_head.3.bias)  min: -0.6203(module.dense_head.heatmap_head.1.weight)  NaN: False  loss_hm=10.9637, loss_cls=0.3209, loss_bbox=6.4734, matched_ious=0.0111, d_time=0.00(0.02), f_time=1.39(1.45), b_time=1.39(1.47)  Time cost: 08:34/1:26:01 [08:36/31:22:45]  Acc_iter 350         Data time: 0.00(0.02)  Forward time: 1.39(1.45)  Batch time: 1.39(1.47)
2025-09-01 21:54:15,518   INFO  Train:    1/20 (  5%) [ 399/3862 ( 10%)]  Loss: 13.00 (116.)  LR: 1.004e-04  Grad: 16.4991  max=1.5642(module.dense_head.heatmap_head.3.bias)  min: -0.3479(module.dense_head.heatmap_head.1.weight)  NaN: False  loss_hm=6.3778, loss_cls=0.3274, loss_bbox=6.1962, matched_ious=0.0142, d_time=0.00(0.02), f_time=1.40(1.44), b_time=1.40(1.46)  Time cost: 09:42/1:24:03 [09:44/31:05:20]  Acc_iter 400         Data time: 0.00(0.02)  Forward time: 1.40(1.44)  Batch time: 1.40(1.46)
2025-09-01 21:55:30,180   INFO  Train:    1/20 (  5%) [ 449/3862 ( 12%)]  Loss: 9.540 (105.)  LR: 1.005e-04  Grad: 12.5380  max=0.9958(module.dense_head.heatmap_head.3.bias)  min: -0.2918(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=4.4345, loss_cls=0.3297, loss_bbox=5.8013, matched_ious=0.0153, d_time=0.00(0.02), f_time=1.36(1.44), b_time=1.37(1.46)  Time cost: 10:57/1:23:05 [10:59/31:09:20]  Acc_iter 450         Data time: 0.00(0.02)  Forward time: 1.36(1.44)  Batch time: 1.37(1.46)
2025-09-01 21:56:37,317   INFO  Train:    1/20 (  5%) [ 499/3862 ( 13%)]  Loss: 10.67 (95.1)  LR: 1.006e-04  Grad: 13.0877  max=0.5887(module.dense_head.heatmap_head.3.bias)  min: -0.3398(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=3.6949, loss_cls=0.3336, loss_bbox=5.5403, matched_ious=0.0187, d_time=0.00(0.02), f_time=1.27(1.43), b_time=1.27(1.45)  Time cost: 12:04/1:21:12 [12:06/30:53:03]  Acc_iter 500         Data time: 0.00(0.02)  Forward time: 1.27(1.43)  Batch time: 1.27(1.45)
2025-09-01 21:57:45,235   INFO  Train:    1/20 (  5%) [ 549/3862 ( 14%)]  Loss: 9.513 (87.3)  LR: 1.007e-04  Grad: 17.3876  max=0.5257(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5487(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=3.3441, loss_cls=0.3347, loss_bbox=6.1636, matched_ious=0.0169, d_time=0.00(0.02), f_time=1.32(1.43), b_time=1.33(1.44)  Time cost: 13:12/1:19:32 [13:14/30:41:20]  Acc_iter 550         Data time: 0.00(0.02)  Forward time: 1.32(1.43)  Batch time: 1.33(1.44)
2025-09-01 21:58:52,926   INFO  Train:    1/20 (  5%) [ 599/3862 ( 16%)]  Loss: 9.527 (80.9)  LR: 1.008e-04  Grad: 20.4427  max=0.7693(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6291(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=3.0985, loss_cls=0.3366, loss_bbox=6.2858, matched_ious=0.0163, d_time=0.00(0.01), f_time=1.34(1.42), b_time=1.35(1.43)  Time cost: 14:20/1:17:57 [14:21/30:30:54]  Acc_iter 600         Data time: 0.00(0.01)  Forward time: 1.34(1.42)  Batch time: 1.35(1.43)
2025-09-01 21:59:59,124   INFO  Train:    1/20 (  5%) [ 649/3862 ( 17%)]  Loss: 8.430 (75.4)  LR: 1.010e-04  Grad: 23.1524  max=0.9042(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7741(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=2.9220, loss_cls=0.3357, loss_bbox=6.0046, matched_ious=0.0194, d_time=0.00(0.01), f_time=1.28(1.41), b_time=1.28(1.43)  Time cost: 15:26/1:16:18 [15:28/30:18:57]  Acc_iter 650         Data time: 0.00(0.01)  Forward time: 1.28(1.41)  Batch time: 1.28(1.43)
2025-09-01 22:01:05,979   INFO  Train:    1/20 (  5%) [ 699/3862 ( 18%)]  Loss: 8.199 (70.6)  LR: 1.011e-04  Grad: 24.7019  max=0.9113(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8048(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=2.6770, loss_cls=0.3285, loss_bbox=5.7905, matched_ious=0.0236, d_time=0.00(0.01), f_time=2.18(1.40), b_time=2.19(1.42)  Time cost: 16:33/1:14:47 [16:35/30:09:46]  Acc_iter 700         Data time: 0.00(0.01)  Forward time: 2.18(1.40)  Batch time: 2.19(1.42)
2025-09-01 22:02:16,947   INFO  Train:    1/20 (  5%) [ 749/3862 ( 19%)]  Loss: 7.280 (66.5)  LR: 1.013e-04  Grad: 26.2909  max=1.0181(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9566(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=2.5786, loss_cls=0.3238, loss_bbox=5.6665, matched_ious=0.0275, d_time=0.00(0.01), f_time=1.33(1.40), b_time=1.33(1.42)  Time cost: 17:44/1:13:36 [17:46/30:08:39]  Acc_iter 750         Data time: 0.00(0.01)  Forward time: 1.33(1.40)  Batch time: 1.33(1.42)
2025-09-01 22:03:23,277   INFO  Train:    1/20 (  5%) [ 799/3862 ( 21%)]  Loss: 7.817 (62.8)  LR: 1.015e-04  Grad: 27.3448  max=0.9889(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8868(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=2.4713, loss_cls=0.3197, loss_bbox=5.3251, matched_ious=0.0320, d_time=0.01(0.01), f_time=1.43(1.40), b_time=1.43(1.41)  Time cost: 18:50/1:12:07 [18:52/30:00:08]  Acc_iter 800         Data time: 0.01(0.01)  Forward time: 1.43(1.40)  Batch time: 1.43(1.41)
2025-09-01 22:04:30,269   INFO  Train:    1/20 (  5%) [ 849/3862 ( 22%)]  Loss: 7.145 (59.6)  LR: 1.017e-04  Grad: 28.0885  max=1.0378(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9374(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=2.3369, loss_cls=0.3299, loss_bbox=4.7224, matched_ious=0.0449, d_time=0.01(0.01), f_time=1.35(1.40), b_time=1.36(1.41)  Time cost: 19:57/1:10:44 [19:59/29:53:28]  Acc_iter 850         Data time: 0.01(0.01)  Forward time: 1.35(1.40)  Batch time: 1.36(1.41)
2025-09-01 22:05:38,139   INFO  Train:    1/20 (  5%) [ 899/3862 ( 23%)]  Loss: 6.715 (56.6)  LR: 1.019e-04  Grad: 29.5225  max=1.0738(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0692(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=2.2441, loss_cls=0.3273, loss_bbox=4.3755, matched_ious=0.0628, d_time=0.00(0.01), f_time=1.36(1.39), b_time=1.36(1.41)  Time cost: 21:05/1:09:25 [21:07/29:48:40]  Acc_iter 900         Data time: 0.00(0.01)  Forward time: 1.36(1.39)  Batch time: 1.36(1.41)
2025-09-01 22:06:44,850   INFO  Train:    1/20 (  5%) [ 949/3862 ( 25%)]  Loss: 6.049 (54.0)  LR: 1.021e-04  Grad: 30.3391  max=1.1161(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1123(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=2.1505, loss_cls=0.3215, loss_bbox=4.2796, matched_ious=0.0755, d_time=0.00(0.01), f_time=1.33(1.39), b_time=1.33(1.40)  Time cost: 22:11/1:08:04 [22:13/29:42:43]  Acc_iter 950         Data time: 0.00(0.01)  Forward time: 1.33(1.39)  Batch time: 1.33(1.40)
2025-09-01 22:07:52,327   INFO  Train:    1/20 (  5%) [ 999/3862 ( 26%)]  Loss: 6.349 (51.6)  LR: 1.023e-04  Grad: 31.1464  max=1.1225(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1536(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=2.0728, loss_cls=0.3223, loss_bbox=3.8089, matched_ious=0.0919, d_time=0.01(0.01), f_time=1.23(1.39), b_time=1.24(1.40)  Time cost: 23:19/1:06:46 [23:21/29:38:12]  Acc_iter 1000        Data time: 0.01(0.01)  Forward time: 1.23(1.39)  Batch time: 1.24(1.40)
2025-09-01 22:09:00,102   INFO  Train:    1/20 (  5%) [1049/3862 ( 27%)]  Loss: 5.711 (49.4)  LR: 1.026e-04  Grad: 31.7727  max=1.1088(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1615(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.9900, loss_cls=0.3157, loss_bbox=3.6189, matched_ious=0.1062, d_time=0.00(0.01), f_time=1.31(1.39), b_time=1.31(1.40)  Time cost: 24:27/1:05:30 [24:29/29:34:21]  Acc_iter 1050        Data time: 0.00(0.01)  Forward time: 1.31(1.39)  Batch time: 1.31(1.40)
2025-09-01 22:10:10,806   INFO  Train:    1/20 (  5%) [1099/3862 ( 28%)]  Loss: 5.709 (47.5)  LR: 1.028e-04  Grad: 32.6281  max=1.0943(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2020(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.9342, loss_cls=0.3052, loss_bbox=3.4623, matched_ious=0.1176, d_time=0.00(0.01), f_time=1.28(1.39), b_time=1.29(1.40)  Time cost: 25:37/1:04:22 [25:39/29:34:11]  Acc_iter 1100        Data time: 0.00(0.01)  Forward time: 1.28(1.39)  Batch time: 1.29(1.40)
2025-09-01 22:11:17,777   INFO  Train:    1/20 (  5%) [1149/3862 ( 30%)]  Loss: 5.586 (45.6)  LR: 1.031e-04  Grad: 33.2345  max=1.1053(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1933(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.9098, loss_cls=0.3053, loss_bbox=3.2613, matched_ious=0.1305, d_time=0.00(0.01), f_time=1.24(1.38), b_time=1.25(1.40)  Time cost: 26:44/1:03:06 [26:46/29:29:47]  Acc_iter 1150        Data time: 0.00(0.01)  Forward time: 1.24(1.38)  Batch time: 1.25(1.40)
2025-09-01 22:12:25,528   INFO  Train:    1/20 (  5%) [1199/3862 ( 31%)]  Loss: 5.224 (43.9)  LR: 1.033e-04  Grad: 33.9511  max=1.1030(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2071(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.8302, loss_cls=0.2948, loss_bbox=3.1063, matched_ious=0.1434, d_time=0.00(0.01), f_time=1.41(1.38), b_time=1.42(1.39)  Time cost: 27:52/1:01:51 [27:54/29:26:29]  Acc_iter 1200        Data time: 0.00(0.01)  Forward time: 1.41(1.38)  Batch time: 1.42(1.39)
2025-09-01 22:13:33,724   INFO  Train:    1/20 (  5%) [1249/3862 ( 32%)]  Loss: 4.515 (42.4)  LR: 1.036e-04  Grad: 34.3978  max=1.1302(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1826(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.8089, loss_cls=0.2980, loss_bbox=2.9168, matched_ious=0.1543, d_time=0.00(0.01), f_time=1.29(1.38), b_time=1.29(1.39)  Time cost: 29:00/1:00:38 [29:02/29:23:48]  Acc_iter 1250        Data time: 0.00(0.01)  Forward time: 1.29(1.38)  Batch time: 1.29(1.39)
2025-09-01 22:14:41,319   INFO  Train:    1/20 (  5%) [1299/3862 ( 34%)]  Loss: 4.604 (40.9)  LR: 1.039e-04  Grad: 34.8795  max=1.1417(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1850(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.7561, loss_cls=0.2915, loss_bbox=2.7780, matched_ious=0.1592, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.30(1.39)  Time cost: 30:08/59:25 [30:10/29:20:39]  Acc_iter 1300        Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.30(1.39)
2025-09-01 22:15:49,410   INFO  Train:    1/20 (  5%) [1349/3862 ( 35%)]  Loss: 4.483 (39.6)  LR: 1.042e-04  Grad: 35.0000  max=1.1502(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1778(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.6855, loss_cls=0.2782, loss_bbox=2.8138, matched_ious=0.1635, d_time=0.01(0.01), f_time=2.31(1.38), b_time=2.32(1.39)  Time cost: 31:16/58:13 [31:18/29:18:08]  Acc_iter 1350        Data time: 0.01(0.01)  Forward time: 2.31(1.38)  Batch time: 2.32(1.39)
2025-09-01 22:16:59,173   INFO  Train:    1/20 (  5%) [1399/3862 ( 36%)]  Loss: 4.413 (38.4)  LR: 1.045e-04  Grad: 34.8508  max=1.1374(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1510(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.6751, loss_cls=0.2777, loss_bbox=2.5967, matched_ious=0.1753, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.33(1.39)  Time cost: 32:26/57:04 [32:28/29:17:13]  Acc_iter 1400        Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.33(1.39)
2025-09-01 22:18:06,197   INFO  Train:    1/20 (  5%) [1449/3862 ( 38%)]  Loss: 4.524 (37.2)  LR: 1.049e-04  Grad: 35.0000  max=1.1275(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1497(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.6691, loss_cls=0.2774, loss_bbox=2.6286, matched_ious=0.1810, d_time=0.00(0.01), f_time=1.23(1.38), b_time=1.24(1.39)  Time cost: 33:33/55:50 [33:35/29:13:53]  Acc_iter 1450        Data time: 0.00(0.01)  Forward time: 1.23(1.38)  Batch time: 1.24(1.39)
2025-09-01 22:19:13,901   INFO  Train:    1/20 (  5%) [1499/3862 ( 39%)]  Loss: 4.357 (36.1)  LR: 1.052e-04  Grad: 34.8709  max=1.1246(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1307(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.6333, loss_cls=0.2729, loss_bbox=2.4783, matched_ious=0.1890, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.31(1.39)  Time cost: 34:40/54:38 [34:42/29:11:17]  Acc_iter 1500        Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.31(1.39)
2025-09-01 22:20:22,060   INFO  Train:    1/20 (  5%) [1549/3862 ( 40%)]  Loss: 4.151 (35.1)  LR: 1.056e-04  Grad: 34.9782  max=1.1209(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1406(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.6001, loss_cls=0.2690, loss_bbox=2.4524, matched_ious=0.1971, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.33(1.39)  Time cost: 35:49/53:27 [35:51/29:09:08]  Acc_iter 1550        Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.33(1.39)
2025-09-01 22:21:29,002   INFO  Train:    1/20 (  5%) [1599/3862 ( 41%)]  Loss: 4.511 (34.1)  LR: 1.059e-04  Grad: 34.9991  max=1.1220(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1391(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.5981, loss_cls=0.2686, loss_bbox=2.4735, matched_ious=0.1962, d_time=0.01(0.01), f_time=1.28(1.37), b_time=1.29(1.39)  Time cost: 36:56/52:14 [36:58/29:06:06]  Acc_iter 1600        Data time: 0.01(0.01)  Forward time: 1.28(1.37)  Batch time: 1.29(1.39)
2025-09-01 22:22:40,086   INFO  Train:    1/20 (  5%) [1649/3862 ( 43%)]  Loss: 4.585 (33.2)  LR: 1.063e-04  Grad: 34.9321  max=1.1276(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1377(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.5546, loss_cls=0.2625, loss_bbox=2.4262, matched_ious=0.2037, d_time=0.00(0.01), f_time=2.21(1.38), b_time=2.21(1.39)  Time cost: 38:07/51:07 [38:09/29:06:21]  Acc_iter 1650        Data time: 0.00(0.01)  Forward time: 2.21(1.38)  Batch time: 2.21(1.39)
2025-09-01 22:23:46,780   INFO  Train:    1/20 (  5%) [1699/3862 ( 44%)]  Loss: 3.446 (32.4)  LR: 1.067e-04  Grad: 35.0000  max=1.1247(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1400(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.5761, loss_cls=0.2680, loss_bbox=2.3288, matched_ious=0.2093, d_time=0.00(0.01), f_time=1.28(1.37), b_time=1.28(1.38)  Time cost: 39:13/49:54 [39:15/29:03:16]  Acc_iter 1700        Data time: 0.00(0.01)  Forward time: 1.28(1.37)  Batch time: 1.28(1.38)
2025-09-01 22:24:55,116   INFO  Train:    1/20 (  5%) [1749/3862 ( 45%)]  Loss: 4.030 (31.5)  LR: 1.071e-04  Grad: 35.0000  max=1.1279(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1141(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.4831, loss_cls=0.2535, loss_bbox=2.2535, matched_ious=0.2128, d_time=0.00(0.01), f_time=1.35(1.37), b_time=1.36(1.38)  Time cost: 40:22/48:44 [40:24/29:01:28]  Acc_iter 1750        Data time: 0.00(0.01)  Forward time: 1.35(1.37)  Batch time: 1.36(1.38)
2025-09-01 22:26:02,210   INFO  Train:    1/20 (  5%) [1799/3862 ( 47%)]  Loss: 3.130 (30.8)  LR: 1.075e-04  Grad: 34.9762  max=1.0989(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1249(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.5242, loss_cls=0.2650, loss_bbox=2.2416, matched_ious=0.2182, d_time=0.00(0.01), f_time=1.28(1.37), b_time=1.28(1.38)  Time cost: 41:29/47:33 [41:31/28:58:50]  Acc_iter 1800        Data time: 0.00(0.01)  Forward time: 1.28(1.37)  Batch time: 1.28(1.38)
2025-09-01 22:27:10,054   INFO  Train:    1/20 (  5%) [1849/3862 ( 48%)]  Loss: 3.468 (30.1)  LR: 1.079e-04  Grad: 35.0000  max=1.0915(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1279(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.4512, loss_cls=0.2509, loss_bbox=2.1904, matched_ious=0.2211, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 42:37/46:22 [42:39/28:56:48]  Acc_iter 1850        Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-01 22:28:16,945   INFO  Train:    1/20 (  5%) [1899/3862 ( 49%)]  Loss: 3.409 (29.4)  LR: 1.084e-04  Grad: 34.9782  max=1.0927(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1229(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.4468, loss_cls=0.2505, loss_bbox=2.2274, matched_ious=0.2238, d_time=0.01(0.01), f_time=1.35(1.37), b_time=1.37(1.38)  Time cost: 43:44/45:11 [43:46/28:54:11]  Acc_iter 1900        Data time: 0.01(0.01)  Forward time: 1.35(1.37)  Batch time: 1.37(1.38)
2025-09-01 22:29:25,828   INFO  Train:    1/20 (  5%) [1949/3862 ( 50%)]  Loss: 3.299 (28.7)  LR: 1.088e-04  Grad: 34.9520  max=1.0799(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1222(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.4811, loss_cls=0.2573, loss_bbox=2.1535, matched_ious=0.2283, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.39(1.38)  Time cost: 44:52/44:01 [44:54/28:52:55]  Acc_iter 1950        Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.39(1.38)
2025-09-01 22:30:33,085   INFO  Train:    1/20 (  5%) [1999/3862 ( 52%)]  Loss: 3.793 (28.1)  LR: 1.093e-04  Grad: 35.0000  max=1.0851(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1183(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.4208, loss_cls=0.2479, loss_bbox=2.1066, matched_ious=0.2324, d_time=0.00(0.01), f_time=1.27(1.37), b_time=1.27(1.38)  Time cost: 46:00/42:51 [46:02/28:50:39]  Acc_iter 2000        Data time: 0.00(0.01)  Forward time: 1.27(1.37)  Batch time: 1.27(1.38)
2025-09-01 22:31:41,250   INFO  Train:    1/20 (  5%) [2049/3862 ( 53%)]  Loss: 3.925 (27.5)  LR: 1.097e-04  Grad: 34.9951  max=1.0826(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1359(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.3883, loss_cls=0.2417, loss_bbox=1.9964, matched_ious=0.2345, d_time=0.00(0.01), f_time=1.25(1.37), b_time=1.26(1.38)  Time cost: 47:08/41:41 [47:10/28:48:59]  Acc_iter 2050        Data time: 0.00(0.01)  Forward time: 1.25(1.37)  Batch time: 1.26(1.38)
2025-09-01 22:32:49,278   INFO  Train:    1/20 (  5%) [2099/3862 ( 54%)]  Loss: 4.241 (26.9)  LR: 1.102e-04  Grad: 34.9840  max=1.0763(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1252(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.3935, loss_cls=0.2475, loss_bbox=2.0693, matched_ious=0.2406, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 48:16/40:31 [48:18/28:47:16]  Acc_iter 2100        Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-01 22:33:57,083   INFO  Train:    1/20 (  5%) [2149/3862 ( 56%)]  Loss: 3.807 (26.4)  LR: 1.107e-04  Grad: 34.9249  max=1.0687(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1419(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.3657, loss_cls=0.2461, loss_bbox=1.9311, matched_ious=0.2475, d_time=0.01(0.01), f_time=1.32(1.37), b_time=1.33(1.38)  Time cost: 49:24/39:21 [49:26/28:45:26]  Acc_iter 2150        Data time: 0.01(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.38)
2025-09-01 22:35:04,252   INFO  Train:    1/20 (  5%) [2199/3862 ( 57%)]  Loss: 3.141 (25.9)  LR: 1.112e-04  Grad: 34.9370  max=1.0750(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1342(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.3321, loss_cls=0.2309, loss_bbox=2.0506, matched_ious=0.2404, d_time=0.00(0.01), f_time=1.27(1.37), b_time=1.28(1.38)  Time cost: 50:31/38:11 [50:33/28:43:17]  Acc_iter 2200        Data time: 0.00(0.01)  Forward time: 1.27(1.37)  Batch time: 1.28(1.38)
2025-09-01 22:36:15,314   INFO  Train:    1/20 (  5%) [2249/3862 ( 58%)]  Loss: 3.453 (25.4)  LR: 1.117e-04  Grad: 34.9817  max=1.0838(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1184(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.3220, loss_cls=0.2341, loss_bbox=2.0315, matched_ious=0.2434, d_time=0.00(0.01), f_time=1.40(1.37), b_time=1.40(1.38)  Time cost: 51:42/37:04 [51:44/28:43:21]  Acc_iter 2250        Data time: 0.00(0.01)  Forward time: 1.40(1.37)  Batch time: 1.40(1.38)
2025-09-01 22:37:22,781   INFO  Train:    1/20 (  5%) [2299/3862 ( 60%)]  Loss: 3.265 (24.9)  LR: 1.122e-04  Grad: 34.9968  max=1.0828(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1336(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.3262, loss_cls=0.2364, loss_bbox=1.9502, matched_ious=0.2508, d_time=0.00(0.01), f_time=1.30(1.37), b_time=1.30(1.38)  Time cost: 52:49/35:54 [52:51/28:41:24]  Acc_iter 2300        Data time: 0.00(0.01)  Forward time: 1.30(1.37)  Batch time: 1.30(1.38)
2025-09-01 22:38:31,995   INFO  Train:    1/20 (  5%) [2349/3862 ( 61%)]  Loss: 3.604 (24.4)  LR: 1.128e-04  Grad: 35.0000  max=1.0759(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1255(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.3531, loss_cls=0.2385, loss_bbox=1.9998, matched_ious=0.2432, d_time=0.00(0.01), f_time=1.26(1.37), b_time=1.26(1.38)  Time cost: 53:59/34:45 [54:01/28:40:24]  Acc_iter 2350        Data time: 0.00(0.01)  Forward time: 1.26(1.37)  Batch time: 1.26(1.38)
2025-09-01 22:39:39,263   INFO  Train:    1/20 (  5%) [2399/3862 ( 62%)]  Loss: 3.256 (24.0)  LR: 1.133e-04  Grad: 34.9975  max=1.0631(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1203(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.3105, loss_cls=0.2356, loss_bbox=1.9458, matched_ious=0.2549, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 55:06/33:35 [55:08/28:38:24]  Acc_iter 2400        Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-01 22:40:46,315   INFO  Train:    1/20 (  5%) [2449/3862 ( 63%)]  Loss: 3.459 (23.6)  LR: 1.139e-04  Grad: 34.9923  max=1.0623(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1227(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2864, loss_cls=0.2259, loss_bbox=1.9408, matched_ious=0.2534, d_time=0.01(0.01), f_time=1.32(1.37), b_time=1.33(1.38)  Time cost: 56:13/32:25 [56:15/28:36:19]  Acc_iter 2450        Data time: 0.01(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.38)
2025-09-01 22:41:55,577   INFO  Train:    1/20 (  5%) [2499/3862 ( 65%)]  Loss: 3.856 (23.2)  LR: 1.145e-04  Grad: 34.9526  max=1.0532(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1305(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2805, loss_cls=0.2316, loss_bbox=1.9072, matched_ious=0.2538, d_time=0.00(0.01), f_time=2.25(1.37), b_time=2.25(1.38)  Time cost: 57:22/31:16 [57:24/28:35:23]  Acc_iter 2500        Data time: 0.00(0.01)  Forward time: 2.25(1.37)  Batch time: 2.25(1.38)
2025-09-01 22:43:04,699   INFO  Train:    1/20 (  5%) [2549/3862 ( 66%)]  Loss: 3.653 (22.8)  LR: 1.150e-04  Grad: 35.0000  max=1.0448(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1266(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2720, loss_cls=0.2276, loss_bbox=1.8710, matched_ious=0.2579, d_time=0.00(0.01), f_time=1.25(1.37), b_time=1.26(1.38)  Time cost: 58:31/30:08 [58:33/28:34:22]  Acc_iter 2550        Data time: 0.00(0.01)  Forward time: 1.25(1.37)  Batch time: 1.26(1.38)
2025-09-01 22:44:12,619   INFO  Train:    1/20 (  5%) [2599/3862 ( 67%)]  Loss: 3.406 (22.4)  LR: 1.156e-04  Grad: 34.9874  max=1.0556(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1252(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2691, loss_cls=0.2247, loss_bbox=1.9120, matched_ious=0.2564, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.30(1.38)  Time cost: 59:39/28:58 [59:41/28:32:46]  Acc_iter 2600        Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.30(1.38)
2025-09-01 22:45:21,016   INFO  Train:    1/20 (  5%) [2649/3862 ( 69%)]  Loss: 3.688 (22.1)  LR: 1.162e-04  Grad: 34.9389  max=1.0459(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1205(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2545, loss_cls=0.2267, loss_bbox=1.8472, matched_ious=0.2611, d_time=0.00(0.01), f_time=1.28(1.37), b_time=1.28(1.38)  Time cost: 1:00:48/27:49 [1:00:50/28:31:25]  Acc_iter 2650        Data time: 0.00(0.01)  Forward time: 1.28(1.37)  Batch time: 1.28(1.38)
2025-09-01 22:46:28,087   INFO  Train:    1/20 (  5%) [2699/3862 ( 70%)]  Loss: 3.384 (21.7)  LR: 1.168e-04  Grad: 35.0000  max=1.0569(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1196(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2282, loss_cls=0.2209, loss_bbox=1.8290, matched_ious=0.2695, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.32(1.38)  Time cost: 1:01:55/26:40 [1:01:57/28:29:27]  Acc_iter 2700        Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.32(1.38)
2025-09-01 22:47:35,666   INFO  Train:    1/20 (  5%) [2749/3862 ( 71%)]  Loss: 3.251 (21.4)  LR: 1.175e-04  Grad: 34.9650  max=1.0492(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1209(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2227, loss_cls=0.2230, loss_bbox=1.8106, matched_ious=0.2643, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.32(1.38)  Time cost: 1:03:02/25:30 [1:03:04/28:27:45]  Acc_iter 2750        Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.32(1.38)
2025-09-01 22:48:47,365   INFO  Train:    1/20 (  5%) [2799/3862 ( 72%)]  Loss: 3.052 (21.1)  LR: 1.181e-04  Grad: 35.0000  max=1.0472(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1276(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2177, loss_cls=0.2226, loss_bbox=1.7723, matched_ious=0.2673, d_time=0.01(0.01), f_time=1.62(1.37), b_time=1.63(1.38)  Time cost: 1:04:14/24:23 [1:04:16/28:27:54]  Acc_iter 2800        Data time: 0.01(0.01)  Forward time: 1.62(1.37)  Batch time: 1.63(1.38)
2025-09-01 22:49:55,964   INFO  Train:    1/20 (  5%) [2849/3862 ( 74%)]  Loss: 3.290 (20.7)  LR: 1.188e-04  Grad: 34.9610  max=1.0446(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1220(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2123, loss_cls=0.2204, loss_bbox=1.8333, matched_ious=0.2704, d_time=0.00(0.01), f_time=1.42(1.37), b_time=1.43(1.38)  Time cost: 1:05:23/23:14 [1:05:25/28:26:39]  Acc_iter 2850        Data time: 0.00(0.01)  Forward time: 1.42(1.37)  Batch time: 1.43(1.38)
2025-09-01 22:51:03,871   INFO  Train:    1/20 (  5%) [2899/3862 ( 75%)]  Loss: 3.409 (20.4)  LR: 1.194e-04  Grad: 34.9006  max=1.0355(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1221(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1953, loss_cls=0.2150, loss_bbox=1.8253, matched_ious=0.2697, d_time=0.00(0.01), f_time=1.40(1.37), b_time=1.40(1.38)  Time cost: 1:06:30/22:05 [1:06:32/28:25:07]  Acc_iter 2900        Data time: 0.00(0.01)  Forward time: 1.40(1.37)  Batch time: 1.40(1.38)
2025-09-01 22:52:10,458   INFO  Train:    1/20 (  5%) [2949/3862 ( 76%)]  Loss: 3.120 (20.1)  LR: 1.201e-04  Grad: 34.9587  max=1.0161(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1242(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2027, loss_cls=0.2145, loss_bbox=1.8388, matched_ious=0.2644, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.38(1.38)  Time cost: 1:07:37/20:55 [1:07:39/28:23:02]  Acc_iter 2950        Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.38(1.38)
2025-09-01 22:53:19,036   INFO  Train:    1/20 (  5%) [2999/3862 ( 78%)]  Loss: 3.215 (19.9)  LR: 1.208e-04  Grad: 35.0000  max=1.0245(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1214(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1894, loss_cls=0.2143, loss_bbox=1.7970, matched_ious=0.2681, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.37(1.38)  Time cost: 1:08:46/19:46 [1:08:48/28:21:49]  Acc_iter 3000        Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.37(1.38)
2025-09-01 22:54:27,675   INFO  Train:    1/20 (  5%) [3049/3862 ( 79%)]  Loss: 3.348 (19.6)  LR: 1.215e-04  Grad: 34.9579  max=1.0233(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1140(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1893, loss_cls=0.2201, loss_bbox=1.7950, matched_ious=0.2746, d_time=0.00(0.01), f_time=1.44(1.37), b_time=1.44(1.38)  Time cost: 1:09:54/18:38 [1:09:56/28:20:37]  Acc_iter 3050        Data time: 0.00(0.01)  Forward time: 1.44(1.37)  Batch time: 1.44(1.38)
2025-09-01 22:55:39,161   INFO  Train:    1/20 (  5%) [3099/3862 ( 80%)]  Loss: 2.862 (19.3)  LR: 1.222e-04  Grad: 34.9897  max=1.0159(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1126(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1852, loss_cls=0.2159, loss_bbox=1.7661, matched_ious=0.2797, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.36(1.38)  Time cost: 1:11:06/17:30 [1:11:08/28:20:33]  Acc_iter 3100        Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.36(1.38)
2025-09-01 22:56:48,192   INFO  Train:    1/20 (  5%) [3149/3862 ( 82%)]  Loss: 3.007 (19.1)  LR: 1.229e-04  Grad: 34.9568  max=1.0073(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1139(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1592, loss_cls=0.2079, loss_bbox=1.7646, matched_ious=0.2764, d_time=0.00(0.01), f_time=1.26(1.37), b_time=1.26(1.38)  Time cost: 1:12:15/16:21 [1:12:17/28:19:29]  Acc_iter 3150        Data time: 0.00(0.01)  Forward time: 1.26(1.37)  Batch time: 1.26(1.38)
2025-09-01 22:57:55,470   INFO  Train:    1/20 (  5%) [3199/3862 ( 83%)]  Loss: 3.073 (18.8)  LR: 1.236e-04  Grad: 34.9812  max=1.0145(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1113(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1739, loss_cls=0.2130, loss_bbox=1.7451, matched_ious=0.2762, d_time=0.00(0.01), f_time=1.26(1.37), b_time=1.27(1.38)  Time cost: 1:13:22/15:12 [1:13:24/28:17:45]  Acc_iter 3200        Data time: 0.00(0.01)  Forward time: 1.26(1.37)  Batch time: 1.27(1.38)
2025-09-01 22:59:02,777   INFO  Train:    1/20 (  5%) [3249/3862 ( 84%)]  Loss: 2.820 (18.6)  LR: 1.243e-04  Grad: 34.9528  max=1.0052(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1065(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1548, loss_cls=0.2120, loss_bbox=1.6776, matched_ious=0.2818, d_time=0.00(0.01), f_time=1.35(1.37), b_time=1.36(1.38)  Time cost: 1:14:29/14:03 [1:14:31/28:16:03]  Acc_iter 3250        Data time: 0.00(0.01)  Forward time: 1.35(1.37)  Batch time: 1.36(1.38)
2025-09-01 23:00:11,776   INFO  Train:    1/20 (  5%) [3299/3862 ( 85%)]  Loss: 3.265 (18.3)  LR: 1.251e-04  Grad: 34.9606  max=0.9999(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1087(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1864, loss_cls=0.2132, loss_bbox=1.6946, matched_ious=0.2837, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 1:15:38/12:54 [1:15:40/28:14:59]  Acc_iter 3300        Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-01 23:01:21,635   INFO  Train:    1/20 (  5%) [3349/3862 ( 87%)]  Loss: 2.802 (18.1)  LR: 1.258e-04  Grad: 34.9415  max=0.9997(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1037(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1526, loss_cls=0.2117, loss_bbox=1.7031, matched_ious=0.2832, d_time=0.00(0.01), f_time=1.45(1.37), b_time=1.46(1.38)  Time cost: 1:16:48/11:45 [1:16:50/28:14:14]  Acc_iter 3350        Data time: 0.00(0.01)  Forward time: 1.45(1.37)  Batch time: 1.46(1.38)
2025-09-01 23:02:32,999   INFO  Train:    1/20 (  5%) [3399/3862 ( 88%)]  Loss: 3.664 (17.9)  LR: 1.266e-04  Grad: 35.0000  max=1.0005(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0996(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1329, loss_cls=0.2068, loss_bbox=1.7083, matched_ious=0.2841, d_time=0.00(0.01), f_time=1.27(1.37), b_time=1.28(1.38)  Time cost: 1:18:00/10:37 [1:18:02/28:14:01]  Acc_iter 3400        Data time: 0.00(0.01)  Forward time: 1.27(1.37)  Batch time: 1.28(1.38)
2025-09-01 23:03:40,704   INFO  Train:    1/20 (  5%) [3449/3862 ( 89%)]  Loss: 2.639 (17.7)  LR: 1.274e-04  Grad: 34.9887  max=0.9919(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0985(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1615, loss_cls=0.2096, loss_bbox=1.6712, matched_ious=0.2837, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.32(1.38)  Time cost: 1:19:07/09:28 [1:19:09/28:12:29]  Acc_iter 3450        Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.32(1.38)
2025-09-01 23:04:47,478   INFO  Train:    1/20 (  5%) [3499/3862 ( 91%)]  Loss: 2.805 (17.5)  LR: 1.282e-04  Grad: 34.9771  max=0.9917(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0807(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1605, loss_cls=0.2096, loss_bbox=1.6484, matched_ious=0.2890, d_time=0.03(0.01), f_time=1.36(1.37), b_time=1.39(1.38)  Time cost: 1:20:14/08:19 [1:20:16/28:10:37]  Acc_iter 3500        Data time: 0.03(0.01)  Forward time: 1.36(1.37)  Batch time: 1.39(1.38)
2025-09-01 23:05:54,406   INFO  Train:    1/20 (  5%) [3549/3862 ( 92%)]  Loss: 3.563 (17.3)  LR: 1.290e-04  Grad: 34.9802  max=0.9849(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0845(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0992, loss_cls=0.2034, loss_bbox=1.6124, matched_ious=0.2900, d_time=0.00(0.01), f_time=1.26(1.37), b_time=1.27(1.38)  Time cost: 1:21:21/07:10 [1:21:23/28:08:50]  Acc_iter 3550        Data time: 0.00(0.01)  Forward time: 1.26(1.37)  Batch time: 1.27(1.38)
2025-09-01 23:07:03,367   INFO  Train:    1/20 (  5%) [3599/3862 ( 93%)]  Loss: 2.739 (17.1)  LR: 1.298e-04  Grad: 34.9791  max=0.9786(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0821(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1171, loss_cls=0.2037, loss_bbox=1.6021, matched_ious=0.2885, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.37(1.38)  Time cost: 1:22:30/06:01 [1:22:32/28:07:45]  Acc_iter 3600        Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.37(1.38)
2025-09-01 23:08:14,376   INFO  Train:    1/20 (  5%) [3649/3862 ( 94%)]  Loss: 2.803 (16.9)  LR: 1.306e-04  Grad: 34.9705  max=0.9736(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0793(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0852, loss_cls=0.1948, loss_bbox=1.6568, matched_ious=0.2912, d_time=0.00(0.01), f_time=1.41(1.37), b_time=1.42(1.38)  Time cost: 1:23:41/04:53 [1:23:43/28:07:22]  Acc_iter 3650        Data time: 0.00(0.01)  Forward time: 1.41(1.37)  Batch time: 1.42(1.38)
2025-09-01 23:09:23,522   INFO  Train:    1/20 (  5%) [3699/3862 ( 96%)]  Loss: 2.771 (16.7)  LR: 1.315e-04  Grad: 35.0000  max=0.9707(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1321(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1106, loss_cls=0.2024, loss_bbox=1.6499, matched_ious=0.2945, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.29(1.38)  Time cost: 1:24:50/03:44 [1:24:52/28:06:20]  Acc_iter 3700        Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.29(1.38)
2025-09-01 23:10:31,446   INFO  Train:    1/20 (  5%) [3749/3862 ( 97%)]  Loss: 2.636 (16.5)  LR: 1.323e-04  Grad: 34.9828  max=0.9748(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0756(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1107, loss_cls=0.2014, loss_bbox=1.6317, matched_ious=0.2911, d_time=0.01(0.01), f_time=1.33(1.37), b_time=1.34(1.38)  Time cost: 1:25:58/02:35 [1:26:00/28:04:54]  Acc_iter 3750        Data time: 0.01(0.01)  Forward time: 1.33(1.37)  Batch time: 1.34(1.38)
2025-09-01 23:11:39,744   INFO  Train:    1/20 (  5%) [3799/3862 ( 98%)]  Loss: 3.308 (16.3)  LR: 1.332e-04  Grad: 34.9495  max=0.9664(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0756(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0902, loss_cls=0.1997, loss_bbox=1.6191, matched_ious=0.2964, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.36(1.38)  Time cost: 1:27:06/01:26 [1:27:08/28:03:36]  Acc_iter 3800        Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.36(1.38)
2025-09-01 23:12:48,107   INFO  Train:    1/20 (  5%) [3849/3862 (100%)]  Loss: 2.702 (16.1)  LR: 1.340e-04  Grad: 34.9498  max=0.9670(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0659(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0859, loss_cls=0.2005, loss_bbox=1.5886, matched_ious=0.2984, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.34(1.38)  Time cost: 1:28:15/00:17 [1:28:17/28:02:20]  Acc_iter 3850        Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.34(1.38)
2025-09-01 23:13:04,858   INFO  Train:    1/20 (  5%) [3861/3862 (100%)]  Loss: 3.279 (16.1)  LR: 1.342e-04  Grad: 34.9416  max=0.9704(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0641(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0678, loss_cls=0.1981, loss_bbox=1.5828, matched_ious=0.2990, d_time=0.00(0.01), f_time=1.23(1.37), b_time=1.24(1.38)  Time cost: 1:28:31/00:01 [1:28:33/28:02:08]  Acc_iter 3862        Data time: 0.00(0.01)  Forward time: 1.23(1.37)  Batch time: 1.24(1.38)

                                               [Aepochs:   5%|▌         | 1/20 [1:28:34<28:03:01, 5314.82s/it]epochs:   5%|▌         | 1/20 [1:28:34<28:03:01, 5314.84s/it]epochs:   5%|▌         | 1/20 [1:28:34<28:03:02, 5314.85s/it]epochs:   5%|▌         | 1/20 [1:28:34<28:03:02, 5314.86s/it]epochs:   5%|▌         | 1/20 [1:28:34<28:03:02, 5314.86s/it]epochs:   5%|▌         | 1/20 [1:28:34<28:03:02, 5314.87s/it]epochs:   5%|▌         | 1/20 [1:28:34<28:03:02, 5314.87s/it]epochs:   5%|▌         | 1/20 [1:28:35<28:03:06, 5315.10s/it]
train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-01 23:13:13,054   INFO  Train:    2/20 ( 10%) [   0/3862 (  0%)]  Loss: 3.077 (3.08)  LR: 1.343e-04  Grad: 34.9236  max=0.9689(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0634(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2141, loss_cls=0.2242, loss_bbox=1.6384, matched_ious=0.2875, d_time=3.33(3.33), f_time=3.19(3.19), b_time=6.52(6.52)  Time cost: 00:06/6:43:49 [1:28:42/127:52:44]  Acc_iter 3863        Data time: 3.33(3.33)  Forward time: 3.19(3.19)  Batch time: 6.52(6.52)
2025-09-01 23:14:05,798   INFO  Train:    2/20 ( 10%) [  37/3862 (  1%)]  Loss: 2.819 (2.91)  LR: 1.349e-04  Grad: 34.9662  max=0.9691(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0602(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0922, loss_cls=0.1991, loss_bbox=1.6107, matched_ious=0.2998, d_time=0.00(0.09), f_time=1.41(1.47), b_time=1.42(1.56)  Time cost: 00:59/1:39:00 [1:29:34/31:38:25]  Acc_iter 3900        Data time: 0.00(0.09)  Forward time: 1.41(1.47)  Batch time: 1.42(1.56)
2025-09-01 23:15:15,981   INFO  Train:    2/20 ( 10%) [  87/3862 (  2%)]  Loss: 2.695 (2.88)  LR: 1.358e-04  Grad: 35.0000  max=0.9668(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.4817(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0864, loss_cls=0.1996, loss_bbox=1.5713, matched_ious=0.2969, d_time=0.00(0.04), f_time=1.34(1.43), b_time=1.34(1.47)  Time cost: 02:09/1:32:22 [1:30:45/29:53:24]  Acc_iter 3950        Data time: 0.00(0.04)  Forward time: 1.34(1.43)  Batch time: 1.34(1.47)
2025-09-01 23:16:25,166   INFO  Train:    2/20 ( 10%) [ 137/3862 (  4%)]  Loss: 2.774 (2.86)  LR: 1.367e-04  Grad: 34.9666  max=0.9599(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0559(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0882, loss_cls=0.1984, loss_bbox=1.5472, matched_ious=0.3056, d_time=0.00(0.03), f_time=1.42(1.41), b_time=1.43(1.44)  Time cost: 03:18/1:29:14 [1:31:54/29:14:49]  Acc_iter 4000        Data time: 0.00(0.03)  Forward time: 1.42(1.41)  Batch time: 1.43(1.44)
2025-09-01 23:17:32,584   INFO  Train:    2/20 ( 10%) [ 187/3862 (  5%)]  Loss: 2.807 (2.86)  LR: 1.376e-04  Grad: 34.9373  max=0.9624(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0541(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0570, loss_cls=0.1959, loss_bbox=1.5859, matched_ious=0.3020, d_time=0.00(0.02), f_time=1.38(1.39), b_time=1.39(1.42)  Time cost: 04:25/1:26:35 [1:33:01/28:44:41]  Acc_iter 4050        Data time: 0.00(0.02)  Forward time: 1.38(1.39)  Batch time: 1.39(1.42)
2025-09-01 23:18:40,961   INFO  Train:    2/20 ( 10%) [ 237/3862 (  6%)]  Loss: 3.008 (2.84)  LR: 1.385e-04  Grad: 34.9810  max=0.9578(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0496(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0586, loss_cls=0.1940, loss_bbox=1.5495, matched_ious=0.3044, d_time=0.00(0.02), f_time=1.33(1.39), b_time=1.33(1.41)  Time cost: 05:34/1:24:49 [1:34:10/28:31:39]  Acc_iter 4100        Data time: 0.00(0.02)  Forward time: 1.33(1.39)  Batch time: 1.33(1.41)
2025-09-01 23:19:51,208   INFO  Train:    2/20 ( 10%) [ 287/3862 (  7%)]  Loss: 2.786 (2.85)  LR: 1.395e-04  Grad: 34.9673  max=0.9791(module.vfe.pfn_layers.0.linear.weight)  min: -1.0431(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0608, loss_cls=0.1941, loss_bbox=1.5946, matched_ious=0.3025, d_time=0.00(0.02), f_time=1.27(1.39), b_time=1.28(1.41)  Time cost: 06:44/1:23:40 [1:35:20/28:30:39]  Acc_iter 4150        Data time: 0.00(0.02)  Forward time: 1.27(1.39)  Batch time: 1.28(1.41)
2025-09-01 23:21:01,098   INFO  Train:    2/20 ( 10%) [ 337/3862 (  9%)]  Loss: 2.629 (2.84)  LR: 1.404e-04  Grad: 34.9843  max=0.9519(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0357(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0811, loss_cls=0.1957, loss_bbox=1.5373, matched_ious=0.3097, d_time=0.00(0.02), f_time=1.37(1.39), b_time=1.38(1.40)  Time cost: 07:54/1:22:26 [1:36:30/28:28:18]  Acc_iter 4200        Data time: 0.00(0.02)  Forward time: 1.37(1.39)  Batch time: 1.38(1.40)
2025-09-01 23:22:08,979   INFO  Train:    2/20 ( 10%) [ 387/3862 ( 10%)]  Loss: 2.825 (2.83)  LR: 1.414e-04  Grad: 34.9307  max=0.9452(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1223(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0455, loss_cls=0.1900, loss_bbox=1.5581, matched_ious=0.3085, d_time=0.01(0.02), f_time=1.26(1.38), b_time=1.27(1.40)  Time cost: 09:02/1:20:56 [1:37:38/28:19:59]  Acc_iter 4250        Data time: 0.01(0.02)  Forward time: 1.26(1.38)  Batch time: 1.27(1.40)
2025-09-01 23:23:17,082   INFO  Train:    2/20 ( 10%) [ 437/3862 ( 11%)]  Loss: 2.730 (2.83)  LR: 1.423e-04  Grad: 34.9684  max=1.2508(module.vfe.pfn_layers.0.linear.weight)  min: -1.0313(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0367, loss_cls=0.1858, loss_bbox=1.6023, matched_ious=0.3059, d_time=0.00(0.02), f_time=1.37(1.38), b_time=1.37(1.39)  Time cost: 10:10/1:19:32 [1:38:46/28:13:54]  Acc_iter 4300        Data time: 0.00(0.02)  Forward time: 1.37(1.38)  Batch time: 1.37(1.39)
2025-09-01 23:24:24,833   INFO  Train:    2/20 ( 10%) [ 487/3862 ( 13%)]  Loss: 2.583 (2.83)  LR: 1.433e-04  Grad: 34.9190  max=0.9450(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0343(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0435, loss_cls=0.1855, loss_bbox=1.5293, matched_ious=0.3119, d_time=0.00(0.01), f_time=1.28(1.38), b_time=1.28(1.39)  Time cost: 11:18/1:18:09 [1:39:53/28:07:58]  Acc_iter 4350        Data time: 0.00(0.01)  Forward time: 1.28(1.38)  Batch time: 1.28(1.39)
2025-09-01 23:25:33,288   INFO  Train:    2/20 ( 10%) [ 537/3862 ( 14%)]  Loss: 3.046 (2.82)  LR: 1.443e-04  Grad: 34.9301  max=0.9521(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0371(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0417, loss_cls=0.1856, loss_bbox=1.5705, matched_ious=0.3088, d_time=0.00(0.01), f_time=1.40(1.37), b_time=1.40(1.39)  Time cost: 12:26/1:16:53 [1:41:02/28:04:30]  Acc_iter 4400        Data time: 0.00(0.01)  Forward time: 1.40(1.37)  Batch time: 1.40(1.39)
2025-09-01 23:26:42,008   INFO  Train:    2/20 ( 10%) [ 587/3862 ( 15%)]  Loss: 2.317 (2.82)  LR: 1.453e-04  Grad: 34.9645  max=0.9511(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0368(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0503, loss_cls=0.1880, loss_bbox=1.5325, matched_ious=0.3084, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.37(1.39)  Time cost: 13:35/1:15:40 [1:42:11/28:02:00]  Acc_iter 4450        Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.37(1.39)
2025-09-01 23:27:52,381   INFO  Train:    2/20 ( 10%) [ 637/3862 ( 16%)]  Loss: 3.058 (2.82)  LR: 1.463e-04  Grad: 34.7556  max=0.9427(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0238(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0436, loss_cls=0.1874, loss_bbox=1.5612, matched_ious=0.3115, d_time=0.00(0.01), f_time=1.27(1.38), b_time=1.27(1.39)  Time cost: 14:45/1:14:36 [1:43:21/28:02:50]  Acc_iter 4500        Data time: 0.00(0.01)  Forward time: 1.27(1.38)  Batch time: 1.27(1.39)
2025-09-01 23:29:00,501   INFO  Train:    2/20 ( 10%) [ 687/3862 ( 18%)]  Loss: 3.001 (2.81)  LR: 1.473e-04  Grad: 34.9428  max=0.9442(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.8282(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0166, loss_cls=0.1828, loss_bbox=1.5064, matched_ious=0.3216, d_time=0.01(0.01), f_time=1.25(1.37), b_time=1.27(1.39)  Time cost: 15:53/1:13:21 [1:44:29/27:59:25]  Acc_iter 4550        Data time: 0.01(0.01)  Forward time: 1.25(1.37)  Batch time: 1.27(1.39)
2025-09-01 23:30:08,695   INFO  Train:    2/20 ( 10%) [ 737/3862 ( 19%)]  Loss: 2.304 (2.80)  LR: 1.483e-04  Grad: 34.9888  max=1.1229(module.vfe.pfn_layers.0.linear.weight)  min: -1.4225(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0047, loss_cls=0.1824, loss_bbox=1.4482, matched_ious=0.3233, d_time=0.00(0.01), f_time=1.39(1.37), b_time=1.39(1.39)  Time cost: 17:01/1:12:07 [1:45:37/27:56:26]  Acc_iter 4600        Data time: 0.00(0.01)  Forward time: 1.39(1.37)  Batch time: 1.39(1.39)
2025-09-01 23:31:17,497   INFO  Train:    2/20 ( 10%) [ 787/3862 ( 20%)]  Loss: 2.647 (2.79)  LR: 1.494e-04  Grad: 34.9897  max=0.9531(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.6691(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0255, loss_cls=0.1795, loss_bbox=1.5363, matched_ious=0.3214, d_time=0.01(0.01), f_time=1.27(1.37), b_time=1.28(1.38)  Time cost: 18:10/1:10:56 [1:46:46/27:54:37]  Acc_iter 4650        Data time: 0.01(0.01)  Forward time: 1.27(1.37)  Batch time: 1.28(1.38)
2025-09-01 23:32:26,982   INFO  Train:    2/20 ( 10%) [ 837/3862 ( 22%)]  Loss: 2.851 (2.79)  LR: 1.504e-04  Grad: 34.5214  max=0.9559(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0106(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0080, loss_cls=0.1788, loss_bbox=1.5130, matched_ious=0.3238, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.38(1.38)  Time cost: 19:20/1:09:48 [1:47:56/27:53:52]  Acc_iter 4700        Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.38(1.38)
2025-09-01 23:33:36,674   INFO  Train:    2/20 ( 10%) [ 887/3862 ( 23%)]  Loss: 3.097 (2.78)  LR: 1.515e-04  Grad: 34.6452  max=0.9617(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.3233(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9909, loss_cls=0.1740, loss_bbox=1.5310, matched_ious=0.3196, d_time=0.01(0.01), f_time=1.30(1.37), b_time=1.30(1.39)  Time cost: 20:29/1:08:40 [1:49:05/27:53:21]  Acc_iter 4750        Data time: 0.01(0.01)  Forward time: 1.30(1.37)  Batch time: 1.30(1.39)
2025-09-01 23:34:45,325   INFO  Train:    2/20 ( 10%) [ 937/3862 ( 24%)]  Loss: 2.803 (2.78)  LR: 1.525e-04  Grad: 34.7163  max=0.9680(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0101(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0565, loss_cls=0.1843, loss_bbox=1.5475, matched_ious=0.3251, d_time=0.01(0.01), f_time=1.40(1.37), b_time=1.41(1.38)  Time cost: 21:38/1:07:29 [1:50:14/27:51:25]  Acc_iter 4800        Data time: 0.01(0.01)  Forward time: 1.40(1.37)  Batch time: 1.41(1.38)
2025-09-01 23:35:53,097   INFO  Train:    2/20 ( 10%) [ 987/3862 ( 26%)]  Loss: 2.595 (2.78)  LR: 1.536e-04  Grad: 34.7550  max=0.9728(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1516(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0262, loss_cls=0.1804, loss_bbox=1.4997, matched_ious=0.3272, d_time=0.01(0.01), f_time=1.40(1.37), b_time=1.41(1.38)  Time cost: 22:46/1:06:15 [1:51:22/27:48:30]  Acc_iter 4850        Data time: 0.01(0.01)  Forward time: 1.40(1.37)  Batch time: 1.41(1.38)
2025-09-01 23:37:01,504   INFO  Train:    2/20 ( 10%) [1037/3862 ( 27%)]  Loss: 2.448 (2.77)  LR: 1.547e-04  Grad: 34.7778  max=1.1923(module.vfe.pfn_layers.0.linear.weight)  min: -1.0078(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0149, loss_cls=0.1781, loss_bbox=1.4552, matched_ious=0.3328, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.34(1.38)  Time cost: 23:54/1:05:04 [1:52:30/27:46:29]  Acc_iter 4900        Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.34(1.38)
2025-09-01 23:38:10,828   INFO  Train:    2/20 ( 10%) [1087/3862 ( 28%)]  Loss: 2.325 (2.77)  LR: 1.558e-04  Grad: 34.9282  max=1.7422(module.vfe.pfn_layers.0.linear.weight)  min: -1.0117(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0174, loss_cls=0.1802, loss_bbox=1.4911, matched_ious=0.3331, d_time=0.00(0.01), f_time=1.41(1.37), b_time=1.41(1.38)  Time cost: 25:04/1:03:56 [1:53:39/27:45:34]  Acc_iter 4950        Data time: 0.00(0.01)  Forward time: 1.41(1.37)  Batch time: 1.41(1.38)
2025-09-01 23:39:21,275   INFO  Train:    2/20 ( 10%) [1137/3862 ( 29%)]  Loss: 2.598 (2.76)  LR: 1.569e-04  Grad: 34.9787  max=1.4033(module.vfe.pfn_layers.0.linear.weight)  min: -2.4143(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9926, loss_cls=0.1745, loss_bbox=1.4743, matched_ious=0.3294, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.29(1.38)  Time cost: 26:14/1:02:50 [1:54:50/27:45:49]  Acc_iter 5000        Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.29(1.38)
2025-09-01 23:40:29,154   INFO  Train:    2/20 ( 10%) [1187/3862 ( 31%)]  Loss: 2.495 (2.76)  LR: 1.580e-04  Grad: 34.7883  max=0.9940(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0047(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0122, loss_cls=0.1808, loss_bbox=1.4127, matched_ious=0.3368, d_time=0.01(0.01), f_time=1.35(1.37), b_time=1.36(1.38)  Time cost: 27:22/1:01:38 [1:55:58/27:43:21]  Acc_iter 5050        Data time: 0.01(0.01)  Forward time: 1.35(1.37)  Batch time: 1.36(1.38)
2025-09-01 23:41:38,204   INFO  Train:    2/20 ( 10%) [1237/3862 ( 32%)]  Loss: 2.507 (2.75)  LR: 1.591e-04  Grad: 34.8985  max=1.0083(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0121(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.0275, loss_cls=0.1809, loss_bbox=1.4142, matched_ious=0.3341, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.33(1.38)  Time cost: 28:31/1:00:28 [1:57:07/27:42:08]  Acc_iter 5100        Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.33(1.38)
2025-09-01 23:42:46,985   INFO  Train:    2/20 ( 10%) [1287/3862 ( 33%)]  Loss: 2.337 (2.75)  LR: 1.603e-04  Grad: 34.8983  max=1.2404(module.vfe.pfn_layers.0.linear.weight)  min: -1.0042(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9960, loss_cls=0.1758, loss_bbox=1.4589, matched_ious=0.3353, d_time=0.00(0.01), f_time=1.39(1.37), b_time=1.39(1.38)  Time cost: 29:40/59:19 [1:58:16/27:40:40]  Acc_iter 5150        Data time: 0.00(0.01)  Forward time: 1.39(1.37)  Batch time: 1.39(1.38)
2025-09-01 23:43:56,573   INFO  Train:    2/20 ( 10%) [1337/3862 ( 35%)]  Loss: 2.456 (2.74)  LR: 1.614e-04  Grad: 34.7677  max=1.0175(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9985(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9981, loss_cls=0.1770, loss_bbox=1.4224, matched_ious=0.3364, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.31(1.38)  Time cost: 30:49/58:10 [1:59:25/27:39:57]  Acc_iter 5200        Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.31(1.38)
2025-09-01 23:45:06,750   INFO  Train:    2/20 ( 10%) [1387/3862 ( 36%)]  Loss: 2.693 (2.73)  LR: 1.626e-04  Grad: 34.8391  max=1.0952(module.vfe.pfn_layers.0.linear.weight)  min: -0.9964(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9845, loss_cls=0.1750, loss_bbox=1.3502, matched_ious=0.3425, d_time=0.01(0.01), f_time=1.37(1.37), b_time=1.39(1.38)  Time cost: 31:59/57:03 [2:00:35/27:39:42]  Acc_iter 5250        Data time: 0.01(0.01)  Forward time: 1.37(1.37)  Batch time: 1.39(1.38)
2025-09-01 23:46:15,932   INFO  Train:    2/20 ( 10%) [1437/3862 ( 37%)]  Loss: 2.485 (2.73)  LR: 1.638e-04  Grad: 34.9034  max=1.0246(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.8695(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0068, loss_cls=0.1769, loss_bbox=1.4126, matched_ious=0.3394, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.39(1.38)  Time cost: 33:09/55:54 [2:01:44/27:38:34]  Acc_iter 5300        Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.39(1.38)
2025-09-01 23:47:24,419   INFO  Train:    2/20 ( 10%) [1487/3862 ( 39%)]  Loss: 2.450 (2.72)  LR: 1.649e-04  Grad: 34.8237  max=1.0288(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.6909(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9812, loss_cls=0.1728, loss_bbox=1.4375, matched_ious=0.3441, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.34(1.38)  Time cost: 34:17/54:44 [2:02:53/27:36:52]  Acc_iter 5350        Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.34(1.38)
2025-09-01 23:48:32,477   INFO  Train:    2/20 ( 10%) [1537/3862 ( 40%)]  Loss: 2.664 (2.72)  LR: 1.661e-04  Grad: 34.8688  max=1.2361(module.vfe.pfn_layers.0.linear.weight)  min: -0.9849(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9607, loss_cls=0.1650, loss_bbox=1.4020, matched_ious=0.3478, d_time=0.00(0.01), f_time=1.44(1.37), b_time=1.45(1.38)  Time cost: 35:25/53:33 [2:04:01/27:34:52]  Acc_iter 5400        Data time: 0.00(0.01)  Forward time: 1.44(1.37)  Batch time: 1.45(1.38)
2025-09-01 23:49:41,613   INFO  Train:    2/20 ( 10%) [1587/3862 ( 41%)]  Loss: 2.612 (2.71)  LR: 1.673e-04  Grad: 34.7477  max=1.0393(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9788(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9925, loss_cls=0.1754, loss_bbox=1.3973, matched_ious=0.3449, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.33(1.38)  Time cost: 36:34/52:24 [2:05:10/27:33:44]  Acc_iter 5450        Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.33(1.38)
2025-09-01 23:50:52,200   INFO  Train:    2/20 ( 10%) [1637/3862 ( 42%)]  Loss: 2.936 (2.71)  LR: 1.685e-04  Grad: 34.8567  max=1.9000(module.vfe.pfn_layers.0.linear.weight)  min: -0.9785(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9884, loss_cls=0.1757, loss_bbox=1.4021, matched_ious=0.3467, d_time=0.00(0.01), f_time=1.40(1.37), b_time=1.40(1.38)  Time cost: 37:45/51:17 [2:06:21/27:33:40]  Acc_iter 5500        Data time: 0.00(0.01)  Forward time: 1.40(1.37)  Batch time: 1.40(1.38)
2025-09-01 23:52:00,973   INFO  Train:    2/20 ( 10%) [1687/3862 ( 44%)]  Loss: 2.350 (2.70)  LR: 1.698e-04  Grad: 34.8274  max=1.0533(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9768(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9656, loss_cls=0.1704, loss_bbox=1.3885, matched_ious=0.3492, d_time=0.00(0.01), f_time=1.28(1.37), b_time=1.28(1.38)  Time cost: 38:54/50:07 [2:07:30/27:32:15]  Acc_iter 5550        Data time: 0.00(0.01)  Forward time: 1.28(1.37)  Batch time: 1.28(1.38)
2025-09-01 23:53:09,584   INFO  Train:    2/20 ( 10%) [1737/3862 ( 45%)]  Loss: 2.270 (2.70)  LR: 1.710e-04  Grad: 34.8418  max=1.0594(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9763(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9872, loss_cls=0.1731, loss_bbox=1.3786, matched_ious=0.3510, d_time=0.04(0.01), f_time=1.62(1.37), b_time=1.66(1.38)  Time cost: 40:02/48:57 [2:08:38/27:30:44]  Acc_iter 5600        Data time: 0.04(0.01)  Forward time: 1.62(1.37)  Batch time: 1.66(1.38)
2025-09-01 23:54:17,786   INFO  Train:    2/20 ( 10%) [1787/3862 ( 46%)]  Loss: 2.166 (2.69)  LR: 1.722e-04  Grad: 34.8385  max=1.0671(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.8978(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9689, loss_cls=0.1651, loss_bbox=1.4523, matched_ious=0.3501, d_time=0.02(0.01), f_time=1.35(1.37), b_time=1.37(1.38)  Time cost: 41:11/47:47 [2:09:46/27:28:58]  Acc_iter 5650        Data time: 0.02(0.01)  Forward time: 1.35(1.37)  Batch time: 1.37(1.38)
2025-09-01 23:55:25,623   INFO  Train:    2/20 ( 10%) [1837/3862 ( 48%)]  Loss: 2.761 (2.69)  LR: 1.735e-04  Grad: 34.9766  max=2.6606(module.vfe.pfn_layers.0.linear.weight)  min: -0.9687(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9609, loss_cls=0.1672, loss_bbox=1.3585, matched_ious=0.3461, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.38(1.38)  Time cost: 42:18/46:37 [2:10:54/27:26:59]  Acc_iter 5700        Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.38(1.38)
2025-09-01 23:56:35,758   INFO  Train:    2/20 ( 10%) [1887/3862 ( 49%)]  Loss: 2.675 (2.68)  LR: 1.747e-04  Grad: 34.6229  max=1.0754(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0781(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9498, loss_cls=0.1622, loss_bbox=1.3361, matched_ious=0.3514, d_time=0.00(0.01), f_time=1.35(1.37), b_time=1.35(1.38)  Time cost: 43:28/45:29 [2:12:04/27:26:31]  Acc_iter 5750        Data time: 0.00(0.01)  Forward time: 1.35(1.37)  Batch time: 1.35(1.38)
2025-09-01 23:57:44,737   INFO  Train:    2/20 ( 10%) [1937/3862 ( 50%)]  Loss: 2.347 (2.68)  LR: 1.760e-04  Grad: 34.8853  max=1.0884(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.9309(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9473, loss_cls=0.1635, loss_bbox=1.3784, matched_ious=0.3553, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.37(1.38)  Time cost: 44:37/44:19 [2:13:13/27:25:18]  Acc_iter 5800        Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.37(1.38)
2025-09-01 23:58:54,076   INFO  Train:    2/20 ( 10%) [1987/3862 ( 51%)]  Loss: 1.963 (2.67)  LR: 1.773e-04  Grad: 34.7899  max=1.0965(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.3883(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9484, loss_cls=0.1645, loss_bbox=1.3697, matched_ious=0.3535, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.37(1.38)  Time cost: 45:47/43:11 [2:14:23/27:24:18]  Acc_iter 5850        Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.37(1.38)
2025-09-02 00:00:01,563   INFO  Train:    2/20 ( 10%) [2037/3862 ( 53%)]  Loss: 2.216 (2.67)  LR: 1.786e-04  Grad: 34.8351  max=1.5992(module.vfe.pfn_layers.0.linear.weight)  min: -0.9689(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9513, loss_cls=0.1668, loss_bbox=1.3862, matched_ious=0.3563, d_time=0.00(0.01), f_time=1.45(1.37), b_time=1.45(1.38)  Time cost: 46:54/42:00 [2:15:30/27:22:12]  Acc_iter 5900        Data time: 0.00(0.01)  Forward time: 1.45(1.37)  Batch time: 1.45(1.38)
2025-09-02 00:01:10,523   INFO  Train:    2/20 ( 10%) [2087/3862 ( 54%)]  Loss: 2.667 (2.66)  LR: 1.799e-04  Grad: 34.8527  max=1.1081(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.4509(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9503, loss_cls=0.1615, loss_bbox=1.3726, matched_ious=0.3552, d_time=0.01(0.01), f_time=1.32(1.37), b_time=1.33(1.38)  Time cost: 48:03/40:51 [2:16:39/27:21:00]  Acc_iter 5950        Data time: 0.01(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.38)
2025-09-02 00:02:20,392   INFO  Train:    2/20 ( 10%) [2137/3862 ( 55%)]  Loss: 2.723 (2.66)  LR: 1.812e-04  Grad: 34.6343  max=2.0818(module.vfe.pfn_layers.0.linear.weight)  min: -1.2252(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9632, loss_cls=0.1644, loss_bbox=1.3683, matched_ious=0.3541, d_time=0.00(0.01), f_time=1.44(1.37), b_time=1.44(1.38)  Time cost: 49:13/39:43 [2:17:49/27:20:18]  Acc_iter 6000        Data time: 0.00(0.01)  Forward time: 1.44(1.37)  Batch time: 1.44(1.38)
2025-09-02 00:03:31,292   INFO  Train:    2/20 ( 10%) [2187/3862 ( 57%)]  Loss: 2.663 (2.66)  LR: 1.825e-04  Grad: 34.7736  max=2.3372(module.vfe.pfn_layers.0.linear.weight)  min: -1.5143(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9481, loss_cls=0.1640, loss_bbox=1.3588, matched_ious=0.3597, d_time=0.00(0.01), f_time=1.43(1.37), b_time=1.43(1.38)  Time cost: 50:24/38:35 [2:19:00/27:20:08]  Acc_iter 6050        Data time: 0.00(0.01)  Forward time: 1.43(1.37)  Batch time: 1.43(1.38)
2025-09-02 00:04:39,527   INFO  Train:    2/20 ( 10%) [2237/3862 ( 58%)]  Loss: 2.941 (2.65)  LR: 1.838e-04  Grad: 34.7468  max=2.0686(module.vfe.pfn_layers.0.linear.weight)  min: -0.9502(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9163, loss_cls=0.1584, loss_bbox=1.3452, matched_ious=0.3633, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.32(1.38)  Time cost: 51:32/37:25 [2:20:08/27:18:31]  Acc_iter 6100        Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.32(1.38)
2025-09-02 00:05:47,959   INFO  Train:    2/20 ( 10%) [2287/3862 ( 59%)]  Loss: 2.527 (2.65)  LR: 1.851e-04  Grad: 34.8084  max=1.1167(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.1218(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9590, loss_cls=0.1628, loss_bbox=1.3413, matched_ious=0.3531, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.38(1.38)  Time cost: 52:41/36:16 [2:21:17/27:17:01]  Acc_iter 6150        Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.38(1.38)
2025-09-02 00:06:56,876   INFO  Train:    2/20 ( 10%) [2337/3862 ( 61%)]  Loss: 2.139 (2.64)  LR: 1.865e-04  Grad: 34.8874  max=1.9347(module.vfe.pfn_layers.0.linear.weight)  min: -1.2630(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9421, loss_cls=0.1628, loss_bbox=1.3399, matched_ious=0.3590, d_time=0.01(0.01), f_time=1.26(1.37), b_time=1.27(1.38)  Time cost: 53:50/35:06 [2:22:25/27:15:47]  Acc_iter 6200        Data time: 0.01(0.01)  Forward time: 1.26(1.37)  Batch time: 1.27(1.38)
2025-09-02 00:08:05,119   INFO  Train:    2/20 ( 10%) [2387/3862 ( 62%)]  Loss: 2.036 (2.64)  LR: 1.878e-04  Grad: 34.8751  max=1.1406(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9502(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9424, loss_cls=0.1631, loss_bbox=1.2769, matched_ious=0.3688, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 54:58/33:57 [2:23:34/27:14:13]  Acc_iter 6250        Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 00:09:15,947   INFO  Train:    2/20 ( 10%) [2437/3862 ( 63%)]  Loss: 2.483 (2.63)  LR: 1.892e-04  Grad: 34.6888  max=1.1350(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.3934(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9110, loss_cls=0.1571, loss_bbox=1.2830, matched_ious=0.3698, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.31(1.38)  Time cost: 56:09/32:49 [2:24:45/27:13:56]  Acc_iter 6300        Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.31(1.38)
2025-09-02 00:10:24,427   INFO  Train:    2/20 ( 10%) [2487/3862 ( 64%)]  Loss: 2.664 (2.63)  LR: 1.906e-04  Grad: 34.7850  max=1.1467(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.3017(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9445, loss_cls=0.1620, loss_bbox=1.3849, matched_ious=0.3653, d_time=0.01(0.01), f_time=1.30(1.37), b_time=1.31(1.38)  Time cost: 57:17/31:39 [2:25:53/27:12:29]  Acc_iter 6350        Data time: 0.01(0.01)  Forward time: 1.30(1.37)  Batch time: 1.31(1.38)
2025-09-02 00:11:32,775   INFO  Train:    2/20 ( 10%) [2537/3862 ( 66%)]  Loss: 2.767 (2.62)  LR: 1.919e-04  Grad: 34.6661  max=1.1498(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2444(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9372, loss_cls=0.1621, loss_bbox=1.3199, matched_ious=0.3665, d_time=0.00(0.01), f_time=1.30(1.37), b_time=1.31(1.38)  Time cost: 58:25/30:30 [2:27:01/27:10:59]  Acc_iter 6400        Data time: 0.00(0.01)  Forward time: 1.30(1.37)  Batch time: 1.31(1.38)
2025-09-02 00:12:40,850   INFO  Train:    2/20 ( 10%) [2587/3862 ( 67%)]  Loss: 2.485 (2.62)  LR: 1.933e-04  Grad: 34.7785  max=1.1536(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.4240(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9254, loss_cls=0.1626, loss_bbox=1.2680, matched_ious=0.3718, d_time=0.45(0.01), f_time=1.23(1.37), b_time=1.68(1.38)  Time cost: 59:34/29:20 [2:28:09/27:09:23]  Acc_iter 6450        Data time: 0.45(0.01)  Forward time: 1.23(1.37)  Batch time: 1.68(1.38)
2025-09-02 00:13:49,438   INFO  Train:    2/20 ( 10%) [2637/3862 ( 68%)]  Loss: 2.678 (2.61)  LR: 1.947e-04  Grad: 34.8956  max=1.1565(module.dense_head.decoder.self_attn.in_proj_weight)  min: -3.1415(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9285, loss_cls=0.1601, loss_bbox=1.2863, matched_ious=0.3740, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.37(1.38)  Time cost: 1:00:42/28:11 [2:29:18/27:08:02]  Acc_iter 6500        Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.37(1.38)
2025-09-02 00:14:59,051   INFO  Train:    2/20 ( 10%) [2687/3862 ( 70%)]  Loss: 2.308 (2.61)  LR: 1.961e-04  Grad: 34.7749  max=1.1590(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0896(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9216, loss_cls=0.1562, loss_bbox=1.2891, matched_ious=0.3727, d_time=0.00(0.01), f_time=1.44(1.37), b_time=1.44(1.38)  Time cost: 1:01:52/27:02 [2:30:28/27:07:08]  Acc_iter 6550        Data time: 0.00(0.01)  Forward time: 1.44(1.37)  Batch time: 1.44(1.38)
2025-09-02 00:16:08,012   INFO  Train:    2/20 ( 10%) [2737/3862 ( 71%)]  Loss: 2.428 (2.60)  LR: 1.976e-04  Grad: 34.8232  max=1.1692(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.5812(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9250, loss_cls=0.1557, loss_bbox=1.2765, matched_ious=0.3729, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.34(1.38)  Time cost: 1:03:01/25:53 [2:31:37/27:05:56]  Acc_iter 6600        Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.34(1.38)
2025-09-02 00:17:16,304   INFO  Train:    2/20 ( 10%) [2787/3862 ( 72%)]  Loss: 2.784 (2.60)  LR: 1.990e-04  Grad: 34.7848  max=1.1691(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2859(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9184, loss_cls=0.1568, loss_bbox=1.3392, matched_ious=0.3685, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.36(1.38)  Time cost: 1:04:09/24:44 [2:32:45/27:04:28]  Acc_iter 6650        Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.36(1.38)
2025-09-02 00:18:25,530   INFO  Train:    2/20 ( 10%) [2837/3862 ( 73%)]  Loss: 2.496 (2.60)  LR: 2.004e-04  Grad: 35.0000  max=4.4686(module.vfe.pfn_layers.0.linear.weight)  min: -1.8244(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9367, loss_cls=0.1588, loss_bbox=1.3086, matched_ious=0.3757, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.36(1.38)  Time cost: 1:05:18/23:35 [2:33:54/27:03:23]  Acc_iter 6700        Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.36(1.38)
2025-09-02 00:19:35,344   INFO  Train:    2/20 ( 10%) [2887/3862 ( 75%)]  Loss: 2.233 (2.59)  LR: 2.019e-04  Grad: 34.6928  max=1.5901(module.vfe.pfn_layers.0.linear.weight)  min: -0.9252(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9193, loss_cls=0.1570, loss_bbox=1.3074, matched_ious=0.3753, d_time=0.00(0.01), f_time=1.40(1.37), b_time=1.41(1.38)  Time cost: 1:06:28/22:26 [2:35:04/27:02:33]  Acc_iter 6750        Data time: 0.00(0.01)  Forward time: 1.40(1.37)  Batch time: 1.41(1.38)
2025-09-02 00:20:43,985   INFO  Train:    2/20 ( 10%) [2937/3862 ( 76%)]  Loss: 2.592 (2.59)  LR: 2.033e-04  Grad: 34.9182  max=1.1690(module.dense_head.decoder.self_attn.in_proj_weight)  min: -4.7279(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8972, loss_cls=0.1539, loss_bbox=1.3535, matched_ious=0.3739, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.31(1.38)  Time cost: 1:07:37/21:17 [2:36:13/27:01:14]  Acc_iter 6800        Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.31(1.38)
2025-09-02 00:21:54,345   INFO  Train:    2/20 ( 10%) [2987/3862 ( 77%)]  Loss: 2.232 (2.59)  LR: 2.048e-04  Grad: 34.6298  max=1.1845(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.7319(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9369, loss_cls=0.1579, loss_bbox=1.2842, matched_ious=0.3715, d_time=0.00(0.01), f_time=1.39(1.37), b_time=1.39(1.38)  Time cost: 1:08:47/20:08 [2:37:23/27:00:36]  Acc_iter 6850        Data time: 0.00(0.01)  Forward time: 1.39(1.37)  Batch time: 1.39(1.38)
2025-09-02 00:23:03,352   INFO  Train:    2/20 ( 10%) [3037/3862 ( 79%)]  Loss: 2.837 (2.58)  LR: 2.063e-04  Grad: 34.6523  max=1.1833(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1759(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9281, loss_cls=0.1592, loss_bbox=1.3111, matched_ious=0.3691, d_time=0.00(0.01), f_time=1.27(1.37), b_time=1.27(1.38)  Time cost: 1:09:56/18:59 [2:38:32/26:59:26]  Acc_iter 6900        Data time: 0.00(0.01)  Forward time: 1.27(1.37)  Batch time: 1.27(1.38)
2025-09-02 00:24:12,901   INFO  Train:    2/20 ( 10%) [3087/3862 ( 80%)]  Loss: 1.827 (2.58)  LR: 2.077e-04  Grad: 34.3767  max=1.1760(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9236(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9205, loss_cls=0.1574, loss_bbox=1.3176, matched_ious=0.3773, d_time=0.00(0.01), f_time=2.11(1.37), b_time=2.11(1.38)  Time cost: 1:11:06/17:50 [2:39:41/26:58:28]  Acc_iter 6950        Data time: 0.00(0.01)  Forward time: 2.11(1.37)  Batch time: 2.11(1.38)
2025-09-02 00:25:21,022   INFO  Train:    2/20 ( 10%) [3137/3862 ( 81%)]  Loss: 2.420 (2.58)  LR: 2.092e-04  Grad: 34.4387  max=1.1868(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9276(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9210, loss_cls=0.1581, loss_bbox=1.2814, matched_ious=0.3802, d_time=0.00(0.01), f_time=1.35(1.37), b_time=1.35(1.38)  Time cost: 1:12:14/16:41 [2:40:50/26:56:57]  Acc_iter 7000        Data time: 0.00(0.01)  Forward time: 1.35(1.37)  Batch time: 1.35(1.38)
2025-09-02 00:26:29,710   INFO  Train:    2/20 ( 10%) [3187/3862 ( 83%)]  Loss: 2.734 (2.57)  LR: 2.107e-04  Grad: 34.5158  max=1.1898(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9312(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9071, loss_cls=0.1567, loss_bbox=1.2915, matched_ious=0.3743, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.38(1.38)  Time cost: 1:13:22/15:32 [2:41:58/26:55:40]  Acc_iter 7050        Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.38(1.38)
2025-09-02 00:27:39,498   INFO  Train:    2/20 ( 10%) [3237/3862 ( 84%)]  Loss: 2.377 (2.57)  LR: 2.122e-04  Grad: 34.5687  max=1.1960(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9282(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9306, loss_cls=0.1585, loss_bbox=1.2848, matched_ious=0.3765, d_time=0.01(0.01), f_time=1.35(1.37), b_time=1.36(1.38)  Time cost: 1:14:32/14:23 [2:43:08/26:54:47]  Acc_iter 7100        Data time: 0.01(0.01)  Forward time: 1.35(1.37)  Batch time: 1.36(1.38)
2025-09-02 00:28:48,452   INFO  Train:    2/20 ( 10%) [3287/3862 ( 85%)]  Loss: 1.774 (2.57)  LR: 2.138e-04  Grad: 34.6524  max=1.1997(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9243(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9026, loss_cls=0.1542, loss_bbox=1.2792, matched_ious=0.3823, d_time=0.00(0.01), f_time=1.39(1.37), b_time=1.40(1.38)  Time cost: 1:15:41/13:14 [2:44:17/26:53:35]  Acc_iter 7150        Data time: 0.00(0.01)  Forward time: 1.39(1.37)  Batch time: 1.40(1.38)
2025-09-02 00:29:58,382   INFO  Train:    2/20 ( 10%) [3337/3862 ( 86%)]  Loss: 2.408 (2.56)  LR: 2.153e-04  Grad: 34.5776  max=1.1980(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.9479(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9234, loss_cls=0.1575, loss_bbox=1.2969, matched_ious=0.3818, d_time=0.00(0.01), f_time=1.41(1.37), b_time=1.41(1.38)  Time cost: 1:16:51/12:05 [2:45:27/26:52:44]  Acc_iter 7200        Data time: 0.00(0.01)  Forward time: 1.41(1.37)  Batch time: 1.41(1.38)
2025-09-02 00:31:06,782   INFO  Train:    2/20 ( 10%) [3387/3862 ( 88%)]  Loss: 1.803 (2.56)  LR: 2.168e-04  Grad: 34.5911  max=1.2025(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9216(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9043, loss_cls=0.1545, loss_bbox=1.2342, matched_ious=0.3887, d_time=0.00(0.01), f_time=1.28(1.37), b_time=1.29(1.38)  Time cost: 1:18:00/10:56 [2:46:35/26:51:21]  Acc_iter 7250        Data time: 0.00(0.01)  Forward time: 1.28(1.37)  Batch time: 1.29(1.38)
2025-09-02 00:32:14,668   INFO  Train:    2/20 ( 10%) [3437/3862 ( 89%)]  Loss: 2.402 (2.56)  LR: 2.184e-04  Grad: 34.6857  max=1.2094(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9294(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9191, loss_cls=0.1556, loss_bbox=1.2454, matched_ious=0.3896, d_time=0.01(0.01), f_time=1.36(1.37), b_time=1.37(1.38)  Time cost: 1:19:07/09:46 [2:47:43/26:49:48]  Acc_iter 7300        Data time: 0.01(0.01)  Forward time: 1.36(1.37)  Batch time: 1.37(1.38)
2025-09-02 00:33:24,643   INFO  Train:    2/20 ( 10%) [3487/3862 ( 90%)]  Loss: 2.514 (2.55)  LR: 2.199e-04  Grad: 34.8351  max=1.8266(module.vfe.pfn_layers.0.linear.weight)  min: -0.9297(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8766, loss_cls=0.1516, loss_bbox=1.2327, matched_ious=0.3841, d_time=0.00(0.01), f_time=1.21(1.37), b_time=1.22(1.38)  Time cost: 1:20:17/08:37 [2:48:53/26:48:58]  Acc_iter 7350        Data time: 0.00(0.01)  Forward time: 1.21(1.37)  Batch time: 1.22(1.38)
2025-09-02 00:34:33,613   INFO  Train:    2/20 ( 10%) [3537/3862 ( 92%)]  Loss: 2.767 (2.55)  LR: 2.215e-04  Grad: 34.6278  max=1.2138(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9224(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9025, loss_cls=0.1540, loss_bbox=1.2280, matched_ious=0.3884, d_time=0.00(0.01), f_time=1.41(1.37), b_time=1.41(1.38)  Time cost: 1:21:26/07:28 [2:50:02/26:47:47]  Acc_iter 7400        Data time: 0.00(0.01)  Forward time: 1.41(1.37)  Batch time: 1.41(1.38)
2025-09-02 00:35:41,871   INFO  Train:    2/20 ( 10%) [3587/3862 ( 93%)]  Loss: 2.669 (2.55)  LR: 2.230e-04  Grad: 34.7248  max=2.5314(module.vfe.pfn_layers.0.linear.weight)  min: -0.9226(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8968, loss_cls=0.1514, loss_bbox=1.2528, matched_ious=0.3881, d_time=0.01(0.01), f_time=1.27(1.37), b_time=1.28(1.38)  Time cost: 1:22:35/06:19 [2:51:10/26:46:22]  Acc_iter 7450        Data time: 0.01(0.01)  Forward time: 1.27(1.37)  Batch time: 1.28(1.38)
2025-09-02 00:36:52,133   INFO  Train:    2/20 ( 10%) [3637/3862 ( 94%)]  Loss: 2.315 (2.54)  LR: 2.246e-04  Grad: 34.7132  max=1.2143(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.2438(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8975, loss_cls=0.1554, loss_bbox=1.2172, matched_ious=0.3849, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.34(1.38)  Time cost: 1:23:45/05:10 [2:52:21/26:45:36]  Acc_iter 7500        Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.34(1.38)
2025-09-02 00:38:00,053   INFO  Train:    2/20 ( 10%) [3687/3862 ( 95%)]  Loss: 1.800 (2.54)  LR: 2.262e-04  Grad: 34.8317  max=2.3518(module.vfe.pfn_layers.0.linear.weight)  min: -0.9256(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8872, loss_cls=0.1525, loss_bbox=1.2108, matched_ious=0.3945, d_time=0.01(0.01), f_time=1.34(1.37), b_time=1.35(1.38)  Time cost: 1:24:53/04:01 [2:53:29/26:44:06]  Acc_iter 7550        Data time: 0.01(0.01)  Forward time: 1.34(1.37)  Batch time: 1.35(1.38)
2025-09-02 00:39:09,701   INFO  Train:    2/20 ( 10%) [3737/3862 ( 97%)]  Loss: 2.089 (2.53)  LR: 2.278e-04  Grad: 34.5278  max=1.2231(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9311(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8892, loss_cls=0.1497, loss_bbox=1.2493, matched_ious=0.3899, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.31(1.38)  Time cost: 1:26:02/02:52 [2:54:38/26:43:08]  Acc_iter 7600        Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.31(1.38)
2025-09-02 00:40:17,470   INFO  Train:    2/20 ( 10%) [3787/3862 ( 98%)]  Loss: 2.120 (2.53)  LR: 2.294e-04  Grad: 34.2643  max=1.2215(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9246(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8805, loss_cls=0.1499, loss_bbox=1.2584, matched_ious=0.3934, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.32(1.38)  Time cost: 1:27:10/01:43 [2:55:46/26:41:35]  Acc_iter 7650        Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.32(1.38)
2025-09-02 00:41:24,580   INFO  Train:    2/20 ( 10%) [3837/3862 ( 99%)]  Loss: 2.439 (2.53)  LR: 2.310e-04  Grad: 33.8748  max=1.2099(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2364(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8981, loss_cls=0.1537, loss_bbox=1.3185, matched_ious=0.3844, d_time=0.00(0.01), f_time=1.24(1.37), b_time=1.24(1.38)  Time cost: 1:28:17/00:34 [2:56:53/26:39:51]  Acc_iter 7700        Data time: 0.00(0.01)  Forward time: 1.24(1.37)  Batch time: 1.24(1.38)
2025-09-02 00:41:57,007   INFO  Train:    2/20 ( 10%) [3861/3862 (100%)]  Loss: 2.097 (2.53)  LR: 2.318e-04  Grad: 33.9154  max=1.2082(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.5054(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9203, loss_cls=0.1521, loss_bbox=1.2901, matched_ious=0.3820, d_time=0.00(0.01), f_time=1.15(1.37), b_time=1.15(1.38)  Time cost: 1:28:50/00:01 [2:57:26/26:39:05]  Acc_iter 7724        Data time: 0.00(0.01)  Forward time: 1.15(1.37)  Batch time: 1.15(1.38)

                                               [Aepochs:  10%|█         | 2/20 [2:57:27<26:37:31, 5325.08s/it]epochs:  10%|█         | 2/20 [2:57:27<26:37:31, 5325.08s/it]epochs:  10%|█         | 2/20 [2:57:27<26:37:31, 5325.09s/it]epochs:  10%|█         | 2/20 [2:57:27<26:37:31, 5325.09s/it]epochs:  10%|█         | 2/20 [2:57:27<26:37:31, 5325.09s/it]epochs:  10%|█         | 2/20 [2:57:27<26:37:31, 5325.09s/it]epochs:  10%|█         | 2/20 [2:57:27<26:37:31, 5325.09s/it]epochs:  10%|█         | 2/20 [2:57:27<26:37:32, 5325.16s/it]
train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-02 00:42:05,140   INFO  Train:    3/20 ( 15%) [   0/3862 (  0%)]  Loss: 2.059 (2.06)  LR: 2.318e-04  Grad: 33.8938  max=1.2089(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.3746(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8985, loss_cls=0.1592, loss_bbox=1.0015, matched_ious=0.4055, d_time=3.52(3.52), f_time=2.79(2.79), b_time=6.31(6.31)  Time cost: 00:06/6:35:27 [2:57:34/118:38:22]  Acc_iter 7725        Data time: 3.52(3.52)  Forward time: 2.79(2.79)  Batch time: 6.31(6.31)
2025-09-02 00:42:41,373   INFO  Train:    3/20 ( 15%) [  25/3862 (  1%)]  Loss: 2.484 (2.25)  LR: 2.326e-04  Grad: 33.9662  max=2.1722(module.vfe.pfn_layers.0.linear.weight)  min: -0.9196(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8854, loss_cls=0.1471, loss_bbox=1.2254, matched_ious=0.3884, d_time=0.00(0.14), f_time=1.32(1.50), b_time=1.32(1.64)  Time cost: 00:42/1:44:13 [2:58:10/31:27:44]  Acc_iter 7750        Data time: 0.00(0.14)  Forward time: 1.32(1.50)  Batch time: 1.32(1.64)
2025-09-02 00:43:50,194   INFO  Train:    3/20 ( 15%) [  75/3862 (  2%)]  Loss: 2.132 (2.23)  LR: 2.342e-04  Grad: 33.9806  max=1.2163(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9192(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8685, loss_cls=0.1488, loss_bbox=1.2030, matched_ious=0.3878, d_time=0.00(0.05), f_time=1.36(1.41), b_time=1.37(1.47)  Time cost: 01:51/1:32:20 [2:59:19/28:13:21]  Acc_iter 7800        Data time: 0.00(0.05)  Forward time: 1.36(1.41)  Batch time: 1.37(1.47)
2025-09-02 00:44:59,139   INFO  Train:    3/20 ( 15%) [ 125/3862 (  3%)]  Loss: 2.323 (2.24)  LR: 2.359e-04  Grad: 34.0730  max=1.2206(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.7747(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8782, loss_cls=0.1506, loss_bbox=1.2330, matched_ious=0.3939, d_time=0.01(0.03), f_time=1.34(1.40), b_time=1.36(1.43)  Time cost: 03:00/1:29:02 [3:00:28/27:33:30]  Acc_iter 7850        Data time: 0.01(0.03)  Forward time: 1.34(1.40)  Batch time: 1.36(1.43)
2025-09-02 00:46:09,695   INFO  Train:    3/20 ( 15%) [ 175/3862 (  5%)]  Loss: 2.420 (2.23)  LR: 2.375e-04  Grad: 34.0675  max=1.2228(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9251(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8759, loss_cls=0.1484, loss_bbox=1.1806, matched_ious=0.3962, d_time=0.00(0.03), f_time=1.39(1.40), b_time=1.39(1.43)  Time cost: 04:10/1:27:31 [3:01:38/27:26:07]  Acc_iter 7900        Data time: 0.00(0.03)  Forward time: 1.39(1.40)  Batch time: 1.39(1.43)
2025-09-02 00:47:17,550   INFO  Train:    3/20 ( 15%) [ 225/3862 (  6%)]  Loss: 2.075 (2.25)  LR: 2.392e-04  Grad: 34.1436  max=1.2270(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9300(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9090, loss_cls=0.1533, loss_bbox=1.2405, matched_ious=0.3923, d_time=0.01(0.02), f_time=1.35(1.39), b_time=1.37(1.41)  Time cost: 05:18/1:25:26 [3:02:46/27:07:48]  Acc_iter 7950        Data time: 0.01(0.02)  Forward time: 1.35(1.39)  Batch time: 1.37(1.41)
2025-09-02 00:48:28,517   INFO  Train:    3/20 ( 15%) [ 275/3862 (  7%)]  Loss: 2.350 (2.25)  LR: 2.408e-04  Grad: 34.2259  max=1.2290(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.6668(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8828, loss_cls=0.1502, loss_bbox=1.2090, matched_ious=0.3969, d_time=0.00(0.02), f_time=1.33(1.39), b_time=1.33(1.41)  Time cost: 06:29/1:24:22 [3:03:57/27:08:40]  Acc_iter 8000        Data time: 0.00(0.02)  Forward time: 1.33(1.39)  Batch time: 1.33(1.41)
2025-09-02 00:49:36,957   INFO  Train:    3/20 ( 15%) [ 325/3862 (  8%)]  Loss: 2.339 (2.25)  LR: 2.425e-04  Grad: 34.3542  max=2.1264(module.vfe.pfn_layers.0.linear.weight)  min: -0.9361(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9040, loss_cls=0.1521, loss_bbox=1.2143, matched_ious=0.3888, d_time=0.00(0.02), f_time=1.36(1.39), b_time=1.36(1.41)  Time cost: 07:37/1:22:48 [3:05:06/26:59:59]  Acc_iter 8050        Data time: 0.00(0.02)  Forward time: 1.36(1.39)  Batch time: 1.36(1.41)
2025-09-02 00:50:44,670   INFO  Train:    3/20 ( 15%) [ 375/3862 ( 10%)]  Loss: 2.386 (2.25)  LR: 2.442e-04  Grad: 34.3001  max=1.2305(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9296(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8795, loss_cls=0.1506, loss_bbox=1.1820, matched_ious=0.3947, d_time=0.00(0.02), f_time=1.35(1.38), b_time=1.35(1.40)  Time cost: 08:45/1:21:15 [3:06:13/26:51:04]  Acc_iter 8100        Data time: 0.00(0.02)  Forward time: 1.35(1.38)  Batch time: 1.35(1.40)
2025-09-02 00:51:54,647   INFO  Train:    3/20 ( 15%) [ 425/3862 ( 11%)]  Loss: 2.120 (2.24)  LR: 2.458e-04  Grad: 34.3462  max=1.2411(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9326(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8566, loss_cls=0.1476, loss_bbox=1.2142, matched_ious=0.3952, d_time=0.00(0.01), f_time=1.40(1.38), b_time=1.40(1.40)  Time cost: 09:55/1:20:05 [3:07:23/26:50:06]  Acc_iter 8150        Data time: 0.00(0.01)  Forward time: 1.40(1.38)  Batch time: 1.40(1.40)
2025-09-02 00:53:02,346   INFO  Train:    3/20 ( 15%) [ 475/3862 ( 12%)]  Loss: 2.044 (2.25)  LR: 2.475e-04  Grad: 34.4137  max=1.2526(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9422(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8740, loss_cls=0.1489, loss_bbox=1.2546, matched_ious=0.3932, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.35(1.39)  Time cost: 11:03/1:18:40 [3:08:31/26:43:35]  Acc_iter 8200        Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.35(1.39)
2025-09-02 00:54:12,398   INFO  Train:    3/20 ( 15%) [ 525/3862 ( 14%)]  Loss: 1.973 (2.24)  LR: 2.492e-04  Grad: 34.9239  max=2.3739(module.vfe.pfn_layers.0.linear.weight)  min: -5.0405(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8633, loss_cls=0.1460, loss_bbox=1.2031, matched_ious=0.4031, d_time=0.01(0.01), f_time=1.20(1.38), b_time=1.21(1.39)  Time cost: 12:13/1:17:32 [3:09:41/26:43:14]  Acc_iter 8250        Data time: 0.01(0.01)  Forward time: 1.20(1.38)  Batch time: 1.21(1.39)
2025-09-02 00:55:21,258   INFO  Train:    3/20 ( 15%) [ 575/3862 ( 15%)]  Loss: 2.287 (2.24)  LR: 2.509e-04  Grad: 34.0583  max=1.2293(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.6241(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8816, loss_cls=0.1490, loss_bbox=1.2199, matched_ious=0.3951, d_time=0.01(0.01), f_time=1.31(1.38), b_time=1.32(1.39)  Time cost: 13:22/1:16:18 [3:10:50/26:40:22]  Acc_iter 8300        Data time: 0.01(0.01)  Forward time: 1.31(1.38)  Batch time: 1.32(1.39)
2025-09-02 00:56:28,480   INFO  Train:    3/20 ( 15%) [ 625/3862 ( 16%)]  Loss: 1.855 (2.24)  LR: 2.527e-04  Grad: 34.0663  max=1.2419(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1196(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8709, loss_cls=0.1481, loss_bbox=1.2314, matched_ious=0.3979, d_time=0.01(0.01), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 14:29/1:14:56 [3:11:57/26:34:46]  Acc_iter 8350        Data time: 0.01(0.01)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 00:57:37,970   INFO  Train:    3/20 ( 15%) [ 675/3862 ( 17%)]  Loss: 2.441 (2.24)  LR: 2.544e-04  Grad: 34.1940  max=1.2443(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.3120(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8808, loss_cls=0.1473, loss_bbox=1.1624, matched_ious=0.4007, d_time=0.01(0.01), f_time=1.30(1.38), b_time=1.31(1.39)  Time cost: 15:38/1:13:46 [3:13:07/26:33:40]  Acc_iter 8400        Data time: 0.01(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.39)
2025-09-02 00:58:45,977   INFO  Train:    3/20 ( 15%) [ 725/3862 ( 19%)]  Loss: 1.990 (2.24)  LR: 2.561e-04  Grad: 34.5234  max=3.5985(module.vfe.pfn_layers.0.linear.weight)  min: -4.2997(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8772, loss_cls=0.1493, loss_bbox=1.2382, matched_ious=0.3929, d_time=0.00(0.01), f_time=1.35(1.37), b_time=1.35(1.39)  Time cost: 16:46/1:12:31 [3:14:15/26:30:14]  Acc_iter 8450        Data time: 0.00(0.01)  Forward time: 1.35(1.37)  Batch time: 1.35(1.39)
2025-09-02 00:59:54,783   INFO  Train:    3/20 ( 15%) [ 775/3862 ( 20%)]  Loss: 2.295 (2.24)  LR: 2.578e-04  Grad: 34.0369  max=1.7547(module.vfe.pfn_layers.0.linear.weight)  min: -1.4241(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8531, loss_cls=0.1441, loss_bbox=1.2008, matched_ious=0.3988, d_time=0.00(0.01), f_time=1.39(1.37), b_time=1.39(1.39)  Time cost: 17:55/1:11:19 [3:15:23/26:28:17]  Acc_iter 8500        Data time: 0.00(0.01)  Forward time: 1.39(1.37)  Batch time: 1.39(1.39)
2025-09-02 01:01:04,930   INFO  Train:    3/20 ( 15%) [ 825/3862 ( 21%)]  Loss: 2.202 (2.24)  LR: 2.596e-04  Grad: 34.1505  max=1.9000(module.vfe.pfn_layers.0.linear.weight)  min: -1.7452(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8697, loss_cls=0.1485, loss_bbox=1.1668, matched_ious=0.4057, d_time=0.01(0.01), f_time=1.39(1.38), b_time=1.40(1.39)  Time cost: 19:05/1:10:13 [3:16:33/26:28:17]  Acc_iter 8550        Data time: 0.01(0.01)  Forward time: 1.39(1.38)  Batch time: 1.40(1.39)
2025-09-02 01:02:13,578   INFO  Train:    3/20 ( 15%) [ 875/3862 ( 23%)]  Loss: 2.152 (2.23)  LR: 2.613e-04  Grad: 34.0879  max=1.2531(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.3572(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8504, loss_cls=0.1447, loss_bbox=1.2033, matched_ious=0.4023, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.33(1.39)  Time cost: 20:14/1:09:01 [3:17:42/26:26:11]  Acc_iter 8600        Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.33(1.39)
2025-09-02 01:03:21,649   INFO  Train:    3/20 ( 15%) [ 925/3862 ( 24%)]  Loss: 2.105 (2.23)  LR: 2.631e-04  Grad: 34.3124  max=4.7901(module.vfe.pfn_layers.0.linear.weight)  min: -0.9429(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8855, loss_cls=0.1526, loss_bbox=1.1933, matched_ious=0.4026, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.37(1.39)  Time cost: 21:22/1:07:48 [3:18:50/26:23:29]  Acc_iter 8650        Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.37(1.39)
2025-09-02 01:04:31,286   INFO  Train:    3/20 ( 15%) [ 975/3862 ( 25%)]  Loss: 2.320 (2.23)  LR: 2.649e-04  Grad: 34.2523  max=2.8792(module.vfe.pfn_layers.0.linear.weight)  min: -2.3971(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8595, loss_cls=0.1468, loss_bbox=1.2234, matched_ious=0.3967, d_time=0.00(0.01), f_time=1.28(1.37), b_time=1.28(1.39)  Time cost: 22:32/1:06:40 [3:20:00/26:22:46]  Acc_iter 8700        Data time: 0.00(0.01)  Forward time: 1.28(1.37)  Batch time: 1.28(1.39)
2025-09-02 01:05:38,839   INFO  Train:    3/20 ( 15%) [1025/3862 ( 27%)]  Loss: 2.264 (2.23)  LR: 2.666e-04  Grad: 34.2189  max=1.2548(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.9433(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8403, loss_cls=0.1452, loss_bbox=1.1806, matched_ious=0.4052, d_time=0.00(0.01), f_time=1.49(1.37), b_time=1.49(1.38)  Time cost: 23:39/1:05:26 [3:21:07/26:19:42]  Acc_iter 8750        Data time: 0.00(0.01)  Forward time: 1.49(1.37)  Batch time: 1.49(1.38)
2025-09-02 01:06:50,360   INFO  Train:    3/20 ( 15%) [1075/3862 ( 28%)]  Loss: 2.037 (2.23)  LR: 2.684e-04  Grad: 34.0058  max=1.2532(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.3086(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8547, loss_cls=0.1452, loss_bbox=1.1892, matched_ious=0.3979, d_time=0.01(0.01), f_time=1.26(1.38), b_time=1.27(1.39)  Time cost: 24:51/1:04:22 [3:22:19/26:21:01]  Acc_iter 8800        Data time: 0.01(0.01)  Forward time: 1.26(1.38)  Batch time: 1.27(1.39)
2025-09-02 01:07:58,898   INFO  Train:    3/20 ( 15%) [1125/3862 ( 29%)]  Loss: 2.269 (2.22)  LR: 2.702e-04  Grad: 34.2350  max=1.3143(module.vfe.pfn_layers.0.linear.weight)  min: -3.6584(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8519, loss_cls=0.1436, loss_bbox=1.1584, matched_ious=0.4083, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.35(1.39)  Time cost: 25:59/1:03:11 [3:23:27/26:19:05]  Acc_iter 8850        Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.35(1.39)
2025-09-02 01:09:07,745   INFO  Train:    3/20 ( 15%) [1175/3862 ( 30%)]  Loss: 1.932 (2.22)  LR: 2.720e-04  Grad: 34.0850  max=1.2573(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9461(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8240, loss_cls=0.1395, loss_bbox=1.1868, matched_ious=0.4018, d_time=0.00(0.01), f_time=2.25(1.37), b_time=2.26(1.39)  Time cost: 27:08/1:02:01 [3:24:36/26:17:31]  Acc_iter 8900        Data time: 0.00(0.01)  Forward time: 2.25(1.37)  Batch time: 2.26(1.39)
2025-09-02 01:10:17,289   INFO  Train:    3/20 ( 15%) [1225/3862 ( 32%)]  Loss: 2.010 (2.22)  LR: 2.738e-04  Grad: 34.1307  max=1.2598(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9479(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8432, loss_cls=0.1440, loss_bbox=1.1337, matched_ious=0.4037, d_time=0.01(0.01), f_time=1.32(1.38), b_time=1.33(1.39)  Time cost: 28:18/1:00:52 [3:25:46/26:16:38]  Acc_iter 8950        Data time: 0.01(0.01)  Forward time: 1.32(1.38)  Batch time: 1.33(1.39)
2025-09-02 01:11:26,010   INFO  Train:    3/20 ( 15%) [1275/3862 ( 33%)]  Loss: 2.075 (2.21)  LR: 2.756e-04  Grad: 33.5094  max=1.2204(module.dense_head.decoder.self_attn.in_proj_weight)  min: -3.8112(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8422, loss_cls=0.1411, loss_bbox=1.1515, matched_ious=0.3927, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.35(1.38)  Time cost: 29:27/59:42 [3:26:55/26:15:00]  Acc_iter 9000        Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.35(1.38)
2025-09-02 01:12:35,920   INFO  Train:    3/20 ( 15%) [1325/3862 ( 34%)]  Loss: 2.010 (2.21)  LR: 2.774e-04  Grad: 33.1997  max=1.2282(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9301(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8573, loss_cls=0.1433, loss_bbox=1.2032, matched_ious=0.4075, d_time=0.00(0.01), f_time=1.41(1.38), b_time=1.41(1.39)  Time cost: 30:36/58:34 [3:28:04/26:14:25]  Acc_iter 9050        Data time: 0.00(0.01)  Forward time: 1.41(1.38)  Batch time: 1.41(1.39)
2025-09-02 01:13:46,463   INFO  Train:    3/20 ( 15%) [1375/3862 ( 36%)]  Loss: 2.041 (2.21)  LR: 2.793e-04  Grad: 33.2947  max=1.3238(module.vfe.pfn_layers.0.linear.weight)  min: -0.9299(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8447, loss_cls=0.1436, loss_bbox=1.1379, matched_ious=0.4113, d_time=0.01(0.01), f_time=1.42(1.38), b_time=1.42(1.39)  Time cost: 31:47/57:27 [3:29:15/26:14:19]  Acc_iter 9100        Data time: 0.01(0.01)  Forward time: 1.42(1.38)  Batch time: 1.42(1.39)
2025-09-02 01:14:54,159   INFO  Train:    3/20 ( 15%) [1425/3862 ( 37%)]  Loss: 2.290 (2.21)  LR: 2.811e-04  Grad: 33.4999  max=2.4448(module.vfe.pfn_layers.0.linear.weight)  min: -2.9599(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8446, loss_cls=0.1448, loss_bbox=1.0943, matched_ious=0.4090, d_time=0.00(0.01), f_time=1.29(1.38), b_time=1.29(1.39)  Time cost: 32:55/56:15 [3:30:23/26:11:53]  Acc_iter 9150        Data time: 0.00(0.01)  Forward time: 1.29(1.38)  Batch time: 1.29(1.39)
2025-09-02 01:16:03,305   INFO  Train:    3/20 ( 15%) [1475/3862 ( 38%)]  Loss: 2.505 (2.20)  LR: 2.829e-04  Grad: 33.4088  max=1.2370(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.6643(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8358, loss_cls=0.1397, loss_bbox=1.1586, matched_ious=0.4065, d_time=0.00(0.01), f_time=1.35(1.38), b_time=1.35(1.39)  Time cost: 34:04/55:06 [3:31:32/26:10:39]  Acc_iter 9200        Data time: 0.00(0.01)  Forward time: 1.35(1.38)  Batch time: 1.35(1.39)
2025-09-02 01:17:11,588   INFO  Train:    3/20 ( 15%) [1525/3862 ( 39%)]  Loss: 2.146 (2.20)  LR: 2.848e-04  Grad: 33.4473  max=1.4033(module.vfe.pfn_layers.0.linear.weight)  min: -0.9383(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8581, loss_cls=0.1459, loss_bbox=1.1618, matched_ious=0.4070, d_time=0.00(0.01), f_time=1.30(1.37), b_time=1.31(1.38)  Time cost: 35:12/53:55 [3:32:40/26:08:46]  Acc_iter 9250        Data time: 0.00(0.01)  Forward time: 1.30(1.37)  Batch time: 1.31(1.38)
2025-09-02 01:18:20,281   INFO  Train:    3/20 ( 15%) [1575/3862 ( 41%)]  Loss: 1.947 (2.20)  LR: 2.866e-04  Grad: 33.8249  max=4.6669(module.vfe.pfn_layers.0.linear.weight)  min: -0.9384(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8388, loss_cls=0.1420, loss_bbox=1.1635, matched_ious=0.4074, d_time=0.02(0.01), f_time=1.28(1.37), b_time=1.30(1.38)  Time cost: 36:21/52:45 [3:33:49/26:07:14]  Acc_iter 9300        Data time: 0.02(0.01)  Forward time: 1.28(1.37)  Batch time: 1.30(1.38)
2025-09-02 01:19:30,138   INFO  Train:    3/20 ( 15%) [1625/3862 ( 42%)]  Loss: 1.861 (2.20)  LR: 2.885e-04  Grad: 33.6314  max=1.2865(module.vfe.pfn_layers.0.linear.weight)  min: -2.1110(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8523, loss_cls=0.1450, loss_bbox=1.1602, matched_ious=0.4060, d_time=0.00(0.01), f_time=1.29(1.38), b_time=1.29(1.38)  Time cost: 37:31/51:37 [3:34:59/26:06:32]  Acc_iter 9350        Data time: 0.00(0.01)  Forward time: 1.29(1.38)  Batch time: 1.29(1.38)
2025-09-02 01:20:38,715   INFO  Train:    3/20 ( 15%) [1675/3862 ( 43%)]  Loss: 2.366 (2.20)  LR: 2.903e-04  Grad: 33.7480  max=1.2504(module.dense_head.decoder.self_attn.in_proj_weight)  min: -3.4254(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8158, loss_cls=0.1371, loss_bbox=1.1144, matched_ious=0.4090, d_time=0.03(0.01), f_time=1.33(1.37), b_time=1.36(1.38)  Time cost: 38:39/50:26 [3:36:07/26:04:57]  Acc_iter 9400        Data time: 0.03(0.01)  Forward time: 1.33(1.37)  Batch time: 1.36(1.38)
2025-09-02 01:21:48,437   INFO  Train:    3/20 ( 15%) [1725/3862 ( 45%)]  Loss: 2.251 (2.19)  LR: 2.922e-04  Grad: 33.6854  max=1.2573(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.6996(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8283, loss_cls=0.1427, loss_bbox=1.1543, matched_ious=0.4131, d_time=0.03(0.01), f_time=1.34(1.38), b_time=1.37(1.38)  Time cost: 39:49/49:18 [3:37:17/26:04:08]  Acc_iter 9450        Data time: 0.03(0.01)  Forward time: 1.34(1.38)  Batch time: 1.37(1.38)
2025-09-02 01:22:56,613   INFO  Train:    3/20 ( 15%) [1775/3862 ( 46%)]  Loss: 2.100 (2.19)  LR: 2.941e-04  Grad: 33.8185  max=1.2575(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.4464(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8566, loss_cls=0.1447, loss_bbox=1.1461, matched_ious=0.4096, d_time=0.03(0.01), f_time=1.23(1.37), b_time=1.26(1.38)  Time cost: 40:57/48:07 [3:38:25/26:02:19]  Acc_iter 9500        Data time: 0.03(0.01)  Forward time: 1.23(1.37)  Batch time: 1.26(1.38)
2025-09-02 01:24:05,923   INFO  Train:    3/20 ( 15%) [1825/3862 ( 47%)]  Loss: 2.189 (2.19)  LR: 2.960e-04  Grad: 33.7883  max=1.3944(module.vfe.pfn_layers.0.linear.weight)  min: -0.9560(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8482, loss_cls=0.1448, loss_bbox=1.1753, matched_ious=0.4043, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.33(1.38)  Time cost: 42:06/46:58 [3:39:34/26:01:14]  Acc_iter 9550        Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.33(1.38)
2025-09-02 01:25:16,324   INFO  Train:    3/20 ( 15%) [1875/3862 ( 49%)]  Loss: 2.170 (2.19)  LR: 2.979e-04  Grad: 33.8292  max=1.2562(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.4999(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8543, loss_cls=0.1455, loss_bbox=1.1404, matched_ious=0.4061, d_time=0.00(0.01), f_time=2.20(1.38), b_time=2.20(1.38)  Time cost: 43:17/45:51 [3:40:45/26:00:49]  Acc_iter 9600        Data time: 0.00(0.01)  Forward time: 2.20(1.38)  Batch time: 2.20(1.38)
2025-09-02 01:26:25,267   INFO  Train:    3/20 ( 15%) [1925/3862 ( 50%)]  Loss: 1.969 (2.19)  LR: 2.998e-04  Grad: 34.0977  max=1.2644(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.9087(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8165, loss_cls=0.1374, loss_bbox=1.0919, matched_ious=0.4191, d_time=0.01(0.01), f_time=1.35(1.38), b_time=1.36(1.38)  Time cost: 44:26/44:41 [3:41:54/25:59:30]  Acc_iter 9650        Data time: 0.01(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.38)
2025-09-02 01:27:35,062   INFO  Train:    3/20 ( 15%) [1975/3862 ( 51%)]  Loss: 2.141 (2.18)  LR: 3.017e-04  Grad: 34.2486  max=2.8613(module.vfe.pfn_layers.0.linear.weight)  min: -2.6681(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8270, loss_cls=0.1409, loss_bbox=1.0996, matched_ious=0.4157, d_time=0.00(0.01), f_time=1.39(1.38), b_time=1.39(1.38)  Time cost: 45:36/43:32 [3:43:04/25:58:40]  Acc_iter 9700        Data time: 0.00(0.01)  Forward time: 1.39(1.38)  Batch time: 1.39(1.38)
2025-09-02 01:28:42,944   INFO  Train:    3/20 ( 15%) [2025/3862 ( 52%)]  Loss: 2.519 (2.18)  LR: 3.036e-04  Grad: 34.1691  max=3.1029(module.vfe.pfn_layers.0.linear.weight)  min: -0.9733(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8287, loss_cls=0.1408, loss_bbox=1.0981, matched_ious=0.4176, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.30(1.38)  Time cost: 46:43/42:22 [3:44:12/25:56:46]  Acc_iter 9750        Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.30(1.38)
2025-09-02 01:29:51,190   INFO  Train:    3/20 ( 15%) [2075/3862 ( 54%)]  Loss: 2.073 (2.18)  LR: 3.055e-04  Grad: 34.2358  max=3.8006(module.vfe.pfn_layers.0.linear.weight)  min: -0.9764(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8343, loss_cls=0.1435, loss_bbox=1.0415, matched_ious=0.4226, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.34(1.38)  Time cost: 47:52/41:12 [3:45:20/25:55:06]  Acc_iter 9800        Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.34(1.38)
2025-09-02 01:31:01,310   INFO  Train:    3/20 ( 15%) [2125/3862 ( 55%)]  Loss: 2.253 (2.17)  LR: 3.074e-04  Grad: 34.1144  max=1.2745(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.4527(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8266, loss_cls=0.1421, loss_bbox=1.0909, matched_ious=0.4142, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.34(1.38)  Time cost: 49:02/40:03 [3:46:30/25:54:26]  Acc_iter 9850        Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.34(1.38)
2025-09-02 01:32:11,112   INFO  Train:    3/20 ( 15%) [2175/3862 ( 56%)]  Loss: 1.977 (2.17)  LR: 3.094e-04  Grad: 34.0485  max=3.9971(module.vfe.pfn_layers.0.linear.weight)  min: -0.9709(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8386, loss_cls=0.1424, loss_bbox=1.0971, matched_ious=0.4163, d_time=0.00(0.01), f_time=1.40(1.38), b_time=1.40(1.38)  Time cost: 50:12/38:55 [3:47:40/25:53:36]  Acc_iter 9900        Data time: 0.00(0.01)  Forward time: 1.40(1.38)  Batch time: 1.40(1.38)
2025-09-02 01:33:19,232   INFO  Train:    3/20 ( 15%) [2225/3862 ( 58%)]  Loss: 2.238 (2.17)  LR: 3.113e-04  Grad: 33.9312  max=2.0934(module.vfe.pfn_layers.0.linear.weight)  min: -0.9719(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8175, loss_cls=0.1390, loss_bbox=1.0590, matched_ious=0.4143, d_time=0.01(0.01), f_time=1.42(1.38), b_time=1.43(1.38)  Time cost: 51:20/37:45 [3:48:48/25:51:54]  Acc_iter 9950        Data time: 0.01(0.01)  Forward time: 1.42(1.38)  Batch time: 1.43(1.38)
2025-09-02 01:34:28,926   INFO  Train:    3/20 ( 15%) [2275/3862 ( 59%)]  Loss: 2.495 (2.17)  LR: 3.132e-04  Grad: 34.3927  max=1.2654(module.dense_head.decoder.self_attn.in_proj_weight)  min: -5.4443(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8183, loss_cls=0.1406, loss_bbox=1.1077, matched_ious=0.4176, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.37(1.38)  Time cost: 52:29/36:36 [3:49:57/25:50:59]  Acc_iter 10000       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.37(1.38)
2025-09-02 01:35:36,462   INFO  Train:    3/20 ( 15%) [2325/3862 ( 60%)]  Loss: 2.006 (2.16)  LR: 3.152e-04  Grad: 34.0229  max=1.2708(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1006(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8147, loss_cls=0.1381, loss_bbox=1.1042, matched_ious=0.4116, d_time=0.00(0.01), f_time=1.40(1.37), b_time=1.41(1.38)  Time cost: 53:37/35:26 [3:51:05/25:49:02]  Acc_iter 10050       Data time: 0.00(0.01)  Forward time: 1.40(1.37)  Batch time: 1.41(1.38)
2025-09-02 01:36:45,242   INFO  Train:    3/20 ( 15%) [2375/3862 ( 61%)]  Loss: 2.042 (2.16)  LR: 3.171e-04  Grad: 33.6995  max=1.2551(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.4067(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8236, loss_cls=0.1387, loss_bbox=1.1105, matched_ious=0.4170, d_time=0.00(0.01), f_time=1.43(1.37), b_time=1.43(1.38)  Time cost: 54:46/34:16 [3:52:14/25:47:42]  Acc_iter 10100       Data time: 0.00(0.01)  Forward time: 1.43(1.37)  Batch time: 1.43(1.38)
2025-09-02 01:37:56,241   INFO  Train:    3/20 ( 15%) [2425/3862 ( 63%)]  Loss: 1.752 (2.16)  LR: 3.191e-04  Grad: 33.8511  max=2.2958(module.vfe.pfn_layers.0.linear.weight)  min: -1.7977(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8035, loss_cls=0.1359, loss_bbox=1.0940, matched_ious=0.4162, d_time=0.01(0.01), f_time=1.37(1.38), b_time=1.37(1.38)  Time cost: 55:57/33:08 [3:53:25/25:47:24]  Acc_iter 10150       Data time: 0.01(0.01)  Forward time: 1.37(1.38)  Batch time: 1.37(1.38)
2025-09-02 01:39:04,617   INFO  Train:    3/20 ( 15%) [2475/3862 ( 64%)]  Loss: 1.940 (2.16)  LR: 3.211e-04  Grad: 33.8078  max=1.2540(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.3753(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8118, loss_cls=0.1371, loss_bbox=1.1103, matched_ious=0.4182, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.35(1.38)  Time cost: 57:05/31:58 [3:54:33/25:45:53]  Acc_iter 10200       Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.35(1.38)
2025-09-02 01:40:13,648   INFO  Train:    3/20 ( 15%) [2525/3862 ( 65%)]  Loss: 1.873 (2.16)  LR: 3.230e-04  Grad: 33.8334  max=1.5310(module.vfe.pfn_layers.0.linear.weight)  min: -0.9757(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8097, loss_cls=0.1387, loss_bbox=1.1237, matched_ious=0.4155, d_time=0.00(0.01), f_time=1.26(1.37), b_time=1.27(1.38)  Time cost: 58:14/30:49 [3:55:42/25:44:39]  Acc_iter 10250       Data time: 0.00(0.01)  Forward time: 1.26(1.37)  Batch time: 1.27(1.38)
2025-09-02 01:41:21,829   INFO  Train:    3/20 ( 15%) [2575/3862 ( 67%)]  Loss: 2.096 (2.15)  LR: 3.250e-04  Grad: 33.9230  max=1.2701(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2217(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8104, loss_cls=0.1366, loss_bbox=1.1150, matched_ious=0.4203, d_time=0.00(0.01), f_time=1.35(1.37), b_time=1.35(1.38)  Time cost: 59:22/29:40 [3:56:50/25:43:05]  Acc_iter 10300       Data time: 0.00(0.01)  Forward time: 1.35(1.37)  Batch time: 1.35(1.38)
2025-09-02 01:42:30,586   INFO  Train:    3/20 ( 15%) [2625/3862 ( 68%)]  Loss: 2.274 (2.15)  LR: 3.270e-04  Grad: 33.9139  max=1.2699(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9753(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8168, loss_cls=0.1393, loss_bbox=1.0763, matched_ious=0.4244, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.32(1.38)  Time cost: 1:00:31/28:30 [3:57:59/25:41:46]  Acc_iter 10350       Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.32(1.38)
2025-09-02 01:43:41,815   INFO  Train:    3/20 ( 15%) [2675/3862 ( 69%)]  Loss: 1.747 (2.15)  LR: 3.290e-04  Grad: 34.2198  max=1.2780(module.dense_head.decoder.self_attn.in_proj_weight)  min: -4.4652(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8104, loss_cls=0.1351, loss_bbox=1.0860, matched_ious=0.4249, d_time=0.01(0.01), f_time=1.38(1.38), b_time=1.39(1.38)  Time cost: 1:01:42/27:22 [3:59:10/25:41:28]  Acc_iter 10400       Data time: 0.01(0.01)  Forward time: 1.38(1.38)  Batch time: 1.39(1.38)
2025-09-02 01:44:51,067   INFO  Train:    3/20 ( 15%) [2725/3862 ( 71%)]  Loss: 2.057 (2.15)  LR: 3.310e-04  Grad: 34.0419  max=2.1635(module.vfe.pfn_layers.0.linear.weight)  min: -0.9846(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8212, loss_cls=0.1376, loss_bbox=1.0955, matched_ious=0.4219, d_time=0.00(0.01), f_time=1.41(1.38), b_time=1.41(1.38)  Time cost: 1:02:52/26:13 [4:00:20/25:40:21]  Acc_iter 10450       Data time: 0.00(0.01)  Forward time: 1.41(1.38)  Batch time: 1.41(1.38)
2025-09-02 01:46:00,558   INFO  Train:    3/20 ( 15%) [2775/3862 ( 72%)]  Loss: 2.120 (2.14)  LR: 3.330e-04  Grad: 34.0865  max=1.5456(module.vfe.pfn_layers.0.linear.weight)  min: -0.9811(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8069, loss_cls=0.1361, loss_bbox=1.0747, matched_ious=0.4166, d_time=0.00(0.01), f_time=1.46(1.38), b_time=1.46(1.38)  Time cost: 1:04:01/25:04 [4:01:29/25:39:19]  Acc_iter 10500       Data time: 0.00(0.01)  Forward time: 1.46(1.38)  Batch time: 1.46(1.38)
2025-09-02 01:47:10,986   INFO  Train:    3/20 ( 15%) [2825/3862 ( 73%)]  Loss: 2.469 (2.14)  LR: 3.350e-04  Grad: 34.2578  max=2.7870(module.vfe.pfn_layers.0.linear.weight)  min: -0.9882(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8146, loss_cls=0.1401, loss_bbox=1.0343, matched_ious=0.4252, d_time=0.01(0.01), f_time=1.32(1.38), b_time=1.33(1.38)  Time cost: 1:05:11/23:55 [4:02:40/25:38:39]  Acc_iter 10550       Data time: 0.01(0.01)  Forward time: 1.32(1.38)  Batch time: 1.33(1.38)
2025-09-02 01:48:20,530   INFO  Train:    3/20 ( 15%) [2875/3862 ( 74%)]  Loss: 1.931 (2.14)  LR: 3.370e-04  Grad: 33.2417  max=1.6781(module.vfe.pfn_layers.0.linear.weight)  min: -1.3240(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7937, loss_cls=0.1346, loss_bbox=1.0647, matched_ious=0.4253, d_time=0.01(0.01), f_time=1.32(1.38), b_time=1.33(1.38)  Time cost: 1:06:21/22:46 [4:03:49/25:37:37]  Acc_iter 10600       Data time: 0.01(0.01)  Forward time: 1.32(1.38)  Batch time: 1.33(1.38)
2025-09-02 01:49:29,634   INFO  Train:    3/20 ( 15%) [2925/3862 ( 76%)]  Loss: 2.004 (2.14)  LR: 3.390e-04  Grad: 32.0905  max=1.3024(module.vfe.pfn_layers.0.linear.weight)  min: -0.9336(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8065, loss_cls=0.1373, loss_bbox=1.0825, matched_ious=0.4292, d_time=0.01(0.01), f_time=1.39(1.38), b_time=1.40(1.38)  Time cost: 1:07:30/21:37 [4:04:58/25:36:25]  Acc_iter 10650       Data time: 0.01(0.01)  Forward time: 1.39(1.38)  Batch time: 1.40(1.38)
2025-09-02 01:50:39,881   INFO  Train:    3/20 ( 15%) [2975/3862 ( 77%)]  Loss: 2.011 (2.14)  LR: 3.410e-04  Grad: 32.1784  max=1.4225(module.vfe.pfn_layers.0.linear.weight)  min: -1.1979(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8242, loss_cls=0.1400, loss_bbox=1.1005, matched_ious=0.4222, d_time=0.00(0.01), f_time=1.44(1.38), b_time=1.44(1.38)  Time cost: 1:08:40/20:28 [4:06:08/25:35:39]  Acc_iter 10700       Data time: 0.00(0.01)  Forward time: 1.44(1.38)  Batch time: 1.44(1.38)
2025-09-02 01:51:48,000   INFO  Train:    3/20 ( 15%) [3025/3862 ( 78%)]  Loss: 1.873 (2.13)  LR: 3.431e-04  Grad: 32.4932  max=1.2122(module.dense_head.decoder.self_attn.in_proj_weight)  min: -4.3134(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7959, loss_cls=0.1345, loss_bbox=1.0690, matched_ious=0.4164, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.34(1.38)  Time cost: 1:09:49/19:18 [4:07:17/25:34:06]  Acc_iter 10750       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.38)
2025-09-02 01:52:58,353   INFO  Train:    3/20 ( 15%) [3075/3862 ( 80%)]  Loss: 2.365 (2.13)  LR: 3.451e-04  Grad: 32.2609  max=1.2164(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.7595(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7793, loss_cls=0.1310, loss_bbox=1.0490, matched_ious=0.4223, d_time=0.01(0.01), f_time=1.35(1.38), b_time=1.36(1.38)  Time cost: 1:10:59/18:09 [4:08:27/25:33:21]  Acc_iter 10800       Data time: 0.01(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.38)
2025-09-02 01:54:06,446   INFO  Train:    3/20 ( 15%) [3125/3862 ( 81%)]  Loss: 1.727 (2.13)  LR: 3.471e-04  Grad: 32.2665  max=1.2215(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.5364(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8338, loss_cls=0.1373, loss_bbox=1.0994, matched_ious=0.4180, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.31(1.38)  Time cost: 1:12:07/17:00 [4:09:35/25:31:47]  Acc_iter 10850       Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.38)
2025-09-02 01:55:14,959   INFO  Train:    3/20 ( 15%) [3175/3862 ( 82%)]  Loss: 2.284 (2.13)  LR: 3.492e-04  Grad: 32.3331  max=1.2286(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9369(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8004, loss_cls=0.1371, loss_bbox=1.0533, matched_ious=0.4277, d_time=0.00(0.01), f_time=1.42(1.38), b_time=1.43(1.38)  Time cost: 1:13:15/15:50 [4:10:44/25:30:23]  Acc_iter 10900       Data time: 0.00(0.01)  Forward time: 1.42(1.38)  Batch time: 1.43(1.38)
2025-09-02 01:56:23,379   INFO  Train:    3/20 ( 15%) [3225/3862 ( 84%)]  Loss: 2.276 (2.13)  LR: 3.512e-04  Grad: 32.9107  max=1.2321(module.dense_head.decoder.self_attn.in_proj_weight)  min: -5.7837(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8207, loss_cls=0.1400, loss_bbox=1.0834, matched_ious=0.4275, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.35(1.38)  Time cost: 1:14:24/14:41 [4:11:52/25:28:58]  Acc_iter 10950       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.35(1.38)
2025-09-02 01:57:32,114   INFO  Train:    3/20 ( 15%) [3275/3862 ( 85%)]  Loss: 2.124 (2.13)  LR: 3.533e-04  Grad: 32.7764  max=1.3495(module.vfe.pfn_layers.0.linear.weight)  min: -4.6758(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8215, loss_cls=0.1378, loss_bbox=1.0650, matched_ious=0.4223, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.30(1.38)  Time cost: 1:15:33/13:32 [4:13:01/25:27:40]  Acc_iter 11000       Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.30(1.38)
2025-09-02 01:58:42,129   INFO  Train:    3/20 ( 15%) [3325/3862 ( 86%)]  Loss: 1.957 (2.12)  LR: 3.554e-04  Grad: 32.5701  max=1.2351(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.5996(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8018, loss_cls=0.1345, loss_bbox=1.0757, matched_ious=0.4306, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.37(1.38)  Time cost: 1:16:43/12:23 [4:14:11/25:26:47]  Acc_iter 11050       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.37(1.38)
2025-09-02 01:59:51,007   INFO  Train:    3/20 ( 15%) [3375/3862 ( 87%)]  Loss: 2.245 (2.12)  LR: 3.574e-04  Grad: 32.6246  max=2.7315(module.vfe.pfn_layers.0.linear.weight)  min: -1.0177(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8045, loss_cls=0.1355, loss_bbox=1.0421, matched_ious=0.4285, d_time=0.00(0.01), f_time=1.39(1.38), b_time=1.39(1.38)  Time cost: 1:17:52/11:13 [4:15:20/25:25:31]  Acc_iter 11100       Data time: 0.00(0.01)  Forward time: 1.39(1.38)  Batch time: 1.39(1.38)
2025-09-02 02:01:00,023   INFO  Train:    3/20 ( 15%) [3425/3862 ( 89%)]  Loss: 1.558 (2.12)  LR: 3.595e-04  Grad: 32.5751  max=1.2424(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9551(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7891, loss_cls=0.1334, loss_bbox=1.0160, matched_ious=0.4294, d_time=0.00(0.01), f_time=1.27(1.38), b_time=1.27(1.38)  Time cost: 1:19:01/10:04 [4:16:29/25:24:19]  Acc_iter 11150       Data time: 0.00(0.01)  Forward time: 1.27(1.38)  Batch time: 1.27(1.38)
2025-09-02 02:02:08,700   INFO  Train:    3/20 ( 15%) [3475/3862 ( 90%)]  Loss: 1.896 (2.12)  LR: 3.616e-04  Grad: 32.6636  max=1.4388(module.vfe.pfn_layers.0.linear.weight)  min: -0.9604(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8125, loss_cls=0.1351, loss_bbox=1.1107, matched_ious=0.4216, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.30(1.38)  Time cost: 1:20:09/08:55 [4:17:37/25:23:00]  Acc_iter 11200       Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.30(1.38)
2025-09-02 02:03:19,392   INFO  Train:    3/20 ( 15%) [3525/3862 ( 91%)]  Loss: 2.013 (2.12)  LR: 3.636e-04  Grad: 32.7691  max=1.5185(module.vfe.pfn_layers.0.linear.weight)  min: -1.8759(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7933, loss_cls=0.1342, loss_bbox=1.0500, matched_ious=0.4275, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.37(1.38)  Time cost: 1:21:20/07:46 [4:18:48/25:22:19]  Acc_iter 11250       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.37(1.38)
2025-09-02 02:04:28,458   INFO  Train:    3/20 ( 15%) [3575/3862 ( 93%)]  Loss: 2.028 (2.11)  LR: 3.657e-04  Grad: 32.8516  max=1.2489(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.8939(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7904, loss_cls=0.1307, loss_bbox=1.0912, matched_ious=0.4261, d_time=0.00(0.01), f_time=1.40(1.38), b_time=1.40(1.38)  Time cost: 1:22:29/06:37 [4:19:57/25:21:07]  Acc_iter 11300       Data time: 0.00(0.01)  Forward time: 1.40(1.38)  Batch time: 1.40(1.38)
2025-09-02 02:05:38,572   INFO  Train:    3/20 ( 15%) [3625/3862 ( 94%)]  Loss: 1.890 (2.11)  LR: 3.678e-04  Grad: 31.4494  max=1.7147(module.vfe.pfn_layers.0.linear.weight)  min: -0.9239(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7877, loss_cls=0.1347, loss_bbox=1.0446, matched_ious=0.4280, d_time=0.00(0.01), f_time=1.28(1.38), b_time=1.28(1.38)  Time cost: 1:23:39/05:28 [4:21:07/25:20:14]  Acc_iter 11350       Data time: 0.00(0.01)  Forward time: 1.28(1.38)  Batch time: 1.28(1.38)
2025-09-02 02:06:45,955   INFO  Train:    3/20 ( 15%) [3675/3862 ( 95%)]  Loss: 2.171 (2.11)  LR: 3.699e-04  Grad: 31.5947  max=1.1997(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.9801(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7872, loss_cls=0.1324, loss_bbox=1.0319, matched_ious=0.4314, d_time=0.00(0.01), f_time=1.28(1.38), b_time=1.28(1.38)  Time cost: 1:24:46/04:18 [4:22:15/25:18:32]  Acc_iter 11400       Data time: 0.00(0.01)  Forward time: 1.28(1.38)  Batch time: 1.28(1.38)
2025-09-02 02:07:54,607   INFO  Train:    3/20 ( 15%) [3725/3862 ( 96%)]  Loss: 2.011 (2.11)  LR: 3.720e-04  Grad: 31.5770  max=1.1942(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.1366(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7989, loss_cls=0.1352, loss_bbox=1.0577, matched_ious=0.4288, d_time=0.00(0.01), f_time=1.42(1.38), b_time=1.42(1.38)  Time cost: 1:25:55/03:09 [4:23:23/25:17:14]  Acc_iter 11450       Data time: 0.00(0.01)  Forward time: 1.42(1.38)  Batch time: 1.42(1.38)
2025-09-02 02:09:03,212   INFO  Train:    3/20 ( 15%) [3775/3862 ( 98%)]  Loss: 1.588 (2.11)  LR: 3.741e-04  Grad: 31.6194  max=1.1985(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.0635(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7809, loss_cls=0.1325, loss_bbox=1.0280, matched_ious=0.4312, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.37(1.38)  Time cost: 1:27:04/02:00 [4:24:32/25:15:54]  Acc_iter 11500       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.37(1.38)
2025-09-02 02:10:11,815   INFO  Train:    3/20 ( 15%) [3825/3862 ( 99%)]  Loss: 1.938 (2.10)  LR: 3.762e-04  Grad: 31.9888  max=4.1397(module.vfe.pfn_layers.0.linear.weight)  min: -2.8037(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7668, loss_cls=0.1319, loss_bbox=0.9813, matched_ious=0.4372, d_time=0.01(0.01), f_time=1.24(1.38), b_time=1.25(1.38)  Time cost: 1:28:12/00:51 [4:25:40/25:14:35]  Acc_iter 11550       Data time: 0.01(0.01)  Forward time: 1.24(1.38)  Batch time: 1.25(1.38)
2025-09-02 02:10:59,957   INFO  Train:    3/20 ( 15%) [3861/3862 (100%)]  Loss: 1.840 (2.10)  LR: 3.778e-04  Grad: 31.6589  max=1.2045(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0964(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7860, loss_cls=0.1360, loss_bbox=0.9793, matched_ious=0.4405, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.32(1.38)  Time cost: 1:29:00/00:01 [4:26:29/25:13:17]  Acc_iter 11586       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.32(1.38)

                                               [Aepochs:  15%|█▌        | 3/20 [4:26:29<25:11:02, 5333.09s/it]epochs:  15%|█▌        | 3/20 [4:26:30<25:11:05, 5333.25s/it]epochs:  15%|█▌        | 3/20 [4:26:30<25:11:05, 5333.26s/it]epochs:  15%|█▌        | 3/20 [4:26:30<25:11:05, 5333.26s/it]epochs:  15%|█▌        | 3/20 [4:26:30<25:11:05, 5333.26s/it]epochs:  15%|█▌        | 3/20 [4:26:30<25:11:05, 5333.26s/it]epochs:  15%|█▌        | 3/20 [4:26:30<25:11:05, 5333.27s/it]epochs:  15%|█▌        | 3/20 [4:26:30<25:11:06, 5333.32s/it]
train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-02 02:11:07,957   INFO  Train:    4/20 ( 20%) [   0/3862 (  0%)]  Loss: 2.149 (2.15)  LR: 3.778e-04  Grad: 31.8313  max=1.6166(module.vfe.pfn_layers.0.linear.weight)  min: -3.0442(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8504, loss_cls=0.1260, loss_bbox=1.1731, matched_ious=0.4150, d_time=3.03(3.03), f_time=3.10(3.10), b_time=6.13(6.13)  Time cost: 00:05/6:16:25 [4:26:37/106:39:16]  Acc_iter 11587       Data time: 3.03(3.03)  Forward time: 3.10(3.10)  Batch time: 6.13(6.13)
2025-09-02 02:11:27,224   INFO  Train:    4/20 ( 20%) [  13/3862 (  0%)]  Loss: 1.897 (2.05)  LR: 3.783e-04  Grad: 31.8255  max=1.2019(module.dense_head.decoder.self_attn.in_proj_weight)  min: -3.4891(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8052, loss_cls=0.1302, loss_bbox=1.1092, matched_ious=0.4184, d_time=0.01(0.23), f_time=1.42(1.59), b_time=1.42(1.82)  Time cost: 00:25/1:55:00 [4:26:56/32:41:28]  Acc_iter 11600       Data time: 0.01(0.23)  Forward time: 1.42(1.59)  Batch time: 1.42(1.82)
2025-09-02 02:12:37,010   INFO  Train:    4/20 ( 20%) [  63/3862 (  2%)]  Loss: 2.322 (2.00)  LR: 3.805e-04  Grad: 29.7071  max=1.1254(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8804(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.8173, loss_cls=0.1378, loss_bbox=1.0339, matched_ious=0.4322, d_time=0.01(0.05), f_time=1.48(1.43), b_time=1.49(1.49)  Time cost: 01:34/1:33:53 [4:28:06/27:01:03]  Acc_iter 11650       Data time: 0.01(0.05)  Forward time: 1.48(1.43)  Batch time: 1.49(1.49)
2025-09-02 02:13:46,381   INFO  Train:    4/20 ( 20%) [ 113/3862 (  3%)]  Loss: 2.540 (1.97)  LR: 3.826e-04  Grad: 30.1207  max=1.1296(module.dense_head.decoder.self_attn.in_proj_weight)  min: -4.7169(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7682, loss_cls=0.1303, loss_bbox=1.0328, matched_ious=0.4287, d_time=0.00(0.03), f_time=1.39(1.41), b_time=1.40(1.44)  Time cost: 02:44/1:30:02 [4:29:15/26:14:05]  Acc_iter 11700       Data time: 0.00(0.03)  Forward time: 1.39(1.41)  Batch time: 1.40(1.44)
2025-09-02 02:14:55,752   INFO  Train:    4/20 ( 20%) [ 163/3862 (  4%)]  Loss: 1.968 (1.97)  LR: 3.847e-04  Grad: 29.8222  max=1.1411(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8990(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7732, loss_cls=0.1293, loss_bbox=1.0693, matched_ious=0.4248, d_time=0.01(0.02), f_time=1.34(1.40), b_time=1.35(1.43)  Time cost: 03:53/1:27:49 [4:30:24/25:55:03]  Acc_iter 11750       Data time: 0.01(0.02)  Forward time: 1.34(1.40)  Batch time: 1.35(1.43)
2025-09-02 02:16:04,912   INFO  Train:    4/20 ( 20%) [ 213/3862 (  6%)]  Loss: 1.957 (1.95)  LR: 3.868e-04  Grad: 29.9189  max=1.1423(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.6459(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7473, loss_cls=0.1303, loss_bbox=0.9890, matched_ious=0.4351, d_time=0.04(0.02), f_time=1.63(1.40), b_time=1.68(1.42)  Time cost: 05:02/1:26:03 [4:31:33/25:43:18]  Acc_iter 11800       Data time: 0.04(0.02)  Forward time: 1.63(1.40)  Batch time: 1.68(1.42)
2025-09-02 02:17:14,927   INFO  Train:    4/20 ( 20%) [ 263/3862 (  7%)]  Loss: 1.951 (1.95)  LR: 3.890e-04  Grad: 29.9603  max=1.1500(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8994(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7678, loss_cls=0.1288, loss_bbox=1.0567, matched_ious=0.4300, d_time=0.01(0.02), f_time=1.31(1.39), b_time=1.32(1.41)  Time cost: 06:12/1:24:42 [4:32:43/25:39:05]  Acc_iter 11850       Data time: 0.01(0.02)  Forward time: 1.31(1.39)  Batch time: 1.32(1.41)
2025-09-02 02:18:23,597   INFO  Train:    4/20 ( 20%) [ 313/3862 (  8%)]  Loss: 1.926 (1.95)  LR: 3.911e-04  Grad: 30.0609  max=1.1464(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.8399(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7868, loss_cls=0.1316, loss_bbox=1.0126, matched_ious=0.4301, d_time=0.00(0.02), f_time=1.39(1.39), b_time=1.39(1.41)  Time cost: 07:21/1:23:09 [4:33:52/25:31:11]  Acc_iter 11900       Data time: 0.00(0.02)  Forward time: 1.39(1.39)  Batch time: 1.39(1.41)
2025-09-02 02:19:32,379   INFO  Train:    4/20 ( 20%) [ 363/3862 (  9%)]  Loss: 2.124 (1.95)  LR: 3.933e-04  Grad: 30.4381  max=3.9899(module.vfe.pfn_layers.0.linear.weight)  min: -0.9071(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7971, loss_cls=0.1353, loss_bbox=1.0121, matched_ious=0.4318, d_time=0.01(0.02), f_time=1.35(1.39), b_time=1.36(1.40)  Time cost: 08:30/1:21:45 [4:35:01/25:25:28]  Acc_iter 11950       Data time: 0.01(0.02)  Forward time: 1.35(1.39)  Batch time: 1.36(1.40)
2025-09-02 02:20:40,461   INFO  Train:    4/20 ( 20%) [ 413/3862 ( 11%)]  Loss: 2.153 (1.95)  LR: 3.954e-04  Grad: 31.2481  max=1.1551(module.dense_head.decoder.self_attn.in_proj_weight)  min: -6.5437(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7912, loss_cls=0.1345, loss_bbox=1.0571, matched_ious=0.4293, d_time=0.00(0.02), f_time=1.23(1.38), b_time=1.23(1.40)  Time cost: 09:38/1:20:18 [4:36:09/25:19:01]  Acc_iter 12000       Data time: 0.00(0.02)  Forward time: 1.23(1.38)  Batch time: 1.23(1.40)
2025-09-02 02:21:48,612   INFO  Train:    4/20 ( 20%) [ 463/3862 ( 12%)]  Loss: 1.906 (1.95)  LR: 3.975e-04  Grad: 31.0032  max=1.1585(module.dense_head.decoder.self_attn.in_proj_weight)  min: -5.7069(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7820, loss_cls=0.1324, loss_bbox=1.0234, matched_ious=0.4346, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.32(1.39)  Time cost: 10:46/1:18:55 [4:37:17/25:13:52]  Acc_iter 12050       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.32(1.39)
2025-09-02 02:22:58,933   INFO  Train:    4/20 ( 20%) [ 513/3862 ( 13%)]  Loss: 1.582 (1.95)  LR: 3.997e-04  Grad: 30.3396  max=1.1613(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.3843(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8013, loss_cls=0.1338, loss_bbox=0.9864, matched_ious=0.4412, d_time=0.00(0.01), f_time=1.47(1.38), b_time=1.48(1.40)  Time cost: 11:56/1:17:50 [4:38:27/25:14:06]  Acc_iter 12100       Data time: 0.00(0.01)  Forward time: 1.47(1.38)  Batch time: 1.48(1.40)
2025-09-02 02:24:07,275   INFO  Train:    4/20 ( 20%) [ 563/3862 ( 15%)]  Loss: 2.172 (1.95)  LR: 4.019e-04  Grad: 30.9016  max=6.1314(module.vfe.pfn_layers.0.linear.weight)  min: -0.9139(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7843, loss_cls=0.1310, loss_bbox=1.0429, matched_ious=0.4312, d_time=0.01(0.01), f_time=1.36(1.38), b_time=1.37(1.39)  Time cost: 13:05/1:16:32 [4:39:36/25:10:16]  Acc_iter 12150       Data time: 0.01(0.01)  Forward time: 1.36(1.38)  Batch time: 1.37(1.39)
2025-09-02 02:25:14,955   INFO  Train:    4/20 ( 20%) [ 613/3862 ( 16%)]  Loss: 1.776 (1.94)  LR: 4.040e-04  Grad: 30.3014  max=1.1660(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9170(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7402, loss_cls=0.1266, loss_bbox=0.9909, matched_ious=0.4420, d_time=0.00(0.01), f_time=1.23(1.38), b_time=1.23(1.39)  Time cost: 14:12/1:15:12 [4:40:44/25:05:42]  Acc_iter 12200       Data time: 0.00(0.01)  Forward time: 1.23(1.38)  Batch time: 1.23(1.39)
2025-09-02 02:26:24,610   INFO  Train:    4/20 ( 20%) [ 663/3862 ( 17%)]  Loss: 1.894 (1.94)  LR: 4.062e-04  Grad: 30.5768  max=2.0955(module.vfe.pfn_layers.0.linear.weight)  min: -3.0685(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7929, loss_cls=0.1338, loss_bbox=1.0554, matched_ious=0.4330, d_time=0.00(0.01), f_time=2.17(1.38), b_time=2.18(1.39)  Time cost: 15:22/1:14:04 [4:41:53/25:04:53]  Acc_iter 12250       Data time: 0.00(0.01)  Forward time: 2.17(1.38)  Batch time: 2.18(1.39)
2025-09-02 02:27:33,417   INFO  Train:    4/20 ( 20%) [ 713/3862 ( 18%)]  Loss: 1.916 (1.94)  LR: 4.084e-04  Grad: 30.5020  max=1.1724(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.5735(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8035, loss_cls=0.1359, loss_bbox=0.9857, matched_ious=0.4404, d_time=0.02(0.01), f_time=1.30(1.38), b_time=1.33(1.39)  Time cost: 16:31/1:12:52 [4:43:02/25:02:43]  Acc_iter 12300       Data time: 0.02(0.01)  Forward time: 1.30(1.38)  Batch time: 1.33(1.39)
2025-09-02 02:28:43,163   INFO  Train:    4/20 ( 20%) [ 763/3862 ( 20%)]  Loss: 1.759 (1.94)  LR: 4.105e-04  Grad: 30.7027  max=1.2240(module.vfe.pfn_layers.0.linear.weight)  min: -3.8696(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7696, loss_cls=0.1307, loss_bbox=0.9934, matched_ious=0.4341, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.35(1.39)  Time cost: 17:41/1:11:43 [4:44:12/25:02:01]  Acc_iter 12350       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.35(1.39)
2025-09-02 02:29:51,894   INFO  Train:    4/20 ( 20%) [ 813/3862 ( 21%)]  Loss: 2.081 (1.94)  LR: 4.127e-04  Grad: 30.6312  max=1.7334(module.vfe.pfn_layers.0.linear.weight)  min: -1.4473(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7708, loss_cls=0.1311, loss_bbox=0.9930, matched_ious=0.4403, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.32(1.39)  Time cost: 18:49/1:10:31 [4:45:20/24:59:55]  Acc_iter 12400       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.32(1.39)
2025-09-02 02:31:01,560   INFO  Train:    4/20 ( 20%) [ 863/3862 ( 22%)]  Loss: 1.550 (1.93)  LR: 4.149e-04  Grad: 30.6983  max=2.6986(module.vfe.pfn_layers.0.linear.weight)  min: -0.9273(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7580, loss_cls=0.1291, loss_bbox=0.9892, matched_ious=0.4404, d_time=0.02(0.01), f_time=1.38(1.38), b_time=1.40(1.39)  Time cost: 19:59/1:09:23 [4:46:30/24:59:06]  Acc_iter 12450       Data time: 0.02(0.01)  Forward time: 1.38(1.38)  Batch time: 1.40(1.39)
2025-09-02 02:32:09,860   INFO  Train:    4/20 ( 20%) [ 913/3862 ( 24%)]  Loss: 2.261 (1.93)  LR: 4.171e-04  Grad: 30.5891  max=1.1823(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9348(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7918, loss_cls=0.1330, loss_bbox=1.0285, matched_ious=0.4393, d_time=0.01(0.01), f_time=1.30(1.38), b_time=1.31(1.39)  Time cost: 21:07/1:08:10 [4:47:38/24:56:38]  Acc_iter 12500       Data time: 0.01(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.39)
2025-09-02 02:33:18,890   INFO  Train:    4/20 ( 20%) [ 963/3862 ( 25%)]  Loss: 1.958 (1.93)  LR: 4.193e-04  Grad: 30.8183  max=2.4882(module.vfe.pfn_layers.0.linear.weight)  min: -1.3900(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7718, loss_cls=0.1304, loss_bbox=0.9903, matched_ious=0.4361, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 22:16/1:07:00 [4:48:47/24:55:07]  Acc_iter 12550       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 02:34:28,014   INFO  Train:    4/20 ( 20%) [1013/3862 ( 26%)]  Loss: 1.877 (1.93)  LR: 4.215e-04  Grad: 30.7978  max=1.7441(module.vfe.pfn_layers.0.linear.weight)  min: -1.9207(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7664, loss_cls=0.1288, loss_bbox=0.9863, matched_ious=0.4391, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.31(1.39)  Time cost: 23:25/1:05:50 [4:49:57/24:53:44]  Acc_iter 12600       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.31(1.39)
2025-09-02 02:35:36,331   INFO  Train:    4/20 ( 20%) [1063/3862 ( 28%)]  Loss: 1.470 (1.93)  LR: 4.236e-04  Grad: 30.7551  max=1.1849(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9443(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7755, loss_cls=0.1306, loss_bbox=0.9916, matched_ious=0.4325, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.29(1.39)  Time cost: 24:34/1:04:38 [4:51:05/24:51:34]  Acc_iter 12650       Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.29(1.39)
2025-09-02 02:36:46,057   INFO  Train:    4/20 ( 20%) [1113/3862 ( 29%)]  Loss: 1.718 (1.93)  LR: 4.258e-04  Grad: 30.8407  max=1.3067(module.vfe.pfn_layers.0.linear.weight)  min: -1.4852(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7678, loss_cls=0.1290, loss_bbox=1.0118, matched_ious=0.4362, d_time=0.00(0.01), f_time=1.27(1.38), b_time=1.27(1.39)  Time cost: 25:43/1:03:29 [4:52:15/24:50:50]  Acc_iter 12700       Data time: 0.00(0.01)  Forward time: 1.27(1.38)  Batch time: 1.27(1.39)
2025-09-02 02:37:52,959   INFO  Train:    4/20 ( 20%) [1163/3862 ( 30%)]  Loss: 2.619 (1.92)  LR: 4.280e-04  Grad: 30.8903  max=1.4679(module.vfe.pfn_layers.0.linear.weight)  min: -0.9506(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7620, loss_cls=0.1269, loss_bbox=0.9829, matched_ious=0.4400, d_time=0.01(0.01), f_time=1.35(1.37), b_time=1.36(1.38)  Time cost: 26:50/1:02:15 [4:53:22/24:47:28]  Acc_iter 12750       Data time: 0.01(0.01)  Forward time: 1.35(1.37)  Batch time: 1.36(1.38)
2025-09-02 02:39:03,266   INFO  Train:    4/20 ( 20%) [1213/3862 ( 31%)]  Loss: 2.231 (1.92)  LR: 4.302e-04  Grad: 31.2074  max=3.9530(module.vfe.pfn_layers.0.linear.weight)  min: -1.7340(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7734, loss_cls=0.1312, loss_bbox=1.0265, matched_ious=0.4407, d_time=0.01(0.01), f_time=2.18(1.37), b_time=2.19(1.39)  Time cost: 28:01/1:01:08 [4:54:32/24:47:18]  Acc_iter 12800       Data time: 0.01(0.01)  Forward time: 2.18(1.37)  Batch time: 2.19(1.39)
2025-09-02 02:40:10,958   INFO  Train:    4/20 ( 20%) [1263/3862 ( 33%)]  Loss: 1.702 (1.92)  LR: 4.325e-04  Grad: 31.1785  max=3.5766(module.vfe.pfn_layers.0.linear.weight)  min: -0.9520(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7694, loss_cls=0.1320, loss_bbox=1.0203, matched_ious=0.4338, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.33(1.38)  Time cost: 29:08/59:55 [4:55:40/24:44:50]  Acc_iter 12850       Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.33(1.38)
2025-09-02 02:41:21,163   INFO  Train:    4/20 ( 20%) [1313/3862 ( 34%)]  Loss: 1.937 (1.92)  LR: 4.347e-04  Grad: 31.3019  max=1.2092(module.dense_head.decoder.self_attn.in_proj_weight)  min: -3.2347(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7783, loss_cls=0.1305, loss_bbox=1.0283, matched_ious=0.4345, d_time=0.00(0.01), f_time=2.25(1.37), b_time=2.25(1.38)  Time cost: 30:19/58:48 [4:56:50/24:44:31]  Acc_iter 12900       Data time: 0.00(0.01)  Forward time: 2.25(1.37)  Batch time: 2.25(1.38)
2025-09-02 02:42:29,852   INFO  Train:    4/20 ( 20%) [1363/3862 ( 35%)]  Loss: 1.721 (1.93)  LR: 4.369e-04  Grad: 31.3044  max=3.9165(module.vfe.pfn_layers.0.linear.weight)  min: -0.9545(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7598, loss_cls=0.1260, loss_bbox=1.0414, matched_ious=0.4398, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.37(1.38)  Time cost: 31:27/57:38 [4:57:58/24:42:57]  Acc_iter 12950       Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.37(1.38)
2025-09-02 02:43:37,871   INFO  Train:    4/20 ( 20%) [1413/3862 ( 37%)]  Loss: 1.552 (1.92)  LR: 4.391e-04  Grad: 31.1539  max=1.2126(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.6637(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7500, loss_cls=0.1249, loss_bbox=0.9915, matched_ious=0.4357, d_time=0.01(0.01), f_time=1.29(1.37), b_time=1.30(1.38)  Time cost: 32:35/56:27 [4:59:06/24:40:54]  Acc_iter 13000       Data time: 0.01(0.01)  Forward time: 1.29(1.37)  Batch time: 1.30(1.38)
2025-09-02 02:44:46,061   INFO  Train:    4/20 ( 20%) [1463/3862 ( 38%)]  Loss: 1.774 (1.92)  LR: 4.413e-04  Grad: 31.4487  max=1.7508(module.vfe.pfn_layers.0.linear.weight)  min: -3.3183(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7813, loss_cls=0.1302, loss_bbox=1.0176, matched_ious=0.4400, d_time=0.00(0.01), f_time=1.30(1.37), b_time=1.31(1.38)  Time cost: 33:43/55:16 [5:00:15/24:39:03]  Acc_iter 13050       Data time: 0.00(0.01)  Forward time: 1.30(1.37)  Batch time: 1.31(1.38)
2025-09-02 02:45:55,493   INFO  Train:    4/20 ( 20%) [1513/3862 ( 39%)]  Loss: 1.404 (1.92)  LR: 4.435e-04  Grad: 31.3661  max=1.2145(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.4826(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7915, loss_cls=0.1339, loss_bbox=1.0181, matched_ious=0.4406, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.34(1.38)  Time cost: 34:53/54:07 [5:01:24/24:38:06]  Acc_iter 13100       Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.34(1.38)
2025-09-02 02:47:05,408   INFO  Train:    4/20 ( 20%) [1563/3862 ( 40%)]  Loss: 1.618 (1.92)  LR: 4.458e-04  Grad: 31.3024  max=1.2216(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.7643(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7594, loss_cls=0.1279, loss_bbox=0.9766, matched_ious=0.4436, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 36:03/52:59 [5:02:34/24:37:29]  Acc_iter 13150       Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 02:48:13,746   INFO  Train:    4/20 ( 20%) [1613/3862 ( 42%)]  Loss: 1.592 (1.92)  LR: 4.480e-04  Grad: 31.4135  max=1.8414(module.vfe.pfn_layers.0.linear.weight)  min: -0.9549(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7501, loss_cls=0.1266, loss_bbox=1.0424, matched_ious=0.4459, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 37:11/51:49 [5:03:42/24:35:47]  Acc_iter 13200       Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 02:49:24,268   INFO  Train:    4/20 ( 20%) [1663/3862 ( 43%)]  Loss: 1.749 (1.92)  LR: 4.502e-04  Grad: 31.5737  max=1.6850(module.vfe.pfn_layers.0.linear.weight)  min: -3.1343(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7476, loss_cls=0.1278, loss_bbox=0.9335, matched_ious=0.4463, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.34(1.38)  Time cost: 38:22/50:42 [5:04:53/24:35:32]  Acc_iter 13250       Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.34(1.38)
2025-09-02 02:50:32,059   INFO  Train:    4/20 ( 20%) [1713/3862 ( 44%)]  Loss: 1.871 (1.92)  LR: 4.524e-04  Grad: 31.4230  max=1.2220(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9602(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7958, loss_cls=0.1335, loss_bbox=0.9740, matched_ious=0.4423, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 39:29/49:31 [5:06:01/24:33:31]  Acc_iter 13300       Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 02:51:41,649   INFO  Train:    4/20 ( 20%) [1763/3862 ( 46%)]  Loss: 1.987 (1.92)  LR: 4.547e-04  Grad: 31.5697  max=1.2216(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.7311(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7654, loss_cls=0.1279, loss_bbox=0.9965, matched_ious=0.4387, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.33(1.38)  Time cost: 40:39/48:22 [5:07:10/24:32:38]  Acc_iter 13350       Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.33(1.38)
2025-09-02 02:52:51,111   INFO  Train:    4/20 ( 20%) [1813/3862 ( 47%)]  Loss: 1.972 (1.92)  LR: 4.569e-04  Grad: 31.5476  max=1.2264(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9757(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7669, loss_cls=0.1273, loss_bbox=1.0218, matched_ious=0.4397, d_time=0.00(0.01), f_time=1.40(1.37), b_time=1.41(1.38)  Time cost: 41:49/47:14 [5:08:20/24:31:40]  Acc_iter 13400       Data time: 0.00(0.01)  Forward time: 1.40(1.37)  Batch time: 1.41(1.38)
2025-09-02 02:54:00,522   INFO  Train:    4/20 ( 20%) [1863/3862 ( 48%)]  Loss: 2.134 (1.92)  LR: 4.592e-04  Grad: 31.6645  max=2.2632(module.vfe.pfn_layers.0.linear.weight)  min: -0.9787(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7446, loss_cls=0.1278, loss_bbox=1.0212, matched_ious=0.4407, d_time=0.00(0.01), f_time=1.26(1.37), b_time=1.27(1.38)  Time cost: 42:58/46:05 [5:09:29/24:30:40]  Acc_iter 13450       Data time: 0.00(0.01)  Forward time: 1.26(1.37)  Batch time: 1.27(1.38)
2025-09-02 02:55:08,984   INFO  Train:    4/20 ( 20%) [1913/3862 ( 50%)]  Loss: 1.508 (1.92)  LR: 4.614e-04  Grad: 31.6754  max=1.2281(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.5968(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7854, loss_cls=0.1312, loss_bbox=1.0089, matched_ious=0.4393, d_time=0.03(0.01), f_time=1.23(1.37), b_time=1.26(1.38)  Time cost: 44:06/44:55 [5:10:38/24:29:07]  Acc_iter 13500       Data time: 0.03(0.01)  Forward time: 1.23(1.37)  Batch time: 1.26(1.38)
2025-09-02 02:56:17,815   INFO  Train:    4/20 ( 20%) [1963/3862 ( 51%)]  Loss: 1.587 (1.92)  LR: 4.636e-04  Grad: 31.7007  max=1.2339(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9815(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7516, loss_cls=0.1293, loss_bbox=0.9743, matched_ious=0.4456, d_time=0.01(0.01), f_time=1.25(1.37), b_time=1.26(1.38)  Time cost: 45:15/43:45 [5:11:46/24:27:48]  Acc_iter 13550       Data time: 0.01(0.01)  Forward time: 1.25(1.37)  Batch time: 1.26(1.38)
2025-09-02 02:57:26,082   INFO  Train:    4/20 ( 20%) [2013/3862 ( 52%)]  Loss: 1.484 (1.91)  LR: 4.659e-04  Grad: 31.7554  max=1.3770(module.vfe.pfn_layers.0.linear.weight)  min: -0.9851(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7597, loss_cls=0.1283, loss_bbox=0.9869, matched_ious=0.4436, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.36(1.38)  Time cost: 46:23/42:35 [5:12:55/24:26:11]  Acc_iter 13600       Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.36(1.38)
2025-09-02 02:58:36,788   INFO  Train:    4/20 ( 20%) [2063/3862 ( 53%)]  Loss: 1.925 (1.91)  LR: 4.681e-04  Grad: 31.5742  max=4.2082(module.vfe.pfn_layers.0.linear.weight)  min: -2.5797(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7582, loss_cls=0.1264, loss_bbox=1.0304, matched_ious=0.4458, d_time=0.00(0.01), f_time=1.41(1.37), b_time=1.41(1.38)  Time cost: 47:34/41:28 [5:14:05/24:25:51]  Acc_iter 13650       Data time: 0.00(0.01)  Forward time: 1.41(1.37)  Batch time: 1.41(1.38)
2025-09-02 02:59:45,355   INFO  Train:    4/20 ( 20%) [2113/3862 ( 55%)]  Loss: 1.596 (1.91)  LR: 4.704e-04  Grad: 31.3429  max=2.2730(module.vfe.pfn_layers.0.linear.weight)  min: -0.9715(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7472, loss_cls=0.1259, loss_bbox=0.9726, matched_ious=0.4448, d_time=0.02(0.01), f_time=1.29(1.37), b_time=1.31(1.38)  Time cost: 48:43/40:18 [5:15:14/24:24:24]  Acc_iter 13700       Data time: 0.02(0.01)  Forward time: 1.29(1.37)  Batch time: 1.31(1.38)
2025-09-02 03:00:53,913   INFO  Train:    4/20 ( 20%) [2163/3862 ( 56%)]  Loss: 1.817 (1.91)  LR: 4.726e-04  Grad: 28.9417  max=1.1134(module.dense_head.decoder.self_attn.in_proj_weight)  min: -5.1473(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7333, loss_cls=0.1265, loss_bbox=0.9379, matched_ious=0.4494, d_time=0.01(0.01), f_time=1.33(1.37), b_time=1.34(1.38)  Time cost: 49:51/39:08 [5:16:22/24:22:58]  Acc_iter 13750       Data time: 0.01(0.01)  Forward time: 1.33(1.37)  Batch time: 1.34(1.38)
2025-09-02 03:02:02,972   INFO  Train:    4/20 ( 20%) [2213/3862 ( 57%)]  Loss: 1.991 (1.91)  LR: 4.749e-04  Grad: 28.7728  max=2.9810(module.vfe.pfn_layers.0.linear.weight)  min: -2.4912(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7608, loss_cls=0.1277, loss_bbox=0.9657, matched_ious=0.4483, d_time=0.01(0.01), f_time=1.35(1.37), b_time=1.36(1.38)  Time cost: 51:00/37:59 [5:17:32/24:21:47]  Acc_iter 13800       Data time: 0.01(0.01)  Forward time: 1.35(1.37)  Batch time: 1.36(1.38)
2025-09-02 03:03:11,460   INFO  Train:    4/20 ( 20%) [2263/3862 ( 59%)]  Loss: 2.203 (1.91)  LR: 4.772e-04  Grad: 23.2086  max=2.3417(module.vfe.pfn_layers.0.linear.weight)  min: -1.5952(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7478, loss_cls=0.1270, loss_bbox=0.9763, matched_ious=0.4452, d_time=0.01(0.01), f_time=1.40(1.37), b_time=1.41(1.38)  Time cost: 52:09/36:50 [5:18:40/24:20:20]  Acc_iter 13850       Data time: 0.01(0.01)  Forward time: 1.40(1.37)  Batch time: 1.41(1.38)
2025-09-02 03:04:21,489   INFO  Train:    4/20 ( 20%) [2313/3862 ( 60%)]  Loss: 1.848 (1.91)  LR: 4.794e-04  Grad: 23.3714  max=3.8310(module.vfe.pfn_layers.0.linear.weight)  min: -0.7216(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7403, loss_cls=0.1262, loss_bbox=0.9510, matched_ious=0.4467, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.36(1.38)  Time cost: 53:19/35:41 [5:19:50/24:19:36]  Acc_iter 13900       Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.36(1.38)
2025-09-02 03:05:28,853   INFO  Train:    4/20 ( 20%) [2363/3862 ( 61%)]  Loss: 1.767 (1.91)  LR: 4.817e-04  Grad: 23.1216  max=0.9105(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7293(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7618, loss_cls=0.1279, loss_bbox=1.0140, matched_ious=0.4341, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.33(1.38)  Time cost: 54:26/34:31 [5:20:57/24:17:39]  Acc_iter 13950       Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.33(1.38)
2025-09-02 03:06:38,521   INFO  Train:    4/20 ( 20%) [2413/3862 ( 62%)]  Loss: 1.591 (1.90)  LR: 4.839e-04  Grad: 28.6269  max=14.6392(module.vfe.pfn_layers.0.linear.weight)  min: -7.5263(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7408, loss_cls=0.1259, loss_bbox=0.9241, matched_ious=0.4523, d_time=0.00(0.01), f_time=1.26(1.37), b_time=1.26(1.38)  Time cost: 55:36/33:22 [5:22:07/24:16:46]  Acc_iter 14000       Data time: 0.00(0.01)  Forward time: 1.26(1.37)  Batch time: 1.26(1.38)
2025-09-02 03:07:47,068   INFO  Train:    4/20 ( 20%) [2463/3862 ( 64%)]  Loss: 1.721 (1.90)  LR: 4.862e-04  Grad: 23.5902  max=2.2968(module.vfe.pfn_layers.0.linear.weight)  min: -2.5627(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7562, loss_cls=0.1272, loss_bbox=0.9941, matched_ious=0.4414, d_time=0.01(0.01), f_time=1.32(1.37), b_time=1.33(1.38)  Time cost: 56:44/32:13 [5:23:16/24:15:22]  Acc_iter 14050       Data time: 0.01(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.38)
2025-09-02 03:08:55,868   INFO  Train:    4/20 ( 20%) [2513/3862 ( 65%)]  Loss: 2.004 (1.90)  LR: 4.885e-04  Grad: 23.3341  max=0.9175(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2008(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7347, loss_cls=0.1241, loss_bbox=0.9943, matched_ious=0.4431, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.32(1.38)  Time cost: 57:53/31:04 [5:24:24/24:14:06]  Acc_iter 14100       Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.32(1.38)
2025-09-02 03:10:05,216   INFO  Train:    4/20 ( 20%) [2563/3862 ( 66%)]  Loss: 1.691 (1.90)  LR: 4.907e-04  Grad: 23.3742  max=1.3218(module.vfe.pfn_layers.0.linear.weight)  min: -0.7381(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7321, loss_cls=0.1233, loss_bbox=0.9502, matched_ious=0.4506, d_time=0.00(0.01), f_time=1.47(1.37), b_time=1.48(1.38)  Time cost: 59:03/29:55 [5:25:34/24:13:03]  Acc_iter 14150       Data time: 0.00(0.01)  Forward time: 1.47(1.37)  Batch time: 1.48(1.38)
2025-09-02 03:11:15,301   INFO  Train:    4/20 ( 20%) [2613/3862 ( 68%)]  Loss: 1.616 (1.90)  LR: 4.930e-04  Grad: 23.4516  max=0.9193(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.8510(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7418, loss_cls=0.1242, loss_bbox=0.9379, matched_ious=0.4436, d_time=0.00(0.01), f_time=1.30(1.37), b_time=1.31(1.38)  Time cost: 1:00:13/28:46 [5:26:44/24:12:18]  Acc_iter 14200       Data time: 0.00(0.01)  Forward time: 1.30(1.37)  Batch time: 1.31(1.38)
2025-09-02 03:12:24,023   INFO  Train:    4/20 ( 20%) [2663/3862 ( 69%)]  Loss: 2.362 (1.90)  LR: 4.953e-04  Grad: 23.7782  max=0.9230(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.6492(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7626, loss_cls=0.1262, loss_bbox=1.0130, matched_ious=0.4409, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.36(1.38)  Time cost: 1:01:21/27:37 [5:27:53/24:10:59]  Acc_iter 14250       Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.36(1.38)
2025-09-02 03:13:33,609   INFO  Train:    4/20 ( 20%) [2713/3862 ( 70%)]  Loss: 1.993 (1.90)  LR: 4.975e-04  Grad: 23.5440  max=1.4912(module.vfe.pfn_layers.0.linear.weight)  min: -0.7466(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7378, loss_cls=0.1257, loss_bbox=0.9978, matched_ious=0.4419, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.38(1.38)  Time cost: 1:02:31/26:28 [5:29:02/24:10:02]  Acc_iter 14300       Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.38(1.38)
2025-09-02 03:14:41,823   INFO  Train:    4/20 ( 20%) [2763/3862 ( 72%)]  Loss: 1.756 (1.90)  LR: 4.998e-04  Grad: 23.6671  max=0.9331(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.7156(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7358, loss_cls=0.1229, loss_bbox=0.9919, matched_ious=0.4469, d_time=0.02(0.01), f_time=1.36(1.37), b_time=1.37(1.38)  Time cost: 1:03:39/25:18 [5:30:10/24:08:32]  Acc_iter 14350       Data time: 0.02(0.01)  Forward time: 1.36(1.37)  Batch time: 1.37(1.38)
2025-09-02 03:15:51,101   INFO  Train:    4/20 ( 20%) [2813/3862 ( 73%)]  Loss: 1.744 (1.89)  LR: 5.021e-04  Grad: 23.6540  max=0.9336(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.4334(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7365, loss_cls=0.1242, loss_bbox=0.9501, matched_ious=0.4512, d_time=0.00(0.01), f_time=1.35(1.37), b_time=1.35(1.38)  Time cost: 1:04:48/24:09 [5:31:20/24:07:27]  Acc_iter 14400       Data time: 0.00(0.01)  Forward time: 1.35(1.37)  Batch time: 1.35(1.38)
2025-09-02 03:17:00,120   INFO  Train:    4/20 ( 20%) [2863/3862 ( 74%)]  Loss: 2.222 (1.89)  LR: 5.044e-04  Grad: 23.7452  max=2.0383(module.vfe.pfn_layers.0.linear.weight)  min: -0.8526(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7290, loss_cls=0.1245, loss_bbox=0.9786, matched_ious=0.4454, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.38(1.38)  Time cost: 1:05:58/23:00 [5:32:29/24:06:16]  Acc_iter 14450       Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.38(1.38)
2025-09-02 03:18:09,859   INFO  Train:    4/20 ( 20%) [2913/3862 ( 75%)]  Loss: 2.109 (1.89)  LR: 5.066e-04  Grad: 23.7717  max=1.7083(module.vfe.pfn_layers.0.linear.weight)  min: -0.7546(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7076, loss_cls=0.1194, loss_bbox=0.9280, matched_ious=0.4559, d_time=0.00(0.01), f_time=1.27(1.37), b_time=1.27(1.38)  Time cost: 1:07:07/21:51 [5:33:38/24:05:21]  Acc_iter 14500       Data time: 0.00(0.01)  Forward time: 1.27(1.37)  Batch time: 1.27(1.38)
2025-09-02 03:19:18,116   INFO  Train:    4/20 ( 20%) [2963/3862 ( 77%)]  Loss: 1.884 (1.89)  LR: 5.089e-04  Grad: 24.0619  max=0.9429(module.dense_head.decoder.self_attn.in_proj_weight)  min: -3.7398(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7226, loss_cls=0.1214, loss_bbox=0.9428, matched_ious=0.4520, d_time=0.01(0.01), f_time=1.35(1.37), b_time=1.35(1.38)  Time cost: 1:08:16/20:42 [5:34:47/24:03:53]  Acc_iter 14550       Data time: 0.01(0.01)  Forward time: 1.35(1.37)  Batch time: 1.35(1.38)
2025-09-02 03:20:26,937   INFO  Train:    4/20 ( 20%) [3013/3862 ( 78%)]  Loss: 1.662 (1.89)  LR: 5.112e-04  Grad: 24.2586  max=3.4802(module.vfe.pfn_layers.0.linear.weight)  min: -2.9305(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7355, loss_cls=0.1195, loss_bbox=1.0106, matched_ious=0.4505, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.30(1.38)  Time cost: 1:09:24/19:33 [5:35:56/24:02:39]  Acc_iter 14600       Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.30(1.38)
2025-09-02 03:21:37,016   INFO  Train:    4/20 ( 20%) [3063/3862 ( 79%)]  Loss: 1.893 (1.89)  LR: 5.135e-04  Grad: 24.2747  max=3.4920(module.vfe.pfn_layers.0.linear.weight)  min: -2.7285(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7304, loss_cls=0.1219, loss_bbox=0.9550, matched_ious=0.4524, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.30(1.38)  Time cost: 1:10:34/18:24 [5:37:06/24:01:50]  Acc_iter 14650       Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.30(1.38)
2025-09-02 03:22:46,623   INFO  Train:    4/20 ( 20%) [3113/3862 ( 81%)]  Loss: 1.536 (1.89)  LR: 5.158e-04  Grad: 24.2711  max=3.6652(module.vfe.pfn_layers.0.linear.weight)  min: -1.7100(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7155, loss_cls=0.1208, loss_bbox=0.9334, matched_ious=0.4573, d_time=0.00(0.01), f_time=1.28(1.37), b_time=1.29(1.38)  Time cost: 1:11:44/17:15 [5:38:15/24:00:51]  Acc_iter 14700       Data time: 0.00(0.01)  Forward time: 1.28(1.37)  Batch time: 1.29(1.38)
2025-09-02 03:23:56,003   INFO  Train:    4/20 ( 20%) [3163/3862 ( 82%)]  Loss: 1.492 (1.89)  LR: 5.180e-04  Grad: 24.1777  max=1.3656(module.vfe.pfn_layers.0.linear.weight)  min: -2.7727(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7275, loss_cls=0.1206, loss_bbox=1.0005, matched_ious=0.4466, d_time=0.01(0.01), f_time=1.35(1.37), b_time=1.35(1.38)  Time cost: 1:12:53/16:06 [5:39:25/23:59:47]  Acc_iter 14750       Data time: 0.01(0.01)  Forward time: 1.35(1.37)  Batch time: 1.35(1.38)
2025-09-02 03:25:05,565   INFO  Train:    4/20 ( 20%) [3213/3862 ( 83%)]  Loss: 1.773 (1.88)  LR: 5.203e-04  Grad: 24.1609  max=2.3261(module.vfe.pfn_layers.0.linear.weight)  min: -0.7604(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7377, loss_cls=0.1236, loss_bbox=0.9344, matched_ious=0.4514, d_time=0.00(0.01), f_time=1.30(1.37), b_time=1.30(1.38)  Time cost: 1:14:03/14:57 [5:40:34/23:58:46]  Acc_iter 14800       Data time: 0.00(0.01)  Forward time: 1.30(1.37)  Batch time: 1.30(1.38)
2025-09-02 03:26:13,528   INFO  Train:    4/20 ( 20%) [3263/3862 ( 84%)]  Loss: 1.782 (1.88)  LR: 5.226e-04  Grad: 24.1492  max=1.3585(module.vfe.pfn_layers.0.linear.weight)  min: -0.7640(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7138, loss_cls=0.1188, loss_bbox=0.9401, matched_ious=0.4535, d_time=0.00(0.01), f_time=1.35(1.37), b_time=1.35(1.38)  Time cost: 1:15:11/13:47 [5:41:42/23:57:15]  Acc_iter 14850       Data time: 0.00(0.01)  Forward time: 1.35(1.37)  Batch time: 1.35(1.38)
2025-09-02 03:27:21,821   INFO  Train:    4/20 ( 20%) [3313/3862 ( 86%)]  Loss: 1.811 (1.88)  LR: 5.249e-04  Grad: 24.6198  max=2.7157(module.vfe.pfn_layers.0.linear.weight)  min: -3.7890(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7362, loss_cls=0.1236, loss_bbox=0.9283, matched_ious=0.4575, d_time=0.01(0.01), f_time=1.33(1.37), b_time=1.34(1.38)  Time cost: 1:16:19/12:38 [5:42:50/23:55:50]  Acc_iter 14900       Data time: 0.01(0.01)  Forward time: 1.33(1.37)  Batch time: 1.34(1.38)
2025-09-02 03:28:32,446   INFO  Train:    4/20 ( 20%) [3363/3862 ( 87%)]  Loss: 1.743 (1.88)  LR: 5.272e-04  Grad: 24.3105  max=2.0197(module.vfe.pfn_layers.0.linear.weight)  min: -1.3284(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7643, loss_cls=0.1253, loss_bbox=0.9601, matched_ious=0.4475, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 1:17:30/11:29 [5:44:01/23:55:10]  Acc_iter 14950       Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 03:29:39,820   INFO  Train:    4/20 ( 20%) [3413/3862 ( 88%)]  Loss: 1.988 (1.88)  LR: 5.295e-04  Grad: 25.3084  max=6.7279(module.vfe.pfn_layers.0.linear.weight)  min: -0.7664(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7331, loss_cls=0.1229, loss_bbox=0.9709, matched_ious=0.4539, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.34(1.38)  Time cost: 1:18:37/10:20 [5:45:08/23:53:29]  Acc_iter 15000       Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.34(1.38)
2025-09-02 03:30:48,841   INFO  Train:    4/20 ( 20%) [3463/3862 ( 90%)]  Loss: 1.838 (1.88)  LR: 5.317e-04  Grad: 24.3036  max=0.9756(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7759(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7202, loss_cls=0.1204, loss_bbox=0.9708, matched_ious=0.4523, d_time=0.00(0.01), f_time=1.44(1.37), b_time=1.45(1.38)  Time cost: 1:19:46/09:11 [5:46:17/23:52:18]  Acc_iter 15050       Data time: 0.00(0.01)  Forward time: 1.44(1.37)  Batch time: 1.45(1.38)
2025-09-02 03:31:57,043   INFO  Train:    4/20 ( 20%) [3513/3862 ( 91%)]  Loss: 1.825 (1.88)  LR: 5.340e-04  Grad: 24.5408  max=2.6766(module.vfe.pfn_layers.0.linear.weight)  min: -0.7738(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7338, loss_cls=0.1245, loss_bbox=0.9448, matched_ious=0.4557, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.36(1.38)  Time cost: 1:20:54/08:02 [5:47:26/23:50:53]  Acc_iter 15100       Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.36(1.38)
2025-09-02 03:33:05,660   INFO  Train:    4/20 ( 20%) [3563/3862 ( 92%)]  Loss: 1.892 (1.88)  LR: 5.363e-04  Grad: 24.4371  max=0.9773(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1683(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7115, loss_cls=0.1209, loss_bbox=0.9180, matched_ious=0.4591, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.39(1.38)  Time cost: 1:22:03/06:53 [5:48:34/23:49:36]  Acc_iter 15150       Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.39(1.38)
2025-09-02 03:34:16,355   INFO  Train:    4/20 ( 20%) [3613/3862 ( 94%)]  Loss: 1.678 (1.87)  LR: 5.386e-04  Grad: 24.5087  max=0.9764(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.4258(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7398, loss_cls=0.1229, loss_bbox=0.9379, matched_ious=0.4555, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.38(1.38)  Time cost: 1:23:14/05:44 [5:49:45/23:48:55]  Acc_iter 15200       Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.38(1.38)
2025-09-02 03:35:24,302   INFO  Train:    4/20 ( 20%) [3663/3862 ( 95%)]  Loss: 1.332 (1.87)  LR: 5.409e-04  Grad: 24.6272  max=0.9845(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.3458(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7299, loss_cls=0.1241, loss_bbox=0.9171, matched_ious=0.4586, d_time=0.00(0.01), f_time=1.22(1.37), b_time=1.23(1.38)  Time cost: 1:24:22/04:34 [5:50:53/23:47:27]  Acc_iter 15250       Data time: 0.00(0.01)  Forward time: 1.22(1.37)  Batch time: 1.23(1.38)
2025-09-02 03:36:33,851   INFO  Train:    4/20 ( 20%) [3713/3862 ( 96%)]  Loss: 1.614 (1.87)  LR: 5.432e-04  Grad: 24.6122  max=1.3537(module.vfe.pfn_layers.0.linear.weight)  min: -1.1341(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7304, loss_cls=0.1220, loss_bbox=0.9613, matched_ious=0.4590, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.38(1.38)  Time cost: 1:25:31/03:25 [5:52:02/23:46:25]  Acc_iter 15300       Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.38(1.38)
2025-09-02 03:37:43,705   INFO  Train:    4/20 ( 20%) [3763/3862 ( 97%)]  Loss: 1.696 (1.87)  LR: 5.455e-04  Grad: 24.8403  max=3.1378(module.vfe.pfn_layers.0.linear.weight)  min: -0.7962(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7083, loss_cls=0.1178, loss_bbox=0.9103, matched_ious=0.4611, d_time=0.00(0.01), f_time=1.26(1.37), b_time=1.27(1.38)  Time cost: 1:26:41/02:16 [5:53:12/23:45:29]  Acc_iter 15350       Data time: 0.00(0.01)  Forward time: 1.26(1.37)  Batch time: 1.27(1.38)
2025-09-02 03:38:52,734   INFO  Train:    4/20 ( 20%) [3813/3862 ( 99%)]  Loss: 1.575 (1.87)  LR: 5.478e-04  Grad: 25.0012  max=3.3009(module.vfe.pfn_layers.0.linear.weight)  min: -1.4849(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7213, loss_cls=0.1194, loss_bbox=0.9516, matched_ious=0.4480, d_time=0.01(0.01), f_time=1.30(1.37), b_time=1.30(1.38)  Time cost: 1:27:50/01:07 [5:54:21/23:44:19]  Acc_iter 15400       Data time: 0.01(0.01)  Forward time: 1.30(1.37)  Batch time: 1.30(1.38)
2025-09-02 03:39:58,874   INFO  Train:    4/20 ( 20%) [3861/3862 (100%)]  Loss: 1.774 (1.87)  LR: 5.500e-04  Grad: 24.8144  max=2.0124(module.vfe.pfn_layers.0.linear.weight)  min: -0.8138(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7148, loss_cls=0.1165, loss_bbox=0.9240, matched_ious=0.4580, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.37(1.38)  Time cost: 1:28:56/00:01 [5:55:27/23:43:09]  Acc_iter 15448       Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.37(1.38)

                                               [Aepochs:  20%|██        | 4/20 [5:55:28<23:42:47, 5335.46s/it]epochs:  20%|██        | 4/20 [5:55:28<23:42:48, 5335.52s/it]epochs:  20%|██        | 4/20 [5:55:28<23:42:47, 5335.48s/it]epochs:  20%|██        | 4/20 [5:55:28<23:42:47, 5335.48s/it]epochs:  20%|██        | 4/20 [5:55:28<23:42:47, 5335.48s/it]epochs:  20%|██        | 4/20 [5:55:28<23:42:47, 5335.48s/it]epochs:  20%|██        | 4/20 [5:55:28<23:42:47, 5335.48s/it]epochs:  20%|██        | 4/20 [5:55:29<23:42:48, 5335.53s/it]
train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-02 03:40:07,106   INFO  Train:    5/20 ( 25%) [   0/3862 (  0%)]  Loss: 2.062 (2.06)  LR: 5.500e-04  Grad: 24.8196  max=1.6614(module.vfe.pfn_layers.0.linear.weight)  min: -1.5037(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7975, loss_cls=0.1275, loss_bbox=1.1369, matched_ious=0.4505, d_time=2.95(2.95), f_time=3.46(3.46), b_time=6.41(6.41)  Time cost: 00:06/6:30:18 [5:55:36/104:05:03]  Acc_iter 15449       Data time: 2.95(2.95)  Forward time: 3.46(3.46)  Batch time: 6.41(6.41)
2025-09-02 03:40:08,508   INFO  Train:    5/20 ( 25%) [   1/3862 (  0%)]  Loss: 1.667 (1.86)  LR: 5.500e-04  Grad: 24.9902  max=3.6407(module.vfe.pfn_layers.0.linear.weight)  min: -0.8144(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6688, loss_cls=0.1096, loss_bbox=0.8882, matched_ious=0.4741, d_time=0.01(1.48), f_time=1.38(2.42), b_time=1.39(3.90)  Time cost: 00:07/4:00:15 [5:55:37/64:05:06]  Acc_iter 15450       Data time: 0.01(1.48)  Forward time: 1.38(2.42)  Batch time: 1.39(3.90)
2025-09-02 03:41:18,198   INFO  Train:    5/20 ( 25%) [  51/3862 (  1%)]  Loss: 1.673 (1.77)  LR: 5.523e-04  Grad: 24.9546  max=1.8131(module.vfe.pfn_layers.0.linear.weight)  min: -2.4375(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7414, loss_cls=0.1244, loss_bbox=0.9040, matched_ious=0.4565, d_time=0.00(0.06), f_time=1.45(1.43), b_time=1.45(1.49)  Time cost: 01:17/1:34:14 [5:56:47/25:26:52]  Acc_iter 15500       Data time: 0.00(0.06)  Forward time: 1.45(1.43)  Batch time: 1.45(1.49)
2025-09-02 03:42:26,877   INFO  Train:    5/20 ( 25%) [ 101/3862 (  3%)]  Loss: 1.730 (1.77)  LR: 5.546e-04  Grad: 24.8661  max=1.6037(module.vfe.pfn_layers.0.linear.weight)  min: -1.2086(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7265, loss_cls=0.1214, loss_bbox=0.9184, matched_ious=0.4661, d_time=0.01(0.04), f_time=1.39(1.40), b_time=1.41(1.43)  Time cost: 02:25/1:29:37 [5:57:55/24:30:04]  Acc_iter 15550       Data time: 0.01(0.04)  Forward time: 1.39(1.40)  Batch time: 1.41(1.43)
2025-09-02 03:43:37,564   INFO  Train:    5/20 ( 25%) [ 151/3862 (  4%)]  Loss: 2.211 (1.77)  LR: 5.569e-04  Grad: 26.0051  max=4.8396(module.vfe.pfn_layers.0.linear.weight)  min: -3.9909(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7196, loss_cls=0.1194, loss_bbox=0.9363, matched_ious=0.4560, d_time=0.00(0.03), f_time=1.29(1.40), b_time=1.30(1.43)  Time cost: 03:36/1:28:06 [5:59:06/24:23:27]  Acc_iter 15600       Data time: 0.00(0.03)  Forward time: 1.29(1.40)  Batch time: 1.30(1.43)
2025-09-02 03:44:46,812   INFO  Train:    5/20 ( 25%) [ 201/3862 (  5%)]  Loss: 1.366 (1.78)  LR: 5.592e-04  Grad: 25.5056  max=2.5235(module.vfe.pfn_layers.0.linear.weight)  min: -4.0605(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7372, loss_cls=0.1229, loss_bbox=0.9621, matched_ious=0.4540, d_time=0.01(0.02), f_time=1.36(1.39), b_time=1.37(1.42)  Time cost: 04:45/1:26:19 [6:00:15/24:12:12]  Acc_iter 15650       Data time: 0.01(0.02)  Forward time: 1.36(1.39)  Batch time: 1.37(1.42)
2025-09-02 03:45:56,920   INFO  Train:    5/20 ( 25%) [ 251/3862 (  6%)]  Loss: 1.720 (1.77)  LR: 5.615e-04  Grad: 26.2799  max=1.0236(module.dense_head.decoder.self_attn.in_proj_weight)  min: -7.6964(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6865, loss_cls=0.1167, loss_bbox=0.9224, matched_ious=0.4577, d_time=0.01(0.02), f_time=1.33(1.39), b_time=1.34(1.41)  Time cost: 05:55/1:24:59 [6:01:25/24:08:29]  Acc_iter 15700       Data time: 0.01(0.02)  Forward time: 1.33(1.39)  Batch time: 1.34(1.41)
2025-09-02 03:47:05,702   INFO  Train:    5/20 ( 25%) [ 301/3862 (  8%)]  Loss: 1.949 (1.78)  LR: 5.638e-04  Grad: 25.1094  max=1.5338(module.vfe.pfn_layers.0.linear.weight)  min: -0.8389(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7430, loss_cls=0.1220, loss_bbox=0.9581, matched_ious=0.4559, d_time=0.01(0.02), f_time=1.36(1.39), b_time=1.37(1.41)  Time cost: 07:04/1:23:27 [6:02:34/24:01:06]  Acc_iter 15750       Data time: 0.01(0.02)  Forward time: 1.36(1.39)  Batch time: 1.37(1.41)
2025-09-02 03:48:14,553   INFO  Train:    5/20 ( 25%) [ 351/3862 (  9%)]  Loss: 2.064 (1.78)  LR: 5.661e-04  Grad: 25.1299  max=1.7283(module.vfe.pfn_layers.0.linear.weight)  min: -0.8467(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7288, loss_cls=0.1205, loss_bbox=0.9439, matched_ious=0.4588, d_time=0.00(0.02), f_time=1.28(1.39), b_time=1.29(1.40)  Time cost: 08:13/1:22:02 [6:03:43/23:55:41]  Acc_iter 15800       Data time: 0.00(0.02)  Forward time: 1.28(1.39)  Batch time: 1.29(1.40)
2025-09-02 03:49:22,909   INFO  Train:    5/20 ( 25%) [ 401/3862 ( 10%)]  Loss: 1.706 (1.78)  LR: 5.683e-04  Grad: 25.1251  max=1.0364(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8452(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7338, loss_cls=0.1206, loss_bbox=0.9302, matched_ious=0.4586, d_time=0.00(0.02), f_time=1.26(1.38), b_time=1.27(1.40)  Time cost: 09:21/1:20:37 [6:04:51/23:50:05]  Acc_iter 15850       Data time: 0.00(0.02)  Forward time: 1.26(1.38)  Batch time: 1.27(1.40)
2025-09-02 03:50:33,609   INFO  Train:    5/20 ( 25%) [ 451/3862 ( 12%)]  Loss: 1.989 (1.79)  LR: 5.706e-04  Grad: 25.2705  max=1.0378(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.8152(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7460, loss_cls=0.1220, loss_bbox=0.9631, matched_ious=0.4575, d_time=0.00(0.02), f_time=1.43(1.39), b_time=1.44(1.40)  Time cost: 10:32/1:19:33 [6:06:02/23:50:46]  Acc_iter 15900       Data time: 0.00(0.02)  Forward time: 1.43(1.39)  Batch time: 1.44(1.40)
2025-09-02 03:51:43,144   INFO  Train:    5/20 ( 25%) [ 501/3862 ( 13%)]  Loss: 1.664 (1.79)  LR: 5.729e-04  Grad: 25.5848  max=3.3263(module.vfe.pfn_layers.0.linear.weight)  min: -2.3966(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7155, loss_cls=0.1206, loss_bbox=0.9283, matched_ious=0.4497, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.34(1.40)  Time cost: 11:42/1:18:20 [6:07:12/23:48:42]  Acc_iter 15950       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.40)
2025-09-02 03:52:50,361   INFO  Train:    5/20 ( 25%) [ 551/3862 ( 14%)]  Loss: 1.786 (1.78)  LR: 5.752e-04  Grad: 25.4410  max=2.5348(module.vfe.pfn_layers.0.linear.weight)  min: -0.8626(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7106, loss_cls=0.1192, loss_bbox=0.9153, matched_ious=0.4577, d_time=0.00(0.01), f_time=1.27(1.38), b_time=1.27(1.39)  Time cost: 12:49/1:16:54 [6:08:19/23:42:31]  Acc_iter 16000       Data time: 0.00(0.01)  Forward time: 1.27(1.38)  Batch time: 1.27(1.39)
2025-09-02 03:53:58,823   INFO  Train:    5/20 ( 25%) [ 601/3862 ( 16%)]  Loss: 1.783 (1.78)  LR: 5.775e-04  Grad: 25.4293  max=1.8000(module.vfe.pfn_layers.0.linear.weight)  min: -0.8641(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7181, loss_cls=0.1221, loss_bbox=0.9198, matched_ious=0.4548, d_time=0.01(0.01), f_time=1.35(1.38), b_time=1.36(1.39)  Time cost: 13:57/1:15:38 [6:09:27/23:39:17]  Acc_iter 16050       Data time: 0.01(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.39)
2025-09-02 03:55:08,305   INFO  Train:    5/20 ( 25%) [ 651/3862 ( 17%)]  Loss: 2.351 (1.78)  LR: 5.798e-04  Grad: 26.5182  max=3.0833(module.vfe.pfn_layers.0.linear.weight)  min: -6.1848(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7162, loss_cls=0.1202, loss_bbox=0.9325, matched_ious=0.4506, d_time=0.00(0.01), f_time=1.47(1.38), b_time=1.48(1.39)  Time cost: 15:07/1:14:28 [6:10:37/23:37:58]  Acc_iter 16100       Data time: 0.00(0.01)  Forward time: 1.47(1.38)  Batch time: 1.48(1.39)
2025-09-02 03:56:17,317   INFO  Train:    5/20 ( 25%) [ 701/3862 ( 18%)]  Loss: 2.165 (1.78)  LR: 5.820e-04  Grad: 25.5401  max=1.6257(module.vfe.pfn_layers.0.linear.weight)  min: -1.2736(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7215, loss_cls=0.1235, loss_bbox=0.9155, matched_ious=0.4537, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.37(1.39)  Time cost: 16:16/1:13:16 [6:11:46/23:35:59]  Acc_iter 16150       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.37(1.39)
2025-09-02 03:57:27,176   INFO  Train:    5/20 ( 25%) [ 751/3862 ( 19%)]  Loss: 1.545 (1.78)  LR: 5.843e-04  Grad: 25.5383  max=1.0393(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.5446(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7083, loss_cls=0.1187, loss_bbox=0.9147, matched_ious=0.4558, d_time=0.01(0.01), f_time=1.29(1.38), b_time=1.29(1.39)  Time cost: 17:26/1:12:07 [6:12:56/23:35:16]  Acc_iter 16200       Data time: 0.01(0.01)  Forward time: 1.29(1.38)  Batch time: 1.29(1.39)
2025-09-02 03:58:36,753   INFO  Train:    5/20 ( 25%) [ 801/3862 ( 21%)]  Loss: 1.616 (1.77)  LR: 5.866e-04  Grad: 25.7494  max=1.0492(module.dense_head.decoder.self_attn.in_proj_weight)  min: -3.1912(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7110, loss_cls=0.1183, loss_bbox=0.9231, matched_ious=0.4609, d_time=0.00(0.01), f_time=1.35(1.38), b_time=1.35(1.39)  Time cost: 18:35/1:10:58 [6:14:05/23:34:08]  Acc_iter 16250       Data time: 0.00(0.01)  Forward time: 1.35(1.38)  Batch time: 1.35(1.39)
2025-09-02 03:59:46,099   INFO  Train:    5/20 ( 25%) [ 851/3862 ( 22%)]  Loss: 2.256 (1.77)  LR: 5.889e-04  Grad: 25.9142  max=3.2980(module.vfe.pfn_layers.0.linear.weight)  min: -2.1541(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7185, loss_cls=0.1186, loss_bbox=0.8963, matched_ious=0.4628, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 19:45/1:09:48 [6:15:15/23:32:43]  Acc_iter 16300       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 04:00:55,810   INFO  Train:    5/20 ( 25%) [ 901/3862 ( 23%)]  Loss: 1.799 (1.77)  LR: 5.912e-04  Grad: 25.8605  max=1.0594(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.7664(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7260, loss_cls=0.1232, loss_bbox=0.9399, matched_ious=0.4648, d_time=0.00(0.01), f_time=1.39(1.38), b_time=1.39(1.39)  Time cost: 20:54/1:08:39 [6:16:24/23:31:45]  Acc_iter 16350       Data time: 0.00(0.01)  Forward time: 1.39(1.38)  Batch time: 1.39(1.39)
2025-09-02 04:02:03,375   INFO  Train:    5/20 ( 25%) [ 951/3862 ( 25%)]  Loss: 1.492 (1.77)  LR: 5.934e-04  Grad: 25.7002  max=1.0616(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0936(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7121, loss_cls=0.1172, loss_bbox=0.9360, matched_ious=0.4576, d_time=0.01(0.01), f_time=2.09(1.38), b_time=2.10(1.39)  Time cost: 22:02/1:07:23 [6:17:32/23:28:28]  Acc_iter 16400       Data time: 0.01(0.01)  Forward time: 2.09(1.38)  Batch time: 2.10(1.39)
2025-09-02 04:03:13,667   INFO  Train:    5/20 ( 25%) [1001/3862 ( 26%)]  Loss: 2.168 (1.77)  LR: 5.957e-04  Grad: 26.1938  max=4.5757(module.vfe.pfn_layers.0.linear.weight)  min: -0.8848(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7244, loss_cls=0.1186, loss_bbox=0.9478, matched_ious=0.4547, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.35(1.39)  Time cost: 23:12/1:06:16 [6:18:42/23:28:10]  Acc_iter 16450       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.35(1.39)
2025-09-02 04:04:21,575   INFO  Train:    5/20 ( 25%) [1051/3862 ( 27%)]  Loss: 2.108 (1.77)  LR: 5.980e-04  Grad: 25.8640  max=1.5453(module.vfe.pfn_layers.0.linear.weight)  min: -1.0683(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6917, loss_cls=0.1146, loss_bbox=0.9436, matched_ious=0.4577, d_time=0.00(0.01), f_time=1.29(1.38), b_time=1.30(1.39)  Time cost: 24:20/1:05:02 [6:19:50/23:25:29]  Acc_iter 16500       Data time: 0.00(0.01)  Forward time: 1.29(1.38)  Batch time: 1.30(1.39)
2025-09-02 04:05:29,647   INFO  Train:    5/20 ( 25%) [1101/3862 ( 29%)]  Loss: 1.687 (1.77)  LR: 6.003e-04  Grad: 25.9482  max=1.2785(module.vfe.pfn_layers.0.linear.weight)  min: -1.5920(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7054, loss_cls=0.1181, loss_bbox=0.9041, matched_ious=0.4572, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.32(1.39)  Time cost: 25:28/1:03:49 [6:20:58/23:23:05]  Acc_iter 16550       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.32(1.39)
2025-09-02 04:06:39,511   INFO  Train:    5/20 ( 25%) [1151/3862 ( 30%)]  Loss: 1.462 (1.77)  LR: 6.025e-04  Grad: 25.9336  max=1.0707(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8943(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7018, loss_cls=0.1185, loss_bbox=0.8819, matched_ious=0.4624, d_time=0.00(0.01), f_time=1.35(1.38), b_time=1.35(1.39)  Time cost: 26:38/1:02:41 [6:22:08/23:22:23]  Acc_iter 16600       Data time: 0.00(0.01)  Forward time: 1.35(1.38)  Batch time: 1.35(1.39)
2025-09-02 04:07:47,904   INFO  Train:    5/20 ( 25%) [1201/3862 ( 31%)]  Loss: 1.400 (1.77)  LR: 6.048e-04  Grad: 26.1837  max=2.3223(module.vfe.pfn_layers.0.linear.weight)  min: -2.1041(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7062, loss_cls=0.1176, loss_bbox=0.9248, matched_ious=0.4656, d_time=0.00(0.01), f_time=1.39(1.38), b_time=1.40(1.39)  Time cost: 27:46/1:01:30 [6:23:16/23:20:24]  Acc_iter 16650       Data time: 0.00(0.01)  Forward time: 1.39(1.38)  Batch time: 1.40(1.39)
2025-09-02 04:08:58,245   INFO  Train:    5/20 ( 25%) [1251/3862 ( 32%)]  Loss: 1.552 (1.76)  LR: 6.071e-04  Grad: 26.0316  max=1.0722(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9044(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6825, loss_cls=0.1153, loss_bbox=0.9260, matched_ious=0.4591, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.32(1.39)  Time cost: 28:57/1:00:22 [6:24:27/23:20:03]  Acc_iter 16700       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.32(1.39)
2025-09-02 04:10:06,217   INFO  Train:    5/20 ( 25%) [1301/3862 ( 34%)]  Loss: 1.514 (1.77)  LR: 6.094e-04  Grad: 26.1821  max=1.0747(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.0821(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7234, loss_cls=0.1198, loss_bbox=0.9500, matched_ious=0.4540, d_time=0.01(0.01), f_time=1.39(1.38), b_time=1.40(1.39)  Time cost: 30:05/59:10 [6:25:35/23:17:48]  Acc_iter 16750       Data time: 0.01(0.01)  Forward time: 1.39(1.38)  Batch time: 1.40(1.39)
2025-09-02 04:11:15,202   INFO  Train:    5/20 ( 25%) [1351/3862 ( 35%)]  Loss: 1.775 (1.77)  LR: 6.116e-04  Grad: 26.4438  max=1.0772(module.dense_head.decoder.self_attn.in_proj_weight)  min: -3.9245(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7203, loss_cls=0.1188, loss_bbox=0.9238, matched_ious=0.4606, d_time=0.00(0.01), f_time=1.38(1.38), b_time=1.39(1.39)  Time cost: 31:14/58:00 [6:26:44/23:16:24]  Acc_iter 16800       Data time: 0.00(0.01)  Forward time: 1.38(1.38)  Batch time: 1.39(1.39)
2025-09-02 04:12:23,593   INFO  Train:    5/20 ( 25%) [1401/3862 ( 36%)]  Loss: 1.347 (1.76)  LR: 6.139e-04  Grad: 26.2915  max=1.3298(module.vfe.pfn_layers.0.linear.weight)  min: -1.7667(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7070, loss_cls=0.1177, loss_bbox=0.9084, matched_ious=0.4672, d_time=0.01(0.01), f_time=1.30(1.38), b_time=1.31(1.39)  Time cost: 32:22/56:49 [6:27:52/23:14:35]  Acc_iter 16850       Data time: 0.01(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.39)
2025-09-02 04:13:32,579   INFO  Train:    5/20 ( 25%) [1451/3862 ( 38%)]  Loss: 1.377 (1.76)  LR: 6.162e-04  Grad: 25.7771  max=1.0600(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2219(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7073, loss_cls=0.1199, loss_bbox=0.9087, matched_ious=0.4623, d_time=0.00(0.01), f_time=1.23(1.38), b_time=1.23(1.39)  Time cost: 33:31/55:40 [6:29:01/23:13:13]  Acc_iter 16900       Data time: 0.00(0.01)  Forward time: 1.23(1.38)  Batch time: 1.23(1.39)
2025-09-02 04:14:42,909   INFO  Train:    5/20 ( 25%) [1501/3862 ( 39%)]  Loss: 1.857 (1.76)  LR: 6.184e-04  Grad: 26.2375  max=1.0617(module.dense_head.decoder.self_attn.in_proj_weight)  min: -3.3180(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7049, loss_cls=0.1178, loss_bbox=0.9138, matched_ious=0.4652, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.31(1.39)  Time cost: 34:41/54:32 [6:30:11/23:12:46]  Acc_iter 16950       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.31(1.39)
2025-09-02 04:15:51,868   INFO  Train:    5/20 ( 25%) [1551/3862 ( 40%)]  Loss: 1.819 (1.76)  LR: 6.207e-04  Grad: 26.0507  max=2.8616(module.vfe.pfn_layers.0.linear.weight)  min: -1.7696(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7012, loss_cls=0.1174, loss_bbox=0.8767, matched_ious=0.4665, d_time=0.00(0.01), f_time=1.43(1.38), b_time=1.44(1.39)  Time cost: 35:50/53:22 [6:31:20/23:11:24]  Acc_iter 17000       Data time: 0.00(0.01)  Forward time: 1.43(1.38)  Batch time: 1.44(1.39)
2025-09-02 04:16:59,833   INFO  Train:    5/20 ( 25%) [1601/3862 ( 41%)]  Loss: 1.615 (1.76)  LR: 6.229e-04  Grad: 26.2218  max=3.7860(module.vfe.pfn_layers.0.linear.weight)  min: -1.0747(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6819, loss_cls=0.1151, loss_bbox=0.8898, matched_ious=0.4678, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.31(1.39)  Time cost: 36:58/52:11 [6:32:28/23:09:25]  Acc_iter 17050       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.31(1.39)
2025-09-02 04:18:08,136   INFO  Train:    5/20 ( 25%) [1651/3862 ( 43%)]  Loss: 1.720 (1.76)  LR: 6.252e-04  Grad: 25.9252  max=1.0746(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9146(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6820, loss_cls=0.1128, loss_bbox=0.8919, matched_ious=0.4621, d_time=0.00(0.01), f_time=1.35(1.37), b_time=1.36(1.38)  Time cost: 38:07/51:00 [6:33:37/23:07:41]  Acc_iter 17100       Data time: 0.00(0.01)  Forward time: 1.35(1.37)  Batch time: 1.36(1.38)
2025-09-02 04:19:17,203   INFO  Train:    5/20 ( 25%) [1701/3862 ( 44%)]  Loss: 1.559 (1.76)  LR: 6.274e-04  Grad: 26.0465  max=1.2338(module.vfe.pfn_layers.0.linear.weight)  min: -1.2518(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7108, loss_cls=0.1185, loss_bbox=0.8914, matched_ious=0.4646, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.33(1.38)  Time cost: 39:16/49:51 [6:34:46/23:06:26]  Acc_iter 17150       Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.33(1.38)
2025-09-02 04:20:25,740   INFO  Train:    5/20 ( 25%) [1751/3862 ( 45%)]  Loss: 1.708 (1.75)  LR: 6.297e-04  Grad: 26.0762  max=1.1660(module.vfe.pfn_layers.0.linear.weight)  min: -0.9172(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6829, loss_cls=0.1144, loss_bbox=0.8977, matched_ious=0.4735, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.33(1.38)  Time cost: 40:24/48:41 [6:35:54/23:04:54]  Acc_iter 17200       Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.33(1.38)
2025-09-02 04:21:36,010   INFO  Train:    5/20 ( 25%) [1801/3862 ( 47%)]  Loss: 1.662 (1.75)  LR: 6.319e-04  Grad: 26.5035  max=4.4619(module.vfe.pfn_layers.0.linear.weight)  min: -0.9516(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7112, loss_cls=0.1172, loss_bbox=0.9027, matched_ious=0.4591, d_time=0.01(0.01), f_time=1.38(1.38), b_time=1.39(1.38)  Time cost: 41:34/47:33 [6:37:05/23:04:20]  Acc_iter 17250       Data time: 0.01(0.01)  Forward time: 1.38(1.38)  Batch time: 1.39(1.38)
2025-09-02 04:22:43,935   INFO  Train:    5/20 ( 25%) [1851/3862 ( 48%)]  Loss: 1.659 (1.75)  LR: 6.342e-04  Grad: 26.2102  max=1.7178(module.vfe.pfn_layers.0.linear.weight)  min: -0.9254(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7005, loss_cls=0.1160, loss_bbox=0.9069, matched_ious=0.4625, d_time=0.01(0.01), f_time=1.29(1.37), b_time=1.29(1.38)  Time cost: 42:42/46:22 [6:38:13/23:02:29]  Acc_iter 17300       Data time: 0.01(0.01)  Forward time: 1.29(1.37)  Batch time: 1.29(1.38)
2025-09-02 04:23:53,043   INFO  Train:    5/20 ( 25%) [1901/3862 ( 49%)]  Loss: 2.086 (1.75)  LR: 6.364e-04  Grad: 26.2528  max=1.6908(module.vfe.pfn_layers.0.linear.weight)  min: -0.9241(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7141, loss_cls=0.1174, loss_bbox=0.9324, matched_ious=0.4637, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 43:52/45:13 [6:39:22/23:01:17]  Acc_iter 17350       Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 04:25:00,396   INFO  Train:    5/20 ( 25%) [1951/3862 ( 51%)]  Loss: 1.725 (1.75)  LR: 6.387e-04  Grad: 20.5746  max=2.2621(module.vfe.pfn_layers.0.linear.weight)  min: -0.7193(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6927, loss_cls=0.1155, loss_bbox=0.9414, matched_ious=0.4570, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.29(1.38)  Time cost: 44:59/44:02 [6:40:29/22:59:12]  Acc_iter 17400       Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.29(1.38)
2025-09-02 04:26:09,928   INFO  Train:    5/20 ( 25%) [2001/3862 ( 52%)]  Loss: 1.773 (1.75)  LR: 6.409e-04  Grad: 20.8599  max=1.5058(module.vfe.pfn_layers.0.linear.weight)  min: -3.2346(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7157, loss_cls=0.1197, loss_bbox=0.8950, matched_ious=0.4650, d_time=0.00(0.01), f_time=1.24(1.37), b_time=1.24(1.38)  Time cost: 46:08/42:53 [6:41:38/22:58:14]  Acc_iter 17450       Data time: 0.00(0.01)  Forward time: 1.24(1.37)  Batch time: 1.24(1.38)
2025-09-02 04:27:19,315   INFO  Train:    5/20 ( 25%) [2051/3862 ( 53%)]  Loss: 1.754 (1.75)  LR: 6.432e-04  Grad: 20.7589  max=2.4272(module.vfe.pfn_layers.0.linear.weight)  min: -2.3169(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6970, loss_cls=0.1146, loss_bbox=0.8743, matched_ious=0.4633, d_time=0.01(0.01), f_time=1.38(1.37), b_time=1.40(1.38)  Time cost: 47:18/41:44 [6:42:48/22:57:12]  Acc_iter 17500       Data time: 0.01(0.01)  Forward time: 1.38(1.37)  Batch time: 1.40(1.38)
2025-09-02 04:28:28,892   INFO  Train:    5/20 ( 25%) [2101/3862 ( 54%)]  Loss: 1.259 (1.75)  LR: 6.454e-04  Grad: 20.6252  max=1.5625(module.vfe.pfn_layers.0.linear.weight)  min: -0.7298(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7002, loss_cls=0.1162, loss_bbox=0.8838, matched_ious=0.4698, d_time=0.00(0.01), f_time=1.45(1.37), b_time=1.45(1.38)  Time cost: 48:27/40:36 [6:43:57/22:56:14]  Acc_iter 17550       Data time: 0.00(0.01)  Forward time: 1.45(1.37)  Batch time: 1.45(1.38)
2025-09-02 04:29:36,385   INFO  Train:    5/20 ( 25%) [2151/3862 ( 56%)]  Loss: 1.590 (1.75)  LR: 6.476e-04  Grad: 20.7660  max=1.9887(module.vfe.pfn_layers.0.linear.weight)  min: -0.7314(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6845, loss_cls=0.1164, loss_bbox=0.8473, matched_ious=0.4770, d_time=0.00(0.01), f_time=1.44(1.37), b_time=1.44(1.38)  Time cost: 49:35/39:25 [6:45:05/22:54:19]  Acc_iter 17600       Data time: 0.00(0.01)  Forward time: 1.44(1.37)  Batch time: 1.44(1.38)
2025-09-02 04:30:45,393   INFO  Train:    5/20 ( 25%) [2201/3862 ( 57%)]  Loss: 1.716 (1.75)  LR: 6.499e-04  Grad: 20.8957  max=2.5786(module.vfe.pfn_layers.0.linear.weight)  min: -1.5235(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7087, loss_cls=0.1179, loss_bbox=0.9103, matched_ious=0.4675, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.37(1.38)  Time cost: 50:44/38:16 [6:46:14/22:53:06]  Acc_iter 17650       Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.37(1.38)
2025-09-02 04:31:55,238   INFO  Train:    5/20 ( 25%) [2251/3862 ( 58%)]  Loss: 1.582 (1.74)  LR: 6.521e-04  Grad: 20.9453  max=1.9467(module.vfe.pfn_layers.0.linear.weight)  min: -2.1324(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6797, loss_cls=0.1129, loss_bbox=0.8623, matched_ious=0.4745, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.30(1.38)  Time cost: 51:54/37:07 [6:47:24/22:52:16]  Acc_iter 17700       Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.30(1.38)
2025-09-02 04:33:04,451   INFO  Train:    5/20 ( 25%) [2301/3862 ( 60%)]  Loss: 1.948 (1.74)  LR: 6.543e-04  Grad: 20.7975  max=1.3632(module.vfe.pfn_layers.0.linear.weight)  min: -0.7464(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6900, loss_cls=0.1142, loss_bbox=0.8991, matched_ious=0.4675, d_time=0.00(0.01), f_time=2.15(1.37), b_time=2.15(1.38)  Time cost: 53:03/35:58 [6:48:33/22:51:09]  Acc_iter 17750       Data time: 0.00(0.01)  Forward time: 2.15(1.37)  Batch time: 2.15(1.38)
2025-09-02 04:34:15,225   INFO  Train:    5/20 ( 25%) [2351/3862 ( 61%)]  Loss: 1.958 (1.74)  LR: 6.566e-04  Grad: 20.8481  max=0.9554(module.vfe.pfn_layers.0.linear.weight)  min: -1.2402(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6913, loss_cls=0.1148, loss_bbox=0.9067, matched_ious=0.4623, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.31(1.38)  Time cost: 54:14/34:50 [6:49:44/22:50:41]  Acc_iter 17800       Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.31(1.38)
2025-09-02 04:35:24,530   INFO  Train:    5/20 ( 25%) [2401/3862 ( 62%)]  Loss: 1.927 (1.74)  LR: 6.588e-04  Grad: 20.9858  max=2.2758(module.vfe.pfn_layers.0.linear.weight)  min: -1.0109(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6988, loss_cls=0.1142, loss_bbox=0.9138, matched_ious=0.4703, d_time=0.01(0.01), f_time=1.40(1.37), b_time=1.41(1.38)  Time cost: 55:23/33:41 [6:50:53/22:49:35]  Acc_iter 17850       Data time: 0.01(0.01)  Forward time: 1.40(1.37)  Batch time: 1.41(1.38)
2025-09-02 04:36:33,329   INFO  Train:    5/20 ( 25%) [2451/3862 ( 63%)]  Loss: 1.551 (1.74)  LR: 6.610e-04  Grad: 21.0180  max=2.2439(module.vfe.pfn_layers.0.linear.weight)  min: -0.7487(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6760, loss_cls=0.1125, loss_bbox=0.8504, matched_ious=0.4693, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.37(1.38)  Time cost: 56:32/32:32 [6:52:02/22:48:17]  Acc_iter 17900       Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.37(1.38)
2025-09-02 04:37:41,495   INFO  Train:    5/20 ( 25%) [2501/3862 ( 65%)]  Loss: 1.747 (1.74)  LR: 6.632e-04  Grad: 21.2316  max=2.4538(module.vfe.pfn_layers.0.linear.weight)  min: -1.9647(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6830, loss_cls=0.1154, loss_bbox=0.8819, matched_ious=0.4651, d_time=0.00(0.01), f_time=1.42(1.37), b_time=1.43(1.38)  Time cost: 57:40/31:22 [6:53:10/22:46:43]  Acc_iter 17950       Data time: 0.00(0.01)  Forward time: 1.42(1.37)  Batch time: 1.43(1.38)
2025-09-02 04:38:50,779   INFO  Train:    5/20 ( 25%) [2551/3862 ( 66%)]  Loss: 1.632 (1.74)  LR: 6.654e-04  Grad: 21.0402  max=1.5869(module.vfe.pfn_layers.0.linear.weight)  min: -0.7532(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6733, loss_cls=0.1124, loss_bbox=0.8610, matched_ious=0.4633, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.37(1.38)  Time cost: 58:49/30:13 [6:54:19/22:45:37]  Acc_iter 18000       Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.37(1.38)
2025-09-02 04:40:02,339   INFO  Train:    5/20 ( 25%) [2601/3862 ( 67%)]  Loss: 1.995 (1.74)  LR: 6.676e-04  Grad: 21.0739  max=1.1672(module.vfe.pfn_layers.0.linear.weight)  min: -0.7589(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7009, loss_cls=0.1175, loss_bbox=0.8729, matched_ious=0.4704, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.35(1.38)  Time cost: 1:00:01/29:05 [6:55:31/22:45:23]  Acc_iter 18050       Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.35(1.38)
2025-09-02 04:41:11,442   INFO  Train:    5/20 ( 25%) [2651/3862 ( 69%)]  Loss: 1.676 (1.74)  LR: 6.698e-04  Grad: 21.2253  max=2.3367(module.vfe.pfn_layers.0.linear.weight)  min: -0.7687(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6826, loss_cls=0.1152, loss_bbox=0.9067, matched_ious=0.4671, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.35(1.38)  Time cost: 1:01:10/27:56 [6:56:40/22:44:11]  Acc_iter 18100       Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.35(1.38)
2025-09-02 04:42:18,908   INFO  Train:    5/20 ( 25%) [2701/3862 ( 70%)]  Loss: 1.481 (1.74)  LR: 6.720e-04  Grad: 21.4286  max=2.1713(module.vfe.pfn_layers.0.linear.weight)  min: -1.3710(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6901, loss_cls=0.1162, loss_bbox=0.8888, matched_ious=0.4684, d_time=0.01(0.01), f_time=1.30(1.37), b_time=1.31(1.38)  Time cost: 1:02:17/26:46 [6:57:47/22:42:24]  Acc_iter 18150       Data time: 0.01(0.01)  Forward time: 1.30(1.37)  Batch time: 1.31(1.38)
2025-09-02 04:43:27,596   INFO  Train:    5/20 ( 25%) [2751/3862 ( 71%)]  Loss: 1.688 (1.73)  LR: 6.742e-04  Grad: 21.7574  max=4.1555(module.vfe.pfn_layers.0.linear.weight)  min: -1.4527(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6712, loss_cls=0.1124, loss_bbox=0.9002, matched_ious=0.4686, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.33(1.38)  Time cost: 1:03:26/25:36 [6:58:56/22:41:05]  Acc_iter 18200       Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.38)
2025-09-02 04:44:37,480   INFO  Train:    5/20 ( 25%) [2801/3862 ( 73%)]  Loss: 1.604 (1.73)  LR: 6.764e-04  Grad: 21.4170  max=0.9004(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.1493(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6887, loss_cls=0.1163, loss_bbox=0.8631, matched_ious=0.4698, d_time=0.01(0.01), f_time=1.35(1.37), b_time=1.36(1.38)  Time cost: 1:04:36/24:27 [7:00:06/22:40:11]  Acc_iter 18250       Data time: 0.01(0.01)  Forward time: 1.35(1.37)  Batch time: 1.36(1.38)
2025-09-02 04:45:49,252   INFO  Train:    5/20 ( 25%) [2851/3862 ( 74%)]  Loss: 1.695 (1.73)  LR: 6.786e-04  Grad: 21.5524  max=2.5661(module.vfe.pfn_layers.0.linear.weight)  min: -0.7794(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6953, loss_cls=0.1136, loss_bbox=0.9019, matched_ious=0.4713, d_time=0.00(0.01), f_time=1.26(1.38), b_time=1.27(1.38)  Time cost: 1:05:48/23:19 [7:01:18/22:39:55]  Acc_iter 18300       Data time: 0.00(0.01)  Forward time: 1.26(1.38)  Batch time: 1.27(1.38)
2025-09-02 04:46:56,979   INFO  Train:    5/20 ( 25%) [2901/3862 ( 75%)]  Loss: 1.449 (1.73)  LR: 6.808e-04  Grad: 21.5592  max=0.9078(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2583(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6739, loss_cls=0.1145, loss_bbox=0.8822, matched_ious=0.4765, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.37(1.38)  Time cost: 1:06:55/22:09 [7:02:26/22:38:16]  Acc_iter 18350       Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.37(1.38)
2025-09-02 04:48:05,548   INFO  Train:    5/20 ( 25%) [2951/3862 ( 76%)]  Loss: 1.385 (1.73)  LR: 6.830e-04  Grad: 21.5235  max=0.9098(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.9731(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6692, loss_cls=0.1122, loss_bbox=0.8099, matched_ious=0.4775, d_time=0.00(0.01), f_time=1.44(1.37), b_time=1.44(1.38)  Time cost: 1:08:04/21:00 [7:03:34/22:36:54]  Acc_iter 18400       Data time: 0.00(0.01)  Forward time: 1.44(1.37)  Batch time: 1.44(1.38)
2025-09-02 04:49:13,610   INFO  Train:    5/20 ( 25%) [3001/3862 ( 78%)]  Loss: 1.997 (1.73)  LR: 6.852e-04  Grad: 21.6017  max=1.9701(module.vfe.pfn_layers.0.linear.weight)  min: -0.7881(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6689, loss_cls=0.1114, loss_bbox=0.9003, matched_ious=0.4750, d_time=0.01(0.01), f_time=1.27(1.37), b_time=1.28(1.38)  Time cost: 1:09:12/19:50 [7:04:42/22:35:23]  Acc_iter 18450       Data time: 0.01(0.01)  Forward time: 1.27(1.37)  Batch time: 1.28(1.38)
2025-09-02 04:50:22,422   INFO  Train:    5/20 ( 25%) [3051/3862 ( 79%)]  Loss: 1.413 (1.73)  LR: 6.874e-04  Grad: 21.5951  max=1.0036(module.vfe.pfn_layers.0.linear.weight)  min: -1.2681(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6911, loss_cls=0.1162, loss_bbox=0.8524, matched_ious=0.4687, d_time=0.01(0.01), f_time=1.41(1.37), b_time=1.42(1.38)  Time cost: 1:10:21/18:41 [7:05:51/22:34:07]  Acc_iter 18500       Data time: 0.01(0.01)  Forward time: 1.41(1.37)  Batch time: 1.42(1.38)
2025-09-02 04:51:32,944   INFO  Train:    5/20 ( 25%) [3101/3862 ( 80%)]  Loss: 1.817 (1.73)  LR: 6.896e-04  Grad: 22.1599  max=3.2989(module.vfe.pfn_layers.0.linear.weight)  min: -2.8367(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6748, loss_cls=0.1122, loss_bbox=0.8759, matched_ious=0.4685, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 1:11:31/17:32 [7:07:02/22:33:24]  Acc_iter 18550       Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 04:52:42,210   INFO  Train:    5/20 ( 25%) [3151/3862 ( 82%)]  Loss: 1.562 (1.73)  LR: 6.917e-04  Grad: 21.6619  max=0.9134(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0263(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6726, loss_cls=0.1118, loss_bbox=0.8795, matched_ious=0.4700, d_time=0.00(0.01), f_time=1.30(1.37), b_time=1.30(1.38)  Time cost: 1:12:41/16:23 [7:08:11/22:32:16]  Acc_iter 18600       Data time: 0.00(0.01)  Forward time: 1.30(1.37)  Batch time: 1.30(1.38)
2025-09-02 04:53:50,636   INFO  Train:    5/20 ( 25%) [3201/3862 ( 83%)]  Loss: 1.455 (1.72)  LR: 6.939e-04  Grad: 21.8027  max=0.9193(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.1649(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6688, loss_cls=0.1109, loss_bbox=0.8674, matched_ious=0.4675, d_time=0.00(0.01), f_time=1.27(1.37), b_time=1.27(1.38)  Time cost: 1:13:49/15:14 [7:09:19/22:30:53]  Acc_iter 18650       Data time: 0.00(0.01)  Forward time: 1.27(1.37)  Batch time: 1.27(1.38)
2025-09-02 04:54:58,995   INFO  Train:    5/20 ( 25%) [3251/3862 ( 84%)]  Loss: 1.618 (1.72)  LR: 6.961e-04  Grad: 22.0629  max=2.0927(module.vfe.pfn_layers.0.linear.weight)  min: -2.2118(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6786, loss_cls=0.1113, loss_bbox=0.8650, matched_ious=0.4701, d_time=0.00(0.01), f_time=1.28(1.37), b_time=1.29(1.38)  Time cost: 1:14:57/14:05 [7:10:28/22:29:30]  Acc_iter 18700       Data time: 0.00(0.01)  Forward time: 1.28(1.37)  Batch time: 1.29(1.38)
2025-09-02 04:56:07,765   INFO  Train:    5/20 ( 25%) [3301/3862 ( 85%)]  Loss: 1.666 (1.72)  LR: 6.982e-04  Grad: 22.1199  max=2.8148(module.vfe.pfn_layers.0.linear.weight)  min: -1.7173(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6627, loss_cls=0.1118, loss_bbox=0.8402, matched_ious=0.4789, d_time=0.37(0.01), f_time=1.29(1.37), b_time=1.66(1.38)  Time cost: 1:16:06/12:55 [7:11:36/22:28:14]  Acc_iter 18750       Data time: 0.37(0.01)  Forward time: 1.29(1.37)  Batch time: 1.66(1.38)
2025-09-02 04:57:16,948   INFO  Train:    5/20 ( 25%) [3351/3862 ( 87%)]  Loss: 1.487 (1.72)  LR: 7.004e-04  Grad: 21.9662  max=1.8925(module.vfe.pfn_layers.0.linear.weight)  min: -0.8105(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6886, loss_cls=0.1103, loss_bbox=0.9069, matched_ious=0.4688, d_time=0.00(0.01), f_time=2.13(1.37), b_time=2.14(1.38)  Time cost: 1:17:15/11:46 [7:12:46/22:27:05]  Acc_iter 18800       Data time: 0.00(0.01)  Forward time: 2.13(1.37)  Batch time: 2.14(1.38)
2025-09-02 04:58:25,504   INFO  Train:    5/20 ( 25%) [3401/3862 ( 88%)]  Loss: 1.508 (1.72)  LR: 7.025e-04  Grad: 22.0356  max=2.2237(module.vfe.pfn_layers.0.linear.weight)  min: -0.8127(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6977, loss_cls=0.1148, loss_bbox=0.8737, matched_ious=0.4685, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.32(1.38)  Time cost: 1:18:24/10:37 [7:13:54/22:25:46]  Acc_iter 18850       Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.32(1.38)
2025-09-02 04:59:34,345   INFO  Train:    5/20 ( 25%) [3451/3862 ( 89%)]  Loss: 1.760 (1.72)  LR: 7.047e-04  Grad: 22.1393  max=2.5784(module.vfe.pfn_layers.0.linear.weight)  min: -0.8155(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6884, loss_cls=0.1125, loss_bbox=0.8771, matched_ious=0.4732, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.33(1.38)  Time cost: 1:19:33/09:28 [7:15:03/22:24:31]  Acc_iter 18900       Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.38)
2025-09-02 05:00:43,172   INFO  Train:    5/20 ( 25%) [3501/3862 ( 91%)]  Loss: 1.557 (1.72)  LR: 7.068e-04  Grad: 22.2725  max=3.1564(module.vfe.pfn_layers.0.linear.weight)  min: -0.8185(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6699, loss_cls=0.1130, loss_bbox=0.8308, matched_ious=0.4780, d_time=0.01(0.01), f_time=1.33(1.37), b_time=1.34(1.38)  Time cost: 1:20:42/08:19 [7:16:12/22:23:17]  Acc_iter 18950       Data time: 0.01(0.01)  Forward time: 1.33(1.37)  Batch time: 1.34(1.38)
2025-09-02 05:01:52,102   INFO  Train:    5/20 ( 25%) [3551/3862 ( 92%)]  Loss: 1.920 (1.72)  LR: 7.090e-04  Grad: 22.0725  max=0.9397(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8208(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6924, loss_cls=0.1135, loss_bbox=0.8882, matched_ious=0.4764, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.37(1.38)  Time cost: 1:21:51/07:09 [7:17:21/22:22:05]  Acc_iter 19000       Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.37(1.38)
2025-09-02 05:03:00,579   INFO  Train:    5/20 ( 25%) [3601/3862 ( 93%)]  Loss: 1.785 (1.72)  LR: 7.111e-04  Grad: 22.2193  max=1.9893(module.vfe.pfn_layers.0.linear.weight)  min: -0.8231(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6658, loss_cls=0.1107, loss_bbox=0.8773, matched_ious=0.4722, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.36(1.38)  Time cost: 1:22:59/06:00 [7:18:29/22:20:45]  Acc_iter 19050       Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.36(1.38)
2025-09-02 05:04:11,584   INFO  Train:    5/20 ( 25%) [3651/3862 ( 95%)]  Loss: 1.855 (1.72)  LR: 7.132e-04  Grad: 22.3647  max=1.6535(module.vfe.pfn_layers.0.linear.weight)  min: -1.4587(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6690, loss_cls=0.1119, loss_bbox=0.8680, matched_ious=0.4734, d_time=0.00(0.01), f_time=2.17(1.37), b_time=2.17(1.38)  Time cost: 1:24:10/04:51 [7:19:40/22:20:06]  Acc_iter 19100       Data time: 0.00(0.01)  Forward time: 2.17(1.37)  Batch time: 2.17(1.38)
2025-09-02 05:05:20,083   INFO  Train:    5/20 ( 25%) [3701/3862 ( 96%)]  Loss: 1.803 (1.72)  LR: 7.154e-04  Grad: 22.3108  max=1.5926(module.vfe.pfn_layers.0.linear.weight)  min: -0.8329(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6771, loss_cls=0.1102, loss_bbox=0.8460, matched_ious=0.4790, d_time=0.00(0.01), f_time=1.27(1.37), b_time=1.28(1.38)  Time cost: 1:25:19/03:42 [7:20:49/22:18:46]  Acc_iter 19150       Data time: 0.00(0.01)  Forward time: 1.27(1.37)  Batch time: 1.28(1.38)
2025-09-02 05:06:29,350   INFO  Train:    5/20 ( 25%) [3751/3862 ( 97%)]  Loss: 1.733 (1.71)  LR: 7.175e-04  Grad: 22.3620  max=0.9606(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.8890(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6684, loss_cls=0.1098, loss_bbox=0.8178, matched_ious=0.4725, d_time=0.00(0.01), f_time=1.42(1.37), b_time=1.43(1.38)  Time cost: 1:26:28/02:33 [7:21:58/22:17:39]  Acc_iter 19200       Data time: 0.00(0.01)  Forward time: 1.42(1.37)  Batch time: 1.43(1.38)
2025-09-02 05:07:36,971   INFO  Train:    5/20 ( 25%) [3801/3862 ( 98%)]  Loss: 1.574 (1.71)  LR: 7.196e-04  Grad: 22.4061  max=0.9583(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.3498(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6711, loss_cls=0.1117, loss_bbox=0.8736, matched_ious=0.4722, d_time=0.01(0.01), f_time=1.28(1.37), b_time=1.29(1.38)  Time cost: 1:27:35/01:24 [7:23:06/22:16:07]  Acc_iter 19250       Data time: 0.01(0.01)  Forward time: 1.28(1.37)  Batch time: 1.29(1.38)
2025-09-02 05:08:45,815   INFO  Train:    5/20 ( 25%) [3851/3862 (100%)]  Loss: 1.369 (1.71)  LR: 7.217e-04  Grad: 22.5251  max=2.5201(module.vfe.pfn_layers.0.linear.weight)  min: -0.8404(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6628, loss_cls=0.1116, loss_bbox=0.8358, matched_ious=0.4814, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.33(1.38)  Time cost: 1:28:44/00:15 [7:24:14/22:14:54]  Acc_iter 19300       Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.33(1.38)
2025-09-02 05:08:59,809   INFO  Train:    5/20 ( 25%) [3861/3862 (100%)]  Loss: 1.361 (1.71)  LR: 7.222e-04  Grad: 22.4727  max=1.0525(module.vfe.pfn_layers.0.linear.weight)  min: -1.3780(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6581, loss_cls=0.1078, loss_bbox=0.8074, matched_ious=0.4805, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.37(1.38)  Time cost: 1:28:58/00:01 [7:24:28/22:14:42]  Acc_iter 19310       Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.37(1.38)

                                               [Aepochs:  25%|██▌       | 5/20 [7:24:29<22:14:21, 5337.45s/it]epochs:  25%|██▌       | 5/20 [7:24:29<22:14:21, 5337.47s/it]epochs:  25%|██▌       | 5/20 [7:24:29<22:14:21, 5337.46s/it]epochs:  25%|██▌       | 5/20 [7:24:29<22:14:21, 5337.46s/it]epochs:  25%|██▌       | 5/20 [7:24:29<22:14:22, 5337.49s/it]epochs:  25%|██▌       | 5/20 [7:24:29<22:14:21, 5337.46s/it]epochs:  25%|██▌       | 5/20 [7:24:29<22:14:21, 5337.46s/it]epochs:  25%|██▌       | 5/20 [7:24:30<22:14:21, 5337.47s/it]
train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-02 05:09:08,023   INFO  Train:    6/20 ( 30%) [   0/3862 (  0%)]  Loss: 1.801 (1.80)  LR: 7.222e-04  Grad: 22.5865  max=0.9653(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.1990(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7234, loss_cls=0.1316, loss_bbox=0.9465, matched_ious=0.4594, d_time=2.80(2.80), f_time=3.50(3.50), b_time=6.30(6.30)  Time cost: 00:06/6:33:32 [7:24:37/98:23:04]  Acc_iter 19311       Data time: 2.80(2.80)  Forward time: 3.50(3.50)  Batch time: 6.30(6.30)
2025-09-02 05:10:01,853   INFO  Train:    6/20 ( 30%) [  39/3862 (  1%)]  Loss: 1.798 (1.61)  LR: 7.239e-04  Grad: 22.4950  max=1.6748(module.vfe.pfn_layers.0.linear.weight)  min: -0.8429(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6699, loss_cls=0.1112, loss_bbox=0.8267, matched_ious=0.4796, d_time=0.00(0.08), f_time=1.33(1.43), b_time=1.33(1.50)  Time cost: 00:59/1:35:32 [7:25:30/24:06:39]  Acc_iter 19350       Data time: 0.00(0.08)  Forward time: 1.33(1.43)  Batch time: 1.33(1.50)
2025-09-02 05:11:11,775   INFO  Train:    6/20 ( 30%) [  89/3862 (  2%)]  Loss: 1.431 (1.66)  LR: 7.260e-04  Grad: 22.6839  max=0.9716(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.7275(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6784, loss_cls=0.1148, loss_bbox=0.9033, matched_ious=0.4674, d_time=0.01(0.04), f_time=1.35(1.40), b_time=1.35(1.45)  Time cost: 02:09/1:30:45 [7:26:40/23:11:21]  Acc_iter 19400       Data time: 0.01(0.04)  Forward time: 1.35(1.40)  Batch time: 1.35(1.45)
2025-09-02 05:12:22,069   INFO  Train:    6/20 ( 30%) [ 139/3862 (  4%)]  Loss: 1.872 (1.66)  LR: 7.281e-04  Grad: 22.5797  max=0.9707(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.6700(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6749, loss_cls=0.1136, loss_bbox=0.8729, matched_ious=0.4705, d_time=0.00(0.03), f_time=1.30(1.40), b_time=1.31(1.43)  Time cost: 03:20/1:28:43 [7:27:51/22:57:16]  Acc_iter 19450       Data time: 0.00(0.03)  Forward time: 1.30(1.40)  Batch time: 1.31(1.43)
2025-09-02 05:13:29,736   INFO  Train:    6/20 ( 30%) [ 189/3862 (  5%)]  Loss: 1.374 (1.66)  LR: 7.302e-04  Grad: 22.6009  max=0.9745(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0771(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6749, loss_cls=0.1124, loss_bbox=0.8840, matched_ious=0.4709, d_time=0.03(0.03), f_time=1.40(1.38), b_time=1.43(1.41)  Time cost: 04:27/1:26:18 [7:28:58/22:36:41]  Acc_iter 19500       Data time: 0.03(0.03)  Forward time: 1.40(1.38)  Batch time: 1.43(1.41)
2025-09-02 05:14:38,727   INFO  Train:    6/20 ( 30%) [ 239/3862 (  6%)]  Loss: 1.693 (1.67)  LR: 7.323e-04  Grad: 21.0137  max=1.3467(module.vfe.pfn_layers.0.linear.weight)  min: -1.0654(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6701, loss_cls=0.1108, loss_bbox=0.8986, matched_ious=0.4732, d_time=0.00(0.02), f_time=1.33(1.38), b_time=1.33(1.40)  Time cost: 05:36/1:24:44 [7:30:07/22:29:30]  Acc_iter 19550       Data time: 0.00(0.02)  Forward time: 1.33(1.38)  Batch time: 1.33(1.40)
2025-09-02 05:15:47,489   INFO  Train:    6/20 ( 30%) [ 289/3862 (  7%)]  Loss: 1.839 (1.65)  LR: 7.343e-04  Grad: 21.3850  max=1.0001(module.vfe.pfn_layers.0.linear.weight)  min: -3.4799(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6642, loss_cls=0.1119, loss_bbox=0.8189, matched_ious=0.4858, d_time=0.00(0.02), f_time=1.39(1.38), b_time=1.39(1.40)  Time cost: 06:45/1:23:17 [7:31:16/22:23:39]  Acc_iter 19600       Data time: 0.00(0.02)  Forward time: 1.39(1.38)  Batch time: 1.39(1.40)
2025-09-02 05:16:56,383   INFO  Train:    6/20 ( 30%) [ 339/3862 (  9%)]  Loss: 1.406 (1.65)  LR: 7.364e-04  Grad: 21.1694  max=1.4822(module.vfe.pfn_layers.0.linear.weight)  min: -1.5027(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6641, loss_cls=0.1127, loss_bbox=0.8705, matched_ious=0.4801, d_time=0.01(0.02), f_time=1.37(1.38), b_time=1.38(1.40)  Time cost: 07:54/1:21:56 [7:32:25/22:19:33]  Acc_iter 19650       Data time: 0.01(0.02)  Forward time: 1.37(1.38)  Batch time: 1.38(1.40)
2025-09-02 05:18:05,090   INFO  Train:    6/20 ( 30%) [ 389/3862 ( 10%)]  Loss: 1.518 (1.65)  LR: 7.385e-04  Grad: 21.9107  max=5.3146(module.vfe.pfn_layers.0.linear.weight)  min: -0.7985(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6675, loss_cls=0.1123, loss_bbox=0.8219, matched_ious=0.4807, d_time=0.00(0.02), f_time=1.40(1.38), b_time=1.40(1.39)  Time cost: 09:03/1:20:37 [7:33:34/22:15:45]  Acc_iter 19700       Data time: 0.00(0.02)  Forward time: 1.40(1.38)  Batch time: 1.40(1.39)
2025-09-02 05:19:15,946   INFO  Train:    6/20 ( 30%) [ 439/3862 ( 11%)]  Loss: 1.456 (1.64)  LR: 7.406e-04  Grad: 21.3022  max=2.6057(module.vfe.pfn_layers.0.linear.weight)  min: -0.8068(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6503, loss_cls=0.1099, loss_bbox=0.8294, matched_ious=0.4744, d_time=0.00(0.02), f_time=1.38(1.38), b_time=1.38(1.40)  Time cost: 10:14/1:19:37 [7:34:45/22:17:14]  Acc_iter 19750       Data time: 0.00(0.02)  Forward time: 1.38(1.38)  Batch time: 1.38(1.40)
2025-09-02 05:20:24,511   INFO  Train:    6/20 ( 30%) [ 489/3862 ( 13%)]  Loss: 1.336 (1.64)  LR: 7.427e-04  Grad: 21.1972  max=0.9233(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8400(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6612, loss_cls=0.1099, loss_bbox=0.8301, matched_ious=0.4810, d_time=0.00(0.02), f_time=1.38(1.38), b_time=1.39(1.39)  Time cost: 11:22/1:18:19 [7:35:53/22:13:42]  Acc_iter 19800       Data time: 0.00(0.02)  Forward time: 1.38(1.38)  Batch time: 1.39(1.39)
2025-09-02 05:21:34,930   INFO  Train:    6/20 ( 30%) [ 539/3862 ( 14%)]  Loss: 1.377 (1.64)  LR: 7.447e-04  Grad: 21.6490  max=3.5151(module.vfe.pfn_layers.0.linear.weight)  min: -0.8334(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6621, loss_cls=0.1112, loss_bbox=0.8947, matched_ious=0.4720, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.32(1.39)  Time cost: 12:33/1:17:14 [7:37:03/22:13:54]  Acc_iter 19850       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.32(1.39)
2025-09-02 05:22:43,733   INFO  Train:    6/20 ( 30%) [ 589/3862 ( 15%)]  Loss: 1.566 (1.64)  LR: 7.468e-04  Grad: 21.3615  max=1.3888(module.vfe.pfn_layers.0.linear.weight)  min: -0.8591(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6665, loss_cls=0.1131, loss_bbox=0.8132, matched_ious=0.4759, d_time=0.00(0.01), f_time=1.35(1.38), b_time=1.35(1.39)  Time cost: 13:41/1:15:59 [7:38:12/22:11:14]  Acc_iter 19900       Data time: 0.00(0.01)  Forward time: 1.35(1.38)  Batch time: 1.35(1.39)
2025-09-02 05:23:52,801   INFO  Train:    6/20 ( 30%) [ 639/3862 ( 17%)]  Loss: 1.840 (1.63)  LR: 7.488e-04  Grad: 21.3386  max=0.9308(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8164(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6546, loss_cls=0.1097, loss_bbox=0.8231, matched_ious=0.4757, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.34(1.39)  Time cost: 14:50/1:14:46 [7:39:21/22:09:12]  Acc_iter 19950       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.34(1.39)
2025-09-02 05:25:02,013   INFO  Train:    6/20 ( 30%) [ 689/3862 ( 18%)]  Loss: 1.552 (1.63)  LR: 7.509e-04  Grad: 21.4998  max=1.0711(module.vfe.pfn_layers.0.linear.weight)  min: -0.8244(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6706, loss_cls=0.1112, loss_bbox=0.8013, matched_ious=0.4802, d_time=0.00(0.01), f_time=1.29(1.38), b_time=1.30(1.39)  Time cost: 16:00/1:13:35 [7:40:31/22:07:30]  Acc_iter 20000       Data time: 0.00(0.01)  Forward time: 1.29(1.38)  Batch time: 1.30(1.39)
2025-09-02 05:26:11,380   INFO  Train:    6/20 ( 30%) [ 739/3862 ( 19%)]  Loss: 1.767 (1.62)  LR: 7.529e-04  Grad: 22.8286  max=1.3825(module.vfe.pfn_layers.0.linear.weight)  min: -6.3554(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6445, loss_cls=0.1075, loss_bbox=0.8315, matched_ious=0.4821, d_time=0.01(0.01), f_time=1.31(1.38), b_time=1.32(1.39)  Time cost: 17:09/1:12:24 [7:41:40/22:06:05]  Acc_iter 20050       Data time: 0.01(0.01)  Forward time: 1.31(1.38)  Batch time: 1.32(1.39)
2025-09-02 05:27:19,739   INFO  Train:    6/20 ( 30%) [ 789/3862 ( 20%)]  Loss: 1.483 (1.62)  LR: 7.550e-04  Grad: 21.5987  max=1.7633(module.vfe.pfn_layers.0.linear.weight)  min: -0.8296(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6610, loss_cls=0.1100, loss_bbox=0.8032, matched_ious=0.4794, d_time=0.02(0.01), f_time=1.48(1.38), b_time=1.50(1.39)  Time cost: 18:17/1:11:10 [7:42:48/22:03:28]  Acc_iter 20100       Data time: 0.02(0.01)  Forward time: 1.48(1.38)  Batch time: 1.50(1.39)
2025-09-02 05:28:29,882   INFO  Train:    6/20 ( 30%) [ 839/3862 ( 22%)]  Loss: 1.381 (1.62)  LR: 7.570e-04  Grad: 21.5563  max=1.1669(module.vfe.pfn_layers.0.linear.weight)  min: -0.8314(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6705, loss_cls=0.1119, loss_bbox=0.8483, matched_ious=0.4784, d_time=0.02(0.01), f_time=1.31(1.38), b_time=1.33(1.39)  Time cost: 19:28/1:10:03 [7:43:58/22:03:03]  Acc_iter 20150       Data time: 0.02(0.01)  Forward time: 1.31(1.38)  Batch time: 1.33(1.39)
2025-09-02 05:29:39,181   INFO  Train:    6/20 ( 30%) [ 889/3862 ( 23%)]  Loss: 1.615 (1.62)  LR: 7.590e-04  Grad: 21.9111  max=2.0964(module.vfe.pfn_layers.0.linear.weight)  min: -0.8319(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6649, loss_cls=0.1107, loss_bbox=0.8207, matched_ious=0.4721, d_time=0.01(0.01), f_time=1.31(1.38), b_time=1.32(1.39)  Time cost: 20:37/1:08:53 [7:45:08/22:01:39]  Acc_iter 20200       Data time: 0.01(0.01)  Forward time: 1.31(1.38)  Batch time: 1.32(1.39)
2025-09-02 05:30:49,003   INFO  Train:    6/20 ( 30%) [ 939/3862 ( 24%)]  Loss: 1.745 (1.62)  LR: 7.611e-04  Grad: 21.9125  max=2.7935(module.vfe.pfn_layers.0.linear.weight)  min: -0.9354(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6575, loss_cls=0.1104, loss_bbox=0.8750, matched_ious=0.4709, d_time=0.01(0.01), f_time=1.47(1.38), b_time=1.48(1.39)  Time cost: 21:47/1:07:44 [7:46:18/22:00:49]  Acc_iter 20250       Data time: 0.01(0.01)  Forward time: 1.47(1.38)  Batch time: 1.48(1.39)
2025-09-02 05:31:58,592   INFO  Train:    6/20 ( 30%) [ 989/3862 ( 26%)]  Loss: 1.760 (1.62)  LR: 7.631e-04  Grad: 21.7913  max=0.9487(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.5348(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6601, loss_cls=0.1110, loss_bbox=0.8311, matched_ious=0.4745, d_time=0.00(0.01), f_time=1.29(1.38), b_time=1.29(1.39)  Time cost: 22:56/1:06:35 [7:47:27/21:59:43]  Acc_iter 20300       Data time: 0.00(0.01)  Forward time: 1.29(1.38)  Batch time: 1.29(1.39)
2025-09-02 05:33:06,418   INFO  Train:    6/20 ( 30%) [1039/3862 ( 27%)]  Loss: 1.891 (1.62)  LR: 7.651e-04  Grad: 21.9573  max=1.9239(module.vfe.pfn_layers.0.linear.weight)  min: -0.8455(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6517, loss_cls=0.1083, loss_bbox=0.8133, matched_ious=0.4775, d_time=0.01(0.01), f_time=1.33(1.38), b_time=1.33(1.39)  Time cost: 24:04/1:05:21 [7:48:35/21:57:00]  Acc_iter 20350       Data time: 0.01(0.01)  Forward time: 1.33(1.38)  Batch time: 1.33(1.39)
2025-09-02 05:34:16,819   INFO  Train:    6/20 ( 30%) [1089/3862 ( 28%)]  Loss: 1.314 (1.62)  LR: 7.671e-04  Grad: 21.8818  max=1.2402(module.vfe.pfn_layers.0.linear.weight)  min: -1.5290(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6449, loss_cls=0.1074, loss_bbox=0.8386, matched_ious=0.4796, d_time=0.01(0.01), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 25:14/1:04:14 [7:49:45/21:56:40]  Acc_iter 20400       Data time: 0.01(0.01)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 05:35:25,451   INFO  Train:    6/20 ( 30%) [1139/3862 ( 29%)]  Loss: 1.341 (1.62)  LR: 7.691e-04  Grad: 22.0222  max=2.7409(module.vfe.pfn_layers.0.linear.weight)  min: -0.8520(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6793, loss_cls=0.1126, loss_bbox=0.8226, matched_ious=0.4782, d_time=0.01(0.01), f_time=1.30(1.38), b_time=1.31(1.39)  Time cost: 26:23/1:03:02 [7:50:54/21:54:48]  Acc_iter 20450       Data time: 0.01(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.39)
2025-09-02 05:36:34,596   INFO  Train:    6/20 ( 30%) [1189/3862 ( 31%)]  Loss: 1.427 (1.61)  LR: 7.711e-04  Grad: 22.0073  max=1.5957(module.vfe.pfn_layers.0.linear.weight)  min: -1.1319(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6483, loss_cls=0.1091, loss_bbox=0.8044, matched_ious=0.4798, d_time=0.00(0.01), f_time=1.43(1.38), b_time=1.43(1.39)  Time cost: 27:32/1:01:52 [7:52:03/21:53:24]  Acc_iter 20500       Data time: 0.00(0.01)  Forward time: 1.43(1.38)  Batch time: 1.43(1.39)
2025-09-02 05:37:43,399   INFO  Train:    6/20 ( 30%) [1239/3862 ( 32%)]  Loss: 1.620 (1.61)  LR: 7.731e-04  Grad: 21.9822  max=0.9607(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8541(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6674, loss_cls=0.1102, loss_bbox=0.8324, matched_ious=0.4841, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.32(1.39)  Time cost: 28:41/1:00:41 [7:53:12/21:51:45]  Acc_iter 20550       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.32(1.39)
2025-09-02 05:38:51,227   INFO  Train:    6/20 ( 30%) [1289/3862 ( 33%)]  Loss: 1.760 (1.62)  LR: 7.751e-04  Grad: 22.0846  max=1.4315(module.vfe.pfn_layers.0.linear.weight)  min: -1.3510(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6553, loss_cls=0.1087, loss_bbox=0.8670, matched_ious=0.4740, d_time=0.01(0.01), f_time=1.32(1.38), b_time=1.33(1.39)  Time cost: 29:49/59:28 [7:54:20/21:49:26]  Acc_iter 20600       Data time: 0.01(0.01)  Forward time: 1.32(1.38)  Batch time: 1.33(1.39)
2025-09-02 05:39:59,736   INFO  Train:    6/20 ( 30%) [1339/3862 ( 35%)]  Loss: 1.220 (1.61)  LR: 7.770e-04  Grad: 22.1948  max=1.5994(module.vfe.pfn_layers.0.linear.weight)  min: -1.6498(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6424, loss_cls=0.1081, loss_bbox=0.8229, matched_ious=0.4784, d_time=0.00(0.01), f_time=1.41(1.38), b_time=1.41(1.39)  Time cost: 30:57/58:18 [7:55:28/21:47:41]  Acc_iter 20650       Data time: 0.00(0.01)  Forward time: 1.41(1.38)  Batch time: 1.41(1.39)
2025-09-02 05:41:08,975   INFO  Train:    6/20 ( 30%) [1389/3862 ( 36%)]  Loss: 1.465 (1.61)  LR: 7.790e-04  Grad: 22.1957  max=1.0697(module.vfe.pfn_layers.0.linear.weight)  min: -1.2567(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6451, loss_cls=0.1079, loss_bbox=0.8518, matched_ious=0.4816, d_time=0.00(0.01), f_time=1.19(1.38), b_time=1.19(1.39)  Time cost: 32:07/57:08 [7:56:38/21:46:28]  Acc_iter 20700       Data time: 0.00(0.01)  Forward time: 1.19(1.38)  Batch time: 1.19(1.39)
2025-09-02 05:42:18,806   INFO  Train:    6/20 ( 30%) [1439/3862 ( 37%)]  Loss: 1.709 (1.61)  LR: 7.810e-04  Grad: 22.5889  max=1.5783(module.vfe.pfn_layers.0.linear.weight)  min: -3.8620(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6487, loss_cls=0.1090, loss_bbox=0.8689, matched_ious=0.4784, d_time=0.00(0.01), f_time=2.21(1.38), b_time=2.21(1.39)  Time cost: 33:16/56:00 [7:57:47/21:45:39]  Acc_iter 20750       Data time: 0.00(0.01)  Forward time: 2.21(1.38)  Batch time: 2.21(1.39)
2025-09-02 05:43:27,359   INFO  Train:    6/20 ( 30%) [1489/3862 ( 39%)]  Loss: 1.750 (1.61)  LR: 7.829e-04  Grad: 23.0393  max=1.9636(module.vfe.pfn_layers.0.linear.weight)  min: -5.2548(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6656, loss_cls=0.1115, loss_bbox=0.8351, matched_ious=0.4838, d_time=0.00(0.01), f_time=1.29(1.38), b_time=1.29(1.39)  Time cost: 34:25/54:49 [7:58:56/21:44:00]  Acc_iter 20800       Data time: 0.00(0.01)  Forward time: 1.29(1.38)  Batch time: 1.29(1.39)
2025-09-02 05:44:37,682   INFO  Train:    6/20 ( 30%) [1539/3862 ( 40%)]  Loss: 1.647 (1.61)  LR: 7.849e-04  Grad: 22.3116  max=1.4214(module.vfe.pfn_layers.0.linear.weight)  min: -0.8908(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6628, loss_cls=0.1094, loss_bbox=0.8669, matched_ious=0.4778, d_time=0.00(0.01), f_time=1.39(1.38), b_time=1.40(1.39)  Time cost: 35:35/53:41 [8:00:06/21:43:27]  Acc_iter 20850       Data time: 0.00(0.01)  Forward time: 1.39(1.38)  Batch time: 1.40(1.39)
2025-09-02 05:45:45,748   INFO  Train:    6/20 ( 30%) [1589/3862 ( 41%)]  Loss: 1.460 (1.62)  LR: 7.868e-04  Grad: 22.3155  max=0.9762(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8672(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6642, loss_cls=0.1078, loss_bbox=0.8675, matched_ious=0.4798, d_time=0.01(0.01), f_time=1.30(1.38), b_time=1.31(1.39)  Time cost: 36:43/52:30 [8:01:14/21:41:33]  Acc_iter 20900       Data time: 0.01(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.39)
2025-09-02 05:46:55,595   INFO  Train:    6/20 ( 30%) [1639/3862 ( 42%)]  Loss: 1.551 (1.61)  LR: 7.888e-04  Grad: 22.5106  max=2.0385(module.vfe.pfn_layers.0.linear.weight)  min: -1.3486(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6432, loss_cls=0.1085, loss_bbox=0.7919, matched_ious=0.4873, d_time=0.01(0.01), f_time=1.35(1.38), b_time=1.36(1.39)  Time cost: 37:53/51:21 [8:02:24/21:40:42]  Acc_iter 20950       Data time: 0.01(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.39)
2025-09-02 05:48:05,630   INFO  Train:    6/20 ( 30%) [1689/3862 ( 44%)]  Loss: 1.684 (1.61)  LR: 7.907e-04  Grad: 22.4850  max=1.4206(module.vfe.pfn_layers.0.linear.weight)  min: -0.8672(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6514, loss_cls=0.1075, loss_bbox=0.8134, matched_ious=0.4887, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.37(1.39)  Time cost: 39:03/50:13 [8:03:34/21:39:56]  Acc_iter 21000       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.37(1.39)
2025-09-02 05:49:14,999   INFO  Train:    6/20 ( 30%) [1739/3862 ( 45%)]  Loss: 1.950 (1.61)  LR: 7.927e-04  Grad: 22.5420  max=0.9911(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2872(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6405, loss_cls=0.1073, loss_bbox=0.8286, matched_ious=0.4810, d_time=0.00(0.01), f_time=1.39(1.38), b_time=1.40(1.39)  Time cost: 40:13/49:04 [8:04:44/21:38:48]  Acc_iter 21050       Data time: 0.00(0.01)  Forward time: 1.39(1.38)  Batch time: 1.40(1.39)
2025-09-02 05:50:25,264   INFO  Train:    6/20 ( 30%) [1789/3862 ( 46%)]  Loss: 1.682 (1.61)  LR: 7.946e-04  Grad: 22.6148  max=1.3626(module.vfe.pfn_layers.0.linear.weight)  min: -1.7523(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6385, loss_cls=0.1068, loss_bbox=0.8310, matched_ious=0.4815, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.34(1.39)  Time cost: 41:23/47:56 [8:05:54/21:38:08]  Acc_iter 21100       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.34(1.39)
2025-09-02 05:51:33,189   INFO  Train:    6/20 ( 30%) [1839/3862 ( 48%)]  Loss: 1.576 (1.61)  LR: 7.965e-04  Grad: 22.5969  max=0.9905(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0691(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6302, loss_cls=0.1062, loss_bbox=0.7824, matched_ious=0.4890, d_time=0.00(0.01), f_time=1.44(1.38), b_time=1.44(1.39)  Time cost: 42:31/46:45 [8:07:02/21:36:14]  Acc_iter 21150       Data time: 0.00(0.01)  Forward time: 1.44(1.38)  Batch time: 1.44(1.39)
2025-09-02 05:52:42,346   INFO  Train:    6/20 ( 30%) [1889/3862 ( 49%)]  Loss: 1.447 (1.61)  LR: 7.984e-04  Grad: 23.1310  max=3.1568(module.vfe.pfn_layers.0.linear.weight)  min: -1.1152(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6439, loss_cls=0.1072, loss_bbox=0.8018, matched_ious=0.4850, d_time=0.01(0.01), f_time=1.34(1.38), b_time=1.35(1.39)  Time cost: 43:40/45:35 [8:08:11/21:35:00]  Acc_iter 21200       Data time: 0.01(0.01)  Forward time: 1.34(1.38)  Batch time: 1.35(1.39)
2025-09-02 05:53:52,529   INFO  Train:    6/20 ( 30%) [1939/3862 ( 50%)]  Loss: 1.352 (1.60)  LR: 8.003e-04  Grad: 22.7400  max=0.9953(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8799(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6296, loss_cls=0.1070, loss_bbox=0.8050, matched_ious=0.4818, d_time=0.01(0.01), f_time=1.39(1.38), b_time=1.40(1.39)  Time cost: 44:50/44:27 [8:09:21/21:34:15]  Acc_iter 21250       Data time: 0.01(0.01)  Forward time: 1.39(1.38)  Batch time: 1.40(1.39)
2025-09-02 05:55:02,781   INFO  Train:    6/20 ( 30%) [1989/3862 ( 52%)]  Loss: 1.384 (1.60)  LR: 8.022e-04  Grad: 22.7356  max=0.9991(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8807(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6431, loss_cls=0.1076, loss_bbox=0.8193, matched_ious=0.4853, d_time=0.00(0.01), f_time=1.39(1.38), b_time=1.39(1.39)  Time cost: 46:00/43:18 [8:10:31/21:33:31]  Acc_iter 21300       Data time: 0.00(0.01)  Forward time: 1.39(1.38)  Batch time: 1.39(1.39)
2025-09-02 05:56:11,632   INFO  Train:    6/20 ( 30%) [2039/3862 ( 53%)]  Loss: 1.284 (1.60)  LR: 8.041e-04  Grad: 23.2020  max=3.8447(module.vfe.pfn_layers.0.linear.weight)  min: -0.8836(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6478, loss_cls=0.1056, loss_bbox=0.8302, matched_ious=0.4835, d_time=0.00(0.01), f_time=1.23(1.38), b_time=1.23(1.39)  Time cost: 47:09/42:08 [8:11:40/21:32:08]  Acc_iter 21350       Data time: 0.00(0.01)  Forward time: 1.23(1.38)  Batch time: 1.23(1.39)
2025-09-02 05:57:21,362   INFO  Train:    6/20 ( 30%) [2089/3862 ( 54%)]  Loss: 1.901 (1.60)  LR: 8.060e-04  Grad: 22.8599  max=1.1749(module.vfe.pfn_layers.0.linear.weight)  min: -0.8839(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6432, loss_cls=0.1054, loss_bbox=0.8128, matched_ious=0.4788, d_time=0.00(0.01), f_time=1.39(1.38), b_time=1.39(1.39)  Time cost: 48:19/40:59 [8:12:50/21:31:08]  Acc_iter 21400       Data time: 0.00(0.01)  Forward time: 1.39(1.38)  Batch time: 1.39(1.39)
2025-09-02 05:58:29,461   INFO  Train:    6/20 ( 30%) [2139/3862 ( 55%)]  Loss: 1.677 (1.60)  LR: 8.079e-04  Grad: 23.8079  max=4.4992(module.vfe.pfn_layers.0.linear.weight)  min: -3.0339(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6549, loss_cls=0.1091, loss_bbox=0.8271, matched_ious=0.4857, d_time=0.01(0.01), f_time=1.35(1.38), b_time=1.36(1.39)  Time cost: 49:27/39:49 [8:13:58/21:29:26]  Acc_iter 21450       Data time: 0.01(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.39)
2025-09-02 05:59:39,435   INFO  Train:    6/20 ( 30%) [2189/3862 ( 57%)]  Loss: 1.354 (1.60)  LR: 8.097e-04  Grad: 22.9214  max=1.0029(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8892(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6684, loss_cls=0.1131, loss_bbox=0.8276, matched_ious=0.4856, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 50:37/38:40 [8:15:08/21:28:33]  Acc_iter 21500       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 06:00:49,070   INFO  Train:    6/20 ( 30%) [2239/3862 ( 58%)]  Loss: 1.391 (1.60)  LR: 8.116e-04  Grad: 22.9601  max=1.0046(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8885(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6244, loss_cls=0.1058, loss_bbox=0.7943, matched_ious=0.4852, d_time=0.01(0.01), f_time=1.35(1.38), b_time=1.36(1.39)  Time cost: 51:47/37:31 [8:16:18/21:27:31]  Acc_iter 21550       Data time: 0.01(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.39)
2025-09-02 06:01:59,144   INFO  Train:    6/20 ( 30%) [2289/3862 ( 59%)]  Loss: 1.447 (1.60)  LR: 8.135e-04  Grad: 20.9957  max=0.9079(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.8696(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6404, loss_cls=0.1073, loss_bbox=0.8412, matched_ious=0.4788, d_time=0.00(0.01), f_time=1.26(1.38), b_time=1.26(1.39)  Time cost: 52:57/36:22 [8:17:28/21:26:39]  Acc_iter 21600       Data time: 0.00(0.01)  Forward time: 1.26(1.38)  Batch time: 1.26(1.39)
2025-09-02 06:03:08,448   INFO  Train:    6/20 ( 30%) [2339/3862 ( 61%)]  Loss: 1.610 (1.60)  LR: 8.153e-04  Grad: 20.9285  max=0.9092(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.4812(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6183, loss_cls=0.1036, loss_bbox=0.7558, matched_ious=0.4948, d_time=0.00(0.01), f_time=1.35(1.38), b_time=1.35(1.39)  Time cost: 54:06/35:13 [8:18:37/21:25:28]  Acc_iter 21650       Data time: 0.00(0.01)  Forward time: 1.35(1.38)  Batch time: 1.35(1.39)
2025-09-02 06:04:16,541   INFO  Train:    6/20 ( 30%) [2389/3862 ( 62%)]  Loss: 1.344 (1.60)  LR: 8.172e-04  Grad: 20.9600  max=1.3801(module.vfe.pfn_layers.0.linear.weight)  min: -0.8033(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6343, loss_cls=0.1049, loss_bbox=0.8082, matched_ious=0.4912, d_time=0.01(0.01), f_time=1.43(1.38), b_time=1.44(1.39)  Time cost: 55:14/34:02 [8:19:45/21:23:49]  Acc_iter 21700       Data time: 0.01(0.01)  Forward time: 1.43(1.38)  Batch time: 1.44(1.39)
2025-09-02 06:05:26,296   INFO  Train:    6/20 ( 30%) [2439/3862 ( 63%)]  Loss: 1.395 (1.59)  LR: 8.190e-04  Grad: 21.3640  max=2.5895(module.vfe.pfn_layers.0.linear.weight)  min: -2.4438(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6205, loss_cls=0.1025, loss_bbox=0.7887, matched_ious=0.4937, d_time=0.00(0.01), f_time=1.53(1.38), b_time=1.53(1.39)  Time cost: 56:24/32:53 [8:20:55/21:22:49]  Acc_iter 21750       Data time: 0.00(0.01)  Forward time: 1.53(1.38)  Batch time: 1.53(1.39)
2025-09-02 06:06:36,370   INFO  Train:    6/20 ( 30%) [2489/3862 ( 64%)]  Loss: 1.546 (1.59)  LR: 8.208e-04  Grad: 21.0233  max=0.9090(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9900(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6208, loss_cls=0.1037, loss_bbox=0.8371, matched_ious=0.4826, d_time=0.01(0.01), f_time=1.35(1.38), b_time=1.36(1.39)  Time cost: 57:34/31:44 [8:22:05/21:21:55]  Acc_iter 21800       Data time: 0.01(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.39)
2025-09-02 06:07:44,138   INFO  Train:    6/20 ( 30%) [2539/3862 ( 66%)]  Loss: 1.679 (1.59)  LR: 8.226e-04  Grad: 21.0868  max=0.9464(module.vfe.pfn_layers.0.linear.weight)  min: -0.8074(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6385, loss_cls=0.1059, loss_bbox=0.8050, matched_ious=0.4852, d_time=0.03(0.01), f_time=1.34(1.38), b_time=1.36(1.39)  Time cost: 58:42/30:34 [8:23:13/21:20:11]  Acc_iter 21850       Data time: 0.03(0.01)  Forward time: 1.34(1.38)  Batch time: 1.36(1.39)
2025-09-02 06:08:53,434   INFO  Train:    6/20 ( 30%) [2589/3862 ( 67%)]  Loss: 1.501 (1.59)  LR: 8.245e-04  Grad: 21.1022  max=0.9166(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8068(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6329, loss_cls=0.1065, loss_bbox=0.8145, matched_ious=0.4843, d_time=0.00(0.01), f_time=1.43(1.38), b_time=1.43(1.39)  Time cost: 59:51/29:25 [8:24:22/21:19:01]  Acc_iter 21900       Data time: 0.00(0.01)  Forward time: 1.43(1.38)  Batch time: 1.43(1.39)
2025-09-02 06:10:02,284   INFO  Train:    6/20 ( 30%) [2639/3862 ( 68%)]  Loss: 1.372 (1.59)  LR: 8.263e-04  Grad: 21.5370  max=3.1877(module.vfe.pfn_layers.0.linear.weight)  min: -0.8303(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6308, loss_cls=0.1052, loss_bbox=0.8182, matched_ious=0.4873, d_time=0.01(0.01), f_time=1.30(1.38), b_time=1.31(1.39)  Time cost: 1:01:00/28:15 [8:25:31/21:17:41]  Acc_iter 21950       Data time: 0.01(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.39)
2025-09-02 06:11:11,199   INFO  Train:    6/20 ( 30%) [2689/3862 ( 70%)]  Loss: 1.629 (1.59)  LR: 8.281e-04  Grad: 21.4364  max=2.0536(module.vfe.pfn_layers.0.linear.weight)  min: -1.9889(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6445, loss_cls=0.1087, loss_bbox=0.8030, matched_ious=0.4854, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 1:02:09/27:06 [8:26:40/21:16:24]  Acc_iter 22000       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 06:12:20,388   INFO  Train:    6/20 ( 30%) [2739/3862 ( 71%)]  Loss: 1.436 (1.59)  LR: 8.299e-04  Grad: 21.2572  max=0.9223(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8071(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6340, loss_cls=0.1063, loss_bbox=0.8263, matched_ious=0.4849, d_time=0.00(0.01), f_time=1.27(1.38), b_time=1.28(1.39)  Time cost: 1:03:18/25:56 [8:27:49/21:15:12]  Acc_iter 22050       Data time: 0.00(0.01)  Forward time: 1.27(1.38)  Batch time: 1.28(1.39)
2025-09-02 06:13:29,999   INFO  Train:    6/20 ( 30%) [2789/3862 ( 72%)]  Loss: 1.162 (1.59)  LR: 8.317e-04  Grad: 21.3632  max=0.9256(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2626(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6521, loss_cls=0.1076, loss_bbox=0.8257, matched_ious=0.4836, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.31(1.39)  Time cost: 1:04:28/24:47 [8:28:59/21:14:08]  Acc_iter 22100       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.31(1.39)
2025-09-02 06:14:39,353   INFO  Train:    6/20 ( 30%) [2839/3862 ( 74%)]  Loss: 1.479 (1.59)  LR: 8.334e-04  Grad: 22.4188  max=1.7768(module.vfe.pfn_layers.0.linear.weight)  min: -5.9297(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6464, loss_cls=0.1067, loss_bbox=0.8066, matched_ious=0.4865, d_time=0.00(0.01), f_time=1.44(1.38), b_time=1.44(1.39)  Time cost: 1:05:37/23:38 [8:30:08/21:12:59]  Acc_iter 22150       Data time: 0.00(0.01)  Forward time: 1.44(1.38)  Batch time: 1.44(1.39)
2025-09-02 06:15:48,563   INFO  Train:    6/20 ( 30%) [2889/3862 ( 75%)]  Loss: 1.456 (1.59)  LR: 8.352e-04  Grad: 21.4253  max=0.9230(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8084(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6301, loss_cls=0.1062, loss_bbox=0.7865, matched_ious=0.4823, d_time=0.00(0.01), f_time=1.39(1.38), b_time=1.40(1.39)  Time cost: 1:06:46/22:28 [8:31:17/21:11:48]  Acc_iter 22200       Data time: 0.00(0.01)  Forward time: 1.39(1.38)  Batch time: 1.40(1.39)
2025-09-02 06:16:56,703   INFO  Train:    6/20 ( 30%) [2939/3862 ( 76%)]  Loss: 1.331 (1.59)  LR: 8.370e-04  Grad: 21.5754  max=1.2248(module.vfe.pfn_layers.0.linear.weight)  min: -0.8190(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6353, loss_cls=0.1058, loss_bbox=0.8229, matched_ious=0.4885, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 1:07:54/21:19 [8:32:25/21:10:17]  Acc_iter 22250       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 06:18:06,386   INFO  Train:    6/20 ( 30%) [2989/3862 ( 77%)]  Loss: 1.202 (1.59)  LR: 8.387e-04  Grad: 21.5709  max=0.9289(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8320(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6348, loss_cls=0.1049, loss_bbox=0.7864, matched_ious=0.4845, d_time=0.00(0.01), f_time=1.41(1.38), b_time=1.41(1.39)  Time cost: 1:09:04/20:10 [8:33:35/21:09:14]  Acc_iter 22300       Data time: 0.00(0.01)  Forward time: 1.41(1.38)  Batch time: 1.41(1.39)
2025-09-02 06:19:15,066   INFO  Train:    6/20 ( 30%) [3039/3862 ( 79%)]  Loss: 1.214 (1.59)  LR: 8.405e-04  Grad: 21.8592  max=0.9276(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.5722(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6270, loss_cls=0.1032, loss_bbox=0.7882, matched_ious=0.4909, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.32(1.39)  Time cost: 1:10:13/19:00 [8:34:44/21:07:54]  Acc_iter 22350       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.32(1.39)
2025-09-02 06:20:24,512   INFO  Train:    6/20 ( 30%) [3089/3862 ( 80%)]  Loss: 1.470 (1.58)  LR: 8.422e-04  Grad: 16.2022  max=2.6757(module.vfe.pfn_layers.0.linear.weight)  min: -0.7749(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6245, loss_cls=0.1034, loss_bbox=0.7848, matched_ious=0.4902, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.31(1.39)  Time cost: 1:11:22/17:51 [8:35:53/21:06:47]  Acc_iter 22400       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.31(1.39)
2025-09-02 06:21:33,173   INFO  Train:    6/20 ( 30%) [3139/3862 ( 81%)]  Loss: 1.292 (1.58)  LR: 8.440e-04  Grad: 15.8000  max=1.2339(module.vfe.pfn_layers.0.linear.weight)  min: -0.6076(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6417, loss_cls=0.1079, loss_bbox=0.7872, matched_ious=0.4865, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 1:12:31/16:41 [8:37:02/21:05:27]  Acc_iter 22450       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 06:22:41,453   INFO  Train:    6/20 ( 30%) [3189/3862 ( 83%)]  Loss: 1.557 (1.58)  LR: 8.457e-04  Grad: 15.9834  max=1.6869(module.vfe.pfn_layers.0.linear.weight)  min: -1.5494(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6160, loss_cls=0.1021, loss_bbox=0.7935, matched_ious=0.4927, d_time=0.00(0.01), f_time=1.40(1.38), b_time=1.40(1.39)  Time cost: 1:13:39/15:32 [8:38:10/21:04:00]  Acc_iter 22500       Data time: 0.00(0.01)  Forward time: 1.40(1.38)  Batch time: 1.40(1.39)
2025-09-02 06:23:50,094   INFO  Train:    6/20 ( 30%) [3239/3862 ( 84%)]  Loss: 1.716 (1.58)  LR: 8.474e-04  Grad: 16.7152  max=1.7991(module.vfe.pfn_layers.0.linear.weight)  min: -4.1934(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6254, loss_cls=0.1041, loss_bbox=0.7815, matched_ious=0.4873, d_time=0.00(0.01), f_time=1.42(1.38), b_time=1.42(1.39)  Time cost: 1:14:48/14:23 [8:39:19/21:02:40]  Acc_iter 22550       Data time: 0.00(0.01)  Forward time: 1.42(1.38)  Batch time: 1.42(1.39)
2025-09-02 06:24:58,982   INFO  Train:    6/20 ( 30%) [3289/3862 ( 85%)]  Loss: 1.412 (1.58)  LR: 8.491e-04  Grad: 15.9056  max=0.8266(module.vfe.pfn_layers.0.linear.weight)  min: -0.6197(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6284, loss_cls=0.1032, loss_bbox=0.8082, matched_ious=0.4862, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.30(1.39)  Time cost: 1:15:57/13:13 [8:40:28/21:01:25]  Acc_iter 22600       Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.30(1.39)
2025-09-02 06:26:09,216   INFO  Train:    6/20 ( 30%) [3339/3862 ( 86%)]  Loss: 1.444 (1.58)  LR: 8.508e-04  Grad: 16.7441  max=1.9191(module.vfe.pfn_layers.0.linear.weight)  min: -3.9753(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6335, loss_cls=0.1053, loss_bbox=0.8011, matched_ious=0.4896, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.37(1.39)  Time cost: 1:17:07/12:04 [8:41:38/21:00:32]  Acc_iter 22650       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.37(1.39)
2025-09-02 06:27:18,353   INFO  Train:    6/20 ( 30%) [3389/3862 ( 88%)]  Loss: 1.272 (1.58)  LR: 8.525e-04  Grad: 15.9712  max=0.6863(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6222(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6257, loss_cls=0.1049, loss_bbox=0.7889, matched_ious=0.4857, d_time=0.00(0.01), f_time=1.45(1.38), b_time=1.45(1.39)  Time cost: 1:18:16/10:55 [8:42:47/20:59:20]  Acc_iter 22700       Data time: 0.00(0.01)  Forward time: 1.45(1.38)  Batch time: 1.45(1.39)
2025-09-02 06:28:26,784   INFO  Train:    6/20 ( 30%) [3439/3862 ( 89%)]  Loss: 1.489 (1.58)  LR: 8.542e-04  Grad: 16.1259  max=1.4860(module.vfe.pfn_layers.0.linear.weight)  min: -0.7124(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6296, loss_cls=0.1068, loss_bbox=0.7835, matched_ious=0.4929, d_time=0.00(0.01), f_time=1.35(1.38), b_time=1.35(1.39)  Time cost: 1:19:24/09:45 [8:43:55/20:57:58]  Acc_iter 22750       Data time: 0.00(0.01)  Forward time: 1.35(1.38)  Batch time: 1.35(1.39)
2025-09-02 06:29:36,738   INFO  Train:    6/20 ( 30%) [3489/3862 ( 90%)]  Loss: 1.364 (1.58)  LR: 8.559e-04  Grad: 16.2689  max=1.3916(module.vfe.pfn_layers.0.linear.weight)  min: -1.1853(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6435, loss_cls=0.1090, loss_bbox=0.7986, matched_ious=0.4927, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.37(1.39)  Time cost: 1:20:34/08:36 [8:45:05/20:56:59]  Acc_iter 22800       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.37(1.39)
2025-09-02 06:30:44,266   INFO  Train:    6/20 ( 30%) [3539/3862 ( 92%)]  Loss: 1.710 (1.58)  LR: 8.576e-04  Grad: 16.2579  max=1.0861(module.vfe.pfn_layers.0.linear.weight)  min: -1.2661(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6218, loss_cls=0.1022, loss_bbox=0.8016, matched_ious=0.4887, d_time=0.00(0.01), f_time=1.26(1.38), b_time=1.27(1.38)  Time cost: 1:21:42/07:27 [8:46:13/20:55:23]  Acc_iter 22850       Data time: 0.00(0.01)  Forward time: 1.26(1.38)  Batch time: 1.27(1.38)
2025-09-02 06:31:54,256   INFO  Train:    6/20 ( 30%) [3589/3862 ( 93%)]  Loss: 1.585 (1.58)  LR: 8.592e-04  Grad: 16.2504  max=0.9744(module.vfe.pfn_layers.0.linear.weight)  min: -0.6308(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6119, loss_cls=0.1010, loss_bbox=0.7829, matched_ious=0.4899, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.30(1.39)  Time cost: 1:22:52/06:18 [8:47:23/20:54:25]  Acc_iter 22900       Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.30(1.39)
2025-09-02 06:33:03,633   INFO  Train:    6/20 ( 30%) [3639/3862 ( 94%)]  Loss: 1.550 (1.58)  LR: 8.609e-04  Grad: 16.2910  max=0.6945(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6374(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6015, loss_cls=0.1022, loss_bbox=0.7653, matched_ious=0.4929, d_time=0.00(0.01), f_time=1.38(1.38), b_time=1.38(1.39)  Time cost: 1:24:01/05:08 [8:48:32/20:53:18]  Acc_iter 22950       Data time: 0.00(0.01)  Forward time: 1.38(1.38)  Batch time: 1.38(1.39)
2025-09-02 06:34:11,951   INFO  Train:    6/20 ( 30%) [3689/3862 ( 96%)]  Loss: 1.551 (1.57)  LR: 8.626e-04  Grad: 16.3894  max=1.3231(module.vfe.pfn_layers.0.linear.weight)  min: -0.6457(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6297, loss_cls=0.1059, loss_bbox=0.7912, matched_ious=0.4912, d_time=0.00(0.01), f_time=1.26(1.38), b_time=1.26(1.38)  Time cost: 1:25:10/03:59 [8:49:41/20:51:55]  Acc_iter 23000       Data time: 0.00(0.01)  Forward time: 1.26(1.38)  Batch time: 1.26(1.38)
2025-09-02 06:35:21,076   INFO  Train:    6/20 ( 30%) [3739/3862 ( 97%)]  Loss: 1.945 (1.57)  LR: 8.642e-04  Grad: 12.5171  max=1.5168(module.vfe.pfn_layers.0.linear.weight)  min: -2.6488(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6148, loss_cls=0.1029, loss_bbox=0.7874, matched_ious=0.4888, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.31(1.38)  Time cost: 1:26:19/02:50 [8:50:50/20:50:44]  Acc_iter 23050       Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.38)
2025-09-02 06:36:30,341   INFO  Train:    6/20 ( 30%) [3789/3862 ( 98%)]  Loss: 1.370 (1.57)  LR: 8.658e-04  Grad: 12.1303  max=0.5179(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4807(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6163, loss_cls=0.1042, loss_bbox=0.7509, matched_ious=0.4940, d_time=0.01(0.01), f_time=1.39(1.38), b_time=1.40(1.38)  Time cost: 1:27:28/01:41 [8:51:59/20:49:35]  Acc_iter 23100       Data time: 0.01(0.01)  Forward time: 1.39(1.38)  Batch time: 1.40(1.38)
2025-09-02 06:37:38,819   INFO  Train:    6/20 ( 30%) [3839/3862 ( 99%)]  Loss: 1.738 (1.57)  LR: 8.675e-04  Grad: 12.1919  max=0.5142(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4902(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6346, loss_cls=0.1052, loss_bbox=0.8125, matched_ious=0.4845, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.33(1.38)  Time cost: 1:28:36/00:31 [8:53:07/20:48:15]  Acc_iter 23150       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.33(1.38)
2025-09-02 06:38:08,284   INFO  Train:    6/20 ( 30%) [3861/3862 (100%)]  Loss: 1.554 (1.57)  LR: 8.682e-04  Grad: 12.2778  max=1.1036(module.vfe.pfn_layers.0.linear.weight)  min: -0.4920(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6420, loss_cls=0.1075, loss_bbox=0.8104, matched_ious=0.4911, d_time=0.00(0.01), f_time=1.26(1.38), b_time=1.27(1.38)  Time cost: 1:29:06/00:01 [8:53:37/20:47:31]  Acc_iter 23172       Data time: 0.00(0.01)  Forward time: 1.26(1.38)  Batch time: 1.27(1.38)

                                               [Aepochs:  30%|███       | 6/20 [8:53:38<20:46:17, 5341.23s/it]epochs:  30%|███       | 6/20 [8:53:38<20:46:17, 5341.22s/it]epochs:  30%|███       | 6/20 [8:53:38<20:46:17, 5341.22s/it]epochs:  30%|███       | 6/20 [8:53:38<20:46:17, 5341.22s/it]epochs:  30%|███       | 6/20 [8:53:38<20:46:17, 5341.22s/it]epochs:  30%|███       | 6/20 [8:53:38<20:46:17, 5341.24s/it]epochs:  30%|███       | 6/20 [8:53:38<20:46:17, 5341.23s/it]epochs:  30%|███       | 6/20 [8:53:38<20:46:17, 5341.23s/it]
train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-02 06:38:16,688   INFO  Train:    7/20 ( 35%) [   0/3862 (  0%)]  Loss: 1.515 (1.52)  LR: 8.682e-04  Grad: 12.6015  max=0.5222(module.vfe.pfn_layers.0.linear.weight)  min: -2.3313(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5931, loss_cls=0.0975, loss_bbox=0.8245, matched_ious=0.4855, d_time=3.28(3.28), f_time=3.16(3.16), b_time=6.44(6.44)  Time cost: 00:06/6:44:43 [8:53:45/94:26:14]  Acc_iter 23173       Data time: 3.28(3.28)  Forward time: 3.16(3.16)  Batch time: 6.44(6.44)
2025-09-02 06:38:54,246   INFO  Train:    7/20 ( 35%) [  27/3862 (  1%)]  Loss: 1.788 (1.55)  LR: 8.691e-04  Grad: 13.5203  max=3.7016(module.vfe.pfn_layers.0.linear.weight)  min: -1.5846(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6193, loss_cls=0.1027, loss_bbox=0.8261, matched_ious=0.4850, d_time=0.00(0.13), f_time=1.35(1.45), b_time=1.35(1.57)  Time cost: 00:43/1:40:05 [8:54:23/23:30:28]  Acc_iter 23200       Data time: 0.00(0.13)  Forward time: 1.35(1.45)  Batch time: 1.35(1.57)
2025-09-02 06:40:05,380   INFO  Train:    7/20 ( 35%) [  77/3862 (  2%)]  Loss: 1.761 (1.51)  LR: 8.707e-04  Grad: 12.3556  max=0.5192(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0483(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6101, loss_cls=0.1010, loss_bbox=0.7814, matched_ious=0.4859, d_time=0.01(0.05), f_time=1.27(1.43), b_time=1.28(1.48)  Time cost: 01:54/1:32:59 [8:55:34/22:06:30]  Acc_iter 23250       Data time: 0.01(0.05)  Forward time: 1.27(1.43)  Batch time: 1.28(1.48)
2025-09-02 06:41:13,139   INFO  Train:    7/20 ( 35%) [ 127/3862 (  3%)]  Loss: 1.645 (1.52)  LR: 8.723e-04  Grad: 12.5878  max=1.6933(module.vfe.pfn_layers.0.linear.weight)  min: -1.0978(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6324, loss_cls=0.1053, loss_bbox=0.8052, matched_ious=0.4885, d_time=0.00(0.03), f_time=1.37(1.40), b_time=1.38(1.43)  Time cost: 03:02/1:28:52 [8:56:42/21:23:29]  Acc_iter 23300       Data time: 0.00(0.03)  Forward time: 1.37(1.40)  Batch time: 1.38(1.43)
2025-09-02 06:42:23,647   INFO  Train:    7/20 ( 35%) [ 177/3862 (  5%)]  Loss: 1.658 (1.53)  LR: 8.739e-04  Grad: 12.5324  max=1.4170(module.vfe.pfn_layers.0.linear.weight)  min: -0.7535(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6424, loss_cls=0.1065, loss_bbox=0.7804, matched_ious=0.4899, d_time=0.00(0.02), f_time=1.38(1.40), b_time=1.39(1.42)  Time cost: 04:13/1:27:22 [8:57:52/21:17:52]  Acc_iter 23350       Data time: 0.00(0.02)  Forward time: 1.38(1.40)  Batch time: 1.39(1.42)
2025-09-02 06:43:32,739   INFO  Train:    7/20 ( 35%) [ 227/3862 (  6%)]  Loss: 1.793 (1.52)  LR: 8.755e-04  Grad: 12.7887  max=1.2788(module.vfe.pfn_layers.0.linear.weight)  min: -2.2237(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6039, loss_cls=0.1022, loss_bbox=0.7981, matched_ious=0.4918, d_time=0.01(0.02), f_time=1.46(1.39), b_time=1.47(1.41)  Time cost: 05:22/1:25:39 [8:59:01/21:08:39]  Acc_iter 23400       Data time: 0.01(0.02)  Forward time: 1.46(1.39)  Batch time: 1.47(1.41)
2025-09-02 06:44:40,951   INFO  Train:    7/20 ( 35%) [ 277/3862 (  7%)]  Loss: 1.347 (1.51)  LR: 8.770e-04  Grad: 12.6203  max=0.7282(module.vfe.pfn_layers.0.linear.weight)  min: -0.7566(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5977, loss_cls=0.0992, loss_bbox=0.7837, matched_ious=0.4939, d_time=0.03(0.02), f_time=1.37(1.39), b_time=1.39(1.41)  Time cost: 06:30/1:23:56 [9:00:10/20:59:29]  Acc_iter 23450       Data time: 0.03(0.02)  Forward time: 1.37(1.39)  Batch time: 1.39(1.41)
2025-09-02 06:45:50,910   INFO  Train:    7/20 ( 35%) [ 327/3862 (  8%)]  Loss: 1.451 (1.51)  LR: 8.786e-04  Grad: 12.5945  max=0.9196(module.vfe.pfn_layers.0.linear.weight)  min: -0.6207(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6326, loss_cls=0.1055, loss_bbox=0.7668, matched_ious=0.4929, d_time=0.00(0.02), f_time=1.42(1.39), b_time=1.43(1.40)  Time cost: 07:40/1:22:43 [9:01:19/20:57:32]  Acc_iter 23500       Data time: 0.00(0.02)  Forward time: 1.42(1.39)  Batch time: 1.43(1.40)
2025-09-02 06:46:58,694   INFO  Train:    7/20 ( 35%) [ 377/3862 ( 10%)]  Loss: 1.706 (1.52)  LR: 8.802e-04  Grad: 13.3831  max=1.7068(module.vfe.pfn_layers.0.linear.weight)  min: -3.9047(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6191, loss_cls=0.1047, loss_bbox=0.8209, matched_ious=0.4915, d_time=0.00(0.02), f_time=1.37(1.38), b_time=1.37(1.40)  Time cost: 08:48/1:21:10 [9:02:27/20:50:39]  Acc_iter 23550       Data time: 0.00(0.02)  Forward time: 1.37(1.38)  Batch time: 1.37(1.40)
2025-09-02 06:48:07,397   INFO  Train:    7/20 ( 35%) [ 427/3862 ( 11%)]  Loss: 1.490 (1.51)  LR: 8.817e-04  Grad: 13.1023  max=0.7601(module.vfe.pfn_layers.0.linear.weight)  min: -1.7854(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6116, loss_cls=0.1035, loss_bbox=0.7529, matched_ious=0.4937, d_time=0.00(0.02), f_time=1.44(1.38), b_time=1.45(1.40)  Time cost: 09:56/1:19:51 [9:03:36/20:47:01]  Acc_iter 23600       Data time: 0.00(0.02)  Forward time: 1.44(1.38)  Batch time: 1.45(1.40)
2025-09-02 06:49:16,327   INFO  Train:    7/20 ( 35%) [ 477/3862 ( 12%)]  Loss: 1.735 (1.51)  LR: 8.833e-04  Grad: 12.9330  max=1.3086(module.vfe.pfn_layers.0.linear.weight)  min: -0.9494(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6228, loss_cls=0.1035, loss_bbox=0.7786, matched_ious=0.4903, d_time=0.00(0.02), f_time=1.34(1.38), b_time=1.34(1.39)  Time cost: 11:05/1:18:35 [9:04:45/20:44:19]  Acc_iter 23650       Data time: 0.00(0.02)  Forward time: 1.34(1.38)  Batch time: 1.34(1.39)
2025-09-02 06:50:26,230   INFO  Train:    7/20 ( 35%) [ 527/3862 ( 14%)]  Loss: 1.426 (1.51)  LR: 8.848e-04  Grad: 13.7519  max=1.7742(module.vfe.pfn_layers.0.linear.weight)  min: -3.7528(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6115, loss_cls=0.1044, loss_bbox=0.7602, matched_ious=0.4920, d_time=0.01(0.02), f_time=1.23(1.38), b_time=1.25(1.39)  Time cost: 12:15/1:17:27 [9:05:55/20:43:35]  Acc_iter 23700       Data time: 0.01(0.02)  Forward time: 1.23(1.38)  Batch time: 1.25(1.39)
2025-09-02 06:51:35,779   INFO  Train:    7/20 ( 35%) [ 577/3862 ( 15%)]  Loss: 1.483 (1.51)  LR: 8.863e-04  Grad: 12.9057  max=0.5428(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6441(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6271, loss_cls=0.1035, loss_bbox=0.7882, matched_ious=0.4921, d_time=0.00(0.02), f_time=1.42(1.38), b_time=1.42(1.39)  Time cost: 13:25/1:16:17 [9:07:04/20:42:14]  Acc_iter 23750       Data time: 0.00(0.02)  Forward time: 1.42(1.38)  Batch time: 1.42(1.39)
2025-09-02 06:52:45,035   INFO  Train:    7/20 ( 35%) [ 627/3862 ( 16%)]  Loss: 1.583 (1.51)  LR: 8.878e-04  Grad: 12.9306  max=0.8639(module.vfe.pfn_layers.0.linear.weight)  min: -0.5401(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6219, loss_cls=0.1036, loss_bbox=0.8058, matched_ious=0.4894, d_time=0.00(0.01), f_time=1.44(1.38), b_time=1.44(1.39)  Time cost: 14:34/1:15:05 [9:08:14/20:40:29]  Acc_iter 23800       Data time: 0.00(0.01)  Forward time: 1.44(1.38)  Batch time: 1.44(1.39)
2025-09-02 06:53:52,960   INFO  Train:    7/20 ( 35%) [ 677/3862 ( 18%)]  Loss: 1.666 (1.51)  LR: 8.893e-04  Grad: 12.9902  max=0.5521(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5454(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6204, loss_cls=0.1012, loss_bbox=0.7926, matched_ious=0.4979, d_time=0.00(0.01), f_time=1.27(1.38), b_time=1.27(1.39)  Time cost: 15:42/1:13:47 [9:09:22/20:37:04]  Acc_iter 23850       Data time: 0.00(0.01)  Forward time: 1.27(1.38)  Batch time: 1.27(1.39)
2025-09-02 06:55:03,832   INFO  Train:    7/20 ( 35%) [ 727/3862 ( 19%)]  Loss: 1.401 (1.51)  LR: 8.908e-04  Grad: 13.7861  max=0.8759(module.vfe.pfn_layers.0.linear.weight)  min: -3.6260(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6130, loss_cls=0.1005, loss_bbox=0.8124, matched_ious=0.4893, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 16:53/1:12:44 [9:10:32/20:37:34]  Acc_iter 23900       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 06:56:13,781   INFO  Train:    7/20 ( 35%) [ 777/3862 ( 20%)]  Loss: 1.469 (1.51)  LR: 8.923e-04  Grad: 13.1512  max=0.6415(module.vfe.pfn_layers.0.linear.weight)  min: -0.7396(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6326, loss_cls=0.1067, loss_bbox=0.7900, matched_ious=0.4922, d_time=0.02(0.01), f_time=1.29(1.38), b_time=1.30(1.39)  Time cost: 18:03/1:11:35 [9:11:42/20:36:49]  Acc_iter 23950       Data time: 0.02(0.01)  Forward time: 1.29(1.38)  Batch time: 1.30(1.39)
2025-09-02 06:57:23,012   INFO  Train:    7/20 ( 35%) [ 827/3862 ( 21%)]  Loss: 1.698 (1.51)  LR: 8.938e-04  Grad: 13.2592  max=0.8410(module.vfe.pfn_layers.0.linear.weight)  min: -1.2498(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6142, loss_cls=0.1036, loss_bbox=0.7903, matched_ious=0.4937, d_time=0.00(0.01), f_time=2.13(1.38), b_time=2.13(1.39)  Time cost: 19:12/1:10:24 [9:12:52/20:35:13]  Acc_iter 24000       Data time: 0.00(0.01)  Forward time: 2.13(1.38)  Batch time: 2.13(1.39)
2025-09-02 06:58:31,122   INFO  Train:    7/20 ( 35%) [ 877/3862 ( 23%)]  Loss: 1.264 (1.51)  LR: 8.953e-04  Grad: 13.3703  max=1.0049(module.vfe.pfn_layers.0.linear.weight)  min: -1.2968(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6100, loss_cls=0.1018, loss_bbox=0.7707, matched_ious=0.4944, d_time=0.00(0.01), f_time=1.25(1.38), b_time=1.25(1.39)  Time cost: 20:20/1:09:10 [9:14:00/20:32:33]  Acc_iter 24050       Data time: 0.00(0.01)  Forward time: 1.25(1.38)  Batch time: 1.25(1.39)
2025-09-02 06:59:38,805   INFO  Train:    7/20 ( 35%) [ 927/3862 ( 24%)]  Loss: 1.269 (1.51)  LR: 8.967e-04  Grad: 13.6447  max=1.3043(module.vfe.pfn_layers.0.linear.weight)  min: -2.2646(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6099, loss_cls=0.0989, loss_bbox=0.7905, matched_ious=0.4929, d_time=0.00(0.01), f_time=1.27(1.38), b_time=1.27(1.39)  Time cost: 21:28/1:07:54 [9:15:07/20:29:39]  Acc_iter 24100       Data time: 0.00(0.01)  Forward time: 1.27(1.38)  Batch time: 1.27(1.39)
2025-09-02 07:00:49,526   INFO  Train:    7/20 ( 35%) [ 977/3862 ( 25%)]  Loss: 1.697 (1.51)  LR: 8.982e-04  Grad: 13.6424  max=1.7328(module.vfe.pfn_layers.0.linear.weight)  min: -1.1813(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6203, loss_cls=0.1030, loss_bbox=0.7776, matched_ious=0.4887, d_time=0.00(0.01), f_time=2.32(1.38), b_time=2.32(1.39)  Time cost: 22:39/1:06:49 [9:16:18/20:29:40]  Acc_iter 24150       Data time: 0.00(0.01)  Forward time: 2.32(1.38)  Batch time: 2.32(1.39)
2025-09-02 07:01:58,160   INFO  Train:    7/20 ( 35%) [1027/3862 ( 27%)]  Loss: 1.142 (1.51)  LR: 8.996e-04  Grad: 13.3599  max=0.5640(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5633(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6059, loss_cls=0.1010, loss_bbox=0.7703, matched_ious=0.4947, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 23:47/1:05:37 [9:17:27/20:27:47]  Acc_iter 24200       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 07:03:05,668   INFO  Train:    7/20 ( 35%) [1077/3862 ( 28%)]  Loss: 1.359 (1.51)  LR: 9.011e-04  Grad: 13.4303  max=0.5665(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5630(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6020, loss_cls=0.0993, loss_bbox=0.7818, matched_ious=0.4897, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 24:55/1:04:23 [9:18:34/20:25:02]  Acc_iter 24250       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 07:04:16,191   INFO  Train:    7/20 ( 35%) [1127/3862 ( 29%)]  Loss: 1.214 (1.51)  LR: 9.025e-04  Grad: 13.7654  max=1.9503(module.vfe.pfn_layers.0.linear.weight)  min: -0.5649(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6154, loss_cls=0.1018, loss_bbox=0.7799, matched_ious=0.4962, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.31(1.39)  Time cost: 26:05/1:03:16 [9:19:45/20:24:48]  Acc_iter 24300       Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.39)
2025-09-02 07:05:23,937   INFO  Train:    7/20 ( 35%) [1177/3862 ( 30%)]  Loss: 1.477 (1.51)  LR: 9.039e-04  Grad: 13.6604  max=0.5686(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8820(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6053, loss_cls=0.1020, loss_bbox=0.7769, matched_ious=0.4935, d_time=0.01(0.01), f_time=1.26(1.38), b_time=1.27(1.39)  Time cost: 27:13/1:02:03 [9:20:53/20:22:24]  Acc_iter 24350       Data time: 0.01(0.01)  Forward time: 1.26(1.38)  Batch time: 1.27(1.39)
2025-09-02 07:06:32,961   INFO  Train:    7/20 ( 35%) [1227/3862 ( 32%)]  Loss: 1.810 (1.50)  LR: 9.053e-04  Grad: 13.6365  max=0.5674(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5733(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6171, loss_cls=0.1022, loss_bbox=0.7433, matched_ious=0.5013, d_time=0.00(0.01), f_time=1.30(1.37), b_time=1.30(1.39)  Time cost: 28:22/1:00:53 [9:22:02/20:21:01]  Acc_iter 24400       Data time: 0.00(0.01)  Forward time: 1.30(1.37)  Batch time: 1.30(1.39)
2025-09-02 07:07:43,782   INFO  Train:    7/20 ( 35%) [1277/3862 ( 33%)]  Loss: 1.334 (1.50)  LR: 9.067e-04  Grad: 13.6631  max=0.6207(module.vfe.pfn_layers.0.linear.weight)  min: -0.5769(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5969, loss_cls=0.1014, loss_bbox=0.7437, matched_ious=0.4998, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.33(1.39)  Time cost: 29:33/59:47 [9:23:12/20:20:54]  Acc_iter 24450       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.33(1.39)
2025-09-02 07:08:51,491   INFO  Train:    7/20 ( 35%) [1327/3862 ( 34%)]  Loss: 1.604 (1.50)  LR: 9.081e-04  Grad: 13.9151  max=1.5381(module.vfe.pfn_layers.0.linear.weight)  min: -0.9290(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6098, loss_cls=0.1043, loss_bbox=0.7723, matched_ious=0.4921, d_time=0.04(0.01), f_time=1.39(1.37), b_time=1.43(1.39)  Time cost: 30:41/58:34 [9:24:20/20:18:38]  Acc_iter 24500       Data time: 0.04(0.01)  Forward time: 1.39(1.37)  Batch time: 1.43(1.39)
2025-09-02 07:10:01,005   INFO  Train:    7/20 ( 35%) [1377/3862 ( 36%)]  Loss: 1.133 (1.50)  LR: 9.095e-04  Grad: 13.8471  max=0.7599(module.vfe.pfn_layers.0.linear.weight)  min: -1.0797(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6014, loss_cls=0.1018, loss_bbox=0.7495, matched_ious=0.4956, d_time=0.02(0.01), f_time=1.37(1.37), b_time=1.39(1.39)  Time cost: 31:50/57:25 [9:25:30/20:17:36]  Acc_iter 24550       Data time: 0.02(0.01)  Forward time: 1.37(1.37)  Batch time: 1.39(1.39)
2025-09-02 07:11:09,479   INFO  Train:    7/20 ( 35%) [1427/3862 ( 37%)]  Loss: 1.618 (1.50)  LR: 9.109e-04  Grad: 13.9870  max=0.9328(module.vfe.pfn_layers.0.linear.weight)  min: -1.4483(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6154, loss_cls=0.1025, loss_bbox=0.7854, matched_ious=0.4896, d_time=0.00(0.01), f_time=1.49(1.37), b_time=1.50(1.39)  Time cost: 32:59/56:14 [9:26:38/20:15:55]  Acc_iter 24600       Data time: 0.00(0.01)  Forward time: 1.49(1.37)  Batch time: 1.50(1.39)
2025-09-02 07:12:18,114   INFO  Train:    7/20 ( 35%) [1477/3862 ( 38%)]  Loss: 1.500 (1.50)  LR: 9.122e-04  Grad: 13.9486  max=0.5795(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5946(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6277, loss_cls=0.1028, loss_bbox=0.7895, matched_ious=0.4961, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.33(1.39)  Time cost: 34:07/55:04 [9:27:47/20:14:22]  Acc_iter 24650       Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.39)
2025-09-02 07:13:29,328   INFO  Train:    7/20 ( 35%) [1527/3862 ( 40%)]  Loss: 1.699 (1.50)  LR: 9.136e-04  Grad: 13.9890  max=0.5792(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5973(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6243, loss_cls=0.1046, loss_bbox=0.8073, matched_ious=0.4900, d_time=0.01(0.01), f_time=1.34(1.38), b_time=1.35(1.39)  Time cost: 35:18/53:58 [9:28:58/20:14:20]  Acc_iter 24700       Data time: 0.01(0.01)  Forward time: 1.34(1.38)  Batch time: 1.35(1.39)
2025-09-02 07:14:39,227   INFO  Train:    7/20 ( 35%) [1577/3862 ( 41%)]  Loss: 1.454 (1.50)  LR: 9.149e-04  Grad: 14.0573  max=0.5953(module.vfe.pfn_layers.0.linear.weight)  min: -0.5957(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6011, loss_cls=0.1013, loss_bbox=0.7444, matched_ious=0.4967, d_time=0.00(0.01), f_time=1.27(1.38), b_time=1.28(1.39)  Time cost: 36:28/52:49 [9:30:08/20:13:29]  Acc_iter 24750       Data time: 0.00(0.01)  Forward time: 1.27(1.38)  Batch time: 1.28(1.39)
2025-09-02 07:15:49,043   INFO  Train:    7/20 ( 35%) [1627/3862 ( 42%)]  Loss: 1.490 (1.50)  LR: 9.163e-04  Grad: 14.1785  max=0.8045(module.vfe.pfn_layers.0.linear.weight)  min: -0.8253(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6039, loss_cls=0.1006, loss_bbox=0.7594, matched_ious=0.4945, d_time=0.00(0.01), f_time=1.35(1.38), b_time=1.36(1.39)  Time cost: 37:38/51:40 [9:31:18/20:12:35]  Acc_iter 24800       Data time: 0.00(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.39)
2025-09-02 07:16:57,729   INFO  Train:    7/20 ( 35%) [1677/3862 ( 43%)]  Loss: 1.274 (1.50)  LR: 9.176e-04  Grad: 14.2893  max=1.6503(module.vfe.pfn_layers.0.linear.weight)  min: -0.6040(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6227, loss_cls=0.1018, loss_bbox=0.7705, matched_ious=0.5049, d_time=0.01(0.01), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 38:47/50:30 [9:32:26/20:11:03]  Acc_iter 24850       Data time: 0.01(0.01)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 07:18:05,932   INFO  Train:    7/20 ( 35%) [1727/3862 ( 45%)]  Loss: 1.388 (1.50)  LR: 9.189e-04  Grad: 14.2934  max=1.0046(module.vfe.pfn_layers.0.linear.weight)  min: -0.6081(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5926, loss_cls=0.0996, loss_bbox=0.7420, matched_ious=0.4987, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 39:55/49:19 [9:33:34/20:09:20]  Acc_iter 24900       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 07:19:16,898   INFO  Train:    7/20 ( 35%) [1777/3862 ( 46%)]  Loss: 1.554 (1.49)  LR: 9.202e-04  Grad: 14.4494  max=1.5319(module.vfe.pfn_layers.0.linear.weight)  min: -1.2831(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5956, loss_cls=0.1000, loss_bbox=0.7448, matched_ious=0.5047, d_time=0.00(0.01), f_time=1.41(1.38), b_time=1.41(1.39)  Time cost: 41:06/48:12 [9:34:45/20:08:59]  Acc_iter 24950       Data time: 0.00(0.01)  Forward time: 1.41(1.38)  Batch time: 1.41(1.39)
2025-09-02 07:20:26,921   INFO  Train:    7/20 ( 35%) [1827/3862 ( 47%)]  Loss: 1.512 (1.50)  LR: 9.215e-04  Grad: 14.4137  max=0.6820(module.vfe.pfn_layers.0.linear.weight)  min: -1.0344(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6163, loss_cls=0.1050, loss_bbox=0.7958, matched_ious=0.4935, d_time=0.01(0.01), f_time=1.34(1.38), b_time=1.34(1.39)  Time cost: 42:16/47:03 [9:35:55/20:08:09]  Acc_iter 25000       Data time: 0.01(0.01)  Forward time: 1.34(1.38)  Batch time: 1.34(1.39)
2025-09-02 07:21:36,058   INFO  Train:    7/20 ( 35%) [1877/3862 ( 49%)]  Loss: 1.124 (1.49)  LR: 9.228e-04  Grad: 14.4018  max=0.5889(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6184(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6107, loss_cls=0.1042, loss_bbox=0.7628, matched_ious=0.4956, d_time=0.00(0.01), f_time=1.42(1.38), b_time=1.43(1.39)  Time cost: 43:25/45:54 [9:37:05/20:06:53]  Acc_iter 25050       Data time: 0.00(0.01)  Forward time: 1.42(1.38)  Batch time: 1.43(1.39)
2025-09-02 07:22:44,817   INFO  Train:    7/20 ( 35%) [1927/3862 ( 50%)]  Loss: 1.704 (1.49)  LR: 9.241e-04  Grad: 14.5550  max=0.8552(module.vfe.pfn_layers.0.linear.weight)  min: -1.5119(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5965, loss_cls=0.1001, loss_bbox=0.7455, matched_ious=0.5040, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 44:34/44:44 [9:38:13/20:05:27]  Acc_iter 25100       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 07:23:53,027   INFO  Train:    7/20 ( 35%) [1977/3862 ( 51%)]  Loss: 1.614 (1.49)  LR: 9.253e-04  Grad: 14.5122  max=0.5936(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6216(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5925, loss_cls=0.0977, loss_bbox=0.7735, matched_ious=0.4979, d_time=0.01(0.01), f_time=1.26(1.38), b_time=1.27(1.39)  Time cost: 45:42/43:33 [9:39:22/20:03:47]  Acc_iter 25150       Data time: 0.01(0.01)  Forward time: 1.26(1.38)  Batch time: 1.27(1.39)
2025-09-02 07:25:01,860   INFO  Train:    7/20 ( 35%) [2027/3862 ( 52%)]  Loss: 1.307 (1.49)  LR: 9.266e-04  Grad: 14.6423  max=0.5944(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6717(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6082, loss_cls=0.1028, loss_bbox=0.7648, matched_ious=0.4978, d_time=0.01(0.01), f_time=1.42(1.38), b_time=1.43(1.39)  Time cost: 46:51/42:23 [9:40:30/20:02:25]  Acc_iter 25200       Data time: 0.01(0.01)  Forward time: 1.42(1.38)  Batch time: 1.43(1.39)
2025-09-02 07:26:11,867   INFO  Train:    7/20 ( 35%) [2077/3862 ( 54%)]  Loss: 1.188 (1.49)  LR: 9.278e-04  Grad: 14.8017  max=1.2226(module.vfe.pfn_layers.0.linear.weight)  min: -1.1104(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6039, loss_cls=0.0999, loss_bbox=0.7864, matched_ious=0.4999, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.33(1.39)  Time cost: 48:01/41:15 [9:41:40/20:01:33]  Acc_iter 25250       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.33(1.39)
2025-09-02 07:27:20,932   INFO  Train:    7/20 ( 35%) [2127/3862 ( 55%)]  Loss: 1.160 (1.49)  LR: 9.291e-04  Grad: 14.8849  max=1.0095(module.vfe.pfn_layers.0.linear.weight)  min: -1.0993(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6240, loss_cls=0.1047, loss_bbox=0.7963, matched_ious=0.4928, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.33(1.39)  Time cost: 49:10/40:05 [9:42:49/20:00:17]  Acc_iter 25300       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.33(1.39)
2025-09-02 07:28:30,607   INFO  Train:    7/20 ( 35%) [2177/3862 ( 56%)]  Loss: 1.310 (1.49)  LR: 9.303e-04  Grad: 15.0061  max=0.7512(module.vfe.pfn_layers.0.linear.weight)  min: -1.6716(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6005, loss_cls=0.1014, loss_bbox=0.7583, matched_ious=0.4982, d_time=0.00(0.01), f_time=1.29(1.38), b_time=1.29(1.39)  Time cost: 50:20/38:56 [9:43:59/19:59:16]  Acc_iter 25350       Data time: 0.00(0.01)  Forward time: 1.29(1.38)  Batch time: 1.29(1.39)
2025-09-02 07:29:38,677   INFO  Train:    7/20 ( 35%) [2227/3862 ( 58%)]  Loss: 1.526 (1.49)  LR: 9.315e-04  Grad: 14.9170  max=0.5853(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7400(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5963, loss_cls=0.1016, loss_bbox=0.7585, matched_ious=0.5000, d_time=0.01(0.01), f_time=1.22(1.38), b_time=1.23(1.39)  Time cost: 51:28/37:46 [9:45:07/19:57:37]  Acc_iter 25400       Data time: 0.01(0.01)  Forward time: 1.22(1.38)  Batch time: 1.23(1.39)
2025-09-02 07:30:46,301   INFO  Train:    7/20 ( 35%) [2277/3862 ( 59%)]  Loss: 1.276 (1.49)  LR: 9.327e-04  Grad: 15.1270  max=1.9609(module.vfe.pfn_layers.0.linear.weight)  min: -0.7506(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5867, loss_cls=0.0998, loss_bbox=0.7315, matched_ious=0.4967, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.31(1.39)  Time cost: 52:35/36:35 [9:46:15/19:55:50]  Acc_iter 25450       Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.31(1.39)
2025-09-02 07:31:57,228   INFO  Train:    7/20 ( 35%) [2327/3862 ( 60%)]  Loss: 1.371 (1.49)  LR: 9.339e-04  Grad: 15.0078  max=0.5812(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6364(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6115, loss_cls=0.1018, loss_bbox=0.7666, matched_ious=0.4961, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.31(1.39)  Time cost: 53:46/35:27 [9:47:26/19:55:17]  Acc_iter 25500       Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.39)
2025-09-02 07:33:06,102   INFO  Train:    7/20 ( 35%) [2377/3862 ( 62%)]  Loss: 1.140 (1.49)  LR: 9.351e-04  Grad: 15.0640  max=0.5787(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6386(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5792, loss_cls=0.0977, loss_bbox=0.7271, matched_ious=0.5043, d_time=0.01(0.01), f_time=1.38(1.38), b_time=1.39(1.39)  Time cost: 54:55/34:18 [9:48:35/19:53:59]  Acc_iter 25550       Data time: 0.01(0.01)  Forward time: 1.38(1.38)  Batch time: 1.39(1.39)
2025-09-02 07:34:14,933   INFO  Train:    7/20 ( 35%) [2427/3862 ( 63%)]  Loss: 1.311 (1.49)  LR: 9.363e-04  Grad: 15.1000  max=0.5845(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6419(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5882, loss_cls=0.0981, loss_bbox=0.7616, matched_ious=0.5053, d_time=0.00(0.01), f_time=2.21(1.38), b_time=2.21(1.39)  Time cost: 56:04/33:08 [9:49:43/19:52:40]  Acc_iter 25600       Data time: 0.00(0.01)  Forward time: 2.21(1.38)  Batch time: 2.21(1.39)
2025-09-02 07:35:23,871   INFO  Train:    7/20 ( 35%) [2477/3862 ( 64%)]  Loss: 1.582 (1.49)  LR: 9.375e-04  Grad: 15.1611  max=0.6807(module.vfe.pfn_layers.0.linear.weight)  min: -0.6450(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6068, loss_cls=0.1019, loss_bbox=0.7327, matched_ious=0.4974, d_time=0.02(0.01), f_time=1.18(1.37), b_time=1.20(1.39)  Time cost: 57:13/31:59 [9:50:52/19:51:23]  Acc_iter 25650       Data time: 0.02(0.01)  Forward time: 1.18(1.37)  Batch time: 1.20(1.39)
2025-09-02 07:36:32,318   INFO  Train:    7/20 ( 35%) [2527/3862 ( 65%)]  Loss: 1.495 (1.49)  LR: 9.386e-04  Grad: 15.5855  max=1.4319(module.vfe.pfn_layers.0.linear.weight)  min: -1.7668(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6257, loss_cls=0.1030, loss_bbox=0.7479, matched_ious=0.4991, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.38(1.39)  Time cost: 58:21/30:49 [9:52:01/19:49:57]  Acc_iter 25700       Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.38(1.39)
2025-09-02 07:37:41,556   INFO  Train:    7/20 ( 35%) [2577/3862 ( 67%)]  Loss: 1.363 (1.49)  LR: 9.398e-04  Grad: 15.2600  max=0.5847(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6568(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6251, loss_cls=0.1041, loss_bbox=0.7885, matched_ious=0.4957, d_time=0.00(0.01), f_time=1.28(1.37), b_time=1.28(1.39)  Time cost: 59:31/29:40 [9:53:10/19:48:47]  Acc_iter 25750       Data time: 0.00(0.01)  Forward time: 1.28(1.37)  Batch time: 1.28(1.39)
2025-09-02 07:38:52,825   INFO  Train:    7/20 ( 35%) [2627/3862 ( 68%)]  Loss: 2.142 (1.49)  LR: 9.409e-04  Grad: 15.3440  max=0.5831(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6568(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6064, loss_cls=0.1003, loss_bbox=0.7416, matched_ious=0.5007, d_time=0.02(0.01), f_time=1.36(1.38), b_time=1.38(1.39)  Time cost: 1:00:42/28:31 [9:54:21/19:48:17]  Acc_iter 25800       Data time: 0.02(0.01)  Forward time: 1.36(1.38)  Batch time: 1.38(1.39)
2025-09-02 07:40:01,088   INFO  Train:    7/20 ( 35%) [2677/3862 ( 69%)]  Loss: 1.355 (1.49)  LR: 9.420e-04  Grad: 15.6536  max=1.5910(module.vfe.pfn_layers.0.linear.weight)  min: -1.7976(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5910, loss_cls=0.0994, loss_bbox=0.7563, matched_ious=0.5006, d_time=0.00(0.01), f_time=1.29(1.38), b_time=1.29(1.39)  Time cost: 1:01:50/27:21 [9:55:30/19:46:48]  Acc_iter 25850       Data time: 0.00(0.01)  Forward time: 1.29(1.38)  Batch time: 1.29(1.39)
2025-09-02 07:41:10,781   INFO  Train:    7/20 ( 35%) [2727/3862 ( 71%)]  Loss: 1.426 (1.48)  LR: 9.431e-04  Grad: 17.9868  max=4.8899(module.vfe.pfn_layers.0.linear.weight)  min: -4.7771(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5817, loss_cls=0.0974, loss_bbox=0.7575, matched_ious=0.4966, d_time=0.00(0.01), f_time=1.38(1.38), b_time=1.39(1.39)  Time cost: 1:03:00/26:12 [9:56:39/19:45:46]  Acc_iter 25900       Data time: 0.00(0.01)  Forward time: 1.38(1.38)  Batch time: 1.39(1.39)
2025-09-02 07:42:19,505   INFO  Train:    7/20 ( 35%) [2777/3862 ( 72%)]  Loss: 1.535 (1.48)  LR: 9.443e-04  Grad: 15.6351  max=1.1425(module.vfe.pfn_layers.0.linear.weight)  min: -1.2624(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5929, loss_cls=0.1003, loss_bbox=0.7252, matched_ious=0.5010, d_time=0.00(0.01), f_time=1.41(1.38), b_time=1.41(1.39)  Time cost: 1:04:09/25:03 [9:57:48/19:44:26]  Acc_iter 25950       Data time: 0.00(0.01)  Forward time: 1.41(1.38)  Batch time: 1.41(1.39)
2025-09-02 07:43:28,666   INFO  Train:    7/20 ( 35%) [2827/3862 ( 73%)]  Loss: 1.512 (1.48)  LR: 9.454e-04  Grad: 15.9827  max=1.9981(module.vfe.pfn_layers.0.linear.weight)  min: -2.1583(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5738, loss_cls=0.0953, loss_bbox=0.7555, matched_ious=0.4969, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.31(1.39)  Time cost: 1:05:18/23:54 [9:58:57/19:43:15]  Acc_iter 26000       Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.39)
2025-09-02 07:44:40,618   INFO  Train:    7/20 ( 35%) [2877/3862 ( 74%)]  Loss: 1.392 (1.48)  LR: 9.464e-04  Grad: 15.6515  max=0.5817(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6738(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5798, loss_cls=0.0967, loss_bbox=0.7499, matched_ious=0.4973, d_time=0.00(0.01), f_time=1.35(1.38), b_time=1.36(1.39)  Time cost: 1:06:30/22:45 [10:00:09/19:42:54]  Acc_iter 26050       Data time: 0.00(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.39)
2025-09-02 07:45:50,091   INFO  Train:    7/20 ( 35%) [2927/3862 ( 76%)]  Loss: 1.409 (1.48)  LR: 9.475e-04  Grad: 15.7097  max=0.5833(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6739(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5856, loss_cls=0.0964, loss_bbox=0.7683, matched_ious=0.5039, d_time=0.00(0.01), f_time=1.46(1.38), b_time=1.46(1.39)  Time cost: 1:07:39/21:36 [10:01:19/19:41:47]  Acc_iter 26100       Data time: 0.00(0.01)  Forward time: 1.46(1.38)  Batch time: 1.46(1.39)
2025-09-02 07:46:59,766   INFO  Train:    7/20 ( 35%) [2977/3862 ( 77%)]  Loss: 1.129 (1.48)  LR: 9.486e-04  Grad: 18.9004  max=5.4551(module.vfe.pfn_layers.0.linear.weight)  min: -5.9865(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5864, loss_cls=0.0976, loss_bbox=0.7023, matched_ious=0.5065, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.37(1.39)  Time cost: 1:08:49/20:27 [10:02:28/19:40:44]  Acc_iter 26150       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.37(1.39)
2025-09-02 07:48:07,845   INFO  Train:    7/20 ( 35%) [3027/3862 ( 78%)]  Loss: 1.082 (1.48)  LR: 9.496e-04  Grad: 15.8706  max=0.7095(module.vfe.pfn_layers.0.linear.weight)  min: -1.1208(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5978, loss_cls=0.0991, loss_bbox=0.7377, matched_ious=0.4981, d_time=0.00(0.01), f_time=1.40(1.38), b_time=1.41(1.39)  Time cost: 1:09:57/19:17 [10:03:36/19:39:13]  Acc_iter 26200       Data time: 0.00(0.01)  Forward time: 1.40(1.38)  Batch time: 1.41(1.39)
2025-09-02 07:49:17,332   INFO  Train:    7/20 ( 35%) [3077/3862 ( 80%)]  Loss: 1.090 (1.48)  LR: 9.507e-04  Grad: 15.8847  max=0.5838(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6997(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5748, loss_cls=0.0971, loss_bbox=0.7368, matched_ious=0.5000, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.37(1.39)  Time cost: 1:11:06/18:08 [10:04:46/19:38:07]  Acc_iter 26250       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.37(1.39)
2025-09-02 07:50:26,260   INFO  Train:    7/20 ( 35%) [3127/3862 ( 81%)]  Loss: 1.689 (1.48)  LR: 9.517e-04  Grad: 15.9069  max=0.5877(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6806(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5994, loss_cls=0.0990, loss_bbox=0.7396, matched_ious=0.5013, d_time=0.00(0.01), f_time=1.26(1.38), b_time=1.26(1.39)  Time cost: 1:12:15/16:58 [10:05:55/19:36:51]  Acc_iter 26300       Data time: 0.00(0.01)  Forward time: 1.26(1.38)  Batch time: 1.26(1.39)
2025-09-02 07:51:36,913   INFO  Train:    7/20 ( 35%) [3177/3862 ( 82%)]  Loss: 1.652 (1.48)  LR: 9.528e-04  Grad: 16.0677  max=1.3823(module.vfe.pfn_layers.0.linear.weight)  min: -0.8996(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6047, loss_cls=0.0996, loss_bbox=0.7720, matched_ious=0.4977, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 1:13:26/15:49 [10:07:05/19:36:03]  Acc_iter 26350       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 07:52:45,203   INFO  Train:    7/20 ( 35%) [3227/3862 ( 84%)]  Loss: 1.471 (1.48)  LR: 9.538e-04  Grad: 16.0732  max=0.7584(module.vfe.pfn_layers.0.linear.weight)  min: -0.8241(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6085, loss_cls=0.1016, loss_bbox=0.7922, matched_ious=0.5057, d_time=0.00(0.01), f_time=1.35(1.38), b_time=1.35(1.39)  Time cost: 1:14:34/14:40 [10:08:14/19:34:38]  Acc_iter 26400       Data time: 0.00(0.01)  Forward time: 1.35(1.38)  Batch time: 1.35(1.39)
2025-09-02 07:53:54,968   INFO  Train:    7/20 ( 35%) [3277/3862 ( 85%)]  Loss: 2.105 (1.48)  LR: 9.548e-04  Grad: 16.2256  max=0.9827(module.vfe.pfn_layers.0.linear.weight)  min: -0.8666(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6008, loss_cls=0.0997, loss_bbox=0.7828, matched_ious=0.4977, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.31(1.39)  Time cost: 1:15:44/13:31 [10:09:24/19:33:35]  Acc_iter 26450       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.31(1.39)
2025-09-02 07:55:03,315   INFO  Train:    7/20 ( 35%) [3327/3862 ( 86%)]  Loss: 1.235 (1.48)  LR: 9.558e-04  Grad: 16.1839  max=0.7531(module.vfe.pfn_layers.0.linear.weight)  min: -0.6964(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5912, loss_cls=0.0985, loss_bbox=0.7422, matched_ious=0.5028, d_time=0.01(0.01), f_time=1.29(1.38), b_time=1.30(1.39)  Time cost: 1:16:52/12:21 [10:10:32/19:32:11]  Acc_iter 26500       Data time: 0.01(0.01)  Forward time: 1.29(1.38)  Batch time: 1.30(1.39)
2025-09-02 07:56:11,420   INFO  Train:    7/20 ( 35%) [3377/3862 ( 87%)]  Loss: 1.440 (1.48)  LR: 9.568e-04  Grad: 16.3109  max=0.7470(module.vfe.pfn_layers.0.linear.weight)  min: -0.8100(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6048, loss_cls=0.0995, loss_bbox=0.7620, matched_ious=0.5042, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.33(1.39)  Time cost: 1:18:01/11:12 [10:11:40/19:30:44]  Acc_iter 26550       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.33(1.39)
2025-09-02 07:57:22,773   INFO  Train:    7/20 ( 35%) [3427/3862 ( 89%)]  Loss: 1.528 (1.48)  LR: 9.577e-04  Grad: 16.3041  max=0.5947(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7002(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6044, loss_cls=0.1014, loss_bbox=0.7625, matched_ious=0.5000, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.31(1.39)  Time cost: 1:19:12/10:03 [10:12:51/19:30:05]  Acc_iter 26600       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.31(1.39)
2025-09-02 07:58:32,041   INFO  Train:    7/20 ( 35%) [3477/3862 ( 90%)]  Loss: 1.932 (1.48)  LR: 9.587e-04  Grad: 16.4682  max=0.6287(module.vfe.pfn_layers.0.linear.weight)  min: -1.4980(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5987, loss_cls=0.0999, loss_bbox=0.7350, matched_ious=0.5016, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 1:20:21/08:53 [10:14:01/19:28:55]  Acc_iter 26650       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 07:59:41,236   INFO  Train:    7/20 ( 35%) [3527/3862 ( 91%)]  Loss: 1.276 (1.47)  LR: 9.596e-04  Grad: 16.8172  max=0.9527(module.vfe.pfn_layers.0.linear.weight)  min: -2.6265(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5833, loss_cls=0.0991, loss_bbox=0.7126, matched_ious=0.5050, d_time=0.00(0.01), f_time=1.29(1.38), b_time=1.29(1.39)  Time cost: 1:21:30/07:44 [10:15:10/19:27:44]  Acc_iter 26700       Data time: 0.00(0.01)  Forward time: 1.29(1.38)  Batch time: 1.29(1.39)
2025-09-02 08:00:49,186   INFO  Train:    7/20 ( 35%) [3577/3862 ( 93%)]  Loss: 1.234 (1.47)  LR: 9.606e-04  Grad: 20.6907  max=7.3303(module.vfe.pfn_layers.0.linear.weight)  min: -6.2514(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5758, loss_cls=0.0963, loss_bbox=0.7240, matched_ious=0.5055, d_time=0.01(0.01), f_time=1.34(1.38), b_time=1.35(1.39)  Time cost: 1:22:38/06:34 [10:16:18/19:26:16]  Acc_iter 26750       Data time: 0.01(0.01)  Forward time: 1.34(1.38)  Batch time: 1.35(1.39)
2025-09-02 08:01:58,572   INFO  Train:    7/20 ( 35%) [3627/3862 ( 94%)]  Loss: 1.211 (1.47)  LR: 9.615e-04  Grad: 16.5895  max=0.7791(module.vfe.pfn_layers.0.linear.weight)  min: -0.8953(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6034, loss_cls=0.1011, loss_bbox=0.7520, matched_ious=0.5038, d_time=0.00(0.01), f_time=1.46(1.38), b_time=1.46(1.39)  Time cost: 1:23:48/05:25 [10:17:27/19:25:07]  Acc_iter 26800       Data time: 0.00(0.01)  Forward time: 1.46(1.38)  Batch time: 1.46(1.39)
2025-09-02 08:03:08,951   INFO  Train:    7/20 ( 35%) [3677/3862 ( 95%)]  Loss: 1.134 (1.47)  LR: 9.624e-04  Grad: 16.6412  max=0.6029(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9546(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5814, loss_cls=0.0980, loss_bbox=0.7416, matched_ious=0.4978, d_time=0.01(0.01), f_time=1.44(1.38), b_time=1.45(1.39)  Time cost: 1:24:58/04:16 [10:18:38/19:24:13]  Acc_iter 26850       Data time: 0.01(0.01)  Forward time: 1.44(1.38)  Batch time: 1.45(1.39)
2025-09-02 08:04:19,807   INFO  Train:    7/20 ( 35%) [3727/3862 ( 97%)]  Loss: 1.502 (1.47)  LR: 9.633e-04  Grad: 16.8980  max=1.5897(module.vfe.pfn_layers.0.linear.weight)  min: -1.0584(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5856, loss_cls=0.0989, loss_bbox=0.7093, matched_ious=0.4969, d_time=0.02(0.01), f_time=1.44(1.38), b_time=1.45(1.39)  Time cost: 1:26:09/03:07 [10:19:48/19:23:24]  Acc_iter 26900       Data time: 0.02(0.01)  Forward time: 1.44(1.38)  Batch time: 1.45(1.39)
2025-09-02 08:05:28,462   INFO  Train:    7/20 ( 35%) [3777/3862 ( 98%)]  Loss: 1.325 (1.47)  LR: 9.642e-04  Grad: 16.9595  max=1.3625(module.vfe.pfn_layers.0.linear.weight)  min: -1.3728(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5640, loss_cls=0.0941, loss_bbox=0.7167, matched_ious=0.5049, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.35(1.39)  Time cost: 1:27:18/01:57 [10:20:57/19:22:06]  Acc_iter 26950       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.35(1.39)
2025-09-02 08:06:37,521   INFO  Train:    7/20 ( 35%) [3827/3862 ( 99%)]  Loss: 1.247 (1.47)  LR: 9.651e-04  Grad: 16.8702  max=0.8623(module.vfe.pfn_layers.0.linear.weight)  min: -1.1145(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5899, loss_cls=0.0972, loss_bbox=0.7301, matched_ious=0.5081, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.32(1.39)  Time cost: 1:28:27/00:48 [10:22:06/19:20:53]  Acc_iter 27000       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.32(1.39)
2025-09-02 08:07:23,467   INFO  Train:    7/20 ( 35%) [3861/3862 (100%)]  Loss: 0.9812 (1.47)  LR: 9.657e-04  Grad: 16.9493  max=1.2688(module.vfe.pfn_layers.0.linear.weight)  min: -0.7292(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5722, loss_cls=0.0957, loss_bbox=0.6837, matched_ious=0.5093, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.32(1.39)  Time cost: 1:29:13/00:01 [10:22:52/19:19:51]  Acc_iter 27034       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.32(1.39)

                                               [Aepochs:  35%|███▌      | 7/20 [10:22:53<19:18:14, 5345.76s/it]epochs:  35%|███▌      | 7/20 [10:22:53<19:18:14, 5345.76s/it]epochs:  35%|███▌      | 7/20 [10:22:53<19:18:14, 5345.76s/it]epochs:  35%|███▌      | 7/20 [10:22:53<19:18:14, 5345.76s/it]epochs:  35%|███▌      | 7/20 [10:22:53<19:18:15, 5345.78s/it]epochs:  35%|███▌      | 7/20 [10:22:53<19:18:15, 5345.77s/it]epochs:  35%|███▌      | 7/20 [10:22:53<19:18:14, 5345.77s/it]epochs:  35%|███▌      | 7/20 [10:22:53<19:18:15, 5345.80s/it]
train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-02 08:07:32,041   INFO  Train:    8/20 ( 40%) [   0/3862 (  0%)]  Loss: 1.441 (1.44)  LR: 9.657e-04  Grad: 16.9754  max=0.9685(module.vfe.pfn_layers.0.linear.weight)  min: -1.4209(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6570, loss_cls=0.1045, loss_bbox=0.6798, matched_ious=0.5210, d_time=3.08(3.08), f_time=3.61(3.61), b_time=6.69(6.69)  Time cost: 00:06/6:50:49 [10:23:01/89:00:46]  Acc_iter 27035       Data time: 3.08(3.08)  Forward time: 3.61(3.61)  Batch time: 6.69(6.69)
2025-09-02 08:07:52,351   INFO  Train:    8/20 ( 40%) [  15/3862 (  0%)]  Loss: 1.358 (1.43)  LR: 9.660e-04  Grad: 16.9419  max=0.6769(module.vfe.pfn_layers.0.linear.weight)  min: -0.7287(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5769, loss_cls=0.0991, loss_bbox=0.7547, matched_ious=0.5141, d_time=0.00(0.20), f_time=1.33(1.49), b_time=1.33(1.69)  Time cost: 00:26/1:46:50 [10:23:21/23:13:54]  Acc_iter 27050       Data time: 0.00(0.20)  Forward time: 1.33(1.49)  Batch time: 1.33(1.69)
2025-09-02 08:09:02,697   INFO  Train:    8/20 ( 40%) [  65/3862 (  2%)]  Loss: 1.207 (1.42)  LR: 9.669e-04  Grad: 16.9510  max=0.6022(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7336(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5922, loss_cls=0.0980, loss_bbox=0.7322, matched_ious=0.5085, d_time=0.00(0.06), f_time=1.36(1.41), b_time=1.36(1.48)  Time cost: 01:37/1:33:02 [10:24:31/20:28:42]  Acc_iter 27100       Data time: 0.00(0.06)  Forward time: 1.36(1.41)  Batch time: 1.36(1.48)
2025-09-02 08:10:15,067   INFO  Train:    8/20 ( 40%) [ 115/3862 (  3%)]  Loss: 1.202 (1.43)  LR: 9.677e-04  Grad: 17.0148  max=0.6077(module.vfe.pfn_layers.0.linear.weight)  min: -0.7368(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5932, loss_cls=0.1006, loss_bbox=0.7504, matched_ious=0.5029, d_time=0.01(0.04), f_time=1.21(1.43), b_time=1.22(1.46)  Time cost: 02:49/1:31:12 [10:25:44/20:19:14]  Acc_iter 27150       Data time: 0.01(0.04)  Forward time: 1.21(1.43)  Batch time: 1.22(1.46)
2025-09-02 08:11:23,590   INFO  Train:    8/20 ( 40%) [ 165/3862 (  4%)]  Loss: 1.420 (1.44)  LR: 9.686e-04  Grad: 17.0773  max=0.7910(module.vfe.pfn_layers.0.linear.weight)  min: -0.7383(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5949, loss_cls=0.0979, loss_bbox=0.7609, matched_ious=0.5025, d_time=0.00(0.03), f_time=1.31(1.40), b_time=1.32(1.44)  Time cost: 03:57/1:28:18 [10:26:52/19:55:24]  Acc_iter 27200       Data time: 0.00(0.03)  Forward time: 1.31(1.40)  Batch time: 1.32(1.44)
2025-09-02 08:12:33,365   INFO  Train:    8/20 ( 40%) [ 215/3862 (  6%)]  Loss: 1.404 (1.42)  LR: 9.694e-04  Grad: 17.1770  max=0.6053(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1193(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5737, loss_cls=0.0956, loss_bbox=0.6949, matched_ious=0.5131, d_time=0.00(0.03), f_time=1.27(1.40), b_time=1.28(1.43)  Time cost: 05:07/1:26:35 [10:28:02/19:46:55]  Acc_iter 27250       Data time: 0.00(0.03)  Forward time: 1.27(1.40)  Batch time: 1.28(1.43)
2025-09-02 08:13:41,470   INFO  Train:    8/20 ( 40%) [ 265/3862 (  7%)]  Loss: 1.147 (1.43)  LR: 9.702e-04  Grad: 17.2732  max=1.1133(module.vfe.pfn_layers.0.linear.weight)  min: -0.7438(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5916, loss_cls=0.0958, loss_bbox=0.7528, matched_ious=0.5010, d_time=0.00(0.02), f_time=1.30(1.39), b_time=1.30(1.41)  Time cost: 06:15/1:24:41 [10:29:10/19:35:58]  Acc_iter 27300       Data time: 0.00(0.02)  Forward time: 1.30(1.39)  Batch time: 1.30(1.41)
2025-09-02 08:14:49,630   INFO  Train:    8/20 ( 40%) [ 315/3862 (  8%)]  Loss: 1.324 (1.43)  LR: 9.710e-04  Grad: 18.0742  max=4.2002(module.vfe.pfn_layers.0.linear.weight)  min: -0.7494(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5869, loss_cls=0.0975, loss_bbox=0.7549, matched_ious=0.5040, d_time=0.00(0.02), f_time=1.41(1.38), b_time=1.42(1.41)  Time cost: 07:23/1:23:03 [10:30:18/19:28:14]  Acc_iter 27350       Data time: 0.00(0.02)  Forward time: 1.41(1.38)  Batch time: 1.42(1.41)
2025-09-02 08:16:00,706   INFO  Train:    8/20 ( 40%) [ 365/3862 (  9%)]  Loss: 1.083 (1.42)  LR: 9.718e-04  Grad: 17.3403  max=0.6149(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7487(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5758, loss_cls=0.0951, loss_bbox=0.7367, matched_ious=0.5056, d_time=0.00(0.02), f_time=1.35(1.39), b_time=1.35(1.41)  Time cost: 08:35/1:22:01 [10:31:29/19:28:58]  Acc_iter 27400       Data time: 0.00(0.02)  Forward time: 1.35(1.39)  Batch time: 1.35(1.41)
2025-09-02 08:17:08,037   INFO  Train:    8/20 ( 40%) [ 415/3862 ( 11%)]  Loss: 1.487 (1.42)  LR: 9.726e-04  Grad: 17.4856  max=0.9263(module.vfe.pfn_layers.0.linear.weight)  min: -0.7491(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5812, loss_cls=0.0964, loss_bbox=0.7004, matched_ious=0.5139, d_time=0.00(0.02), f_time=1.40(1.38), b_time=1.40(1.40)  Time cost: 09:42/1:20:25 [10:32:37/19:21:44]  Acc_iter 27450       Data time: 0.00(0.02)  Forward time: 1.40(1.38)  Batch time: 1.40(1.40)
2025-09-02 08:18:16,602   INFO  Train:    8/20 ( 40%) [ 465/3862 ( 12%)]  Loss: 1.869 (1.41)  LR: 9.734e-04  Grad: 17.7342  max=1.3369(module.vfe.pfn_layers.0.linear.weight)  min: -1.9514(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5596, loss_cls=0.0928, loss_bbox=0.7263, matched_ious=0.5055, d_time=0.00(0.02), f_time=1.24(1.38), b_time=1.24(1.40)  Time cost: 10:50/1:19:04 [10:33:45/19:17:57]  Acc_iter 27500       Data time: 0.00(0.02)  Forward time: 1.24(1.38)  Batch time: 1.24(1.40)
2025-09-02 08:19:26,064   INFO  Train:    8/20 ( 40%) [ 515/3862 ( 13%)]  Loss: 1.141 (1.42)  LR: 9.742e-04  Grad: 17.5299  max=0.6283(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7626(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5901, loss_cls=0.0992, loss_bbox=0.7492, matched_ious=0.5036, d_time=0.01(0.02), f_time=1.33(1.38), b_time=1.34(1.40)  Time cost: 12:00/1:17:52 [10:34:55/19:16:15]  Acc_iter 27550       Data time: 0.01(0.02)  Forward time: 1.33(1.38)  Batch time: 1.34(1.40)
2025-09-02 08:20:33,771   INFO  Train:    8/20 ( 40%) [ 565/3862 ( 15%)]  Loss: 1.394 (1.42)  LR: 9.749e-04  Grad: 17.6154  max=0.6937(module.vfe.pfn_layers.0.linear.weight)  min: -0.7581(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5804, loss_cls=0.0973, loss_bbox=0.7303, matched_ious=0.5004, d_time=0.01(0.02), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 13:08/1:16:30 [10:36:02/19:12:01]  Acc_iter 27600       Data time: 0.01(0.02)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 08:21:43,861   INFO  Train:    8/20 ( 40%) [ 615/3862 ( 16%)]  Loss: 1.444 (1.41)  LR: 9.757e-04  Grad: 18.2070  max=3.7377(module.vfe.pfn_layers.0.linear.weight)  min: -1.1342(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5585, loss_cls=0.0940, loss_bbox=0.7248, matched_ious=0.5102, d_time=0.00(0.02), f_time=1.22(1.38), b_time=1.22(1.39)  Time cost: 14:18/1:15:23 [10:37:12/19:11:29]  Acc_iter 27650       Data time: 0.00(0.02)  Forward time: 1.22(1.38)  Batch time: 1.22(1.39)
2025-09-02 08:22:52,731   INFO  Train:    8/20 ( 40%) [ 665/3862 ( 17%)]  Loss: 1.663 (1.41)  LR: 9.764e-04  Grad: 17.6789  max=0.6471(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7669(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5800, loss_cls=0.0957, loss_bbox=0.7439, matched_ious=0.5089, d_time=0.00(0.02), f_time=1.41(1.38), b_time=1.41(1.39)  Time cost: 15:27/1:14:10 [10:38:21/19:09:21]  Acc_iter 27700       Data time: 0.00(0.02)  Forward time: 1.41(1.38)  Batch time: 1.41(1.39)
2025-09-02 08:24:01,099   INFO  Train:    8/20 ( 40%) [ 715/3862 ( 19%)]  Loss: 1.330 (1.41)  LR: 9.772e-04  Grad: 17.8084  max=0.9307(module.vfe.pfn_layers.0.linear.weight)  min: -0.7656(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5803, loss_cls=0.0985, loss_bbox=0.7330, matched_ious=0.5089, d_time=0.00(0.02), f_time=1.32(1.38), b_time=1.32(1.39)  Time cost: 16:35/1:12:55 [10:39:30/19:06:46]  Acc_iter 27750       Data time: 0.00(0.02)  Forward time: 1.32(1.38)  Batch time: 1.32(1.39)
2025-09-02 08:25:10,771   INFO  Train:    8/20 ( 40%) [ 765/3862 ( 20%)]  Loss: 1.431 (1.41)  LR: 9.779e-04  Grad: 18.0220  max=1.6310(module.vfe.pfn_layers.0.linear.weight)  min: -1.7564(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5808, loss_cls=0.0984, loss_bbox=0.7430, matched_ious=0.5018, d_time=0.02(0.02), f_time=1.32(1.38), b_time=1.33(1.39)  Time cost: 17:45/1:11:46 [10:40:39/19:05:47]  Acc_iter 27800       Data time: 0.02(0.02)  Forward time: 1.32(1.38)  Batch time: 1.33(1.39)
2025-09-02 08:26:18,408   INFO  Train:    8/20 ( 40%) [ 815/3862 ( 21%)]  Loss: 1.284 (1.41)  LR: 9.786e-04  Grad: 17.9108  max=0.6582(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7704(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5665, loss_cls=0.0956, loss_bbox=0.7122, matched_ious=0.5062, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.32(1.39)  Time cost: 18:52/1:10:29 [10:41:47/19:02:41]  Acc_iter 27850       Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.39)
2025-09-02 08:27:28,524   INFO  Train:    8/20 ( 40%) [ 865/3862 ( 22%)]  Loss: 0.9818 (1.41)  LR: 9.793e-04  Grad: 17.9953  max=0.6479(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0467(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5584, loss_cls=0.0964, loss_bbox=0.6953, matched_ious=0.5080, d_time=0.01(0.02), f_time=1.31(1.37), b_time=1.31(1.39)  Time cost: 20:02/1:09:22 [10:42:57/19:02:14]  Acc_iter 27900       Data time: 0.01(0.02)  Forward time: 1.31(1.37)  Batch time: 1.31(1.39)
2025-09-02 08:28:37,576   INFO  Train:    8/20 ( 40%) [ 915/3862 ( 24%)]  Loss: 1.400 (1.41)  LR: 9.799e-04  Grad: 18.0640  max=1.0582(module.vfe.pfn_layers.0.linear.weight)  min: -0.8464(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5980, loss_cls=0.0989, loss_bbox=0.7297, matched_ious=0.5070, d_time=0.01(0.02), f_time=1.30(1.37), b_time=1.31(1.39)  Time cost: 21:11/1:08:12 [10:44:06/19:00:43]  Acc_iter 27950       Data time: 0.01(0.02)  Forward time: 1.30(1.37)  Batch time: 1.31(1.39)
2025-09-02 08:29:45,015   INFO  Train:    8/20 ( 40%) [ 965/3862 ( 25%)]  Loss: 1.563 (1.41)  LR: 9.806e-04  Grad: 18.1136  max=1.2361(module.vfe.pfn_layers.0.linear.weight)  min: -0.7759(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5679, loss_cls=0.0962, loss_bbox=0.7081, matched_ious=0.5116, d_time=0.00(0.01), f_time=1.41(1.37), b_time=1.41(1.39)  Time cost: 22:19/1:06:56 [10:45:14/18:57:51]  Acc_iter 28000       Data time: 0.00(0.01)  Forward time: 1.41(1.37)  Batch time: 1.41(1.39)
2025-09-02 08:30:54,329   INFO  Train:    8/20 ( 40%) [1015/3862 ( 26%)]  Loss: 1.414 (1.41)  LR: 9.813e-04  Grad: 18.3133  max=1.1566(module.vfe.pfn_layers.0.linear.weight)  min: -1.7209(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5751, loss_cls=0.0963, loss_bbox=0.7375, matched_ious=0.5041, d_time=0.01(0.01), f_time=1.22(1.37), b_time=1.23(1.39)  Time cost: 23:28/1:05:47 [10:46:23/18:56:42]  Acc_iter 28050       Data time: 0.01(0.01)  Forward time: 1.22(1.37)  Batch time: 1.23(1.39)
2025-09-02 08:32:03,819   INFO  Train:    8/20 ( 40%) [1065/3862 ( 28%)]  Loss: 1.211 (1.41)  LR: 9.819e-04  Grad: 18.1699  max=0.7858(module.vfe.pfn_layers.0.linear.weight)  min: -0.7829(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5743, loss_cls=0.0974, loss_bbox=0.7462, matched_ious=0.5079, d_time=0.00(0.01), f_time=1.44(1.37), b_time=1.45(1.39)  Time cost: 24:38/1:04:38 [10:47:32/18:55:40]  Acc_iter 28100       Data time: 0.00(0.01)  Forward time: 1.44(1.37)  Batch time: 1.45(1.39)
2025-09-02 08:33:12,348   INFO  Train:    8/20 ( 40%) [1115/3862 ( 29%)]  Loss: 1.569 (1.41)  LR: 9.826e-04  Grad: 18.4000  max=1.2570(module.vfe.pfn_layers.0.linear.weight)  min: -1.0865(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5676, loss_cls=0.0937, loss_bbox=0.6879, matched_ious=0.5120, d_time=0.00(0.01), f_time=1.49(1.37), b_time=1.49(1.39)  Time cost: 25:46/1:03:27 [10:48:41/18:53:56]  Acc_iter 28150       Data time: 0.00(0.01)  Forward time: 1.49(1.37)  Batch time: 1.49(1.39)
2025-09-02 08:34:24,389   INFO  Train:    8/20 ( 40%) [1165/3862 ( 30%)]  Loss: 1.373 (1.40)  LR: 9.832e-04  Grad: 18.3599  max=0.8400(module.vfe.pfn_layers.0.linear.weight)  min: -0.7784(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5769, loss_cls=0.0962, loss_bbox=0.7066, matched_ious=0.5147, d_time=0.04(0.01), f_time=1.36(1.38), b_time=1.40(1.39)  Time cost: 26:58/1:02:24 [10:49:53/18:54:42]  Acc_iter 28200       Data time: 0.04(0.01)  Forward time: 1.36(1.38)  Batch time: 1.40(1.39)
2025-09-02 08:35:32,881   INFO  Train:    8/20 ( 40%) [1215/3862 ( 31%)]  Loss: 1.089 (1.40)  LR: 9.838e-04  Grad: 18.4389  max=0.6717(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9190(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5662, loss_cls=0.0961, loss_bbox=0.7167, matched_ious=0.5113, d_time=0.01(0.01), f_time=1.33(1.37), b_time=1.34(1.39)  Time cost: 28:07/1:01:12 [10:51:01/18:52:55]  Acc_iter 28250       Data time: 0.01(0.01)  Forward time: 1.33(1.37)  Batch time: 1.34(1.39)
2025-09-02 08:36:41,159   INFO  Train:    8/20 ( 40%) [1265/3862 ( 33%)]  Loss: 1.383 (1.40)  LR: 9.844e-04  Grad: 18.4511  max=0.6767(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7806(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5789, loss_cls=0.0964, loss_bbox=0.7356, matched_ious=0.5095, d_time=0.01(0.01), f_time=1.42(1.37), b_time=1.43(1.39)  Time cost: 29:15/1:00:01 [10:52:10/18:51:03]  Acc_iter 28300       Data time: 0.01(0.01)  Forward time: 1.42(1.37)  Batch time: 1.43(1.39)
2025-09-02 08:37:49,767   INFO  Train:    8/20 ( 40%) [1315/3862 ( 34%)]  Loss: 1.609 (1.40)  LR: 9.850e-04  Grad: 18.5103  max=0.6790(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7853(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5768, loss_cls=0.0988, loss_bbox=0.7162, matched_ious=0.5040, d_time=0.00(0.01), f_time=1.30(1.37), b_time=1.30(1.39)  Time cost: 30:24/58:50 [10:53:18/18:49:27]  Acc_iter 28350       Data time: 0.00(0.01)  Forward time: 1.30(1.37)  Batch time: 1.30(1.39)
2025-09-02 08:38:58,584   INFO  Train:    8/20 ( 40%) [1365/3862 ( 35%)]  Loss: 1.462 (1.40)  LR: 9.856e-04  Grad: 18.7577  max=0.8369(module.vfe.pfn_layers.0.linear.weight)  min: -2.3424(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5678, loss_cls=0.0922, loss_bbox=0.7256, matched_ious=0.5120, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.38(1.39)  Time cost: 31:32/57:40 [10:54:27/18:48:01]  Acc_iter 28400       Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.38(1.39)
2025-09-02 08:40:08,116   INFO  Train:    8/20 ( 40%) [1415/3862 ( 37%)]  Loss: 1.289 (1.40)  LR: 9.861e-04  Grad: 19.6734  max=3.8491(module.vfe.pfn_layers.0.linear.weight)  min: -2.9714(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5775, loss_cls=0.0963, loss_bbox=0.7120, matched_ious=0.5079, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.33(1.39)  Time cost: 32:42/56:31 [10:55:37/18:47:00]  Acc_iter 28450       Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.39)
2025-09-02 08:41:17,679   INFO  Train:    8/20 ( 40%) [1465/3862 ( 38%)]  Loss: 1.501 (1.40)  LR: 9.867e-04  Grad: 18.9813  max=1.7856(module.vfe.pfn_layers.0.linear.weight)  min: -2.0406(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5643, loss_cls=0.0947, loss_bbox=0.7457, matched_ious=0.5053, d_time=0.02(0.01), f_time=1.37(1.37), b_time=1.39(1.39)  Time cost: 33:52/55:22 [10:56:46/18:45:59]  Acc_iter 28500       Data time: 0.02(0.01)  Forward time: 1.37(1.37)  Batch time: 1.39(1.39)
2025-09-02 08:42:26,249   INFO  Train:    8/20 ( 40%) [1515/3862 ( 39%)]  Loss: 1.138 (1.40)  LR: 9.872e-04  Grad: 18.7947  max=0.8017(module.vfe.pfn_layers.0.linear.weight)  min: -1.1067(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5748, loss_cls=0.0970, loss_bbox=0.7118, matched_ious=0.5069, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.30(1.39)  Time cost: 35:00/54:12 [10:57:55/18:44:26]  Acc_iter 28550       Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.30(1.39)
2025-09-02 08:43:35,367   INFO  Train:    8/20 ( 40%) [1565/3862 ( 41%)]  Loss: 1.123 (1.40)  LR: 9.878e-04  Grad: 18.7617  max=0.6897(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8013(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5605, loss_cls=0.0961, loss_bbox=0.7055, matched_ious=0.5115, d_time=0.00(0.01), f_time=1.43(1.37), b_time=1.44(1.39)  Time cost: 36:09/53:02 [10:59:04/18:43:12]  Acc_iter 28600       Data time: 0.00(0.01)  Forward time: 1.43(1.37)  Batch time: 1.44(1.39)
2025-09-02 08:44:43,001   INFO  Train:    8/20 ( 40%) [1615/3862 ( 42%)]  Loss: 1.396 (1.40)  LR: 9.883e-04  Grad: 19.3295  max=2.9757(module.vfe.pfn_layers.0.linear.weight)  min: -1.7184(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5746, loss_cls=0.0962, loss_bbox=0.7324, matched_ious=0.5059, d_time=0.00(0.01), f_time=1.30(1.37), b_time=1.30(1.38)  Time cost: 37:17/51:50 [11:00:12/18:41:13]  Acc_iter 28650       Data time: 0.00(0.01)  Forward time: 1.30(1.37)  Batch time: 1.30(1.38)
2025-09-02 08:45:51,907   INFO  Train:    8/20 ( 40%) [1665/3862 ( 43%)]  Loss: 1.165 (1.40)  LR: 9.888e-04  Grad: 18.9335  max=0.6998(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8134(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5630, loss_cls=0.0942, loss_bbox=0.7215, matched_ious=0.5098, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.31(1.38)  Time cost: 38:26/50:41 [11:01:20/18:39:55]  Acc_iter 28700       Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.31(1.38)
2025-09-02 08:47:03,111   INFO  Train:    8/20 ( 40%) [1715/3862 ( 44%)]  Loss: 1.259 (1.40)  LR: 9.893e-04  Grad: 18.9532  max=0.6946(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8115(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5675, loss_cls=0.0939, loss_bbox=0.7218, matched_ious=0.5069, d_time=0.01(0.01), f_time=1.37(1.37), b_time=1.38(1.39)  Time cost: 39:37/49:34 [11:02:32/18:39:42]  Acc_iter 28750       Data time: 0.01(0.01)  Forward time: 1.37(1.37)  Batch time: 1.38(1.39)
2025-09-02 08:48:12,237   INFO  Train:    8/20 ( 40%) [1765/3862 ( 46%)]  Loss: 1.458 (1.40)  LR: 9.898e-04  Grad: 19.2127  max=2.2438(module.vfe.pfn_layers.0.linear.weight)  min: -0.8153(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5515, loss_cls=0.0948, loss_bbox=0.6885, matched_ious=0.5103, d_time=0.01(0.01), f_time=1.33(1.37), b_time=1.34(1.39)  Time cost: 40:46/48:25 [11:03:41/18:38:29]  Acc_iter 28800       Data time: 0.01(0.01)  Forward time: 1.33(1.37)  Batch time: 1.34(1.39)
2025-09-02 08:49:21,651   INFO  Train:    8/20 ( 40%) [1815/3862 ( 47%)]  Loss: 1.395 (1.40)  LR: 9.903e-04  Grad: 19.1722  max=1.3629(module.vfe.pfn_layers.0.linear.weight)  min: -0.8139(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5840, loss_cls=0.0967, loss_bbox=0.7461, matched_ious=0.5056, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.32(1.39)  Time cost: 41:55/47:16 [11:04:50/18:37:23]  Acc_iter 28850       Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.32(1.39)
2025-09-02 08:50:29,964   INFO  Train:    8/20 ( 40%) [1865/3862 ( 48%)]  Loss: 1.342 (1.40)  LR: 9.908e-04  Grad: 19.2103  max=0.7114(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.4487(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5638, loss_cls=0.0925, loss_bbox=0.7104, matched_ious=0.5106, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.35(1.39)  Time cost: 43:04/46:05 [11:05:59/18:35:49]  Acc_iter 28900       Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.35(1.39)
2025-09-02 08:51:37,665   INFO  Train:    8/20 ( 40%) [1915/3862 ( 50%)]  Loss: 1.386 (1.40)  LR: 9.912e-04  Grad: 20.6861  max=0.7930(module.vfe.pfn_layers.0.linear.weight)  min: -4.9527(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5746, loss_cls=0.0960, loss_bbox=0.7336, matched_ious=0.5085, d_time=0.01(0.01), f_time=1.26(1.37), b_time=1.27(1.38)  Time cost: 44:12/44:54 [11:07:06/18:34:01]  Acc_iter 28950       Data time: 0.01(0.01)  Forward time: 1.26(1.37)  Batch time: 1.27(1.38)
2025-09-02 08:52:48,596   INFO  Train:    8/20 ( 40%) [1965/3862 ( 51%)]  Loss: 1.247 (1.40)  LR: 9.917e-04  Grad: 19.2434  max=0.6969(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8199(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5605, loss_cls=0.0952, loss_bbox=0.7260, matched_ious=0.5051, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.34(1.39)  Time cost: 45:22/43:47 [11:08:17/18:33:34]  Acc_iter 29000       Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.34(1.39)
2025-09-02 08:53:58,749   INFO  Train:    8/20 ( 40%) [2015/3862 ( 52%)]  Loss: 1.470 (1.40)  LR: 9.921e-04  Grad: 19.3280  max=0.6971(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8221(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5767, loss_cls=0.0955, loss_bbox=0.7585, matched_ious=0.5015, d_time=0.00(0.01), f_time=1.41(1.37), b_time=1.41(1.39)  Time cost: 46:33/42:38 [11:09:27/18:32:46]  Acc_iter 29050       Data time: 0.00(0.01)  Forward time: 1.41(1.37)  Batch time: 1.41(1.39)
2025-09-02 08:55:07,245   INFO  Train:    8/20 ( 40%) [2065/3862 ( 53%)]  Loss: 1.513 (1.40)  LR: 9.925e-04  Grad: 19.4752  max=1.1654(module.vfe.pfn_layers.0.linear.weight)  min: -0.8248(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5529, loss_cls=0.0920, loss_bbox=0.6717, matched_ious=0.5147, d_time=0.00(0.01), f_time=1.40(1.37), b_time=1.41(1.39)  Time cost: 47:41/41:28 [11:10:36/18:31:19]  Acc_iter 29100       Data time: 0.00(0.01)  Forward time: 1.40(1.37)  Batch time: 1.41(1.39)
2025-09-02 08:56:15,712   INFO  Train:    8/20 ( 40%) [2115/3862 ( 55%)]  Loss: 1.498 (1.40)  LR: 9.929e-04  Grad: 19.4598  max=0.7119(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8268(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5686, loss_cls=0.0942, loss_bbox=0.7029, matched_ious=0.5070, d_time=0.00(0.01), f_time=1.39(1.37), b_time=1.39(1.38)  Time cost: 48:50/40:19 [11:11:44/18:29:51]  Acc_iter 29150       Data time: 0.00(0.01)  Forward time: 1.39(1.37)  Batch time: 1.39(1.38)
2025-09-02 08:57:24,142   INFO  Train:    8/20 ( 40%) [2165/3862 ( 56%)]  Loss: 1.467 (1.39)  LR: 9.933e-04  Grad: 19.5024  max=0.7104(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8361(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5760, loss_cls=0.0949, loss_bbox=0.7146, matched_ious=0.5076, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.37(1.38)  Time cost: 49:58/39:09 [11:12:53/18:28:25]  Acc_iter 29200       Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.37(1.38)
2025-09-02 08:58:34,421   INFO  Train:    8/20 ( 40%) [2215/3862 ( 57%)]  Loss: 1.309 (1.39)  LR: 9.937e-04  Grad: 19.5358  max=0.7171(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8409(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5655, loss_cls=0.0959, loss_bbox=0.6969, matched_ious=0.5078, d_time=0.00(0.01), f_time=1.35(1.37), b_time=1.35(1.38)  Time cost: 51:08/38:00 [11:14:03/18:27:38]  Acc_iter 29250       Data time: 0.00(0.01)  Forward time: 1.35(1.37)  Batch time: 1.35(1.38)
2025-09-02 08:59:44,829   INFO  Train:    8/20 ( 40%) [2265/3862 ( 59%)]  Loss: 1.798 (1.39)  LR: 9.941e-04  Grad: 19.6092  max=0.7191(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8488(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5640, loss_cls=0.0952, loss_bbox=0.7066, matched_ious=0.5091, d_time=0.00(0.01), f_time=1.41(1.37), b_time=1.42(1.39)  Time cost: 52:19/36:52 [11:15:13/18:26:54]  Acc_iter 29300       Data time: 0.00(0.01)  Forward time: 1.41(1.37)  Batch time: 1.42(1.39)
2025-09-02 09:00:55,427   INFO  Train:    8/20 ( 40%) [2315/3862 ( 60%)]  Loss: 1.445 (1.39)  LR: 9.944e-04  Grad: 19.7219  max=1.0125(module.vfe.pfn_layers.0.linear.weight)  min: -0.9532(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5628, loss_cls=0.0946, loss_bbox=0.7370, matched_ious=0.5112, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 53:29/35:44 [11:16:24/18:26:12]  Acc_iter 29350       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 09:02:03,984   INFO  Train:    8/20 ( 40%) [2365/3862 ( 61%)]  Loss: 1.344 (1.39)  LR: 9.948e-04  Grad: 19.7552  max=0.7214(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8447(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5642, loss_cls=0.0952, loss_bbox=0.7183, matched_ious=0.5151, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.32(1.39)  Time cost: 54:38/34:34 [11:17:33/18:24:48]  Acc_iter 29400       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.32(1.39)
2025-09-02 09:03:12,660   INFO  Train:    8/20 ( 40%) [2415/3862 ( 63%)]  Loss: 1.323 (1.39)  LR: 9.951e-04  Grad: 18.9290  max=1.4132(module.vfe.pfn_layers.0.linear.weight)  min: -0.8097(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5773, loss_cls=0.0964, loss_bbox=0.7202, matched_ious=0.5097, d_time=0.01(0.01), f_time=1.25(1.37), b_time=1.25(1.39)  Time cost: 55:47/33:24 [11:18:41/18:23:27]  Acc_iter 29450       Data time: 0.01(0.01)  Forward time: 1.25(1.37)  Batch time: 1.25(1.39)
2025-09-02 09:04:20,852   INFO  Train:    8/20 ( 40%) [2465/3862 ( 64%)]  Loss: 1.405 (1.39)  LR: 9.955e-04  Grad: 18.9614  max=0.7035(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8101(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5514, loss_cls=0.0913, loss_bbox=0.7126, matched_ious=0.5137, d_time=0.01(0.01), f_time=1.32(1.37), b_time=1.33(1.39)  Time cost: 56:55/32:14 [11:19:49/18:21:57]  Acc_iter 29500       Data time: 0.01(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.39)
2025-09-02 09:05:31,926   INFO  Train:    8/20 ( 40%) [2515/3862 ( 65%)]  Loss: 1.332 (1.39)  LR: 9.958e-04  Grad: 18.9962  max=0.7035(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8118(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5394, loss_cls=0.0926, loss_bbox=0.6753, matched_ious=0.5116, d_time=0.01(0.01), f_time=1.36(1.38), b_time=1.37(1.39)  Time cost: 58:06/31:06 [11:21:00/18:21:22]  Acc_iter 29550       Data time: 0.01(0.01)  Forward time: 1.36(1.38)  Batch time: 1.37(1.39)
2025-09-02 09:06:39,511   INFO  Train:    8/20 ( 40%) [2565/3862 ( 66%)]  Loss: 1.312 (1.39)  LR: 9.961e-04  Grad: 19.0535  max=0.7089(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8191(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5540, loss_cls=0.0943, loss_bbox=0.6973, matched_ious=0.5124, d_time=0.01(0.01), f_time=1.28(1.37), b_time=1.29(1.39)  Time cost: 59:13/29:56 [11:22:08/18:19:41]  Acc_iter 29600       Data time: 0.01(0.01)  Forward time: 1.28(1.37)  Batch time: 1.29(1.39)
2025-09-02 09:07:48,754   INFO  Train:    8/20 ( 40%) [2615/3862 ( 68%)]  Loss: 1.182 (1.39)  LR: 9.964e-04  Grad: 19.0979  max=0.7046(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8182(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5743, loss_cls=0.0974, loss_bbox=0.7100, matched_ious=0.5162, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.37(1.39)  Time cost: 1:00:23/28:47 [11:23:17/18:18:32]  Acc_iter 29650       Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.37(1.39)
2025-09-02 09:08:57,931   INFO  Train:    8/20 ( 40%) [2665/3862 ( 69%)]  Loss: 1.260 (1.39)  LR: 9.967e-04  Grad: 19.1978  max=1.1157(module.vfe.pfn_layers.0.linear.weight)  min: -0.8185(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5686, loss_cls=0.0946, loss_bbox=0.7282, matched_ious=0.5065, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.31(1.39)  Time cost: 1:01:32/27:37 [11:24:26/18:17:21]  Acc_iter 29700       Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.31(1.39)
2025-09-02 09:10:06,349   INFO  Train:    8/20 ( 40%) [2715/3862 ( 70%)]  Loss: 1.385 (1.39)  LR: 9.969e-04  Grad: 19.3186  max=0.9154(module.vfe.pfn_layers.0.linear.weight)  min: -1.1683(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5454, loss_cls=0.0904, loss_bbox=0.6900, matched_ious=0.5129, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.34(1.38)  Time cost: 1:02:40/26:28 [11:25:35/18:15:58]  Acc_iter 29750       Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.34(1.38)
2025-09-02 09:11:14,767   INFO  Train:    8/20 ( 40%) [2765/3862 ( 72%)]  Loss: 1.335 (1.39)  LR: 9.972e-04  Grad: 19.4085  max=1.7293(module.vfe.pfn_layers.0.linear.weight)  min: -0.9587(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5442, loss_cls=0.0902, loss_bbox=0.7008, matched_ious=0.5156, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.34(1.38)  Time cost: 1:03:49/25:18 [11:26:43/18:14:34]  Acc_iter 29800       Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.34(1.38)
2025-09-02 09:12:25,893   INFO  Train:    8/20 ( 40%) [2815/3862 ( 73%)]  Loss: 1.301 (1.39)  LR: 9.975e-04  Grad: 19.5053  max=1.3301(module.vfe.pfn_layers.0.linear.weight)  min: -1.7605(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5476, loss_cls=0.0907, loss_bbox=0.6969, matched_ious=0.5131, d_time=0.01(0.01), f_time=1.34(1.37), b_time=1.34(1.39)  Time cost: 1:05:00/24:10 [11:27:54/18:13:57]  Acc_iter 29850       Data time: 0.01(0.01)  Forward time: 1.34(1.37)  Batch time: 1.34(1.39)
2025-09-02 09:13:35,478   INFO  Train:    8/20 ( 40%) [2865/3862 ( 74%)]  Loss: 1.287 (1.39)  LR: 9.977e-04  Grad: 19.4077  max=0.7160(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8292(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5496, loss_cls=0.0919, loss_bbox=0.7013, matched_ious=0.5176, d_time=0.00(0.01), f_time=1.27(1.37), b_time=1.28(1.39)  Time cost: 1:06:09/23:00 [11:29:04/18:12:54]  Acc_iter 29900       Data time: 0.00(0.01)  Forward time: 1.27(1.37)  Batch time: 1.28(1.39)
2025-09-02 09:14:44,395   INFO  Train:    8/20 ( 40%) [2915/3862 ( 75%)]  Loss: 1.319 (1.38)  LR: 9.979e-04  Grad: 20.7862  max=0.8757(module.vfe.pfn_layers.0.linear.weight)  min: -5.5314(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5552, loss_cls=0.0926, loss_bbox=0.6902, matched_ious=0.5223, d_time=0.01(0.01), f_time=1.37(1.37), b_time=1.38(1.39)  Time cost: 1:07:18/21:51 [11:30:13/18:11:39]  Acc_iter 29950       Data time: 0.01(0.01)  Forward time: 1.37(1.37)  Batch time: 1.38(1.39)
2025-09-02 09:15:52,780   INFO  Train:    8/20 ( 40%) [2965/3862 ( 77%)]  Loss: 1.041 (1.38)  LR: 9.981e-04  Grad: 19.5161  max=0.7214(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8374(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5602, loss_cls=0.0940, loss_bbox=0.6997, matched_ious=0.5106, d_time=0.00(0.01), f_time=1.42(1.37), b_time=1.42(1.38)  Time cost: 1:08:27/20:42 [11:31:21/18:10:16]  Acc_iter 30000       Data time: 0.00(0.01)  Forward time: 1.42(1.37)  Batch time: 1.42(1.38)
2025-09-02 09:17:01,388   INFO  Train:    8/20 ( 40%) [3015/3862 ( 78%)]  Loss: 1.411 (1.38)  LR: 9.983e-04  Grad: 19.6953  max=1.8856(module.vfe.pfn_layers.0.linear.weight)  min: -0.8430(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5495, loss_cls=0.0920, loss_bbox=0.6879, matched_ious=0.5117, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.39(1.38)  Time cost: 1:09:35/19:32 [11:32:30/18:08:57]  Acc_iter 30050       Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.39(1.38)
2025-09-02 09:18:12,795   INFO  Train:    8/20 ( 40%) [3065/3862 ( 79%)]  Loss: 1.358 (1.38)  LR: 9.985e-04  Grad: 19.6254  max=0.7334(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8418(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5417, loss_cls=0.0926, loss_bbox=0.7024, matched_ious=0.5108, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.34(1.39)  Time cost: 1:10:47/18:24 [11:33:41/18:08:21]  Acc_iter 30100       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.34(1.39)
2025-09-02 09:19:22,161   INFO  Train:    8/20 ( 40%) [3115/3862 ( 81%)]  Loss: 1.536 (1.38)  LR: 9.987e-04  Grad: 14.7914  max=1.5655(module.vfe.pfn_layers.0.linear.weight)  min: -1.3000(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5531, loss_cls=0.0927, loss_bbox=0.6913, matched_ious=0.5169, d_time=0.03(0.01), f_time=1.35(1.38), b_time=1.38(1.39)  Time cost: 1:11:56/17:14 [11:34:51/18:07:13]  Acc_iter 30150       Data time: 0.03(0.01)  Forward time: 1.35(1.38)  Batch time: 1.38(1.39)
2025-09-02 09:20:31,591   INFO  Train:    8/20 ( 40%) [3165/3862 ( 82%)]  Loss: 1.702 (1.38)  LR: 9.989e-04  Grad: 14.7290  max=0.6492(module.vfe.pfn_layers.0.linear.weight)  min: -0.6291(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5535, loss_cls=0.0913, loss_bbox=0.7133, matched_ious=0.5145, d_time=0.01(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 1:13:05/16:05 [11:36:00/18:06:06]  Acc_iter 30200       Data time: 0.01(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 09:21:40,421   INFO  Train:    8/20 ( 40%) [3215/3862 ( 83%)]  Loss: 1.179 (1.38)  LR: 9.990e-04  Grad: 14.8750  max=0.9333(module.vfe.pfn_layers.0.linear.weight)  min: -1.4273(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5489, loss_cls=0.0906, loss_bbox=0.6684, matched_ious=0.5210, d_time=0.01(0.01), f_time=1.31(1.38), b_time=1.32(1.39)  Time cost: 1:14:14/14:56 [11:37:09/18:04:51]  Acc_iter 30250       Data time: 0.01(0.01)  Forward time: 1.31(1.38)  Batch time: 1.32(1.39)
2025-09-02 09:22:47,930   INFO  Train:    8/20 ( 40%) [3265/3862 ( 85%)]  Loss: 1.481 (1.38)  LR: 9.992e-04  Grad: 14.8017  max=0.7026(module.vfe.pfn_layers.0.linear.weight)  min: -0.6353(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5528, loss_cls=0.0934, loss_bbox=0.6782, matched_ious=0.5157, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.34(1.38)  Time cost: 1:15:22/13:46 [11:38:16/18:03:16]  Acc_iter 30300       Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.34(1.38)
2025-09-02 09:23:59,498   INFO  Train:    8/20 ( 40%) [3315/3862 ( 86%)]  Loss: 1.695 (1.38)  LR: 9.993e-04  Grad: 14.8811  max=0.8013(module.vfe.pfn_layers.0.linear.weight)  min: -0.6386(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5700, loss_cls=0.0949, loss_bbox=0.7554, matched_ious=0.5076, d_time=0.01(0.01), f_time=1.39(1.38), b_time=1.40(1.39)  Time cost: 1:16:33/12:37 [11:39:28/18:02:40]  Acc_iter 30350       Data time: 0.01(0.01)  Forward time: 1.39(1.38)  Batch time: 1.40(1.39)
2025-09-02 09:25:08,590   INFO  Train:    8/20 ( 40%) [3365/3862 ( 87%)]  Loss: 1.201 (1.38)  LR: 9.994e-04  Grad: 14.9398  max=0.8514(module.vfe.pfn_layers.0.linear.weight)  min: -0.6388(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5445, loss_cls=0.0932, loss_bbox=0.7056, matched_ious=0.5124, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.33(1.39)  Time cost: 1:17:42/11:28 [11:40:37/18:01:29]  Acc_iter 30400       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.33(1.39)
2025-09-02 09:26:17,114   INFO  Train:    8/20 ( 40%) [3415/3862 ( 88%)]  Loss: 1.169 (1.38)  LR: 9.995e-04  Grad: 16.6311  max=4.4300(module.vfe.pfn_layers.0.linear.weight)  min: -3.8201(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5511, loss_cls=0.0926, loss_bbox=0.7011, matched_ious=0.5160, d_time=0.00(0.01), f_time=1.38(1.38), b_time=1.38(1.39)  Time cost: 1:18:51/10:19 [11:41:46/18:00:09]  Acc_iter 30450       Data time: 0.00(0.01)  Forward time: 1.38(1.38)  Batch time: 1.38(1.39)
2025-09-02 09:27:25,422   INFO  Train:    8/20 ( 40%) [3465/3862 ( 90%)]  Loss: 1.262 (1.38)  LR: 9.996e-04  Grad: 15.1793  max=1.6452(module.vfe.pfn_layers.0.linear.weight)  min: -0.6476(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5519, loss_cls=0.0925, loss_bbox=0.7001, matched_ious=0.5140, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.39(1.38)  Time cost: 1:19:59/09:09 [11:42:54/17:58:47]  Acc_iter 30500       Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.39(1.38)
2025-09-02 09:28:34,041   INFO  Train:    8/20 ( 40%) [3515/3862 ( 91%)]  Loss: 1.692 (1.38)  LR: 9.997e-04  Grad: 25.9378  max=4.1127(module.vfe.pfn_layers.0.linear.weight)  min: -18.3357(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5465, loss_cls=0.0908, loss_bbox=0.7022, matched_ious=0.5113, d_time=0.00(0.01), f_time=1.23(1.37), b_time=1.24(1.38)  Time cost: 1:21:08/08:00 [11:44:03/17:57:30]  Acc_iter 30550       Data time: 0.00(0.01)  Forward time: 1.23(1.37)  Batch time: 1.24(1.38)
2025-09-02 09:29:44,084   INFO  Train:    8/20 ( 40%) [3565/3862 ( 92%)]  Loss: 0.9440 (1.38)  LR: 9.998e-04  Grad: 15.1843  max=0.5721(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6524(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5492, loss_cls=0.0920, loss_bbox=0.6791, matched_ious=0.5140, d_time=0.00(0.01), f_time=1.38(1.38), b_time=1.38(1.38)  Time cost: 1:22:18/06:51 [11:45:13/17:56:31]  Acc_iter 30600       Data time: 0.00(0.01)  Forward time: 1.38(1.38)  Batch time: 1.38(1.38)
2025-09-02 09:30:54,211   INFO  Train:    8/20 ( 40%) [3615/3862 ( 94%)]  Loss: 1.759 (1.38)  LR: 9.999e-04  Grad: 15.2800  max=0.6876(module.vfe.pfn_layers.0.linear.weight)  min: -0.6569(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5750, loss_cls=0.0981, loss_bbox=0.7267, matched_ious=0.5106, d_time=0.00(0.01), f_time=1.44(1.38), b_time=1.44(1.39)  Time cost: 1:23:28/05:42 [11:46:23/17:55:33]  Acc_iter 30650       Data time: 0.00(0.01)  Forward time: 1.44(1.38)  Batch time: 1.44(1.39)
2025-09-02 09:32:03,194   INFO  Train:    8/20 ( 40%) [3665/3862 ( 95%)]  Loss: 1.486 (1.38)  LR: 9.999e-04  Grad: 15.3034  max=0.5804(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6572(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5493, loss_cls=0.0905, loss_bbox=0.6704, matched_ious=0.5161, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.35(1.39)  Time cost: 1:24:37/04:32 [11:47:32/17:54:20]  Acc_iter 30700       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.35(1.39)
2025-09-02 09:33:11,801   INFO  Train:    8/20 ( 40%) [3715/3862 ( 96%)]  Loss: 1.142 (1.38)  LR: 9.999e-04  Grad: 15.4956  max=1.2515(module.vfe.pfn_layers.0.linear.weight)  min: -1.7814(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5480, loss_cls=0.0918, loss_bbox=0.7086, matched_ious=0.5144, d_time=0.01(0.01), f_time=1.27(1.37), b_time=1.28(1.38)  Time cost: 1:25:46/03:23 [11:48:40/17:53:03]  Acc_iter 30750       Data time: 0.01(0.01)  Forward time: 1.27(1.37)  Batch time: 1.28(1.38)
2025-09-02 09:34:19,540   INFO  Train:    8/20 ( 40%) [3765/3862 ( 97%)]  Loss: 1.679 (1.38)  LR: 1.000e-03  Grad: 15.3974  max=0.5825(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6650(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5525, loss_cls=0.0930, loss_bbox=0.7001, matched_ious=0.5168, d_time=0.01(0.01), f_time=1.44(1.37), b_time=1.45(1.38)  Time cost: 1:26:53/02:14 [11:49:48/17:51:35]  Acc_iter 30800       Data time: 0.01(0.01)  Forward time: 1.44(1.37)  Batch time: 1.45(1.38)
2025-09-02 09:35:29,937   INFO  Train:    8/20 ( 40%) [3815/3862 ( 99%)]  Loss: 1.040 (1.38)  LR: 1.000e-03  Grad: 15.5433  max=1.5934(module.vfe.pfn_layers.0.linear.weight)  min: -0.6689(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5350, loss_cls=0.0908, loss_bbox=0.6890, matched_ious=0.5147, d_time=0.00(0.01), f_time=1.44(1.37), b_time=1.45(1.38)  Time cost: 1:28:04/01:05 [11:50:59/17:50:40]  Acc_iter 30850       Data time: 0.00(0.01)  Forward time: 1.44(1.37)  Batch time: 1.45(1.38)
2025-09-02 09:36:33,154   INFO  Train:    8/20 ( 40%) [3861/3862 (100%)]  Loss: 1.152 (1.37)  LR: 1.000e-03  Grad: 15.5054  max=0.5897(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6714(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5656, loss_cls=0.0944, loss_bbox=0.6746, matched_ious=0.5199, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.31(1.38)  Time cost: 1:29:07/00:01 [11:52:02/17:49:31]  Acc_iter 30896       Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.31(1.38)

                                               [Aepochs:  40%|████      | 8/20 [11:52:03<17:49:24, 5347.03s/it]epochs:  40%|████      | 8/20 [11:52:03<17:49:24, 5347.03s/it]epochs:  40%|████      | 8/20 [11:52:03<17:49:24, 5347.03s/it]epochs:  40%|████      | 8/20 [11:52:03<17:49:24, 5347.04s/it]epochs:  40%|████      | 8/20 [11:52:03<17:49:24, 5347.04s/it]epochs:  40%|████      | 8/20 [11:52:03<17:49:24, 5347.04s/it]epochs:  40%|████      | 8/20 [11:52:03<17:49:24, 5347.04s/it]epochs:  40%|████      | 8/20 [11:52:03<17:49:24, 5347.02s/it]
train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-02 09:36:41,594   INFO  Train:    9/20 ( 45%) [   0/3862 (  0%)]  Loss: 1.761 (1.76)  LR: 1.000e-03  Grad: 15.5040  max=0.5892(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6712(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7268, loss_cls=0.1117, loss_bbox=0.9222, matched_ious=0.4917, d_time=3.56(3.56), f_time=2.95(2.95), b_time=6.51(6.51)  Time cost: 00:06/6:48:11 [11:52:10/81:38:22]  Acc_iter 30897       Data time: 3.56(3.56)  Forward time: 2.95(2.95)  Batch time: 6.51(6.51)
2025-09-02 09:36:45,852   INFO  Train:    9/20 ( 45%) [   3/3862 (  0%)]  Loss: 1.455 (1.52)  LR: 1.000e-03  Grad: 16.4789  max=3.3269(module.vfe.pfn_layers.0.linear.weight)  min: -4.3112(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5582, loss_cls=0.1020, loss_bbox=0.7757, matched_ious=0.5301, d_time=0.00(0.90), f_time=1.27(1.79), b_time=1.27(2.69)  Time cost: 00:10/2:50:25 [11:52:14/34:06:33]  Acc_iter 30900       Data time: 0.00(0.90)  Forward time: 1.27(1.79)  Batch time: 1.27(2.69)
2025-09-02 09:37:56,849   INFO  Train:    9/20 ( 45%) [  53/3862 (  1%)]  Loss: 1.466 (1.32)  LR: 1.000e-03  Grad: 15.5982  max=0.5894(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1005(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5444, loss_cls=0.0916, loss_bbox=0.6731, matched_ious=0.5247, d_time=0.00(0.07), f_time=1.36(1.44), b_time=1.36(1.51)  Time cost: 01:21/1:35:55 [11:53:25/19:25:47]  Acc_iter 30950       Data time: 0.00(0.07)  Forward time: 1.36(1.44)  Batch time: 1.36(1.51)
2025-09-02 09:39:04,607   INFO  Train:    9/20 ( 45%) [ 103/3862 (  3%)]  Loss: 1.144 (1.32)  LR: 1.000e-03  Grad: 15.8263  max=1.9606(module.vfe.pfn_layers.0.linear.weight)  min: -1.2025(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5448, loss_cls=0.0913, loss_bbox=0.6808, matched_ious=0.5191, d_time=0.01(0.04), f_time=1.30(1.40), b_time=1.31(1.44)  Time cost: 02:29/1:29:57 [11:54:33/18:26:42]  Acc_iter 31000       Data time: 0.01(0.04)  Forward time: 1.30(1.40)  Batch time: 1.31(1.44)
2025-09-02 09:40:12,712   INFO  Train:    9/20 ( 45%) [ 153/3862 (  4%)]  Loss: 1.125 (1.33)  LR: 1.000e-03  Grad: 15.6800  max=0.5885(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6776(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5488, loss_cls=0.0906, loss_bbox=0.7213, matched_ious=0.5149, d_time=0.01(0.03), f_time=1.35(1.38), b_time=1.36(1.41)  Time cost: 03:37/1:27:17 [11:55:41/18:07:03]  Acc_iter 31050       Data time: 0.01(0.03)  Forward time: 1.35(1.38)  Batch time: 1.36(1.41)
2025-09-02 09:41:22,757   INFO  Train:    9/20 ( 45%) [ 203/3862 (  5%)]  Loss: 1.543 (1.34)  LR: 1.000e-03  Grad: 15.7716  max=1.1545(module.vfe.pfn_layers.0.linear.weight)  min: -0.6782(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5545, loss_cls=0.0912, loss_bbox=0.7052, matched_ious=0.5176, d_time=0.00(0.02), f_time=1.36(1.39), b_time=1.37(1.41)  Time cost: 04:47/1:25:56 [11:56:51/18:03:47]  Acc_iter 31100       Data time: 0.00(0.02)  Forward time: 1.36(1.39)  Batch time: 1.37(1.41)
2025-09-02 09:42:32,551   INFO  Train:    9/20 ( 45%) [ 253/3862 (  7%)]  Loss: 1.253 (1.33)  LR: 9.999e-04  Grad: 15.9094  max=1.4521(module.vfe.pfn_layers.0.linear.weight)  min: -1.2836(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5279, loss_cls=0.0901, loss_bbox=0.6749, matched_ious=0.5203, d_time=0.00(0.02), f_time=1.34(1.38), b_time=1.34(1.41)  Time cost: 05:57/1:24:36 [11:58:01/18:00:35]  Acc_iter 31150       Data time: 0.00(0.02)  Forward time: 1.34(1.38)  Batch time: 1.34(1.41)
2025-09-02 09:43:42,124   INFO  Train:    9/20 ( 45%) [ 303/3862 (  8%)]  Loss: 1.251 (1.33)  LR: 9.999e-04  Grad: 15.8618  max=0.7875(module.vfe.pfn_layers.0.linear.weight)  min: -0.6814(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5384, loss_cls=0.0902, loss_bbox=0.7080, matched_ious=0.5173, d_time=0.00(0.02), f_time=1.44(1.38), b_time=1.44(1.40)  Time cost: 07:06/1:23:17 [11:59:11/17:57:29]  Acc_iter 31200       Data time: 0.00(0.02)  Forward time: 1.44(1.38)  Batch time: 1.44(1.40)
2025-09-02 09:44:49,607   INFO  Train:    9/20 ( 45%) [ 353/3862 (  9%)]  Loss: 1.413 (1.33)  LR: 9.999e-04  Grad: 15.9509  max=1.1778(module.vfe.pfn_layers.0.linear.weight)  min: -0.6814(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5291, loss_cls=0.0898, loss_bbox=0.6914, matched_ious=0.5143, d_time=0.01(0.02), f_time=1.37(1.37), b_time=1.37(1.40)  Time cost: 08:14/1:21:40 [12:00:18/17:50:25]  Acc_iter 31250       Data time: 0.01(0.02)  Forward time: 1.37(1.37)  Batch time: 1.37(1.40)
2025-09-02 09:45:57,784   INFO  Train:    9/20 ( 45%) [ 403/3862 ( 10%)]  Loss: 1.150 (1.33)  LR: 9.998e-04  Grad: 15.9960  max=0.5983(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6871(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5545, loss_cls=0.0915, loss_bbox=0.6915, matched_ious=0.5119, d_time=0.00(0.02), f_time=1.43(1.37), b_time=1.43(1.39)  Time cost: 09:22/1:20:16 [12:01:26/17:46:08]  Acc_iter 31300       Data time: 0.00(0.02)  Forward time: 1.43(1.37)  Batch time: 1.43(1.39)
2025-09-02 09:47:06,716   INFO  Train:    9/20 ( 45%) [ 453/3862 ( 12%)]  Loss: 1.674 (1.33)  LR: 9.998e-04  Grad: 16.0693  max=0.7130(module.vfe.pfn_layers.0.linear.weight)  min: -0.7766(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5600, loss_cls=0.0933, loss_bbox=0.6877, matched_ious=0.5135, d_time=0.00(0.02), f_time=2.22(1.37), b_time=2.23(1.39)  Time cost: 10:31/1:19:01 [12:02:35/17:43:49]  Acc_iter 31350       Data time: 0.00(0.02)  Forward time: 2.22(1.37)  Batch time: 2.23(1.39)
2025-09-02 09:48:17,338   INFO  Train:    9/20 ( 45%) [ 503/3862 ( 13%)]  Loss: 1.212 (1.33)  LR: 9.997e-04  Grad: 16.2898  max=0.6009(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.5595(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5359, loss_cls=0.0906, loss_bbox=0.6617, matched_ious=0.5234, d_time=0.01(0.02), f_time=1.32(1.38), b_time=1.33(1.39)  Time cost: 11:42/1:17:59 [12:03:46/17:44:17]  Acc_iter 31400       Data time: 0.01(0.02)  Forward time: 1.32(1.38)  Batch time: 1.33(1.39)
2025-09-02 09:49:26,438   INFO  Train:    9/20 ( 45%) [ 553/3862 ( 14%)]  Loss: 1.313 (1.33)  LR: 9.996e-04  Grad: 16.1921  max=0.6101(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0550(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5395, loss_cls=0.0896, loss_bbox=0.6962, matched_ious=0.5178, d_time=0.00(0.02), f_time=1.43(1.38), b_time=1.43(1.39)  Time cost: 12:51/1:16:46 [12:04:55/17:42:22]  Acc_iter 31450       Data time: 0.00(0.02)  Forward time: 1.43(1.38)  Batch time: 1.43(1.39)
2025-09-02 09:50:34,946   INFO  Train:    9/20 ( 45%) [ 603/3862 ( 16%)]  Loss: 0.9442 (1.32)  LR: 9.996e-04  Grad: 16.3466  max=1.2617(module.vfe.pfn_layers.0.linear.weight)  min: -1.3246(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5476, loss_cls=0.0912, loss_bbox=0.6747, matched_ious=0.5179, d_time=0.00(0.02), f_time=1.40(1.37), b_time=1.40(1.39)  Time cost: 13:59/1:15:30 [12:06:04/17:39:50]  Acc_iter 31500       Data time: 0.00(0.02)  Forward time: 1.40(1.37)  Batch time: 1.40(1.39)
2025-09-02 09:51:43,622   INFO  Train:    9/20 ( 45%) [ 653/3862 ( 17%)]  Loss: 1.339 (1.32)  LR: 9.995e-04  Grad: 16.2852  max=0.6161(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6922(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5417, loss_cls=0.0915, loss_bbox=0.6762, matched_ious=0.5215, d_time=0.00(0.02), f_time=1.35(1.37), b_time=1.35(1.39)  Time cost: 15:08/1:14:17 [12:07:12/17:37:42]  Acc_iter 31550       Data time: 0.00(0.02)  Forward time: 1.35(1.37)  Batch time: 1.35(1.39)
2025-09-02 09:52:52,740   INFO  Train:    9/20 ( 45%) [ 703/3862 ( 18%)]  Loss: 1.268 (1.32)  LR: 9.994e-04  Grad: 16.3412  max=0.6155(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6953(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5490, loss_cls=0.0926, loss_bbox=0.6697, matched_ious=0.5124, d_time=0.00(0.01), f_time=1.35(1.37), b_time=1.36(1.39)  Time cost: 16:17/1:13:06 [12:08:21/17:36:11]  Acc_iter 31600       Data time: 0.00(0.01)  Forward time: 1.35(1.37)  Batch time: 1.36(1.39)
2025-09-02 09:54:01,912   INFO  Train:    9/20 ( 45%) [ 753/3862 ( 19%)]  Loss: 1.009 (1.32)  LR: 9.993e-04  Grad: 16.3484  max=0.6012(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6968(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5401, loss_cls=0.0902, loss_bbox=0.6612, matched_ious=0.5214, d_time=0.00(0.01), f_time=1.40(1.37), b_time=1.40(1.39)  Time cost: 17:26/1:11:55 [12:09:30/17:34:46]  Acc_iter 31650       Data time: 0.00(0.01)  Forward time: 1.40(1.37)  Batch time: 1.40(1.39)
2025-09-02 09:55:13,760   INFO  Train:    9/20 ( 45%) [ 803/3862 ( 21%)]  Loss: 1.193 (1.32)  LR: 9.993e-04  Grad: 16.5021  max=1.4890(module.vfe.pfn_layers.0.linear.weight)  min: -0.6977(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5414, loss_cls=0.0905, loss_bbox=0.7159, matched_ious=0.5170, d_time=0.01(0.01), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 18:38/1:10:55 [12:10:42/17:35:55]  Acc_iter 31700       Data time: 0.01(0.01)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 09:56:22,886   INFO  Train:    9/20 ( 45%) [ 853/3862 ( 22%)]  Loss: 1.371 (1.32)  LR: 9.992e-04  Grad: 17.1252  max=4.1054(module.vfe.pfn_layers.0.linear.weight)  min: -0.7364(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5227, loss_cls=0.0878, loss_bbox=0.6464, matched_ious=0.5213, d_time=0.01(0.01), f_time=1.38(1.38), b_time=1.38(1.39)  Time cost: 19:47/1:09:44 [12:11:51/17:34:22]  Acc_iter 31750       Data time: 0.01(0.01)  Forward time: 1.38(1.38)  Batch time: 1.38(1.39)
2025-09-02 09:57:30,955   INFO  Train:    9/20 ( 45%) [ 903/3862 ( 23%)]  Loss: 1.106 (1.32)  LR: 9.991e-04  Grad: 16.5430  max=0.6284(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7072(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5551, loss_cls=0.0921, loss_bbox=0.7205, matched_ious=0.5133, d_time=0.01(0.01), f_time=1.32(1.38), b_time=1.33(1.39)  Time cost: 20:55/1:08:30 [12:13:00/17:31:59]  Acc_iter 31800       Data time: 0.01(0.01)  Forward time: 1.32(1.38)  Batch time: 1.33(1.39)
2025-09-02 09:58:39,816   INFO  Train:    9/20 ( 45%) [ 953/3862 ( 25%)]  Loss: 1.196 (1.32)  LR: 9.990e-04  Grad: 16.6904  max=2.0809(module.vfe.pfn_layers.0.linear.weight)  min: -0.7071(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5472, loss_cls=0.0910, loss_bbox=0.6832, matched_ious=0.5120, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 22:04/1:07:18 [12:14:08/17:30:22]  Acc_iter 31850       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 09:59:49,099   INFO  Train:    9/20 ( 45%) [1003/3862 ( 26%)]  Loss: 1.277 (1.32)  LR: 9.988e-04  Grad: 16.7160  max=0.6250(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.3979(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5323, loss_cls=0.0905, loss_bbox=0.6750, matched_ious=0.5209, d_time=0.01(0.01), f_time=1.35(1.38), b_time=1.37(1.39)  Time cost: 23:13/1:06:09 [12:15:18/17:29:06]  Acc_iter 31900       Data time: 0.01(0.01)  Forward time: 1.35(1.38)  Batch time: 1.37(1.39)
2025-09-02 10:00:59,072   INFO  Train:    9/20 ( 45%) [1053/3862 ( 27%)]  Loss: 1.268 (1.32)  LR: 9.987e-04  Grad: 16.7445  max=0.8707(module.vfe.pfn_layers.0.linear.weight)  min: -0.7137(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5436, loss_cls=0.0914, loss_bbox=0.6762, matched_ious=0.5200, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 24:23/1:05:01 [12:16:28/17:28:21]  Acc_iter 31950       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 10:02:10,298   INFO  Train:    9/20 ( 45%) [1103/3862 ( 29%)]  Loss: 1.168 (1.32)  LR: 9.986e-04  Grad: 17.5951  max=3.1643(module.vfe.pfn_layers.0.linear.weight)  min: -3.8788(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5534, loss_cls=0.0928, loss_bbox=0.6913, matched_ious=0.5226, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.33(1.39)  Time cost: 25:35/1:03:56 [12:17:39/17:28:24]  Acc_iter 32000       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.33(1.39)
2025-09-02 10:03:18,019   INFO  Train:    9/20 ( 45%) [1153/3862 ( 30%)]  Loss: 1.150 (1.32)  LR: 9.985e-04  Grad: 16.8568  max=0.8673(module.vfe.pfn_layers.0.linear.weight)  min: -0.7219(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5329, loss_cls=0.0904, loss_bbox=0.6937, matched_ious=0.5164, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 26:42/1:02:42 [12:18:47/17:26:04]  Acc_iter 32050       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 10:04:25,762   INFO  Train:    9/20 ( 45%) [1203/3862 ( 31%)]  Loss: 1.314 (1.32)  LR: 9.983e-04  Grad: 17.0242  max=1.7648(module.vfe.pfn_layers.0.linear.weight)  min: -0.7269(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5443, loss_cls=0.0920, loss_bbox=0.6531, matched_ious=0.5184, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 27:50/1:01:29 [12:19:54/17:23:51]  Acc_iter 32100       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 10:05:34,857   INFO  Train:    9/20 ( 45%) [1253/3862 ( 32%)]  Loss: 1.491 (1.32)  LR: 9.982e-04  Grad: 17.0370  max=1.4819(module.vfe.pfn_layers.0.linear.weight)  min: -0.7231(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5404, loss_cls=0.0890, loss_bbox=0.6928, matched_ious=0.5163, d_time=0.00(0.01), f_time=1.43(1.38), b_time=1.43(1.39)  Time cost: 28:59/1:00:19 [12:21:03/17:22:31]  Acc_iter 32150       Data time: 0.00(0.01)  Forward time: 1.43(1.38)  Batch time: 1.43(1.39)
2025-09-02 10:06:48,354   INFO  Train:    9/20 ( 45%) [1303/3862 ( 34%)]  Loss: 1.060 (1.32)  LR: 9.981e-04  Grad: 16.9901  max=0.6422(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7266(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5311, loss_cls=0.0868, loss_bbox=0.6507, matched_ious=0.5263, d_time=0.44(0.01), f_time=1.41(1.38), b_time=1.85(1.39)  Time cost: 30:13/59:18 [12:22:17/17:23:45]  Acc_iter 32200       Data time: 0.44(0.01)  Forward time: 1.41(1.38)  Batch time: 1.85(1.39)
2025-09-02 10:07:56,909   INFO  Train:    9/20 ( 45%) [1353/3862 ( 35%)]  Loss: 1.593 (1.32)  LR: 9.979e-04  Grad: 17.1160  max=0.6520(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2234(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5433, loss_cls=0.0905, loss_bbox=0.6920, matched_ious=0.5229, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.37(1.39)  Time cost: 31:21/58:06 [12:23:25/17:22:04]  Acc_iter 32250       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.37(1.39)
2025-09-02 10:09:05,970   INFO  Train:    9/20 ( 45%) [1403/3862 ( 36%)]  Loss: 1.574 (1.32)  LR: 9.977e-04  Grad: 17.0992  max=0.8380(module.vfe.pfn_layers.0.linear.weight)  min: -0.7203(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5387, loss_cls=0.0894, loss_bbox=0.7073, matched_ious=0.5153, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.34(1.39)  Time cost: 32:30/56:56 [12:24:35/17:20:41]  Acc_iter 32300       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.34(1.39)
2025-09-02 10:10:13,988   INFO  Train:    9/20 ( 45%) [1453/3862 ( 38%)]  Loss: 1.045 (1.32)  LR: 9.976e-04  Grad: 17.2123  max=1.2406(module.vfe.pfn_layers.0.linear.weight)  min: -0.7260(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5590, loss_cls=0.0936, loss_bbox=0.6853, matched_ious=0.5181, d_time=0.00(0.01), f_time=1.40(1.38), b_time=1.40(1.39)  Time cost: 33:38/55:44 [12:25:43/17:18:46]  Acc_iter 32350       Data time: 0.00(0.01)  Forward time: 1.40(1.38)  Batch time: 1.40(1.39)
2025-09-02 10:11:21,967   INFO  Train:    9/20 ( 45%) [1503/3862 ( 39%)]  Loss: 1.284 (1.32)  LR: 9.974e-04  Grad: 17.2479  max=0.6615(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0440(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5183, loss_cls=0.0887, loss_bbox=0.6533, matched_ious=0.5192, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 34:46/54:32 [12:26:51/17:16:54]  Acc_iter 32400       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 10:12:32,461   INFO  Train:    9/20 ( 45%) [1553/3862 ( 40%)]  Loss: 1.295 (1.32)  LR: 9.972e-04  Grad: 17.3200  max=0.6734(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7361(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5293, loss_cls=0.0880, loss_bbox=0.6792, matched_ious=0.5243, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.34(1.39)  Time cost: 35:57/53:25 [12:28:01/17:16:17]  Acc_iter 32450       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.34(1.39)
2025-09-02 10:13:43,477   INFO  Train:    9/20 ( 45%) [1603/3862 ( 42%)]  Loss: 1.410 (1.32)  LR: 9.971e-04  Grad: 17.3571  max=1.0079(module.vfe.pfn_layers.0.linear.weight)  min: -0.7344(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5393, loss_cls=0.0891, loss_bbox=0.6882, matched_ious=0.5178, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 37:08/52:18 [12:29:12/17:15:52]  Acc_iter 32500       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 10:14:52,368   INFO  Train:    9/20 ( 45%) [1653/3862 ( 43%)]  Loss: 1.425 (1.32)  LR: 9.969e-04  Grad: 17.5120  max=0.6572(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.0907(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5517, loss_cls=0.0928, loss_bbox=0.6647, matched_ious=0.5276, d_time=0.00(0.01), f_time=1.47(1.38), b_time=1.47(1.39)  Time cost: 38:17/51:07 [12:30:21/17:14:27]  Acc_iter 32550       Data time: 0.00(0.01)  Forward time: 1.47(1.38)  Batch time: 1.47(1.39)
2025-09-02 10:16:00,517   INFO  Train:    9/20 ( 45%) [1703/3862 ( 44%)]  Loss: 1.290 (1.32)  LR: 9.967e-04  Grad: 17.4884  max=0.6637(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7458(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5291, loss_cls=0.0884, loss_bbox=0.6664, matched_ious=0.5194, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 39:25/49:56 [12:31:29/17:12:44]  Acc_iter 32600       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 10:17:08,388   INFO  Train:    9/20 ( 45%) [1753/3862 ( 45%)]  Loss: 1.256 (1.31)  LR: 9.965e-04  Grad: 19.0367  max=6.2424(module.vfe.pfn_layers.0.linear.weight)  min: -3.5727(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5353, loss_cls=0.0906, loss_bbox=0.6419, matched_ious=0.5242, d_time=0.00(0.01), f_time=1.43(1.38), b_time=1.43(1.39)  Time cost: 40:33/48:45 [12:32:37/17:10:56]  Acc_iter 32650       Data time: 0.00(0.01)  Forward time: 1.43(1.38)  Batch time: 1.43(1.39)
2025-09-02 10:18:17,102   INFO  Train:    9/20 ( 45%) [1803/3862 ( 47%)]  Loss: 1.315 (1.31)  LR: 9.963e-04  Grad: 17.5996  max=0.9439(module.vfe.pfn_layers.0.linear.weight)  min: -0.7530(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5364, loss_cls=0.0906, loss_bbox=0.6551, matched_ious=0.5232, d_time=0.01(0.01), f_time=2.20(1.38), b_time=2.21(1.39)  Time cost: 41:41/47:35 [12:33:46/17:09:31]  Acc_iter 32700       Data time: 0.01(0.01)  Forward time: 2.20(1.38)  Batch time: 2.21(1.39)
2025-09-02 10:19:29,258   INFO  Train:    9/20 ( 45%) [1853/3862 ( 48%)]  Loss: 1.184 (1.31)  LR: 9.961e-04  Grad: 18.1481  max=1.9785(module.vfe.pfn_layers.0.linear.weight)  min: -3.8027(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5396, loss_cls=0.0908, loss_bbox=0.6584, matched_ious=0.5223, d_time=0.01(0.01), f_time=1.23(1.38), b_time=1.24(1.39)  Time cost: 42:54/46:29 [12:34:58/17:09:29]  Acc_iter 32750       Data time: 0.01(0.01)  Forward time: 1.23(1.38)  Batch time: 1.24(1.39)
2025-09-02 10:20:38,334   INFO  Train:    9/20 ( 45%) [1903/3862 ( 49%)]  Loss: 1.067 (1.31)  LR: 9.958e-04  Grad: 17.7799  max=0.7914(module.vfe.pfn_layers.0.linear.weight)  min: -1.7970(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5259, loss_cls=0.0888, loss_bbox=0.6708, matched_ious=0.5225, d_time=0.00(0.01), f_time=1.40(1.38), b_time=1.41(1.39)  Time cost: 44:03/45:19 [12:36:07/17:08:11]  Acc_iter 32800       Data time: 0.00(0.01)  Forward time: 1.40(1.38)  Batch time: 1.41(1.39)
2025-09-02 10:21:46,977   INFO  Train:    9/20 ( 45%) [1953/3862 ( 51%)]  Loss: 0.9878 (1.31)  LR: 9.956e-04  Grad: 15.4902  max=1.1314(module.vfe.pfn_layers.0.linear.weight)  min: -0.6626(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5248, loss_cls=0.0891, loss_bbox=0.6611, matched_ious=0.5212, d_time=0.00(0.01), f_time=1.41(1.38), b_time=1.42(1.39)  Time cost: 45:11/44:09 [12:37:16/17:06:45]  Acc_iter 32850       Data time: 0.00(0.01)  Forward time: 1.41(1.38)  Batch time: 1.42(1.39)
2025-09-02 10:22:54,644   INFO  Train:    9/20 ( 45%) [2003/3862 ( 52%)]  Loss: 1.204 (1.31)  LR: 9.954e-04  Grad: 15.4851  max=0.5925(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6635(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5315, loss_cls=0.0884, loss_bbox=0.6595, matched_ious=0.5255, d_time=0.00(0.01), f_time=1.45(1.38), b_time=1.45(1.39)  Time cost: 46:19/42:58 [12:38:23/17:04:57]  Acc_iter 32900       Data time: 0.00(0.01)  Forward time: 1.45(1.38)  Batch time: 1.45(1.39)
2025-09-02 10:24:02,258   INFO  Train:    9/20 ( 45%) [2053/3862 ( 53%)]  Loss: 1.214 (1.31)  LR: 9.952e-04  Grad: 15.5949  max=0.5953(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.3980(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5293, loss_cls=0.0879, loss_bbox=0.6681, matched_ious=0.5181, d_time=0.00(0.01), f_time=1.35(1.38), b_time=1.36(1.39)  Time cost: 47:27/41:47 [12:39:31/17:03:10]  Acc_iter 32950       Data time: 0.00(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.39)
2025-09-02 10:25:14,745   INFO  Train:    9/20 ( 45%) [2103/3862 ( 54%)]  Loss: 1.227 (1.31)  LR: 9.949e-04  Grad: 15.6082  max=0.5944(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6680(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5458, loss_cls=0.0913, loss_bbox=0.7221, matched_ious=0.5196, d_time=0.00(0.01), f_time=1.26(1.38), b_time=1.26(1.39)  Time cost: 48:39/40:40 [12:40:43/17:03:08]  Acc_iter 33000       Data time: 0.00(0.01)  Forward time: 1.26(1.38)  Batch time: 1.26(1.39)
2025-09-02 10:26:23,676   INFO  Train:    9/20 ( 45%) [2153/3862 ( 56%)]  Loss: 1.599 (1.31)  LR: 9.947e-04  Grad: 15.7668  max=1.4778(module.vfe.pfn_layers.0.linear.weight)  min: -1.0509(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5288, loss_cls=0.0886, loss_bbox=0.6434, matched_ious=0.5235, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.33(1.39)  Time cost: 49:48/39:31 [12:41:52/17:01:49]  Acc_iter 33050       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.33(1.39)
2025-09-02 10:27:31,155   INFO  Train:    9/20 ( 45%) [2203/3862 ( 57%)]  Loss: 1.080 (1.31)  LR: 9.944e-04  Grad: 15.8292  max=1.2410(module.vfe.pfn_layers.0.linear.weight)  min: -0.6768(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5331, loss_cls=0.0904, loss_bbox=0.6416, matched_ious=0.5244, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.37(1.39)  Time cost: 50:55/38:20 [12:43:00/17:00:02]  Acc_iter 33100       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.37(1.39)
2025-09-02 10:28:39,115   INFO  Train:    9/20 ( 45%) [2253/3862 ( 58%)]  Loss: 1.047 (1.31)  LR: 9.942e-04  Grad: 15.8136  max=0.5934(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9342(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5148, loss_cls=0.0859, loss_bbox=0.6733, matched_ious=0.5243, d_time=0.01(0.01), f_time=1.25(1.38), b_time=1.26(1.39)  Time cost: 52:03/37:09 [12:44:08/16:58:26]  Acc_iter 33150       Data time: 0.01(0.01)  Forward time: 1.25(1.38)  Batch time: 1.26(1.39)
2025-09-02 10:29:46,778   INFO  Train:    9/20 ( 45%) [2303/3862 ( 60%)]  Loss: 1.302 (1.31)  LR: 9.939e-04  Grad: 15.8390  max=0.5995(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6820(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5600, loss_cls=0.0925, loss_bbox=0.6839, matched_ious=0.5256, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.31(1.39)  Time cost: 53:11/35:59 [12:45:15/16:56:46]  Acc_iter 33200       Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.39)
2025-09-02 10:30:58,677   INFO  Train:    9/20 ( 45%) [2353/3862 ( 61%)]  Loss: 1.197 (1.31)  LR: 9.937e-04  Grad: 15.9538  max=1.3327(module.vfe.pfn_layers.0.linear.weight)  min: -0.6879(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5243, loss_cls=0.0890, loss_bbox=0.6350, matched_ious=0.5269, d_time=0.00(0.01), f_time=2.15(1.38), b_time=2.15(1.39)  Time cost: 54:23/34:51 [12:46:27/16:56:26]  Acc_iter 33250       Data time: 0.00(0.01)  Forward time: 2.15(1.38)  Batch time: 2.15(1.39)
2025-09-02 10:32:08,774   INFO  Train:    9/20 ( 45%) [2403/3862 ( 62%)]  Loss: 1.103 (1.31)  LR: 9.934e-04  Grad: 16.2215  max=2.6229(module.vfe.pfn_layers.0.linear.weight)  min: -0.7675(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5165, loss_cls=0.0875, loss_bbox=0.6226, matched_ious=0.5317, d_time=0.00(0.01), f_time=1.35(1.38), b_time=1.35(1.39)  Time cost: 55:33/33:43 [12:47:37/16:55:31]  Acc_iter 33300       Data time: 0.00(0.01)  Forward time: 1.35(1.38)  Batch time: 1.35(1.39)
2025-09-02 10:33:17,812   INFO  Train:    9/20 ( 45%) [2453/3862 ( 64%)]  Loss: 1.125 (1.30)  LR: 9.931e-04  Grad: 15.9954  max=0.6037(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6899(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5247, loss_cls=0.0869, loss_bbox=0.6464, matched_ious=0.5290, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.33(1.39)  Time cost: 56:42/32:33 [12:48:46/16:54:15]  Acc_iter 33350       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.33(1.39)
2025-09-02 10:34:25,229   INFO  Train:    9/20 ( 45%) [2503/3862 ( 65%)]  Loss: 1.371 (1.30)  LR: 9.928e-04  Grad: 16.0580  max=0.6084(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6915(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5354, loss_cls=0.0892, loss_bbox=0.6717, matched_ious=0.5253, d_time=0.01(0.01), f_time=1.44(1.38), b_time=1.45(1.39)  Time cost: 57:49/31:23 [12:49:54/16:52:33]  Acc_iter 33400       Data time: 0.01(0.01)  Forward time: 1.44(1.38)  Batch time: 1.45(1.39)
2025-09-02 10:35:33,806   INFO  Train:    9/20 ( 45%) [2553/3862 ( 66%)]  Loss: 1.658 (1.30)  LR: 9.925e-04  Grad: 16.7334  max=0.7008(module.vfe.pfn_layers.0.linear.weight)  min: -4.3705(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5260, loss_cls=0.0881, loss_bbox=0.6698, matched_ious=0.5226, d_time=0.00(0.01), f_time=1.38(1.38), b_time=1.38(1.39)  Time cost: 58:58/30:13 [12:51:02/16:51:12]  Acc_iter 33450       Data time: 0.00(0.01)  Forward time: 1.38(1.38)  Batch time: 1.38(1.39)
2025-09-02 10:36:43,566   INFO  Train:    9/20 ( 45%) [2603/3862 ( 67%)]  Loss: 1.468 (1.30)  LR: 9.922e-04  Grad: 16.1611  max=0.5974(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6908(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5125, loss_cls=0.0861, loss_bbox=0.6553, matched_ious=0.5242, d_time=0.01(0.01), f_time=1.25(1.38), b_time=1.25(1.39)  Time cost: 1:00:08/29:04 [12:52:12/16:50:11]  Acc_iter 33500       Data time: 0.01(0.01)  Forward time: 1.25(1.38)  Batch time: 1.25(1.39)
2025-09-02 10:37:53,153   INFO  Train:    9/20 ( 45%) [2653/3862 ( 69%)]  Loss: 1.194 (1.30)  LR: 9.919e-04  Grad: 16.2172  max=0.5959(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6962(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5412, loss_cls=0.0877, loss_bbox=0.6357, matched_ious=0.5238, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.37(1.39)  Time cost: 1:01:17/27:55 [12:53:22/16:49:06]  Acc_iter 33550       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.37(1.39)
2025-09-02 10:39:02,724   INFO  Train:    9/20 ( 45%) [2703/3862 ( 70%)]  Loss: 1.254 (1.30)  LR: 9.916e-04  Grad: 16.3087  max=0.6089(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6960(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5284, loss_cls=0.0888, loss_bbox=0.6443, matched_ious=0.5248, d_time=0.01(0.01), f_time=1.24(1.38), b_time=1.25(1.39)  Time cost: 1:02:27/26:46 [12:54:31/16:48:02]  Acc_iter 33600       Data time: 0.01(0.01)  Forward time: 1.24(1.38)  Batch time: 1.25(1.39)
2025-09-02 10:40:10,731   INFO  Train:    9/20 ( 45%) [2753/3862 ( 71%)]  Loss: 1.251 (1.30)  LR: 9.913e-04  Grad: 16.3709  max=0.6164(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7635(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5303, loss_cls=0.0883, loss_bbox=0.6561, matched_ious=0.5241, d_time=0.00(0.01), f_time=1.29(1.38), b_time=1.30(1.39)  Time cost: 1:03:35/25:36 [12:55:39/16:46:32]  Acc_iter 33650       Data time: 0.00(0.01)  Forward time: 1.29(1.38)  Batch time: 1.30(1.39)
2025-09-02 10:41:17,988   INFO  Train:    9/20 ( 45%) [2803/3862 ( 73%)]  Loss: 1.331 (1.30)  LR: 9.910e-04  Grad: 16.5213  max=1.5812(module.vfe.pfn_layers.0.linear.weight)  min: -0.8898(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5404, loss_cls=0.0901, loss_bbox=0.6759, matched_ious=0.5241, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.39(1.38)  Time cost: 1:04:42/24:26 [12:56:47/16:44:51]  Acc_iter 33700       Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.39(1.38)
2025-09-02 10:42:27,329   INFO  Train:    9/20 ( 45%) [2853/3862 ( 74%)]  Loss: 1.233 (1.30)  LR: 9.907e-04  Grad: 16.5250  max=1.5276(module.vfe.pfn_layers.0.linear.weight)  min: -0.6998(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5348, loss_cls=0.0897, loss_bbox=0.6966, matched_ious=0.5174, d_time=0.00(0.01), f_time=1.45(1.37), b_time=1.45(1.38)  Time cost: 1:05:52/23:17 [12:57:56/16:43:44]  Acc_iter 33750       Data time: 0.00(0.01)  Forward time: 1.45(1.37)  Batch time: 1.45(1.38)
2025-09-02 10:43:37,537   INFO  Train:    9/20 ( 45%) [2903/3862 ( 75%)]  Loss: 1.014 (1.30)  LR: 9.903e-04  Grad: 16.5312  max=0.6075(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7246(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5287, loss_cls=0.0884, loss_bbox=0.6437, matched_ious=0.5245, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.34(1.39)  Time cost: 1:07:02/22:08 [12:59:06/16:42:49]  Acc_iter 33800       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.34(1.39)
2025-09-02 10:44:46,032   INFO  Train:    9/20 ( 45%) [2953/3862 ( 76%)]  Loss: 1.520 (1.30)  LR: 9.900e-04  Grad: 16.5925  max=0.7002(module.vfe.pfn_layers.0.linear.weight)  min: -0.7008(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5273, loss_cls=0.0906, loss_bbox=0.6635, matched_ious=0.5270, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.33(1.38)  Time cost: 1:08:10/20:58 [13:00:15/16:41:29]  Acc_iter 33850       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.33(1.38)
2025-09-02 10:45:54,859   INFO  Train:    9/20 ( 45%) [3003/3862 ( 78%)]  Loss: 1.198 (1.30)  LR: 9.897e-04  Grad: 16.6344  max=0.6077(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9374(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5213, loss_cls=0.0882, loss_bbox=0.6731, matched_ious=0.5246, d_time=0.00(0.01), f_time=1.24(1.37), b_time=1.24(1.38)  Time cost: 1:09:19/19:49 [13:01:23/16:40:13]  Acc_iter 33900       Data time: 0.00(0.01)  Forward time: 1.24(1.37)  Batch time: 1.24(1.38)
2025-09-02 10:47:03,296   INFO  Train:    9/20 ( 45%) [3053/3862 ( 79%)]  Loss: 1.101 (1.30)  LR: 9.893e-04  Grad: 16.7753  max=1.4976(module.vfe.pfn_layers.0.linear.weight)  min: -0.8610(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5283, loss_cls=0.0906, loss_bbox=0.6866, matched_ious=0.5273, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.33(1.38)  Time cost: 1:10:28/18:40 [13:02:32/16:38:53]  Acc_iter 33950       Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.38)
2025-09-02 10:48:11,108   INFO  Train:    9/20 ( 45%) [3103/3862 ( 80%)]  Loss: 1.444 (1.30)  LR: 9.890e-04  Grad: 16.7581  max=0.6156(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9289(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5367, loss_cls=0.0896, loss_bbox=0.6907, matched_ious=0.5226, d_time=0.01(0.01), f_time=1.33(1.37), b_time=1.33(1.38)  Time cost: 1:11:35/17:30 [13:03:40/16:37:24]  Acc_iter 34000       Data time: 0.01(0.01)  Forward time: 1.33(1.37)  Batch time: 1.33(1.38)
2025-09-02 10:49:21,128   INFO  Train:    9/20 ( 45%) [3153/3862 ( 82%)]  Loss: 1.188 (1.30)  LR: 9.886e-04  Grad: 16.8407  max=0.6327(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7188(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5346, loss_cls=0.0886, loss_bbox=0.6866, matched_ious=0.5263, d_time=0.00(0.01), f_time=1.35(1.37), b_time=1.36(1.38)  Time cost: 1:12:45/16:21 [13:04:50/16:36:26]  Acc_iter 34050       Data time: 0.00(0.01)  Forward time: 1.35(1.37)  Batch time: 1.36(1.38)
2025-09-02 10:50:30,061   INFO  Train:    9/20 ( 45%) [3203/3862 ( 83%)]  Loss: 1.524 (1.30)  LR: 9.883e-04  Grad: 16.9345  max=1.6516(module.vfe.pfn_layers.0.linear.weight)  min: -0.7176(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5271, loss_cls=0.0875, loss_bbox=0.6579, matched_ious=0.5258, d_time=0.00(0.01), f_time=1.40(1.37), b_time=1.41(1.38)  Time cost: 1:13:54/15:12 [13:05:59/16:35:13]  Acc_iter 34100       Data time: 0.00(0.01)  Forward time: 1.40(1.37)  Batch time: 1.41(1.38)
2025-09-02 10:51:39,723   INFO  Train:    9/20 ( 45%) [3253/3862 ( 84%)]  Loss: 1.236 (1.30)  LR: 9.879e-04  Grad: 16.9453  max=0.6470(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7172(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5291, loss_cls=0.0882, loss_bbox=0.6634, matched_ious=0.5259, d_time=0.00(0.01), f_time=1.49(1.37), b_time=1.49(1.38)  Time cost: 1:15:04/14:03 [13:07:08/16:34:09]  Acc_iter 34150       Data time: 0.00(0.01)  Forward time: 1.49(1.37)  Batch time: 1.49(1.38)
2025-09-02 10:52:47,323   INFO  Train:    9/20 ( 45%) [3303/3862 ( 86%)]  Loss: 1.339 (1.30)  LR: 9.875e-04  Grad: 21.9867  max=7.1895(module.vfe.pfn_layers.0.linear.weight)  min: -11.0885(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5229, loss_cls=0.0875, loss_bbox=0.6550, matched_ious=0.5242, d_time=0.01(0.01), f_time=1.38(1.37), b_time=1.38(1.38)  Time cost: 1:16:12/12:53 [13:08:16/16:32:40]  Acc_iter 34200       Data time: 0.01(0.01)  Forward time: 1.38(1.37)  Batch time: 1.38(1.38)
2025-09-02 10:53:55,595   INFO  Train:    9/20 ( 45%) [3353/3862 ( 87%)]  Loss: 1.720 (1.30)  LR: 9.871e-04  Grad: 17.3885  max=0.8218(module.vfe.pfn_layers.0.linear.weight)  min: -2.5724(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5210, loss_cls=0.0867, loss_bbox=0.6810, matched_ious=0.5229, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.33(1.38)  Time cost: 1:17:20/11:44 [13:09:24/16:31:19]  Acc_iter 34250       Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.38)
2025-09-02 10:55:05,687   INFO  Train:    9/20 ( 45%) [3403/3862 ( 88%)]  Loss: 1.059 (1.30)  LR: 9.868e-04  Grad: 17.2431  max=0.6440(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.8555(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5281, loss_cls=0.0865, loss_bbox=0.6732, matched_ious=0.5272, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.36(1.38)  Time cost: 1:18:30/10:35 [13:10:34/16:30:21]  Acc_iter 34300       Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.36(1.38)
2025-09-02 10:56:13,993   INFO  Train:    9/20 ( 45%) [3453/3862 ( 89%)]  Loss: 1.341 (1.30)  LR: 9.864e-04  Grad: 17.1609  max=0.6532(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7309(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5370, loss_cls=0.0900, loss_bbox=0.6644, matched_ious=0.5289, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 1:19:38/09:25 [13:11:43/16:29:01]  Acc_iter 34350       Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 10:57:25,304   INFO  Train:    9/20 ( 45%) [3503/3862 ( 91%)]  Loss: 0.9544 (1.30)  LR: 9.860e-04  Grad: 17.2231  max=0.6491(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7331(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5142, loss_cls=0.0903, loss_bbox=0.6271, matched_ious=0.5317, d_time=0.00(0.01), f_time=1.39(1.37), b_time=1.40(1.38)  Time cost: 1:20:50/08:16 [13:12:54/16:28:18]  Acc_iter 34400       Data time: 0.00(0.01)  Forward time: 1.39(1.37)  Batch time: 1.40(1.38)
2025-09-02 10:58:34,115   INFO  Train:    9/20 ( 45%) [3553/3862 ( 92%)]  Loss: 1.130 (1.30)  LR: 9.856e-04  Grad: 17.2727  max=0.6526(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7365(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5171, loss_cls=0.0873, loss_bbox=0.6528, matched_ious=0.5235, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.38(1.38)  Time cost: 1:21:58/07:07 [13:14:03/16:27:04]  Acc_iter 34450       Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.38(1.38)
2025-09-02 10:59:43,220   INFO  Train:    9/20 ( 45%) [3603/3862 ( 93%)]  Loss: 1.412 (1.30)  LR: 9.852e-04  Grad: 17.3201  max=0.6548(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7408(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5492, loss_cls=0.0893, loss_bbox=0.6914, matched_ious=0.5207, d_time=0.01(0.01), f_time=1.32(1.37), b_time=1.33(1.38)  Time cost: 1:23:07/05:58 [13:15:12/16:25:53]  Acc_iter 34500       Data time: 0.01(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.38)
2025-09-02 11:00:53,311   INFO  Train:    9/20 ( 45%) [3653/3862 ( 95%)]  Loss: 0.9338 (1.30)  LR: 9.847e-04  Grad: 17.4377  max=0.6749(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7483(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5126, loss_cls=0.0857, loss_bbox=0.6536, matched_ious=0.5237, d_time=0.01(0.01), f_time=1.24(1.37), b_time=1.25(1.38)  Time cost: 1:24:18/04:49 [13:16:22/16:24:55]  Acc_iter 34550       Data time: 0.01(0.01)  Forward time: 1.24(1.37)  Batch time: 1.25(1.38)
2025-09-02 11:02:02,430   INFO  Train:    9/20 ( 45%) [3703/3862 ( 96%)]  Loss: 1.124 (1.30)  LR: 9.843e-04  Grad: 17.6386  max=1.6898(module.vfe.pfn_layers.0.linear.weight)  min: -0.7497(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5187, loss_cls=0.0865, loss_bbox=0.6200, matched_ious=0.5304, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.33(1.38)  Time cost: 1:25:27/03:40 [13:17:31/16:23:44]  Acc_iter 34600       Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.33(1.38)
2025-09-02 11:03:12,295   INFO  Train:    9/20 ( 45%) [3753/3862 ( 97%)]  Loss: 1.353 (1.30)  LR: 9.839e-04  Grad: 17.6019  max=0.7813(module.vfe.pfn_layers.0.linear.weight)  min: -0.9820(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5201, loss_cls=0.0875, loss_bbox=0.6455, matched_ious=0.5262, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.29(1.38)  Time cost: 1:26:37/02:30 [13:18:41/16:22:42]  Acc_iter 34650       Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.29(1.38)
2025-09-02 11:04:20,053   INFO  Train:    9/20 ( 45%) [3803/3862 ( 98%)]  Loss: 1.682 (1.30)  LR: 9.835e-04  Grad: 17.6503  max=1.4264(module.vfe.pfn_layers.0.linear.weight)  min: -0.7518(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5371, loss_cls=0.0891, loss_bbox=0.6727, matched_ious=0.5278, d_time=0.00(0.01), f_time=1.39(1.37), b_time=1.39(1.38)  Time cost: 1:27:44/01:21 [13:19:49/16:21:17]  Acc_iter 34700       Data time: 0.00(0.01)  Forward time: 1.39(1.37)  Batch time: 1.39(1.38)
2025-09-02 11:05:27,432   INFO  Train:    9/20 ( 45%) [3853/3862 (100%)]  Loss: 1.016 (1.30)  LR: 9.830e-04  Grad: 17.6692  max=0.8317(module.vfe.pfn_layers.0.linear.weight)  min: -0.7508(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5164, loss_cls=0.0852, loss_bbox=0.6670, matched_ious=0.5210, d_time=0.00(0.01), f_time=1.44(1.37), b_time=1.45(1.38)  Time cost: 1:28:52/00:12 [13:20:56/16:19:48]  Acc_iter 34750       Data time: 0.00(0.01)  Forward time: 1.44(1.37)  Batch time: 1.45(1.38)
2025-09-02 11:05:38,060   INFO  Train:    9/20 ( 45%) [3861/3862 (100%)]  Loss: 1.407 (1.30)  LR: 9.830e-04  Grad: 17.6509  max=0.6759(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7511(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5045, loss_cls=0.0848, loss_bbox=0.6181, matched_ious=0.5265, d_time=0.01(0.01), f_time=1.35(1.37), b_time=1.37(1.38)  Time cost: 1:29:02/00:01 [13:21:07/16:19:32]  Acc_iter 34758       Data time: 0.01(0.01)  Forward time: 1.35(1.37)  Batch time: 1.37(1.38)

                                               [Aepochs:  45%|████▌     | 9/20 [13:21:07<16:20:08, 5346.24s/it]epochs:  45%|████▌     | 9/20 [13:21:08<16:20:09, 5346.32s/it]epochs:  45%|████▌     | 9/20 [13:21:08<16:20:09, 5346.33s/it]epochs:  45%|████▌     | 9/20 [13:21:08<16:20:09, 5346.34s/it]epochs:  45%|████▌     | 9/20 [13:21:08<16:20:09, 5346.33s/it]epochs:  45%|████▌     | 9/20 [13:21:08<16:20:09, 5346.34s/it]epochs:  45%|████▌     | 9/20 [13:21:08<16:20:09, 5346.34s/it]epochs:  45%|████▌     | 9/20 [13:21:08<16:20:09, 5346.33s/it]
train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-02 11:05:46,491   INFO  Train:   10/20 ( 50%) [   0/3862 (  0%)]  Loss: 1.333 (1.33)  LR: 9.830e-04  Grad: 17.6365  max=0.6752(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7505(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6112, loss_cls=0.0982, loss_bbox=0.6241, matched_ious=0.5253, d_time=2.79(2.79), f_time=3.84(3.84), b_time=6.63(6.63)  Time cost: 00:06/6:44:57 [13:21:15/74:14:27]  Acc_iter 34759       Data time: 2.79(2.79)  Forward time: 3.84(3.84)  Batch time: 6.63(6.63)
2025-09-02 11:06:43,294   INFO  Train:   10/20 ( 50%) [  41/3862 (  1%)]  Loss: 1.289 (1.24)  LR: 9.826e-04  Grad: 18.6104  max=3.1270(module.vfe.pfn_layers.0.linear.weight)  min: -4.8291(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5065, loss_cls=0.0847, loss_bbox=0.6509, matched_ious=0.5268, d_time=0.00(0.07), f_time=1.46(1.44), b_time=1.46(1.51)  Time cost: 01:03/1:35:39 [13:22:12/17:42:34]  Acc_iter 34800       Data time: 0.00(0.07)  Forward time: 1.46(1.44)  Batch time: 1.46(1.51)
2025-09-02 11:07:53,535   INFO  Train:   10/20 ( 50%) [  91/3862 (  2%)]  Loss: 1.022 (1.27)  LR: 9.822e-04  Grad: 17.7784  max=0.6823(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7625(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5359, loss_cls=0.0895, loss_bbox=0.6728, matched_ious=0.5207, d_time=0.00(0.05), f_time=1.28(1.40), b_time=1.29(1.45)  Time cost: 02:13/1:31:04 [13:23:22/17:03:50]  Acc_iter 34850       Data time: 0.00(0.05)  Forward time: 1.28(1.40)  Batch time: 1.29(1.45)
2025-09-02 11:09:02,913   INFO  Train:   10/20 ( 50%) [ 141/3862 (  4%)]  Loss: 1.322 (1.27)  LR: 9.817e-04  Grad: 17.8124  max=0.6804(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7643(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5258, loss_cls=0.0875, loss_bbox=0.6599, matched_ious=0.5252, d_time=0.02(0.04), f_time=1.44(1.39), b_time=1.46(1.43)  Time cost: 03:22/1:28:31 [13:24:31/16:47:24]  Acc_iter 34900       Data time: 0.02(0.04)  Forward time: 1.44(1.39)  Batch time: 1.46(1.43)
2025-09-02 11:10:12,380   INFO  Train:   10/20 ( 50%) [ 191/3862 (  5%)]  Loss: 1.157 (1.27)  LR: 9.812e-04  Grad: 17.9531  max=1.5015(module.vfe.pfn_layers.0.linear.weight)  min: -1.1335(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5021, loss_cls=0.0827, loss_bbox=0.6569, matched_ious=0.5312, d_time=0.00(0.03), f_time=1.36(1.39), b_time=1.37(1.42)  Time cost: 04:32/1:26:44 [13:25:41/16:39:12]  Acc_iter 34950       Data time: 0.00(0.03)  Forward time: 1.36(1.39)  Batch time: 1.37(1.42)
2025-09-02 11:11:19,865   INFO  Train:   10/20 ( 50%) [ 241/3862 (  6%)]  Loss: 1.742 (1.27)  LR: 9.808e-04  Grad: 17.9354  max=0.6843(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8702(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5273, loss_cls=0.0898, loss_bbox=0.6602, matched_ious=0.5307, d_time=0.00(0.02), f_time=1.36(1.38), b_time=1.36(1.40)  Time cost: 05:39/1:24:42 [13:26:48/16:28:08]  Acc_iter 35000       Data time: 0.00(0.02)  Forward time: 1.36(1.38)  Batch time: 1.36(1.40)
2025-09-02 11:12:28,502   INFO  Train:   10/20 ( 50%) [ 291/3862 (  8%)]  Loss: 1.465 (1.27)  LR: 9.803e-04  Grad: 17.9724  max=0.6786(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7625(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5254, loss_cls=0.0856, loss_bbox=0.6738, matched_ious=0.5243, d_time=0.00(0.02), f_time=1.37(1.38), b_time=1.37(1.40)  Time cost: 06:48/1:23:13 [13:27:57/16:23:15]  Acc_iter 35050       Data time: 0.00(0.02)  Forward time: 1.37(1.38)  Batch time: 1.37(1.40)
2025-09-02 11:13:37,641   INFO  Train:   10/20 ( 50%) [ 341/3862 (  9%)]  Loss: 1.008 (1.27)  LR: 9.798e-04  Grad: 18.0425  max=0.8071(module.vfe.pfn_layers.0.linear.weight)  min: -0.7653(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4979, loss_cls=0.0831, loss_bbox=0.6593, matched_ious=0.5296, d_time=0.00(0.02), f_time=1.34(1.38), b_time=1.34(1.40)  Time cost: 07:57/1:21:55 [13:29:06/16:20:30]  Acc_iter 35100       Data time: 0.00(0.02)  Forward time: 1.34(1.38)  Batch time: 1.34(1.40)
2025-09-02 11:14:45,598   INFO  Train:   10/20 ( 50%) [ 391/3862 ( 10%)]  Loss: 1.275 (1.27)  LR: 9.794e-04  Grad: 18.0890  max=0.8708(module.vfe.pfn_layers.0.linear.weight)  min: -0.7706(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5216, loss_cls=0.0867, loss_bbox=0.6830, matched_ious=0.5215, d_time=0.00(0.02), f_time=1.41(1.37), b_time=1.41(1.39)  Time cost: 09:05/1:20:29 [13:30:14/16:16:02]  Acc_iter 35150       Data time: 0.00(0.02)  Forward time: 1.41(1.37)  Batch time: 1.41(1.39)
2025-09-02 11:15:54,759   INFO  Train:   10/20 ( 50%) [ 441/3862 ( 11%)]  Loss: 1.344 (1.27)  LR: 9.789e-04  Grad: 15.7379  max=1.1132(module.vfe.pfn_layers.0.linear.weight)  min: -1.0974(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5008, loss_cls=0.0836, loss_bbox=0.6552, matched_ious=0.5207, d_time=0.00(0.02), f_time=1.39(1.37), b_time=1.40(1.39)  Time cost: 10:14/1:19:16 [13:31:23/16:14:14]  Acc_iter 35200       Data time: 0.00(0.02)  Forward time: 1.39(1.37)  Batch time: 1.40(1.39)
2025-09-02 11:17:02,410   INFO  Train:   10/20 ( 50%) [ 491/3862 ( 13%)]  Loss: 1.313 (1.26)  LR: 9.784e-04  Grad: 15.6977  max=0.5888(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6723(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5123, loss_cls=0.0837, loss_bbox=0.6514, matched_ious=0.5259, d_time=0.00(0.02), f_time=1.32(1.37), b_time=1.33(1.39)  Time cost: 11:22/1:17:54 [13:32:31/16:10:25]  Acc_iter 35250       Data time: 0.00(0.02)  Forward time: 1.32(1.37)  Batch time: 1.33(1.39)
2025-09-02 11:18:10,827   INFO  Train:   10/20 ( 50%) [ 541/3862 ( 14%)]  Loss: 0.9787 (1.26)  LR: 9.779e-04  Grad: 15.7367  max=0.5935(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6714(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5060, loss_cls=0.0834, loss_bbox=0.6459, matched_ious=0.5285, d_time=0.01(0.02), f_time=1.36(1.37), b_time=1.36(1.39)  Time cost: 12:30/1:16:39 [13:33:39/16:08:05]  Acc_iter 35300       Data time: 0.01(0.02)  Forward time: 1.36(1.37)  Batch time: 1.36(1.39)
2025-09-02 11:19:19,380   INFO  Train:   10/20 ( 50%) [ 591/3862 ( 15%)]  Loss: 1.350 (1.26)  LR: 9.774e-04  Grad: 16.2516  max=3.4623(module.vfe.pfn_layers.0.linear.weight)  min: -1.0057(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5154, loss_cls=0.0848, loss_bbox=0.6702, matched_ious=0.5297, d_time=0.00(0.02), f_time=1.31(1.37), b_time=1.31(1.38)  Time cost: 13:39/1:15:26 [13:34:48/16:06:06]  Acc_iter 35350       Data time: 0.00(0.02)  Forward time: 1.31(1.37)  Batch time: 1.31(1.38)
2025-09-02 11:20:28,951   INFO  Train:   10/20 ( 50%) [ 641/3862 ( 17%)]  Loss: 1.323 (1.26)  LR: 9.769e-04  Grad: 15.1495  max=1.6969(module.vfe.pfn_layers.0.linear.weight)  min: -0.6345(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5216, loss_cls=0.0887, loss_bbox=0.6200, matched_ious=0.5327, d_time=0.00(0.02), f_time=1.32(1.37), b_time=1.33(1.38)  Time cost: 14:48/1:14:18 [13:35:58/16:05:22]  Acc_iter 35400       Data time: 0.00(0.02)  Forward time: 1.32(1.37)  Batch time: 1.33(1.38)
2025-09-02 11:21:38,888   INFO  Train:   10/20 ( 50%) [ 691/3862 ( 18%)]  Loss: 1.006 (1.26)  LR: 9.764e-04  Grad: 15.0577  max=0.5789(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6370(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5171, loss_cls=0.0890, loss_bbox=0.6549, matched_ious=0.5297, d_time=0.02(0.02), f_time=1.41(1.37), b_time=1.43(1.39)  Time cost: 15:58/1:13:13 [13:37:07/16:04:56]  Acc_iter 35450       Data time: 0.02(0.02)  Forward time: 1.41(1.37)  Batch time: 1.43(1.39)
2025-09-02 11:22:46,816   INFO  Train:   10/20 ( 50%) [ 741/3862 ( 19%)]  Loss: 1.256 (1.26)  LR: 9.759e-04  Grad: 15.0926  max=0.5776(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6388(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5180, loss_cls=0.0854, loss_bbox=0.6540, matched_ious=0.5277, d_time=0.01(0.02), f_time=1.41(1.37), b_time=1.41(1.38)  Time cost: 17:06/1:11:58 [13:38:15/16:02:31]  Acc_iter 35500       Data time: 0.01(0.02)  Forward time: 1.41(1.37)  Batch time: 1.41(1.38)
2025-09-02 11:23:55,308   INFO  Train:   10/20 ( 50%) [ 791/3862 ( 20%)]  Loss: 1.372 (1.26)  LR: 9.753e-04  Grad: 15.1716  max=0.7656(module.vfe.pfn_layers.0.linear.weight)  min: -0.6423(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5278, loss_cls=0.0895, loss_bbox=0.6343, matched_ious=0.5345, d_time=0.01(0.01), f_time=1.29(1.37), b_time=1.30(1.38)  Time cost: 18:15/1:10:46 [13:39:24/16:00:46]  Acc_iter 35550       Data time: 0.01(0.01)  Forward time: 1.29(1.37)  Batch time: 1.30(1.38)
2025-09-02 11:25:04,482   INFO  Train:   10/20 ( 50%) [ 841/3862 ( 22%)]  Loss: 1.172 (1.26)  LR: 9.748e-04  Grad: 15.2181  max=0.5890(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9412(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5189, loss_cls=0.0869, loss_bbox=0.6498, matched_ious=0.5261, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.34(1.38)  Time cost: 19:24/1:09:37 [13:40:33/15:59:39]  Acc_iter 35600       Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.34(1.38)
2025-09-02 11:26:13,724   INFO  Train:   10/20 ( 50%) [ 891/3862 ( 23%)]  Loss: 1.144 (1.26)  LR: 9.743e-04  Grad: 15.2446  max=0.5833(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6515(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5174, loss_cls=0.0872, loss_bbox=0.6358, matched_ious=0.5216, d_time=0.01(0.01), f_time=1.33(1.37), b_time=1.35(1.38)  Time cost: 20:33/1:08:28 [13:41:42/15:58:35]  Acc_iter 35650       Data time: 0.01(0.01)  Forward time: 1.33(1.37)  Batch time: 1.35(1.38)
2025-09-02 11:27:23,176   INFO  Train:   10/20 ( 50%) [ 941/3862 ( 24%)]  Loss: 1.218 (1.26)  LR: 9.737e-04  Grad: 15.2897  max=0.5810(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6579(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5051, loss_cls=0.0856, loss_bbox=0.6596, matched_ious=0.5294, d_time=0.00(0.01), f_time=1.28(1.37), b_time=1.28(1.38)  Time cost: 21:42/1:07:20 [13:42:52/15:57:39]  Acc_iter 35700       Data time: 0.00(0.01)  Forward time: 1.28(1.37)  Batch time: 1.28(1.38)
2025-09-02 11:28:31,449   INFO  Train:   10/20 ( 50%) [ 991/3862 ( 26%)]  Loss: 1.107 (1.26)  LR: 9.732e-04  Grad: 15.3510  max=0.7206(module.vfe.pfn_layers.0.linear.weight)  min: -0.6595(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5066, loss_cls=0.0850, loss_bbox=0.6707, matched_ious=0.5295, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.38(1.38)  Time cost: 22:51/1:06:08 [13:44:00/15:55:53]  Acc_iter 35750       Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.38(1.38)
2025-09-02 11:29:40,719   INFO  Train:   10/20 ( 50%) [1041/3862 ( 27%)]  Loss: 1.638 (1.26)  LR: 9.726e-04  Grad: 15.5007  max=1.7026(module.vfe.pfn_layers.0.linear.weight)  min: -0.9468(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5085, loss_cls=0.0843, loss_bbox=0.6290, matched_ious=0.5303, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.38(1.38)  Time cost: 24:00/1:04:59 [13:45:09/15:54:50]  Acc_iter 35800       Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.38(1.38)
2025-09-02 11:30:48,218   INFO  Train:   10/20 ( 50%) [1091/3862 ( 28%)]  Loss: 1.270 (1.26)  LR: 9.721e-04  Grad: 15.5166  max=0.9374(module.vfe.pfn_layers.0.linear.weight)  min: -1.0482(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5161, loss_cls=0.0856, loss_bbox=0.6892, matched_ious=0.5259, d_time=0.00(0.01), f_time=1.23(1.37), b_time=1.23(1.38)  Time cost: 25:08/1:03:46 [13:46:17/15:52:39]  Acc_iter 35850       Data time: 0.00(0.01)  Forward time: 1.23(1.37)  Batch time: 1.23(1.38)
2025-09-02 11:31:58,123   INFO  Train:   10/20 ( 50%) [1141/3862 ( 30%)]  Loss: 0.8784 (1.26)  LR: 9.715e-04  Grad: 15.9360  max=1.5228(module.vfe.pfn_layers.0.linear.weight)  min: -3.5329(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5176, loss_cls=0.0856, loss_bbox=0.6323, matched_ious=0.5299, d_time=0.00(0.01), f_time=1.39(1.37), b_time=1.39(1.38)  Time cost: 26:17/1:02:39 [13:47:27/15:52:01]  Acc_iter 35900       Data time: 0.00(0.01)  Forward time: 1.39(1.37)  Batch time: 1.39(1.38)
2025-09-02 11:33:09,387   INFO  Train:   10/20 ( 50%) [1191/3862 ( 31%)]  Loss: 1.986 (1.26)  LR: 9.710e-04  Grad: 15.6901  max=2.0523(module.vfe.pfn_layers.0.linear.weight)  min: -0.6649(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5180, loss_cls=0.0851, loss_bbox=0.6425, matched_ious=0.5311, d_time=0.00(0.01), f_time=1.40(1.37), b_time=1.40(1.38)  Time cost: 27:29/1:01:35 [13:48:38/15:52:08]  Acc_iter 35950       Data time: 0.00(0.01)  Forward time: 1.40(1.37)  Batch time: 1.40(1.38)
2025-09-02 11:34:17,565   INFO  Train:   10/20 ( 50%) [1241/3862 ( 32%)]  Loss: 1.045 (1.25)  LR: 9.704e-04  Grad: 15.6071  max=0.5915(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6867(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5063, loss_cls=0.0845, loss_bbox=0.6332, matched_ious=0.5295, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.31(1.38)  Time cost: 28:37/1:00:24 [13:49:46/15:50:25]  Acc_iter 36000       Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.31(1.38)
2025-09-02 11:35:24,809   INFO  Train:   10/20 ( 50%) [1291/3862 ( 33%)]  Loss: 1.405 (1.26)  LR: 9.698e-04  Grad: 15.7458  max=1.3088(module.vfe.pfn_layers.0.linear.weight)  min: -0.6714(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5157, loss_cls=0.0849, loss_bbox=0.6608, matched_ious=0.5294, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 29:44/59:11 [13:50:53/15:48:16]  Acc_iter 36050       Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 11:36:35,223   INFO  Train:   10/20 ( 50%) [1341/3862 ( 35%)]  Loss: 1.432 (1.25)  LR: 9.692e-04  Grad: 15.8260  max=1.3390(module.vfe.pfn_layers.0.linear.weight)  min: -0.7693(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4889, loss_cls=0.0824, loss_bbox=0.6366, matched_ious=0.5288, d_time=0.00(0.01), f_time=1.35(1.37), b_time=1.35(1.38)  Time cost: 30:55/58:04 [13:52:04/15:47:48]  Acc_iter 36100       Data time: 0.00(0.01)  Forward time: 1.35(1.37)  Batch time: 1.35(1.38)
2025-09-02 11:37:44,285   INFO  Train:   10/20 ( 50%) [1391/3862 ( 36%)]  Loss: 1.567 (1.25)  LR: 9.686e-04  Grad: 15.7834  max=0.8307(module.vfe.pfn_layers.0.linear.weight)  min: -0.6710(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5221, loss_cls=0.0859, loss_bbox=0.6846, matched_ious=0.5267, d_time=0.00(0.01), f_time=1.40(1.37), b_time=1.41(1.38)  Time cost: 32:04/56:55 [13:53:13/15:46:37]  Acc_iter 36150       Data time: 0.00(0.01)  Forward time: 1.40(1.37)  Batch time: 1.41(1.38)
2025-09-02 11:38:54,674   INFO  Train:   10/20 ( 50%) [1441/3862 ( 37%)]  Loss: 0.9605 (1.25)  LR: 9.680e-04  Grad: 15.7667  max=0.5945(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6720(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5059, loss_cls=0.0843, loss_bbox=0.6411, matched_ious=0.5307, d_time=0.02(0.01), f_time=2.19(1.37), b_time=2.21(1.38)  Time cost: 33:14/55:48 [13:54:23/15:46:05]  Acc_iter 36200       Data time: 0.02(0.01)  Forward time: 2.19(1.37)  Batch time: 2.21(1.38)
2025-09-02 11:40:03,195   INFO  Train:   10/20 ( 50%) [1491/3862 ( 39%)]  Loss: 1.424 (1.25)  LR: 9.674e-04  Grad: 15.8636  max=0.6041(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9041(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5145, loss_cls=0.0844, loss_bbox=0.6577, matched_ious=0.5322, d_time=0.00(0.01), f_time=1.27(1.37), b_time=1.28(1.38)  Time cost: 34:22/54:38 [13:55:32/15:44:38]  Acc_iter 36250       Data time: 0.00(0.01)  Forward time: 1.27(1.37)  Batch time: 1.28(1.38)
2025-09-02 11:41:11,321   INFO  Train:   10/20 ( 50%) [1541/3862 ( 40%)]  Loss: 1.272 (1.25)  LR: 9.668e-04  Grad: 15.9606  max=0.7761(module.vfe.pfn_layers.0.linear.weight)  min: -0.6879(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5050, loss_cls=0.0847, loss_bbox=0.6183, matched_ious=0.5331, d_time=0.02(0.01), f_time=1.35(1.37), b_time=1.37(1.38)  Time cost: 35:31/53:27 [13:56:40/15:43:02]  Acc_iter 36300       Data time: 0.02(0.01)  Forward time: 1.35(1.37)  Batch time: 1.37(1.38)
2025-09-02 11:42:20,957   INFO  Train:   10/20 ( 50%) [1591/3862 ( 41%)]  Loss: 1.434 (1.25)  LR: 9.662e-04  Grad: 16.2228  max=1.4583(module.vfe.pfn_layers.0.linear.weight)  min: -2.3631(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5210, loss_cls=0.0864, loss_bbox=0.6397, matched_ious=0.5302, d_time=0.01(0.01), f_time=1.33(1.37), b_time=1.34(1.38)  Time cost: 36:40/52:19 [13:57:50/15:42:07]  Acc_iter 36350       Data time: 0.01(0.01)  Forward time: 1.33(1.37)  Batch time: 1.34(1.38)
2025-09-02 11:43:30,110   INFO  Train:   10/20 ( 50%) [1641/3862 ( 42%)]  Loss: 1.013 (1.25)  LR: 9.656e-04  Grad: 16.0584  max=0.7649(module.vfe.pfn_layers.0.linear.weight)  min: -0.6946(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5047, loss_cls=0.0838, loss_bbox=0.6255, matched_ious=0.5289, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.36(1.38)  Time cost: 37:49/51:10 [13:58:59/15:40:58]  Acc_iter 36400       Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.36(1.38)
2025-09-02 11:44:40,002   INFO  Train:   10/20 ( 50%) [1691/3862 ( 44%)]  Loss: 1.104 (1.25)  LR: 9.650e-04  Grad: 16.1327  max=1.3210(module.vfe.pfn_layers.0.linear.weight)  min: -0.6925(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5192, loss_cls=0.0848, loss_bbox=0.6631, matched_ious=0.5325, d_time=0.01(0.01), f_time=1.32(1.37), b_time=1.33(1.38)  Time cost: 38:59/50:02 [14:00:09/15:40:08]  Acc_iter 36450       Data time: 0.01(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.38)
2025-09-02 11:45:49,578   INFO  Train:   10/20 ( 50%) [1741/3862 ( 45%)]  Loss: 1.306 (1.25)  LR: 9.644e-04  Grad: 16.1561  max=0.6199(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6944(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5250, loss_cls=0.0882, loss_bbox=0.6287, matched_ious=0.5277, d_time=0.00(0.01), f_time=1.43(1.37), b_time=1.44(1.38)  Time cost: 40:09/48:53 [14:01:18/15:39:09]  Acc_iter 36500       Data time: 0.00(0.01)  Forward time: 1.43(1.37)  Batch time: 1.44(1.38)
2025-09-02 11:46:58,352   INFO  Train:   10/20 ( 50%) [1791/3862 ( 46%)]  Loss: 1.012 (1.25)  LR: 9.637e-04  Grad: 16.2111  max=0.6191(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0728(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5040, loss_cls=0.0841, loss_bbox=0.6481, matched_ious=0.5284, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.37(1.38)  Time cost: 41:18/47:43 [14:02:27/15:37:51]  Acc_iter 36550       Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.37(1.38)
2025-09-02 11:48:07,040   INFO  Train:   10/20 ( 50%) [1841/3862 ( 48%)]  Loss: 1.040 (1.25)  LR: 9.631e-04  Grad: 18.7571  max=0.8851(module.vfe.pfn_layers.0.linear.weight)  min: -8.8285(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5076, loss_cls=0.0860, loss_bbox=0.6573, matched_ious=0.5311, d_time=0.00(0.01), f_time=1.39(1.37), b_time=1.39(1.38)  Time cost: 42:26/46:34 [14:03:36/15:36:32]  Acc_iter 36600       Data time: 0.00(0.01)  Forward time: 1.39(1.37)  Batch time: 1.39(1.38)
2025-09-02 11:49:15,609   INFO  Train:   10/20 ( 50%) [1891/3862 ( 49%)]  Loss: 1.204 (1.25)  LR: 9.625e-04  Grad: 16.3026  max=0.6205(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6972(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5302, loss_cls=0.0868, loss_bbox=0.6282, matched_ious=0.5312, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.30(1.38)  Time cost: 43:35/45:24 [14:04:44/15:35:11]  Acc_iter 36650       Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.30(1.38)
2025-09-02 11:50:24,225   INFO  Train:   10/20 ( 50%) [1941/3862 ( 50%)]  Loss: 1.266 (1.25)  LR: 9.618e-04  Grad: 16.4773  max=0.7537(module.vfe.pfn_layers.0.linear.weight)  min: -1.8405(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5230, loss_cls=0.0872, loss_bbox=0.6650, matched_ious=0.5249, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 44:44/44:15 [14:05:53/15:33:51]  Acc_iter 36700       Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 11:51:35,288   INFO  Train:   10/20 ( 50%) [1991/3862 ( 52%)]  Loss: 0.9591 (1.25)  LR: 9.612e-04  Grad: 16.6689  max=2.0824(module.vfe.pfn_layers.0.linear.weight)  min: -0.8239(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4982, loss_cls=0.0847, loss_bbox=0.6177, matched_ious=0.5321, d_time=0.01(0.01), f_time=2.18(1.37), b_time=2.18(1.38)  Time cost: 45:55/43:07 [14:07:04/15:33:22]  Acc_iter 36750       Data time: 0.01(0.01)  Forward time: 2.18(1.37)  Batch time: 2.18(1.38)
2025-09-02 11:52:44,520   INFO  Train:   10/20 ( 50%) [2041/3862 ( 53%)]  Loss: 1.438 (1.25)  LR: 9.605e-04  Grad: 16.5053  max=0.6276(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7538(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5084, loss_cls=0.0860, loss_bbox=0.6248, matched_ious=0.5315, d_time=0.01(0.01), f_time=1.34(1.37), b_time=1.35(1.38)  Time cost: 47:04/41:58 [14:08:13/15:32:14]  Acc_iter 36800       Data time: 0.01(0.01)  Forward time: 1.34(1.37)  Batch time: 1.35(1.38)
2025-09-02 11:53:53,370   INFO  Train:   10/20 ( 50%) [2091/3862 ( 54%)]  Loss: 1.243 (1.25)  LR: 9.598e-04  Grad: 16.5392  max=0.6323(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7060(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4906, loss_cls=0.0828, loss_bbox=0.6385, matched_ious=0.5303, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.32(1.38)  Time cost: 48:13/40:49 [14:09:22/15:30:59]  Acc_iter 36850       Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.32(1.38)
2025-09-02 11:55:01,706   INFO  Train:   10/20 ( 50%) [2141/3862 ( 55%)]  Loss: 1.161 (1.25)  LR: 9.592e-04  Grad: 16.6761  max=0.7497(module.vfe.pfn_layers.0.linear.weight)  min: -0.8271(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5024, loss_cls=0.0848, loss_bbox=0.6422, matched_ious=0.5269, d_time=0.00(0.01), f_time=1.24(1.37), b_time=1.24(1.38)  Time cost: 49:21/39:39 [14:10:30/15:29:34]  Acc_iter 36900       Data time: 0.00(0.01)  Forward time: 1.24(1.37)  Batch time: 1.24(1.38)
2025-09-02 11:56:10,331   INFO  Train:   10/20 ( 50%) [2191/3862 ( 57%)]  Loss: 1.032 (1.25)  LR: 9.585e-04  Grad: 16.6936  max=0.6525(module.vfe.pfn_layers.0.linear.weight)  min: -0.7088(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5175, loss_cls=0.0872, loss_bbox=0.6540, matched_ious=0.5233, d_time=0.01(0.01), f_time=1.33(1.37), b_time=1.34(1.38)  Time cost: 50:30/38:29 [14:11:39/15:28:16]  Acc_iter 36950       Data time: 0.01(0.01)  Forward time: 1.33(1.37)  Batch time: 1.34(1.38)
2025-09-02 11:57:18,608   INFO  Train:   10/20 ( 50%) [2241/3862 ( 58%)]  Loss: 1.716 (1.25)  LR: 9.578e-04  Grad: 16.7398  max=0.6383(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7114(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4965, loss_cls=0.0842, loss_bbox=0.6318, matched_ious=0.5322, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 51:38/37:20 [14:12:47/15:26:52]  Acc_iter 37000       Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 11:58:29,205   INFO  Train:   10/20 ( 50%) [2291/3862 ( 59%)]  Loss: 1.270 (1.25)  LR: 9.571e-04  Grad: 16.8290  max=0.6402(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8065(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5092, loss_cls=0.0849, loss_bbox=0.6404, matched_ious=0.5310, d_time=0.00(0.01), f_time=1.28(1.37), b_time=1.29(1.38)  Time cost: 52:49/36:12 [14:13:58/15:26:09]  Acc_iter 37050       Data time: 0.00(0.01)  Forward time: 1.28(1.37)  Batch time: 1.29(1.38)
2025-09-02 11:59:37,799   INFO  Train:   10/20 ( 50%) [2341/3862 ( 61%)]  Loss: 1.390 (1.25)  LR: 9.564e-04  Grad: 16.8490  max=0.6438(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7166(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4996, loss_cls=0.0828, loss_bbox=0.6261, matched_ious=0.5337, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.36(1.38)  Time cost: 53:57/35:02 [14:15:06/15:24:51]  Acc_iter 37100       Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.36(1.38)
2025-09-02 12:00:45,202   INFO  Train:   10/20 ( 50%) [2391/3862 ( 62%)]  Loss: 0.8984 (1.25)  LR: 9.558e-04  Grad: 16.8697  max=0.6405(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7193(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5037, loss_cls=0.0839, loss_bbox=0.6200, matched_ious=0.5346, d_time=0.01(0.01), f_time=1.34(1.37), b_time=1.35(1.38)  Time cost: 55:05/33:52 [14:16:14/15:23:13]  Acc_iter 37150       Data time: 0.01(0.01)  Forward time: 1.34(1.37)  Batch time: 1.35(1.38)
2025-09-02 12:01:54,561   INFO  Train:   10/20 ( 50%) [2441/3862 ( 63%)]  Loss: 1.514 (1.25)  LR: 9.551e-04  Grad: 17.0660  max=1.1502(module.vfe.pfn_layers.0.linear.weight)  min: -1.9188(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4948, loss_cls=0.0829, loss_bbox=0.6487, matched_ious=0.5302, d_time=0.00(0.01), f_time=1.39(1.37), b_time=1.39(1.38)  Time cost: 56:14/32:43 [14:17:23/15:22:08]  Acc_iter 37200       Data time: 0.00(0.01)  Forward time: 1.39(1.37)  Batch time: 1.39(1.38)
2025-09-02 12:03:03,445   INFO  Train:   10/20 ( 50%) [2491/3862 ( 65%)]  Loss: 1.167 (1.25)  LR: 9.543e-04  Grad: 17.0315  max=0.6905(module.vfe.pfn_layers.0.linear.weight)  min: -0.7308(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5213, loss_cls=0.0875, loss_bbox=0.6618, matched_ious=0.5331, d_time=0.01(0.01), f_time=1.37(1.37), b_time=1.38(1.38)  Time cost: 57:23/31:34 [14:18:32/15:20:56]  Acc_iter 37250       Data time: 0.01(0.01)  Forward time: 1.37(1.37)  Batch time: 1.38(1.38)
2025-09-02 12:04:12,863   INFO  Train:   10/20 ( 50%) [2541/3862 ( 66%)]  Loss: 1.383 (1.25)  LR: 9.536e-04  Grad: 17.0253  max=0.6451(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7314(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4980, loss_cls=0.0828, loss_bbox=0.6492, matched_ious=0.5298, d_time=0.00(0.01), f_time=1.27(1.37), b_time=1.28(1.38)  Time cost: 58:32/30:25 [14:19:41/15:19:52]  Acc_iter 37300       Data time: 0.00(0.01)  Forward time: 1.27(1.37)  Batch time: 1.28(1.38)
2025-09-02 12:05:21,397   INFO  Train:   10/20 ( 50%) [2591/3862 ( 67%)]  Loss: 1.534 (1.24)  LR: 9.529e-04  Grad: 17.0938  max=0.6472(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7340(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4959, loss_cls=0.0818, loss_bbox=0.6205, matched_ious=0.5307, d_time=0.01(0.01), f_time=1.32(1.37), b_time=1.33(1.38)  Time cost: 59:41/29:16 [14:20:50/15:18:34]  Acc_iter 37350       Data time: 0.01(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.38)
2025-09-02 12:06:29,199   INFO  Train:   10/20 ( 50%) [2641/3862 ( 68%)]  Loss: 1.001 (1.24)  LR: 9.522e-04  Grad: 17.1134  max=0.6488(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7360(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4940, loss_cls=0.0807, loss_bbox=0.6120, matched_ious=0.5349, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.30(1.38)  Time cost: 1:00:48/28:06 [14:21:58/15:17:06]  Acc_iter 37400       Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.30(1.38)
2025-09-02 12:07:38,226   INFO  Train:   10/20 ( 50%) [2691/3862 ( 70%)]  Loss: 1.138 (1.24)  LR: 9.515e-04  Grad: 20.0687  max=9.2533(module.vfe.pfn_layers.0.linear.weight)  min: -0.7400(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4868, loss_cls=0.0823, loss_bbox=0.6134, matched_ious=0.5364, d_time=0.01(0.01), f_time=1.35(1.37), b_time=1.36(1.38)  Time cost: 1:01:58/26:57 [14:23:07/15:15:56]  Acc_iter 37450       Data time: 0.01(0.01)  Forward time: 1.35(1.37)  Batch time: 1.36(1.38)
2025-09-02 12:08:45,783   INFO  Train:   10/20 ( 50%) [2741/3862 ( 71%)]  Loss: 1.100 (1.24)  LR: 9.507e-04  Grad: 17.2395  max=0.7082(module.vfe.pfn_layers.0.linear.weight)  min: -0.7375(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4851, loss_cls=0.0806, loss_bbox=0.6356, matched_ious=0.5336, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.29(1.38)  Time cost: 1:03:05/25:47 [14:24:14/15:14:26]  Acc_iter 37500       Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.29(1.38)
2025-09-02 12:09:55,226   INFO  Train:   10/20 ( 50%) [2791/3862 ( 72%)]  Loss: 1.124 (1.24)  LR: 9.500e-04  Grad: 17.3210  max=0.6655(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9679(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5035, loss_cls=0.0805, loss_bbox=0.6424, matched_ious=0.5383, d_time=0.01(0.01), f_time=1.25(1.37), b_time=1.26(1.38)  Time cost: 1:04:15/24:38 [14:25:24/15:13:22]  Acc_iter 37550       Data time: 0.01(0.01)  Forward time: 1.25(1.37)  Batch time: 1.26(1.38)
2025-09-02 12:11:03,362   INFO  Train:   10/20 ( 50%) [2841/3862 ( 74%)]  Loss: 1.028 (1.24)  LR: 9.493e-04  Grad: 17.3569  max=0.6628(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7418(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4928, loss_cls=0.0827, loss_bbox=0.6426, matched_ious=0.5340, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.30(1.38)  Time cost: 1:05:23/23:29 [14:26:32/15:12:01]  Acc_iter 37600       Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.30(1.38)
2025-09-02 12:12:12,884   INFO  Train:   10/20 ( 50%) [2891/3862 ( 75%)]  Loss: 1.165 (1.24)  LR: 9.485e-04  Grad: 17.4178  max=0.6644(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7417(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4905, loss_cls=0.0813, loss_bbox=0.6339, matched_ious=0.5299, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.33(1.38)  Time cost: 1:06:32/22:20 [14:27:41/15:10:59]  Acc_iter 37650       Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.33(1.38)
2025-09-02 12:13:22,468   INFO  Train:   10/20 ( 50%) [2941/3862 ( 76%)]  Loss: 1.003 (1.24)  LR: 9.478e-04  Grad: 17.4472  max=0.6679(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7479(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5104, loss_cls=0.0848, loss_bbox=0.6236, matched_ious=0.5374, d_time=0.01(0.01), f_time=1.23(1.37), b_time=1.24(1.38)  Time cost: 1:07:42/21:11 [14:28:51/15:09:57]  Acc_iter 37700       Data time: 0.01(0.01)  Forward time: 1.23(1.37)  Batch time: 1.24(1.38)
2025-09-02 12:14:30,272   INFO  Train:   10/20 ( 50%) [2991/3862 ( 77%)]  Loss: 1.092 (1.24)  LR: 9.470e-04  Grad: 17.7668  max=2.6528(module.vfe.pfn_layers.0.linear.weight)  min: -1.3679(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5005, loss_cls=0.0821, loss_bbox=0.6279, matched_ious=0.5376, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 1:08:50/20:02 [14:29:59/15:08:32]  Acc_iter 37750       Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 12:15:39,331   INFO  Train:   10/20 ( 50%) [3041/3862 ( 79%)]  Loss: 1.558 (1.24)  LR: 9.462e-04  Grad: 6.5278  max=0.5887(module.vfe.pfn_layers.0.linear.weight)  min: -1.4771(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4996, loss_cls=0.0833, loss_bbox=0.6187, matched_ious=0.5302, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.36(1.38)  Time cost: 1:09:59/18:53 [14:31:08/15:07:23]  Acc_iter 37800       Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.36(1.38)
2025-09-02 12:16:48,845   INFO  Train:   10/20 ( 50%) [3091/3862 ( 80%)]  Loss: 1.441 (1.24)  LR: 9.455e-04  Grad: 6.6123  max=1.3696(module.vfe.pfn_layers.0.linear.weight)  min: -1.1371(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5132, loss_cls=0.0839, loss_bbox=0.6168, matched_ious=0.5327, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.39(1.38)  Time cost: 1:11:08/17:44 [14:32:17/15:06:21]  Acc_iter 37850       Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.39(1.38)
2025-09-02 12:17:57,780   INFO  Train:   10/20 ( 50%) [3141/3862 ( 81%)]  Loss: 0.9211 (1.24)  LR: 9.447e-04  Grad: 6.5066  max=0.5004(module.vfe.pfn_layers.0.linear.weight)  min: -0.5904(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4937, loss_cls=0.0817, loss_bbox=0.6191, matched_ious=0.5338, d_time=0.00(0.01), f_time=1.28(1.37), b_time=1.28(1.38)  Time cost: 1:12:17/16:35 [14:33:26/15:05:10]  Acc_iter 37900       Data time: 0.00(0.01)  Forward time: 1.28(1.37)  Batch time: 1.28(1.38)
2025-09-02 12:19:06,405   INFO  Train:   10/20 ( 50%) [3191/3862 ( 83%)]  Loss: 1.745 (1.24)  LR: 9.439e-04  Grad: 6.8144  max=0.8636(module.vfe.pfn_layers.0.linear.weight)  min: -1.6833(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4871, loss_cls=0.0825, loss_bbox=0.6162, matched_ious=0.5296, d_time=0.03(0.01), f_time=1.33(1.37), b_time=1.36(1.38)  Time cost: 1:13:26/15:26 [14:34:35/15:03:56]  Acc_iter 37950       Data time: 0.03(0.01)  Forward time: 1.33(1.37)  Batch time: 1.36(1.38)
2025-09-02 12:20:15,458   INFO  Train:   10/20 ( 50%) [3241/3862 ( 84%)]  Loss: 1.064 (1.24)  LR: 9.432e-04  Grad: 6.6202  max=0.3403(module.vfe.pfn_layers.0.linear.weight)  min: -0.4363(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4973, loss_cls=0.0823, loss_bbox=0.6464, matched_ious=0.5355, d_time=0.03(0.01), f_time=1.29(1.37), b_time=1.32(1.38)  Time cost: 1:14:35/14:17 [14:35:44/15:02:48]  Acc_iter 38000       Data time: 0.03(0.01)  Forward time: 1.29(1.37)  Batch time: 1.32(1.38)
2025-09-02 12:21:25,138   INFO  Train:   10/20 ( 50%) [3291/3862 ( 85%)]  Loss: 1.320 (1.24)  LR: 9.424e-04  Grad: 6.6505  max=0.3936(module.vfe.pfn_layers.0.linear.weight)  min: -0.3013(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4855, loss_cls=0.0820, loss_bbox=0.5934, matched_ious=0.5335, d_time=0.00(0.01), f_time=1.30(1.37), b_time=1.30(1.38)  Time cost: 1:15:44/13:08 [14:36:54/15:01:47]  Acc_iter 38050       Data time: 0.00(0.01)  Forward time: 1.30(1.37)  Batch time: 1.30(1.38)
2025-09-02 12:22:35,258   INFO  Train:   10/20 ( 50%) [3341/3862 ( 87%)]  Loss: 1.463 (1.24)  LR: 9.416e-04  Grad: 6.8161  max=0.7543(module.vfe.pfn_layers.0.linear.weight)  min: -0.9965(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4940, loss_cls=0.0842, loss_bbox=0.6354, matched_ious=0.5324, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.32(1.38)  Time cost: 1:16:55/11:59 [14:38:04/15:00:50]  Acc_iter 38100       Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.32(1.38)
2025-09-02 12:23:44,482   INFO  Train:   10/20 ( 50%) [3391/3862 ( 88%)]  Loss: 1.577 (1.23)  LR: 9.408e-04  Grad: 6.8079  max=0.8690(module.vfe.pfn_layers.0.linear.weight)  min: -0.3135(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4922, loss_cls=0.0832, loss_bbox=0.6255, matched_ious=0.5324, d_time=0.00(0.01), f_time=1.43(1.37), b_time=1.44(1.38)  Time cost: 1:18:04/10:50 [14:39:13/14:59:43]  Acc_iter 38150       Data time: 0.00(0.01)  Forward time: 1.43(1.37)  Batch time: 1.44(1.38)
2025-09-02 12:24:52,071   INFO  Train:   10/20 ( 50%) [3441/3862 ( 89%)]  Loss: 1.051 (1.23)  LR: 9.400e-04  Grad: 6.8611  max=0.5328(module.vfe.pfn_layers.0.linear.weight)  min: -0.6397(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4811, loss_cls=0.0792, loss_bbox=0.6516, matched_ious=0.5302, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.32(1.38)  Time cost: 1:19:11/09:41 [14:40:21/14:58:18]  Acc_iter 38200       Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.32(1.38)
2025-09-02 12:26:00,779   INFO  Train:   10/20 ( 50%) [3491/3862 ( 90%)]  Loss: 1.373 (1.23)  LR: 9.392e-04  Grad: 6.8653  max=0.5588(module.vfe.pfn_layers.0.linear.weight)  min: -0.5520(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4906, loss_cls=0.0805, loss_bbox=0.6199, matched_ious=0.5357, d_time=0.00(0.01), f_time=1.45(1.37), b_time=1.45(1.38)  Time cost: 1:20:20/08:32 [14:41:29/14:57:05]  Acc_iter 38250       Data time: 0.00(0.01)  Forward time: 1.45(1.37)  Batch time: 1.45(1.38)
2025-09-02 12:27:09,677   INFO  Train:   10/20 ( 50%) [3541/3862 ( 92%)]  Loss: 1.018 (1.23)  LR: 9.384e-04  Grad: 6.9012  max=0.2785(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2841(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4870, loss_cls=0.0809, loss_bbox=0.6206, matched_ious=0.5347, d_time=0.00(0.01), f_time=1.41(1.37), b_time=1.42(1.38)  Time cost: 1:21:29/07:23 [14:42:38/14:55:55]  Acc_iter 38300       Data time: 0.00(0.01)  Forward time: 1.41(1.37)  Batch time: 1.42(1.38)
2025-09-02 12:28:19,249   INFO  Train:   10/20 ( 50%) [3591/3862 ( 93%)]  Loss: 1.238 (1.23)  LR: 9.375e-04  Grad: 6.9862  max=0.6775(module.vfe.pfn_layers.0.linear.weight)  min: -0.4449(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4807, loss_cls=0.0810, loss_bbox=0.6378, matched_ious=0.5315, d_time=0.01(0.01), f_time=1.36(1.37), b_time=1.37(1.38)  Time cost: 1:22:39/06:14 [14:43:48/14:54:52]  Acc_iter 38350       Data time: 0.01(0.01)  Forward time: 1.36(1.37)  Batch time: 1.37(1.38)
2025-09-02 12:29:26,904   INFO  Train:   10/20 ( 50%) [3641/3862 ( 94%)]  Loss: 1.017 (1.23)  LR: 9.367e-04  Grad: 6.9886  max=0.3616(module.vfe.pfn_layers.0.linear.weight)  min: -0.3029(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4923, loss_cls=0.0828, loss_bbox=0.6096, matched_ious=0.5333, d_time=0.01(0.01), f_time=1.29(1.37), b_time=1.29(1.38)  Time cost: 1:23:46/05:05 [14:44:55/14:53:28]  Acc_iter 38400       Data time: 0.01(0.01)  Forward time: 1.29(1.37)  Batch time: 1.29(1.38)
2025-09-02 12:30:35,390   INFO  Train:   10/20 ( 50%) [3691/3862 ( 96%)]  Loss: 1.445 (1.23)  LR: 9.359e-04  Grad: 7.0946  max=0.7706(module.vfe.pfn_layers.0.linear.weight)  min: -0.3083(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4896, loss_cls=0.0796, loss_bbox=0.6293, matched_ious=0.5308, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.29(1.38)  Time cost: 1:24:55/03:55 [14:46:04/14:52:14]  Acc_iter 38450       Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.29(1.38)
2025-09-02 12:31:43,164   INFO  Train:   10/20 ( 50%) [3741/3862 ( 97%)]  Loss: 1.375 (1.23)  LR: 9.350e-04  Grad: 7.2929  max=0.2943(module.vfe.pfn_layers.0.linear.weight)  min: -1.2167(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4685, loss_cls=0.0792, loss_bbox=0.6231, matched_ious=0.5321, d_time=0.01(0.01), f_time=1.39(1.37), b_time=1.40(1.38)  Time cost: 1:26:02/02:46 [14:47:12/14:50:52]  Acc_iter 38500       Data time: 0.01(0.01)  Forward time: 1.39(1.37)  Batch time: 1.40(1.38)
2025-09-02 12:32:50,624   INFO  Train:   10/20 ( 50%) [3791/3862 ( 98%)]  Loss: 1.148 (1.23)  LR: 9.342e-04  Grad: 7.8879  max=3.2570(module.vfe.pfn_layers.0.linear.weight)  min: -0.3170(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4810, loss_cls=0.0797, loss_bbox=0.6075, matched_ious=0.5371, d_time=0.00(0.01), f_time=1.30(1.37), b_time=1.31(1.38)  Time cost: 1:27:10/01:37 [14:48:19/14:49:27]  Acc_iter 38550       Data time: 0.00(0.01)  Forward time: 1.30(1.37)  Batch time: 1.31(1.38)
2025-09-02 12:34:00,730   INFO  Train:   10/20 ( 50%) [3841/3862 ( 99%)]  Loss: 1.282 (1.23)  LR: 9.334e-04  Grad: 7.2125  max=0.3049(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3402(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5060, loss_cls=0.0846, loss_bbox=0.6263, matched_ious=0.5399, d_time=0.45(0.01), f_time=1.33(1.37), b_time=1.77(1.38)  Time cost: 1:28:20/00:28 [14:49:29/14:48:30]  Acc_iter 38600       Data time: 0.45(0.01)  Forward time: 1.33(1.37)  Batch time: 1.77(1.38)
2025-09-02 12:34:27,372   INFO  Train:   10/20 ( 50%) [3861/3862 (100%)]  Loss: 1.135 (1.23)  LR: 9.330e-04  Grad: 7.2570  max=0.5978(module.vfe.pfn_layers.0.linear.weight)  min: -0.3204(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4817, loss_cls=0.0815, loss_bbox=0.6390, matched_ious=0.5357, d_time=0.00(0.01), f_time=1.28(1.37), b_time=1.29(1.38)  Time cost: 1:28:47/00:01 [14:49:56/14:47:53]  Acc_iter 38620       Data time: 0.00(0.01)  Forward time: 1.28(1.37)  Batch time: 1.29(1.38)

                                               [Aepochs:  50%|█████     | 10/20 [14:49:57<14:50:10, 5341.10s/it]epochs:  50%|█████     | 10/20 [14:49:57<14:50:10, 5341.08s/it]epochs:  50%|█████     | 10/20 [14:49:57<14:50:10, 5341.08s/it]epochs:  50%|█████     | 10/20 [14:49:57<14:50:10, 5341.08s/it]epochs:  50%|█████     | 10/20 [14:49:57<14:50:10, 5341.08s/it]epochs:  50%|█████     | 10/20 [14:49:57<14:50:10, 5341.08s/it]epochs:  50%|█████     | 10/20 [14:49:57<14:50:10, 5341.08s/it]epochs:  50%|█████     | 10/20 [14:49:57<14:50:10, 5341.10s/it]
train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-02 12:34:36,238   INFO  Train:   11/20 ( 55%) [   0/3862 (  0%)]  Loss: 1.305 (1.30)  LR: 9.330e-04  Grad: 7.2672  max=0.6872(module.vfe.pfn_layers.0.linear.weight)  min: -0.3201(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5557, loss_cls=0.0882, loss_bbox=0.6607, matched_ious=0.4944, d_time=2.85(2.85), f_time=4.14(4.14), b_time=6.99(6.99)  Time cost: 00:06/6:54:25 [14:50:05/69:04:10]  Acc_iter 38621       Data time: 2.85(2.85)  Forward time: 4.14(4.14)  Batch time: 6.99(6.99)
2025-09-02 12:35:16,713   INFO  Train:   11/20 ( 55%) [  29/3862 (  1%)]  Loss: 0.9910 (1.20)  LR: 9.325e-04  Grad: 10.0986  max=0.7383(module.vfe.pfn_layers.0.linear.weight)  min: -6.5098(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4940, loss_cls=0.0820, loss_bbox=0.6231, matched_ious=0.5338, d_time=0.00(0.10), f_time=1.33(1.48), b_time=1.34(1.58)  Time cost: 00:46/1:39:53 [14:50:45/16:45:44]  Acc_iter 38650       Data time: 0.00(0.10)  Forward time: 1.33(1.48)  Batch time: 1.34(1.58)
2025-09-02 12:36:25,093   INFO  Train:   11/20 ( 55%) [  79/3862 (  2%)]  Loss: 1.050 (1.18)  LR: 9.317e-04  Grad: 8.0496  max=1.8414(module.vfe.pfn_layers.0.linear.weight)  min: -2.5444(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4847, loss_cls=0.0784, loss_bbox=0.6113, matched_ious=0.5345, d_time=0.00(0.04), f_time=1.48(1.40), b_time=1.48(1.45)  Time cost: 01:55/1:30:51 [14:51:54/15:25:44]  Acc_iter 38700       Data time: 0.00(0.04)  Forward time: 1.48(1.40)  Batch time: 1.48(1.45)
2025-09-02 12:37:34,749   INFO  Train:   11/20 ( 55%) [ 129/3862 (  3%)]  Loss: 1.437 (1.20)  LR: 9.308e-04  Grad: 7.3966  max=0.6205(module.vfe.pfn_layers.0.linear.weight)  min: -0.3220(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4930, loss_cls=0.0815, loss_bbox=0.6442, matched_ious=0.5326, d_time=0.00(0.03), f_time=1.40(1.40), b_time=1.40(1.43)  Time cost: 03:04/1:28:30 [14:53:03/15:12:40]  Acc_iter 38750       Data time: 0.00(0.03)  Forward time: 1.40(1.40)  Batch time: 1.40(1.43)
2025-09-02 12:38:43,916   INFO  Train:   11/20 ( 55%) [ 179/3862 (  5%)]  Loss: 1.536 (1.19)  LR: 9.299e-04  Grad: 7.4577  max=0.7138(module.vfe.pfn_layers.0.linear.weight)  min: -0.4093(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4778, loss_cls=0.0788, loss_bbox=0.6046, matched_ious=0.5403, d_time=0.00(0.02), f_time=1.33(1.39), b_time=1.33(1.41)  Time cost: 04:14/1:26:39 [14:54:12/15:04:29]  Acc_iter 38800       Data time: 0.00(0.02)  Forward time: 1.33(1.39)  Batch time: 1.33(1.41)
2025-09-02 12:39:53,013   INFO  Train:   11/20 ( 55%) [ 229/3862 (  6%)]  Loss: 1.302 (1.19)  LR: 9.291e-04  Grad: 7.6626  max=0.3433(module.vfe.pfn_layers.0.linear.weight)  min: -1.2459(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4881, loss_cls=0.0810, loss_bbox=0.6141, matched_ious=0.5365, d_time=0.01(0.02), f_time=1.31(1.39), b_time=1.31(1.41)  Time cost: 05:23/1:25:05 [14:55:22/14:59:10]  Acc_iter 38850       Data time: 0.01(0.02)  Forward time: 1.31(1.39)  Batch time: 1.31(1.41)
2025-09-02 12:41:03,163   INFO  Train:   11/20 ( 55%) [ 279/3862 (  7%)]  Loss: 0.8364 (1.18)  LR: 9.282e-04  Grad: 7.6024  max=0.6235(module.vfe.pfn_layers.0.linear.weight)  min: -0.3224(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4805, loss_cls=0.0810, loss_bbox=0.6043, matched_ious=0.5368, d_time=0.00(0.02), f_time=1.41(1.39), b_time=1.41(1.41)  Time cost: 06:33/1:23:53 [14:56:32/14:57:44]  Acc_iter 38900       Data time: 0.00(0.02)  Forward time: 1.41(1.39)  Batch time: 1.41(1.41)
2025-09-02 12:42:11,214   INFO  Train:   11/20 ( 55%) [ 329/3862 (  9%)]  Loss: 1.359 (1.19)  LR: 9.273e-04  Grad: 7.7204  max=0.8702(module.vfe.pfn_layers.0.linear.weight)  min: -0.4824(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4845, loss_cls=0.0794, loss_bbox=0.6462, matched_ious=0.5328, d_time=0.01(0.02), f_time=1.27(1.38), b_time=1.28(1.40)  Time cost: 07:41/1:22:19 [14:57:40/14:52:19]  Acc_iter 38950       Data time: 0.01(0.02)  Forward time: 1.27(1.38)  Batch time: 1.28(1.40)
2025-09-02 12:43:20,479   INFO  Train:   11/20 ( 55%) [ 379/3862 ( 10%)]  Loss: 1.132 (1.19)  LR: 9.264e-04  Grad: 8.0456  max=0.3226(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.9922(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5044, loss_cls=0.0841, loss_bbox=0.6214, matched_ious=0.5337, d_time=0.01(0.02), f_time=1.44(1.38), b_time=1.44(1.40)  Time cost: 08:50/1:21:04 [14:58:49/14:50:04]  Acc_iter 39000       Data time: 0.01(0.02)  Forward time: 1.44(1.38)  Batch time: 1.44(1.40)
2025-09-02 12:44:28,398   INFO  Train:   11/20 ( 55%) [ 429/3862 ( 11%)]  Loss: 1.353 (1.19)  LR: 9.256e-04  Grad: 7.9298  max=1.2460(module.vfe.pfn_layers.0.linear.weight)  min: -0.3313(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4998, loss_cls=0.0822, loss_bbox=0.6256, matched_ious=0.5325, d_time=0.00(0.01), f_time=1.39(1.38), b_time=1.39(1.39)  Time cost: 09:58/1:19:39 [14:59:57/14:46:05]  Acc_iter 39050       Data time: 0.00(0.01)  Forward time: 1.39(1.38)  Batch time: 1.39(1.39)
2025-09-02 12:45:38,478   INFO  Train:   11/20 ( 55%) [ 479/3862 ( 12%)]  Loss: 1.335 (1.19)  LR: 9.247e-04  Grad: 7.8888  max=0.6854(module.vfe.pfn_layers.0.linear.weight)  min: -0.3320(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4961, loss_cls=0.0806, loss_bbox=0.6124, matched_ious=0.5334, d_time=0.01(0.01), f_time=1.27(1.38), b_time=1.27(1.39)  Time cost: 11:08/1:18:32 [15:01:07/14:45:33]  Acc_iter 39100       Data time: 0.01(0.01)  Forward time: 1.27(1.38)  Batch time: 1.27(1.39)
2025-09-02 12:46:48,299   INFO  Train:   11/20 ( 55%) [ 529/3862 ( 14%)]  Loss: 1.402 (1.19)  LR: 9.238e-04  Grad: 8.1651  max=1.8701(module.vfe.pfn_layers.0.linear.weight)  min: -0.6259(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4901, loss_cls=0.0802, loss_bbox=0.6099, matched_ious=0.5327, d_time=0.01(0.01), f_time=1.43(1.38), b_time=1.44(1.39)  Time cost: 12:18/1:17:24 [15:02:17/14:44:35]  Acc_iter 39150       Data time: 0.01(0.01)  Forward time: 1.43(1.38)  Batch time: 1.44(1.39)
2025-09-02 12:47:57,244   INFO  Train:   11/20 ( 55%) [ 579/3862 ( 15%)]  Loss: 1.430 (1.19)  LR: 9.229e-04  Grad: 26.8404  max=23.4187(module.vfe.pfn_layers.0.linear.weight)  min: -9.7400(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4835, loss_cls=0.0817, loss_bbox=0.6038, matched_ious=0.5348, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.37(1.39)  Time cost: 13:27/1:16:10 [15:03:26/14:42:38]  Acc_iter 39200       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.37(1.39)
2025-09-02 12:49:06,193   INFO  Train:   11/20 ( 55%) [ 629/3862 ( 16%)]  Loss: 1.372 (1.19)  LR: 9.220e-04  Grad: 9.0393  max=4.1072(module.vfe.pfn_layers.0.linear.weight)  min: -0.3328(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4947, loss_cls=0.0816, loss_bbox=0.6329, matched_ious=0.5364, d_time=0.00(0.01), f_time=1.22(1.38), b_time=1.23(1.39)  Time cost: 14:36/1:14:57 [15:04:35/14:40:49]  Acc_iter 39250       Data time: 0.00(0.01)  Forward time: 1.22(1.38)  Batch time: 1.23(1.39)
2025-09-02 12:50:13,863   INFO  Train:   11/20 ( 55%) [ 679/3862 ( 18%)]  Loss: 1.221 (1.19)  LR: 9.211e-04  Grad: 8.0921  max=0.3314(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3331(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4838, loss_cls=0.0817, loss_bbox=0.6024, matched_ious=0.5420, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 15:44/1:13:39 [15:05:42/14:37:54]  Acc_iter 39300       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 12:51:23,530   INFO  Train:   11/20 ( 55%) [ 729/3862 ( 19%)]  Loss: 1.167 (1.19)  LR: 9.201e-04  Grad: 8.1408  max=0.3863(module.vfe.pfn_layers.0.linear.weight)  min: -0.3375(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4902, loss_cls=0.0829, loss_bbox=0.5952, matched_ious=0.5330, d_time=0.00(0.01), f_time=1.38(1.38), b_time=1.38(1.39)  Time cost: 16:53/1:12:30 [15:06:52/14:36:58]  Acc_iter 39350       Data time: 0.00(0.01)  Forward time: 1.38(1.38)  Batch time: 1.38(1.39)
2025-09-02 12:52:33,595   INFO  Train:   11/20 ( 55%) [ 779/3862 ( 20%)]  Loss: 1.431 (1.19)  LR: 9.192e-04  Grad: 8.2977  max=0.6000(module.vfe.pfn_layers.0.linear.weight)  min: -1.1090(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4999, loss_cls=0.0824, loss_bbox=0.6087, matched_ious=0.5347, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.32(1.39)  Time cost: 18:03/1:11:23 [15:08:02/14:36:19]  Acc_iter 39400       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.32(1.39)
2025-09-02 12:53:43,061   INFO  Train:   11/20 ( 55%) [ 829/3862 ( 21%)]  Loss: 1.276 (1.19)  LR: 9.183e-04  Grad: 8.8811  max=2.3321(module.vfe.pfn_layers.0.linear.weight)  min: -2.0730(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4773, loss_cls=0.0775, loss_bbox=0.6131, matched_ious=0.5352, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.32(1.39)  Time cost: 19:13/1:10:14 [15:09:12/14:35:09]  Acc_iter 39450       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.32(1.39)
2025-09-02 12:54:52,114   INFO  Train:   11/20 ( 55%) [ 879/3862 ( 23%)]  Loss: 1.399 (1.19)  LR: 9.174e-04  Grad: 8.3113  max=0.3944(module.vfe.pfn_layers.0.linear.weight)  min: -0.3992(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4980, loss_cls=0.0837, loss_bbox=0.6336, matched_ious=0.5361, d_time=0.01(0.01), f_time=1.39(1.38), b_time=1.40(1.39)  Time cost: 20:22/1:09:03 [15:10:21/14:33:42]  Acc_iter 39500       Data time: 0.01(0.01)  Forward time: 1.39(1.38)  Batch time: 1.40(1.39)
2025-09-02 12:56:00,366   INFO  Train:   11/20 ( 55%) [ 929/3862 ( 24%)]  Loss: 1.641 (1.19)  LR: 9.164e-04  Grad: 8.3373  max=0.3482(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3444(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4948, loss_cls=0.0795, loss_bbox=0.6135, matched_ious=0.5391, d_time=0.00(0.01), f_time=1.38(1.38), b_time=1.38(1.39)  Time cost: 21:30/1:07:50 [15:11:29/14:31:44]  Acc_iter 39550       Data time: 0.00(0.01)  Forward time: 1.38(1.38)  Batch time: 1.38(1.39)
2025-09-02 12:57:08,739   INFO  Train:   11/20 ( 55%) [ 979/3862 ( 25%)]  Loss: 1.248 (1.19)  LR: 9.155e-04  Grad: 8.6135  max=1.1531(module.vfe.pfn_layers.0.linear.weight)  min: -1.3029(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4874, loss_cls=0.0797, loss_bbox=0.6363, matched_ious=0.5323, d_time=0.01(0.01), f_time=1.41(1.38), b_time=1.42(1.39)  Time cost: 22:38/1:06:37 [15:12:37/14:29:55]  Acc_iter 39600       Data time: 0.01(0.01)  Forward time: 1.41(1.38)  Batch time: 1.42(1.39)
2025-09-02 12:58:19,399   INFO  Train:   11/20 ( 55%) [1029/3862 ( 27%)]  Loss: 1.361 (1.19)  LR: 9.145e-04  Grad: 8.5034  max=0.6412(module.vfe.pfn_layers.0.linear.weight)  min: -0.3567(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4852, loss_cls=0.0808, loss_bbox=0.6079, matched_ious=0.5385, d_time=0.00(0.01), f_time=1.28(1.38), b_time=1.29(1.39)  Time cost: 23:49/1:05:32 [15:13:48/14:29:34]  Acc_iter 39650       Data time: 0.00(0.01)  Forward time: 1.28(1.38)  Batch time: 1.29(1.39)
2025-09-02 12:59:28,067   INFO  Train:   11/20 ( 55%) [1079/3862 ( 28%)]  Loss: 1.041 (1.19)  LR: 9.136e-04  Grad: 8.6601  max=0.6789(module.vfe.pfn_layers.0.linear.weight)  min: -1.1042(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4820, loss_cls=0.0811, loss_bbox=0.6277, matched_ious=0.5317, d_time=0.01(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 24:58/1:04:20 [15:14:57/14:28:00]  Acc_iter 39700       Data time: 0.01(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 13:00:36,814   INFO  Train:   11/20 ( 55%) [1129/3862 ( 29%)]  Loss: 1.025 (1.19)  LR: 9.126e-04  Grad: 8.6208  max=0.3644(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8919(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4905, loss_cls=0.0823, loss_bbox=0.6209, matched_ious=0.5341, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.32(1.39)  Time cost: 26:07/1:03:09 [15:16:05/14:26:30]  Acc_iter 39750       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.32(1.39)
2025-09-02 13:01:45,209   INFO  Train:   11/20 ( 55%) [1179/3862 ( 31%)]  Loss: 1.171 (1.19)  LR: 9.117e-04  Grad: 8.6518  max=0.5110(module.vfe.pfn_layers.0.linear.weight)  min: -0.3624(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4714, loss_cls=0.0807, loss_bbox=0.5904, matched_ious=0.5359, d_time=0.01(0.01), f_time=1.37(1.37), b_time=1.38(1.39)  Time cost: 27:15/1:01:58 [15:17:14/14:24:50]  Acc_iter 39800       Data time: 0.01(0.01)  Forward time: 1.37(1.37)  Batch time: 1.38(1.39)
2025-09-02 13:02:53,170   INFO  Train:   11/20 ( 55%) [1229/3862 ( 32%)]  Loss: 1.073 (1.19)  LR: 9.107e-04  Grad: 9.0995  max=0.8196(module.vfe.pfn_layers.0.linear.weight)  min: -2.1517(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4878, loss_cls=0.0795, loss_bbox=0.6140, matched_ious=0.5330, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.39(1.39)  Time cost: 28:23/1:00:46 [15:18:22/14:23:01]  Acc_iter 39850       Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.39(1.39)
2025-09-02 13:04:04,810   INFO  Train:   11/20 ( 55%) [1279/3862 ( 33%)]  Loss: 1.336 (1.19)  LR: 9.097e-04  Grad: 8.7726  max=0.5401(module.vfe.pfn_layers.0.linear.weight)  min: -0.3664(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4819, loss_cls=0.0785, loss_bbox=0.6314, matched_ious=0.5360, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.33(1.39)  Time cost: 29:35/59:41 [15:19:33/14:23:01]  Acc_iter 39900       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.33(1.39)
2025-09-02 13:05:13,782   INFO  Train:   11/20 ( 55%) [1329/3862 ( 34%)]  Loss: 1.107 (1.19)  LR: 9.088e-04  Grad: 9.0278  max=0.6068(module.vfe.pfn_layers.0.linear.weight)  min: -1.7685(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4806, loss_cls=0.0814, loss_bbox=0.6172, matched_ious=0.5350, d_time=0.01(0.01), f_time=1.30(1.38), b_time=1.31(1.39)  Time cost: 30:43/58:31 [15:20:42/14:21:42]  Acc_iter 39950       Data time: 0.01(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.39)
2025-09-02 13:06:23,816   INFO  Train:   11/20 ( 55%) [1379/3862 ( 36%)]  Loss: 1.152 (1.19)  LR: 9.078e-04  Grad: 8.8929  max=0.7385(module.vfe.pfn_layers.0.linear.weight)  min: -0.7260(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5005, loss_cls=0.0824, loss_bbox=0.6541, matched_ious=0.5320, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 31:54/57:23 [15:21:52/14:20:52]  Acc_iter 40000       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 13:07:32,367   INFO  Train:   11/20 ( 55%) [1429/3862 ( 37%)]  Loss: 1.374 (1.19)  LR: 9.068e-04  Grad: 8.9456  max=0.4612(module.vfe.pfn_layers.0.linear.weight)  min: -0.4783(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4821, loss_cls=0.0787, loss_bbox=0.6132, matched_ious=0.5400, d_time=0.01(0.01), f_time=1.26(1.38), b_time=1.26(1.39)  Time cost: 33:02/56:13 [15:23:01/14:19:21]  Acc_iter 40050       Data time: 0.01(0.01)  Forward time: 1.26(1.38)  Batch time: 1.26(1.39)
2025-09-02 13:08:40,891   INFO  Train:   11/20 ( 55%) [1479/3862 ( 38%)]  Loss: 1.336 (1.19)  LR: 9.058e-04  Grad: 8.9341  max=0.3845(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3734(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4999, loss_cls=0.0823, loss_bbox=0.6184, matched_ious=0.5316, d_time=0.00(0.01), f_time=1.27(1.37), b_time=1.27(1.39)  Time cost: 34:11/55:02 [15:24:09/14:17:52]  Acc_iter 40100       Data time: 0.00(0.01)  Forward time: 1.27(1.37)  Batch time: 1.27(1.39)
2025-09-02 13:09:50,736   INFO  Train:   11/20 ( 55%) [1529/3862 ( 40%)]  Loss: 1.290 (1.19)  LR: 9.048e-04  Grad: 8.9609  max=0.3885(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3779(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4862, loss_cls=0.0800, loss_bbox=0.5931, matched_ious=0.5395, d_time=0.01(0.01), f_time=1.39(1.38), b_time=1.40(1.39)  Time cost: 35:20/53:54 [15:25:19/14:16:56]  Acc_iter 40150       Data time: 0.01(0.01)  Forward time: 1.39(1.38)  Batch time: 1.40(1.39)
2025-09-02 13:10:58,918   INFO  Train:   11/20 ( 55%) [1579/3862 ( 41%)]  Loss: 1.073 (1.19)  LR: 9.038e-04  Grad: 9.4113  max=0.3843(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.7295(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4883, loss_cls=0.0800, loss_bbox=0.6107, matched_ious=0.5411, d_time=0.03(0.01), f_time=1.39(1.37), b_time=1.42(1.39)  Time cost: 36:29/52:43 [15:26:27/14:15:20]  Acc_iter 40200       Data time: 0.03(0.01)  Forward time: 1.39(1.37)  Batch time: 1.42(1.39)
2025-09-02 13:12:07,578   INFO  Train:   11/20 ( 55%) [1629/3862 ( 42%)]  Loss: 1.146 (1.19)  LR: 9.028e-04  Grad: 9.4663  max=2.2010(module.vfe.pfn_layers.0.linear.weight)  min: -0.4535(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4750, loss_cls=0.0770, loss_bbox=0.5958, matched_ious=0.5397, d_time=0.00(0.01), f_time=1.41(1.37), b_time=1.41(1.39)  Time cost: 37:37/51:33 [15:27:36/14:13:57]  Acc_iter 40250       Data time: 0.00(0.01)  Forward time: 1.41(1.37)  Batch time: 1.41(1.39)
2025-09-02 13:13:17,837   INFO  Train:   11/20 ( 55%) [1679/3862 ( 43%)]  Loss: 1.072 (1.19)  LR: 9.018e-04  Grad: 9.1421  max=0.3858(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5485(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4959, loss_cls=0.0832, loss_bbox=0.6295, matched_ious=0.5351, d_time=0.01(0.01), f_time=1.32(1.37), b_time=1.34(1.39)  Time cost: 38:48/50:25 [15:28:46/14:13:10]  Acc_iter 40300       Data time: 0.01(0.01)  Forward time: 1.32(1.37)  Batch time: 1.34(1.39)
2025-09-02 13:14:27,797   INFO  Train:   11/20 ( 55%) [1729/3862 ( 45%)]  Loss: 1.291 (1.19)  LR: 9.008e-04  Grad: 9.3432  max=0.8342(module.vfe.pfn_layers.0.linear.weight)  min: -1.5842(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4884, loss_cls=0.0819, loss_bbox=0.6350, matched_ious=0.5318, d_time=0.01(0.01), f_time=2.15(1.38), b_time=2.16(1.39)  Time cost: 39:57/49:16 [15:29:56/14:12:15]  Acc_iter 40350       Data time: 0.01(0.01)  Forward time: 2.15(1.38)  Batch time: 2.16(1.39)
2025-09-02 13:15:34,905   INFO  Train:   11/20 ( 55%) [1779/3862 ( 46%)]  Loss: 1.378 (1.19)  LR: 8.998e-04  Grad: 9.2280  max=0.3941(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3947(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4821, loss_cls=0.0805, loss_bbox=0.6123, matched_ious=0.5372, d_time=0.00(0.01), f_time=1.28(1.37), b_time=1.29(1.39)  Time cost: 41:05/48:04 [15:31:03/14:10:20]  Acc_iter 40400       Data time: 0.00(0.01)  Forward time: 1.28(1.37)  Batch time: 1.29(1.39)
2025-09-02 13:16:44,939   INFO  Train:   11/20 ( 55%) [1829/3862 ( 47%)]  Loss: 0.9411 (1.19)  LR: 8.988e-04  Grad: 9.5168  max=0.3905(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.5680(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4960, loss_cls=0.0819, loss_bbox=0.6148, matched_ious=0.5381, d_time=0.02(0.01), f_time=1.33(1.37), b_time=1.35(1.39)  Time cost: 42:15/46:56 [15:32:14/14:09:27]  Acc_iter 40450       Data time: 0.02(0.01)  Forward time: 1.33(1.37)  Batch time: 1.35(1.39)
2025-09-02 13:17:52,725   INFO  Train:   11/20 ( 55%) [1879/3862 ( 49%)]  Loss: 1.043 (1.19)  LR: 8.977e-04  Grad: 9.4164  max=0.4035(module.vfe.pfn_layers.0.linear.weight)  min: -0.6312(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4955, loss_cls=0.0803, loss_bbox=0.6155, matched_ious=0.5389, d_time=0.00(0.01), f_time=1.48(1.37), b_time=1.48(1.38)  Time cost: 43:22/45:45 [15:33:21/14:07:49]  Acc_iter 40500       Data time: 0.00(0.01)  Forward time: 1.48(1.37)  Batch time: 1.48(1.38)
2025-09-02 13:19:01,128   INFO  Train:   11/20 ( 55%) [1929/3862 ( 50%)]  Loss: 1.705 (1.19)  LR: 8.967e-04  Grad: 9.4180  max=0.3881(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4016(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4882, loss_cls=0.0812, loss_bbox=0.5985, matched_ious=0.5424, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.33(1.38)  Time cost: 44:31/44:35 [15:34:30/14:06:24]  Acc_iter 40550       Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.33(1.38)
2025-09-02 13:20:11,131   INFO  Train:   11/20 ( 55%) [1979/3862 ( 51%)]  Loss: 1.123 (1.19)  LR: 8.957e-04  Grad: 9.5497  max=0.4781(module.vfe.pfn_layers.0.linear.weight)  min: -0.8336(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4607, loss_cls=0.0762, loss_bbox=0.6009, matched_ious=0.5383, d_time=0.01(0.01), f_time=1.40(1.37), b_time=1.41(1.38)  Time cost: 45:41/43:27 [15:35:40/14:05:29]  Acc_iter 40600       Data time: 0.01(0.01)  Forward time: 1.40(1.37)  Batch time: 1.41(1.38)
2025-09-02 13:21:19,155   INFO  Train:   11/20 ( 55%) [2029/3862 ( 53%)]  Loss: 1.135 (1.18)  LR: 8.946e-04  Grad: 9.4906  max=0.4077(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4077(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4716, loss_cls=0.0807, loss_bbox=0.5889, matched_ious=0.5405, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.34(1.38)  Time cost: 46:49/42:16 [15:36:48/14:03:58]  Acc_iter 40650       Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.34(1.38)
2025-09-02 13:22:29,460   INFO  Train:   11/20 ( 55%) [2079/3862 ( 54%)]  Loss: 1.229 (1.18)  LR: 8.936e-04  Grad: 9.5822  max=0.3945(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4128(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4901, loss_cls=0.0798, loss_bbox=0.6317, matched_ious=0.5346, d_time=0.01(0.01), f_time=1.36(1.37), b_time=1.38(1.38)  Time cost: 47:59/41:08 [15:37:58/14:03:09]  Acc_iter 40700       Data time: 0.01(0.01)  Forward time: 1.36(1.37)  Batch time: 1.38(1.38)
2025-09-02 13:23:37,396   INFO  Train:   11/20 ( 55%) [2129/3862 ( 55%)]  Loss: 1.509 (1.18)  LR: 8.926e-04  Grad: 9.6112  max=0.4422(module.vfe.pfn_layers.0.linear.weight)  min: -0.4138(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4789, loss_cls=0.0797, loss_bbox=0.6050, matched_ious=0.5390, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.34(1.38)  Time cost: 49:07/39:58 [15:39:06/14:01:38]  Acc_iter 40750       Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.34(1.38)
2025-09-02 13:24:46,360   INFO  Train:   11/20 ( 55%) [2179/3862 ( 56%)]  Loss: 0.8961 (1.18)  LR: 8.915e-04  Grad: 9.6540  max=0.3936(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4495(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4666, loss_cls=0.0782, loss_bbox=0.6145, matched_ious=0.5372, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.36(1.38)  Time cost: 50:16/38:48 [15:40:15/14:00:24]  Acc_iter 40800       Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.36(1.38)
2025-09-02 13:25:56,394   INFO  Train:   11/20 ( 55%) [2229/3862 ( 58%)]  Loss: 1.029 (1.18)  LR: 8.904e-04  Grad: 9.6756  max=0.3908(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4183(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4742, loss_cls=0.0791, loss_bbox=0.6156, matched_ious=0.5389, d_time=0.00(0.01), f_time=1.25(1.37), b_time=1.25(1.38)  Time cost: 51:26/37:40 [15:41:25/13:59:29]  Acc_iter 40850       Data time: 0.00(0.01)  Forward time: 1.25(1.37)  Batch time: 1.25(1.38)
2025-09-02 13:27:04,718   INFO  Train:   11/20 ( 55%) [2279/3862 ( 59%)]  Loss: 1.051 (1.18)  LR: 8.894e-04  Grad: 9.7377  max=0.4010(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4543(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4720, loss_cls=0.0782, loss_bbox=0.5712, matched_ious=0.5399, d_time=0.01(0.01), f_time=1.28(1.37), b_time=1.29(1.38)  Time cost: 52:34/36:30 [15:42:33/13:58:06]  Acc_iter 40900       Data time: 0.01(0.01)  Forward time: 1.28(1.37)  Batch time: 1.29(1.38)
2025-09-02 13:28:14,092   INFO  Train:   11/20 ( 55%) [2329/3862 ( 60%)]  Loss: 0.9702 (1.18)  LR: 8.883e-04  Grad: 9.8489  max=0.7433(module.vfe.pfn_layers.0.linear.weight)  min: -0.4155(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4643, loss_cls=0.0786, loss_bbox=0.6014, matched_ious=0.5403, d_time=0.00(0.01), f_time=1.41(1.37), b_time=1.41(1.38)  Time cost: 53:44/35:21 [15:43:43/13:57:00]  Acc_iter 40950       Data time: 0.00(0.01)  Forward time: 1.41(1.37)  Batch time: 1.41(1.38)
2025-09-02 13:29:22,177   INFO  Train:   11/20 ( 55%) [2379/3862 ( 62%)]  Loss: 1.143 (1.18)  LR: 8.873e-04  Grad: 9.8383  max=0.4022(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5965(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4803, loss_cls=0.0798, loss_bbox=0.5931, matched_ious=0.5391, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.32(1.38)  Time cost: 54:52/34:11 [15:44:51/13:55:34]  Acc_iter 41000       Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.32(1.38)
2025-09-02 13:30:31,239   INFO  Train:   11/20 ( 55%) [2429/3862 ( 63%)]  Loss: 1.488 (1.18)  LR: 8.862e-04  Grad: 9.9307  max=1.0520(module.vfe.pfn_layers.0.linear.weight)  min: -0.4147(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4794, loss_cls=0.0791, loss_bbox=0.6049, matched_ious=0.5444, d_time=0.00(0.01), f_time=1.42(1.37), b_time=1.43(1.38)  Time cost: 56:01/33:02 [15:46:00/13:54:23]  Acc_iter 41050       Data time: 0.00(0.01)  Forward time: 1.42(1.37)  Batch time: 1.43(1.38)
2025-09-02 13:31:39,979   INFO  Train:   11/20 ( 55%) [2479/3862 ( 64%)]  Loss: 1.109 (1.18)  LR: 8.851e-04  Grad: 9.9033  max=0.3947(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4175(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4630, loss_cls=0.0776, loss_bbox=0.6028, matched_ious=0.5411, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.31(1.38)  Time cost: 57:10/31:52 [15:47:09/13:53:07]  Acc_iter 41100       Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.31(1.38)
2025-09-02 13:32:49,804   INFO  Train:   11/20 ( 55%) [2529/3862 ( 65%)]  Loss: 0.9087 (1.18)  LR: 8.840e-04  Grad: 10.0132  max=0.5972(module.vfe.pfn_layers.0.linear.weight)  min: -0.6676(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4823, loss_cls=0.0800, loss_bbox=0.6096, matched_ious=0.5399, d_time=0.00(0.01), f_time=1.39(1.37), b_time=1.40(1.38)  Time cost: 58:20/30:44 [15:48:18/13:52:08]  Acc_iter 41150       Data time: 0.00(0.01)  Forward time: 1.39(1.37)  Batch time: 1.40(1.38)
2025-09-02 13:33:59,091   INFO  Train:   11/20 ( 55%) [2579/3862 ( 67%)]  Loss: 1.212 (1.18)  LR: 8.829e-04  Grad: 10.0163  max=0.3911(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5907(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4899, loss_cls=0.0799, loss_bbox=0.6172, matched_ious=0.5433, d_time=0.01(0.01), f_time=1.28(1.37), b_time=1.29(1.38)  Time cost: 59:29/29:34 [15:49:28/13:51:00]  Acc_iter 41200       Data time: 0.01(0.01)  Forward time: 1.28(1.37)  Batch time: 1.29(1.38)
2025-09-02 13:35:08,854   INFO  Train:   11/20 ( 55%) [2629/3862 ( 68%)]  Loss: 1.117 (1.18)  LR: 8.818e-04  Grad: 10.0647  max=0.4198(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4136(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4600, loss_cls=0.0763, loss_bbox=0.5884, matched_ious=0.5420, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.33(1.38)  Time cost: 1:00:39/28:26 [15:50:37/13:49:59]  Acc_iter 41250       Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.38)
2025-09-02 13:36:17,950   INFO  Train:   11/20 ( 55%) [2679/3862 ( 69%)]  Loss: 0.9916 (1.18)  LR: 8.807e-04  Grad: 10.1108  max=0.4062(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5462(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4707, loss_cls=0.0783, loss_bbox=0.6198, matched_ious=0.5370, d_time=0.01(0.01), f_time=1.42(1.37), b_time=1.43(1.38)  Time cost: 1:01:48/27:16 [15:51:47/13:48:49]  Acc_iter 41300       Data time: 0.01(0.01)  Forward time: 1.42(1.37)  Batch time: 1.43(1.38)
2025-09-02 13:37:28,092   INFO  Train:   11/20 ( 55%) [2729/3862 ( 71%)]  Loss: 1.055 (1.18)  LR: 8.796e-04  Grad: 10.1717  max=0.6750(module.vfe.pfn_layers.0.linear.weight)  min: -0.4221(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4590, loss_cls=0.0760, loss_bbox=0.5911, matched_ious=0.5343, d_time=0.01(0.01), f_time=1.41(1.37), b_time=1.41(1.38)  Time cost: 1:02:58/26:08 [15:52:57/13:47:52]  Acc_iter 41350       Data time: 0.01(0.01)  Forward time: 1.41(1.37)  Batch time: 1.41(1.38)
2025-09-02 13:38:37,471   INFO  Train:   11/20 ( 55%) [2779/3862 ( 72%)]  Loss: 0.8995 (1.18)  LR: 8.785e-04  Grad: 10.2570  max=0.4195(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4366(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4687, loss_cls=0.0806, loss_bbox=0.5763, matched_ious=0.5470, d_time=0.01(0.01), f_time=1.40(1.37), b_time=1.40(1.38)  Time cost: 1:04:07/24:58 [15:54:06/13:46:45]  Acc_iter 41400       Data time: 0.01(0.01)  Forward time: 1.40(1.37)  Batch time: 1.40(1.38)
2025-09-02 13:39:47,775   INFO  Train:   11/20 ( 55%) [2829/3862 ( 73%)]  Loss: 1.281 (1.18)  LR: 8.774e-04  Grad: 10.3826  max=1.1005(module.vfe.pfn_layers.0.linear.weight)  min: -0.4380(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4689, loss_cls=0.0779, loss_bbox=0.5980, matched_ious=0.5378, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 1:05:17/23:50 [15:55:16/13:45:50]  Acc_iter 41450       Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 13:40:56,493   INFO  Train:   11/20 ( 55%) [2879/3862 ( 75%)]  Loss: 1.482 (1.18)  LR: 8.763e-04  Grad: 10.3716  max=0.4346(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5262(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4770, loss_cls=0.0799, loss_bbox=0.6529, matched_ious=0.5357, d_time=0.01(0.01), f_time=1.34(1.37), b_time=1.35(1.38)  Time cost: 1:06:26/22:40 [15:56:25/13:44:34]  Acc_iter 41500       Data time: 0.01(0.01)  Forward time: 1.34(1.37)  Batch time: 1.35(1.38)
2025-09-02 13:42:06,053   INFO  Train:   11/20 ( 55%) [2929/3862 ( 76%)]  Loss: 1.274 (1.18)  LR: 8.752e-04  Grad: 10.3911  max=0.4438(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5546(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4821, loss_cls=0.0796, loss_bbox=0.5899, matched_ious=0.5441, d_time=0.01(0.01), f_time=1.43(1.37), b_time=1.43(1.38)  Time cost: 1:07:36/21:31 [15:57:35/13:43:30]  Acc_iter 41550       Data time: 0.01(0.01)  Forward time: 1.43(1.37)  Batch time: 1.43(1.38)
2025-09-02 13:43:14,009   INFO  Train:   11/20 ( 55%) [2979/3862 ( 77%)]  Loss: 1.040 (1.18)  LR: 8.741e-04  Grad: 10.4361  max=0.4457(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4303(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4735, loss_cls=0.0803, loss_bbox=0.5773, matched_ious=0.5455, d_time=0.03(0.01), f_time=1.30(1.37), b_time=1.33(1.38)  Time cost: 1:08:44/20:22 [15:58:43/13:42:05]  Acc_iter 41600       Data time: 0.03(0.01)  Forward time: 1.30(1.37)  Batch time: 1.33(1.38)
2025-09-02 13:44:23,772   INFO  Train:   11/20 ( 55%) [3029/3862 ( 78%)]  Loss: 1.302 (1.17)  LR: 8.729e-04  Grad: 10.4932  max=0.4440(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4362(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4622, loss_cls=0.0753, loss_bbox=0.5756, matched_ious=0.5425, d_time=0.01(0.01), f_time=1.32(1.37), b_time=1.33(1.38)  Time cost: 1:09:53/19:12 [15:59:52/13:41:03]  Acc_iter 41650       Data time: 0.01(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.38)
2025-09-02 13:45:31,859   INFO  Train:   11/20 ( 55%) [3079/3862 ( 80%)]  Loss: 1.170 (1.17)  LR: 8.718e-04  Grad: 10.5596  max=0.5845(module.vfe.pfn_layers.0.linear.weight)  min: -0.5745(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4628, loss_cls=0.0783, loss_bbox=0.5844, matched_ious=0.5420, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 1:11:02/18:03 [16:01:00/13:39:41]  Acc_iter 41700       Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 13:46:40,393   INFO  Train:   11/20 ( 55%) [3129/3862 ( 81%)]  Loss: 1.106 (1.17)  LR: 8.707e-04  Grad: 10.5692  max=0.4501(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4456(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4584, loss_cls=0.0766, loss_bbox=0.5863, matched_ious=0.5438, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.34(1.38)  Time cost: 1:12:10/16:54 [16:02:09/13:38:24]  Acc_iter 41750       Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.34(1.38)
2025-09-02 13:47:50,008   INFO  Train:   11/20 ( 55%) [3179/3862 ( 82%)]  Loss: 1.272 (1.17)  LR: 8.695e-04  Grad: 10.6193  max=0.4469(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4375(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4713, loss_cls=0.0798, loss_bbox=0.5650, matched_ious=0.5482, d_time=0.01(0.01), f_time=1.33(1.37), b_time=1.34(1.38)  Time cost: 1:13:20/15:45 [16:03:19/13:37:20]  Acc_iter 41800       Data time: 0.01(0.01)  Forward time: 1.33(1.37)  Batch time: 1.34(1.38)
2025-09-02 13:48:59,285   INFO  Train:   11/20 ( 55%) [3229/3862 ( 84%)]  Loss: 1.185 (1.17)  LR: 8.684e-04  Grad: 10.6724  max=0.4575(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5230(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4676, loss_cls=0.0771, loss_bbox=0.5943, matched_ious=0.5437, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.37(1.38)  Time cost: 1:14:29/14:35 [16:04:28/13:36:11]  Acc_iter 41850       Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.37(1.38)
2025-09-02 13:50:10,167   INFO  Train:   11/20 ( 55%) [3279/3862 ( 85%)]  Loss: 1.065 (1.17)  LR: 8.672e-04  Grad: 10.7377  max=0.4680(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6076(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4662, loss_cls=0.0769, loss_bbox=0.5881, matched_ious=0.5473, d_time=0.01(0.01), f_time=1.32(1.37), b_time=1.33(1.38)  Time cost: 1:15:40/13:27 [16:05:39/13:35:21]  Acc_iter 41900       Data time: 0.01(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.38)
2025-09-02 13:51:18,140   INFO  Train:   11/20 ( 55%) [3329/3862 ( 86%)]  Loss: 0.9979 (1.17)  LR: 8.661e-04  Grad: 16.1925  max=1.4773(module.vfe.pfn_layers.0.linear.weight)  min: -11.9484(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4585, loss_cls=0.0771, loss_bbox=0.5977, matched_ious=0.5388, d_time=0.02(0.01), f_time=1.37(1.37), b_time=1.38(1.38)  Time cost: 1:16:48/12:17 [16:06:47/13:33:58]  Acc_iter 41950       Data time: 0.02(0.01)  Forward time: 1.37(1.37)  Batch time: 1.38(1.38)
2025-09-02 13:52:26,558   INFO  Train:   11/20 ( 55%) [3379/3862 ( 87%)]  Loss: 1.408 (1.17)  LR: 8.649e-04  Grad: 10.7691  max=0.4609(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4357(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4578, loss_cls=0.0754, loss_bbox=0.5816, matched_ious=0.5476, d_time=0.02(0.01), f_time=1.35(1.37), b_time=1.37(1.38)  Time cost: 1:17:56/11:08 [16:07:55/13:32:41]  Acc_iter 42000       Data time: 0.02(0.01)  Forward time: 1.35(1.37)  Batch time: 1.37(1.38)
2025-09-02 13:53:35,872   INFO  Train:   11/20 ( 55%) [3429/3862 ( 89%)]  Loss: 1.095 (1.17)  LR: 8.638e-04  Grad: 10.9452  max=0.6272(module.vfe.pfn_layers.0.linear.weight)  min: -1.4094(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4721, loss_cls=0.0789, loss_bbox=0.6008, matched_ious=0.5421, d_time=0.01(0.01), f_time=1.34(1.37), b_time=1.34(1.38)  Time cost: 1:19:06/09:59 [16:09:04/13:31:33]  Acc_iter 42050       Data time: 0.01(0.01)  Forward time: 1.34(1.37)  Batch time: 1.34(1.38)
2025-09-02 13:54:44,145   INFO  Train:   11/20 ( 55%) [3479/3862 ( 90%)]  Loss: 0.9931 (1.17)  LR: 8.626e-04  Grad: 10.5240  max=1.2470(module.vfe.pfn_layers.0.linear.weight)  min: -5.7296(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4723, loss_cls=0.0786, loss_bbox=0.5791, matched_ious=0.5456, d_time=0.03(0.01), f_time=1.38(1.37), b_time=1.41(1.38)  Time cost: 1:20:14/08:49 [16:10:13/13:30:15]  Acc_iter 42100       Data time: 0.03(0.01)  Forward time: 1.38(1.37)  Batch time: 1.41(1.38)
2025-09-02 13:55:53,680   INFO  Train:   11/20 ( 55%) [3529/3862 ( 91%)]  Loss: 1.109 (1.17)  LR: 8.614e-04  Grad: 8.5052  max=0.3952(module.vfe.pfn_layers.0.linear.weight)  min: -0.5485(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4659, loss_cls=0.0779, loss_bbox=0.6065, matched_ious=0.5349, d_time=0.01(0.01), f_time=1.27(1.37), b_time=1.28(1.38)  Time cost: 1:21:23/07:40 [16:11:22/13:29:09]  Acc_iter 42150       Data time: 0.01(0.01)  Forward time: 1.27(1.37)  Batch time: 1.28(1.38)
2025-09-02 13:57:01,645   INFO  Train:   11/20 ( 55%) [3579/3862 ( 93%)]  Loss: 1.093 (1.17)  LR: 8.603e-04  Grad: 8.5359  max=0.3532(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4614(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4513, loss_cls=0.0752, loss_bbox=0.5528, matched_ious=0.5427, d_time=0.01(0.01), f_time=1.26(1.37), b_time=1.26(1.38)  Time cost: 1:22:31/06:31 [16:12:30/13:27:48]  Acc_iter 42200       Data time: 0.01(0.01)  Forward time: 1.26(1.37)  Batch time: 1.26(1.38)
2025-09-02 13:58:08,597   INFO  Train:   11/20 ( 55%) [3629/3862 ( 94%)]  Loss: 1.070 (1.17)  LR: 8.591e-04  Grad: 8.5514  max=0.3633(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3579(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4513, loss_cls=0.0748, loss_bbox=0.5546, matched_ious=0.5445, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.34(1.38)  Time cost: 1:23:38/05:22 [16:13:37/13:26:18]  Acc_iter 42250       Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.34(1.38)
2025-09-02 13:59:16,382   INFO  Train:   11/20 ( 55%) [3679/3862 ( 95%)]  Loss: 1.060 (1.17)  LR: 8.579e-04  Grad: 8.6213  max=0.4188(module.vfe.pfn_layers.0.linear.weight)  min: -0.3572(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4836, loss_cls=0.0800, loss_bbox=0.6021, matched_ious=0.5385, d_time=0.00(0.01), f_time=1.35(1.37), b_time=1.36(1.38)  Time cost: 1:24:46/04:12 [16:14:45/13:24:56]  Acc_iter 42300       Data time: 0.00(0.01)  Forward time: 1.35(1.37)  Batch time: 1.36(1.38)
2025-09-02 14:00:25,526   INFO  Train:   11/20 ( 55%) [3729/3862 ( 97%)]  Loss: 1.109 (1.16)  LR: 8.567e-04  Grad: 8.7087  max=0.3673(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7311(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4513, loss_cls=0.0746, loss_bbox=0.5367, matched_ious=0.5467, d_time=0.00(0.01), f_time=1.28(1.37), b_time=1.28(1.38)  Time cost: 1:25:55/03:03 [16:15:54/13:23:47]  Acc_iter 42350       Data time: 0.00(0.01)  Forward time: 1.28(1.37)  Batch time: 1.28(1.38)
2025-09-02 14:01:34,986   INFO  Train:   11/20 ( 55%) [3779/3862 ( 98%)]  Loss: 1.001 (1.16)  LR: 8.555e-04  Grad: 8.7280  max=0.5538(module.vfe.pfn_layers.0.linear.weight)  min: -0.6015(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4654, loss_cls=0.0779, loss_bbox=0.5840, matched_ious=0.5450, d_time=0.03(0.01), f_time=1.35(1.37), b_time=1.37(1.38)  Time cost: 1:27:05/01:54 [16:17:04/13:22:41]  Acc_iter 42400       Data time: 0.03(0.01)  Forward time: 1.35(1.37)  Batch time: 1.37(1.38)
2025-09-02 14:02:43,794   INFO  Train:   11/20 ( 55%) [3829/3862 ( 99%)]  Loss: 1.021 (1.16)  LR: 8.543e-04  Grad: 8.7863  max=0.3754(module.vfe.pfn_layers.0.linear.weight)  min: -0.9812(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4766, loss_cls=0.0796, loss_bbox=0.6082, matched_ious=0.5394, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.30(1.38)  Time cost: 1:28:13/00:45 [16:18:12/13:21:29]  Acc_iter 42450       Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.30(1.38)
2025-09-02 14:03:26,089   INFO  Train:   11/20 ( 55%) [3861/3862 (100%)]  Loss: 1.011 (1.16)  LR: 8.536e-04  Grad: 8.7450  max=0.3533(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5658(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4841, loss_cls=0.0809, loss_bbox=0.6477, matched_ious=0.5375, d_time=0.00(0.01), f_time=1.25(1.37), b_time=1.25(1.38)  Time cost: 1:28:56/00:01 [16:18:55/13:20:27]  Acc_iter 42482       Data time: 0.00(0.01)  Forward time: 1.25(1.37)  Batch time: 1.25(1.38)

                                               [Aepochs:  55%|█████▌    | 11/20 [16:18:56<13:21:03, 5340.38s/it]epochs:  55%|█████▌    | 11/20 [16:18:56<13:21:03, 5340.36s/it]epochs:  55%|█████▌    | 11/20 [16:18:56<13:21:03, 5340.36s/it]epochs:  55%|█████▌    | 11/20 [16:18:56<13:21:03, 5340.36s/it]epochs:  55%|█████▌    | 11/20 [16:18:56<13:21:03, 5340.36s/it]epochs:  55%|█████▌    | 11/20 [16:18:56<13:21:03, 5340.37s/it]epochs:  55%|█████▌    | 11/20 [16:18:56<13:21:03, 5340.36s/it]epochs:  55%|█████▌    | 11/20 [16:18:56<13:21:03, 5340.36s/it]
train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-02 14:03:34,524   INFO  Train:   12/20 ( 60%) [   0/3862 (  0%)]  Loss: 0.9937 (0.994)  LR: 8.536e-04  Grad: 8.7118  max=0.3523(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3676(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4236, loss_cls=0.0807, loss_bbox=0.4894, matched_ious=0.5500, d_time=3.37(3.37), f_time=3.20(3.20), b_time=6.58(6.58)  Time cost: 00:06/6:42:07 [16:19:03/60:19:07]  Acc_iter 42483       Data time: 3.37(3.37)  Forward time: 3.20(3.20)  Batch time: 6.58(6.58)
2025-09-02 14:03:57,941   INFO  Train:   12/20 ( 60%) [  17/3862 (  0%)]  Loss: 1.243 (1.11)  LR: 8.531e-04  Grad: 8.7747  max=0.3541(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3662(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4588, loss_cls=0.0792, loss_bbox=0.5837, matched_ious=0.5359, d_time=0.00(0.20), f_time=1.39(1.47), b_time=1.39(1.67)  Time cost: 00:29/1:45:36 [16:19:27/15:54:15]  Acc_iter 42500       Data time: 0.00(0.20)  Forward time: 1.39(1.47)  Batch time: 1.39(1.67)
2025-09-02 14:05:06,480   INFO  Train:   12/20 ( 60%) [  67/3862 (  2%)]  Loss: 1.108 (1.11)  LR: 8.519e-04  Grad: 8.8492  max=0.5470(module.vfe.pfn_layers.0.linear.weight)  min: -0.3669(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4681, loss_cls=0.0755, loss_bbox=0.5693, matched_ious=0.5430, d_time=0.02(0.07), f_time=1.36(1.38), b_time=1.37(1.45)  Time cost: 01:38/1:31:20 [16:20:35/13:54:59]  Acc_iter 42550       Data time: 0.02(0.07)  Forward time: 1.36(1.38)  Batch time: 1.37(1.45)
2025-09-02 14:06:15,887   INFO  Train:   12/20 ( 60%) [ 117/3862 (  3%)]  Loss: 0.9612 (1.10)  LR: 8.507e-04  Grad: 8.9040  max=0.5148(module.vfe.pfn_layers.0.linear.weight)  min: -0.3694(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4577, loss_cls=0.0767, loss_bbox=0.5461, matched_ious=0.5460, d_time=0.00(0.04), f_time=1.28(1.38), b_time=1.29(1.42)  Time cost: 02:47/1:28:39 [16:21:44/13:40:05]  Acc_iter 42600       Data time: 0.00(0.04)  Forward time: 1.28(1.38)  Batch time: 1.29(1.42)
2025-09-02 14:07:24,856   INFO  Train:   12/20 ( 60%) [ 167/3862 (  4%)]  Loss: 1.054 (1.10)  LR: 8.495e-04  Grad: 8.9163  max=0.3806(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3663(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4495, loss_cls=0.0742, loss_bbox=0.5883, matched_ious=0.5472, d_time=0.03(0.04), f_time=1.34(1.37), b_time=1.37(1.41)  Time cost: 03:56/1:26:43 [16:22:53/13:31:51]  Acc_iter 42650       Data time: 0.03(0.04)  Forward time: 1.34(1.37)  Batch time: 1.37(1.41)
2025-09-02 14:08:35,542   INFO  Train:   12/20 ( 60%) [ 217/3862 (  6%)]  Loss: 1.088 (1.11)  LR: 8.483e-04  Grad: 9.0553  max=0.6746(module.vfe.pfn_layers.0.linear.weight)  min: -0.3717(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4753, loss_cls=0.0778, loss_bbox=0.5889, matched_ious=0.5437, d_time=0.00(0.03), f_time=1.32(1.38), b_time=1.32(1.41)  Time cost: 05:07/1:25:37 [16:24:04/13:31:24]  Acc_iter 42700       Data time: 0.00(0.03)  Forward time: 1.32(1.38)  Batch time: 1.32(1.41)
2025-09-02 14:09:43,409   INFO  Train:   12/20 ( 60%) [ 267/3862 (  7%)]  Loss: 1.178 (1.11)  LR: 8.471e-04  Grad: 9.0704  max=0.4611(module.vfe.pfn_layers.0.linear.weight)  min: -0.3767(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4517, loss_cls=0.0750, loss_bbox=0.5688, matched_ious=0.5370, d_time=0.03(0.03), f_time=1.45(1.37), b_time=1.48(1.40)  Time cost: 06:15/1:23:52 [16:25:12/13:24:38]  Acc_iter 42750       Data time: 0.03(0.03)  Forward time: 1.45(1.37)  Batch time: 1.48(1.40)
2025-09-02 14:10:52,283   INFO  Train:   12/20 ( 60%) [ 317/3862 (  8%)]  Loss: 0.8953 (1.11)  LR: 8.459e-04  Grad: 9.1856  max=0.5915(module.vfe.pfn_layers.0.linear.weight)  min: -0.4031(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4456, loss_cls=0.0733, loss_bbox=0.5716, matched_ious=0.5484, d_time=0.00(0.03), f_time=1.33(1.37), b_time=1.33(1.40)  Time cost: 07:24/1:22:29 [16:26:21/13:21:28]  Acc_iter 42800       Data time: 0.00(0.03)  Forward time: 1.33(1.37)  Batch time: 1.33(1.40)
2025-09-02 14:12:01,065   INFO  Train:   12/20 ( 60%) [ 367/3862 ( 10%)]  Loss: 0.9821 (1.11)  LR: 8.447e-04  Grad: 9.1959  max=0.5226(module.vfe.pfn_layers.0.linear.weight)  min: -0.3806(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4680, loss_cls=0.0767, loss_bbox=0.6129, matched_ious=0.5368, d_time=0.00(0.02), f_time=1.30(1.37), b_time=1.31(1.39)  Time cost: 08:32/1:21:10 [16:27:30/13:18:41]  Acc_iter 42850       Data time: 0.00(0.02)  Forward time: 1.30(1.37)  Batch time: 1.31(1.39)
2025-09-02 14:13:10,599   INFO  Train:   12/20 ( 60%) [ 417/3862 ( 11%)]  Loss: 1.474 (1.11)  LR: 8.434e-04  Grad: 9.2677  max=0.9369(module.vfe.pfn_layers.0.linear.weight)  min: -0.4298(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4724, loss_cls=0.0786, loss_bbox=0.5767, matched_ious=0.5412, d_time=0.01(0.02), f_time=1.33(1.37), b_time=1.34(1.39)  Time cost: 09:42/1:19:59 [16:28:39/13:17:21]  Acc_iter 42900       Data time: 0.01(0.02)  Forward time: 1.33(1.37)  Batch time: 1.34(1.39)
2025-09-02 14:14:22,129   INFO  Train:   12/20 ( 60%) [ 467/3862 ( 12%)]  Loss: 1.073 (1.12)  LR: 8.422e-04  Grad: 9.3269  max=0.5302(module.vfe.pfn_layers.0.linear.weight)  min: -0.3967(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4694, loss_cls=0.0774, loss_bbox=0.5990, matched_ious=0.5425, d_time=0.00(0.02), f_time=1.37(1.38), b_time=1.38(1.40)  Time cost: 10:53/1:19:03 [16:29:51/13:18:28]  Acc_iter 42950       Data time: 0.00(0.02)  Forward time: 1.37(1.38)  Batch time: 1.38(1.40)
2025-09-02 14:15:30,334   INFO  Train:   12/20 ( 60%) [ 517/3862 ( 13%)]  Loss: 1.101 (1.12)  LR: 8.409e-04  Grad: 9.3936  max=0.5736(module.vfe.pfn_layers.0.linear.weight)  min: -0.3921(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4541, loss_cls=0.0746, loss_bbox=0.5736, matched_ious=0.5413, d_time=0.00(0.02), f_time=1.28(1.38), b_time=1.28(1.39)  Time cost: 12:02/1:17:42 [16:30:59/13:15:29]  Acc_iter 43000       Data time: 0.00(0.02)  Forward time: 1.28(1.38)  Batch time: 1.28(1.39)
2025-09-02 14:16:38,803   INFO  Train:   12/20 ( 60%) [ 567/3862 ( 15%)]  Loss: 1.356 (1.12)  LR: 8.397e-04  Grad: 9.6448  max=2.0898(module.vfe.pfn_layers.0.linear.weight)  min: -0.4091(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4620, loss_cls=0.0748, loss_bbox=0.5967, matched_ious=0.5446, d_time=0.00(0.02), f_time=1.39(1.37), b_time=1.39(1.39)  Time cost: 13:10/1:16:25 [16:32:07/13:13:06]  Acc_iter 43050       Data time: 0.00(0.02)  Forward time: 1.39(1.37)  Batch time: 1.39(1.39)
2025-09-02 14:17:46,276   INFO  Train:   12/20 ( 60%) [ 617/3862 ( 16%)]  Loss: 0.9119 (1.12)  LR: 8.385e-04  Grad: 9.5841  max=1.3341(module.vfe.pfn_layers.0.linear.weight)  min: -0.4857(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4686, loss_cls=0.0789, loss_bbox=0.6038, matched_ious=0.5407, d_time=0.00(0.02), f_time=1.25(1.37), b_time=1.25(1.39)  Time cost: 14:18/1:15:05 [16:33:15/13:09:59]  Acc_iter 43100       Data time: 0.00(0.02)  Forward time: 1.25(1.37)  Batch time: 1.25(1.39)
2025-09-02 14:18:57,120   INFO  Train:   12/20 ( 60%) [ 667/3862 ( 17%)]  Loss: 1.145 (1.12)  LR: 8.372e-04  Grad: 9.5548  max=0.4241(module.vfe.pfn_layers.0.linear.weight)  min: -0.4003(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4653, loss_cls=0.0779, loss_bbox=0.5833, matched_ious=0.5442, d_time=0.01(0.02), f_time=1.36(1.37), b_time=1.37(1.39)  Time cost: 15:28/1:14:02 [16:34:26/13:10:03]  Acc_iter 43150       Data time: 0.01(0.02)  Forward time: 1.36(1.37)  Batch time: 1.37(1.39)
2025-09-02 14:20:06,738   INFO  Train:   12/20 ( 60%) [ 717/3862 ( 19%)]  Loss: 0.9545 (1.12)  LR: 8.360e-04  Grad: 9.5948  max=0.3920(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4072(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4610, loss_cls=0.0776, loss_bbox=0.5956, matched_ious=0.5410, d_time=0.01(0.02), f_time=1.45(1.38), b_time=1.45(1.39)  Time cost: 16:38/1:12:53 [16:35:35/13:08:57]  Acc_iter 43200       Data time: 0.01(0.02)  Forward time: 1.45(1.38)  Batch time: 1.45(1.39)
2025-09-02 14:21:17,372   INFO  Train:   12/20 ( 60%) [ 767/3862 ( 20%)]  Loss: 1.155 (1.12)  LR: 8.347e-04  Grad: 9.6016  max=0.3903(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6217(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4597, loss_cls=0.0730, loss_bbox=0.5973, matched_ious=0.5425, d_time=0.00(0.02), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 17:49/1:11:48 [16:36:46/13:08:36]  Acc_iter 43250       Data time: 0.00(0.02)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 14:22:25,572   INFO  Train:   12/20 ( 60%) [ 817/3862 ( 21%)]  Loss: 1.097 (1.12)  LR: 8.334e-04  Grad: 9.8026  max=0.9749(module.vfe.pfn_layers.0.linear.weight)  min: -1.3212(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4606, loss_cls=0.0777, loss_bbox=0.5823, matched_ious=0.5492, d_time=0.00(0.01), f_time=1.38(1.38), b_time=1.39(1.39)  Time cost: 18:57/1:10:33 [16:37:54/13:06:29]  Acc_iter 43300       Data time: 0.00(0.01)  Forward time: 1.38(1.38)  Batch time: 1.39(1.39)
2025-09-02 14:23:34,368   INFO  Train:   12/20 ( 60%) [ 867/3862 ( 22%)]  Loss: 1.085 (1.12)  LR: 8.322e-04  Grad: 9.6929  max=0.5451(module.vfe.pfn_layers.0.linear.weight)  min: -0.4104(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4602, loss_cls=0.0767, loss_bbox=0.5852, matched_ious=0.5448, d_time=0.00(0.01), f_time=1.27(1.38), b_time=1.27(1.39)  Time cost: 20:06/1:09:21 [16:39:03/13:04:51]  Acc_iter 43350       Data time: 0.00(0.01)  Forward time: 1.27(1.38)  Batch time: 1.27(1.39)
2025-09-02 14:24:43,108   INFO  Train:   12/20 ( 60%) [ 917/3862 ( 24%)]  Loss: 1.299 (1.12)  LR: 8.309e-04  Grad: 9.7174  max=0.3959(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4160(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4727, loss_cls=0.0809, loss_bbox=0.5930, matched_ious=0.5431, d_time=0.01(0.02), f_time=1.32(1.37), b_time=1.33(1.39)  Time cost: 21:14/1:08:09 [16:40:12/13:03:15]  Acc_iter 43400       Data time: 0.01(0.02)  Forward time: 1.32(1.37)  Batch time: 1.33(1.39)
2025-09-02 14:25:53,335   INFO  Train:   12/20 ( 60%) [ 967/3862 ( 25%)]  Loss: 1.247 (1.12)  LR: 8.296e-04  Grad: 9.8814  max=1.5098(module.vfe.pfn_layers.0.linear.weight)  min: -0.4172(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4619, loss_cls=0.0784, loss_bbox=0.6017, matched_ious=0.5431, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.34(1.39)  Time cost: 22:25/1:07:02 [16:41:22/13:02:33]  Acc_iter 43450       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.34(1.39)
2025-09-02 14:27:02,654   INFO  Train:   12/20 ( 60%) [1017/3862 ( 26%)]  Loss: 1.326 (1.12)  LR: 8.284e-04  Grad: 9.8162  max=0.3947(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4143(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4582, loss_cls=0.0773, loss_bbox=0.5810, matched_ious=0.5483, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.34(1.39)  Time cost: 23:34/1:05:52 [16:42:31/13:01:18]  Acc_iter 43500       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.34(1.39)
2025-09-02 14:28:10,701   INFO  Train:   12/20 ( 60%) [1067/3862 ( 28%)]  Loss: 1.015 (1.12)  LR: 8.271e-04  Grad: 9.8972  max=0.4340(module.vfe.pfn_layers.0.linear.weight)  min: -0.4106(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4555, loss_cls=0.0753, loss_bbox=0.5642, matched_ious=0.5451, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.33(1.39)  Time cost: 24:42/1:04:39 [16:43:39/12:59:24]  Acc_iter 43550       Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.33(1.39)
2025-09-02 14:29:19,441   INFO  Train:   12/20 ( 60%) [1117/3862 ( 29%)]  Loss: 1.255 (1.12)  LR: 8.258e-04  Grad: 9.9462  max=0.4039(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4134(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4590, loss_cls=0.0781, loss_bbox=0.6145, matched_ious=0.5397, d_time=0.01(0.01), f_time=1.45(1.37), b_time=1.45(1.39)  Time cost: 25:51/1:03:28 [16:44:48/12:57:55]  Acc_iter 43600       Data time: 0.01(0.01)  Forward time: 1.45(1.37)  Batch time: 1.45(1.39)
2025-09-02 14:30:27,439   INFO  Train:   12/20 ( 60%) [1167/3862 ( 30%)]  Loss: 0.9752 (1.12)  LR: 8.245e-04  Grad: 10.2310  max=1.3139(module.vfe.pfn_layers.0.linear.weight)  min: -1.6798(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4490, loss_cls=0.0724, loss_bbox=0.5640, matched_ious=0.5452, d_time=0.00(0.01), f_time=2.21(1.37), b_time=2.22(1.39)  Time cost: 26:59/1:02:15 [16:45:56/12:56:06]  Acc_iter 43650       Data time: 0.00(0.01)  Forward time: 2.21(1.37)  Batch time: 2.22(1.39)
2025-09-02 14:31:37,242   INFO  Train:   12/20 ( 60%) [1217/3862 ( 32%)]  Loss: 1.108 (1.12)  LR: 8.232e-04  Grad: 10.1101  max=1.1136(module.vfe.pfn_layers.0.linear.weight)  min: -0.5301(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4755, loss_cls=0.0780, loss_bbox=0.6050, matched_ious=0.5447, d_time=0.00(0.01), f_time=2.22(1.37), b_time=2.22(1.39)  Time cost: 28:08/1:01:07 [16:47:06/12:55:10]  Acc_iter 43700       Data time: 0.00(0.01)  Forward time: 2.22(1.37)  Batch time: 2.22(1.39)
2025-09-02 14:32:45,545   INFO  Train:   12/20 ( 60%) [1267/3862 ( 33%)]  Loss: 1.125 (1.12)  LR: 8.219e-04  Grad: 10.0969  max=0.8543(module.vfe.pfn_layers.0.linear.weight)  min: -0.4234(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4548, loss_cls=0.0752, loss_bbox=0.5383, matched_ious=0.5503, d_time=0.02(0.01), f_time=1.35(1.37), b_time=1.37(1.39)  Time cost: 29:17/59:56 [16:48:14/12:53:33]  Acc_iter 43750       Data time: 0.02(0.01)  Forward time: 1.35(1.37)  Batch time: 1.37(1.39)
2025-09-02 14:33:55,033   INFO  Train:   12/20 ( 60%) [1317/3862 ( 34%)]  Loss: 0.8511 (1.12)  LR: 8.206e-04  Grad: 10.4313  max=2.3684(module.vfe.pfn_layers.0.linear.weight)  min: -0.4223(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4475, loss_cls=0.0749, loss_bbox=0.5726, matched_ious=0.5483, d_time=0.00(0.01), f_time=1.26(1.37), b_time=1.27(1.39)  Time cost: 30:26/58:47 [16:49:24/12:52:29]  Acc_iter 43800       Data time: 0.00(0.01)  Forward time: 1.26(1.37)  Batch time: 1.27(1.39)
2025-09-02 14:35:04,791   INFO  Train:   12/20 ( 60%) [1367/3862 ( 35%)]  Loss: 0.9734 (1.12)  LR: 8.193e-04  Grad: 10.1243  max=0.4074(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4230(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4665, loss_cls=0.0750, loss_bbox=0.5665, matched_ious=0.5461, d_time=0.01(0.01), f_time=1.33(1.37), b_time=1.34(1.39)  Time cost: 31:36/57:38 [16:50:33/12:51:31]  Acc_iter 43850       Data time: 0.01(0.01)  Forward time: 1.33(1.37)  Batch time: 1.34(1.39)
2025-09-02 14:36:12,279   INFO  Train:   12/20 ( 60%) [1417/3862 ( 37%)]  Loss: 1.312 (1.12)  LR: 8.180e-04  Grad: 10.1854  max=0.4042(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4819(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4641, loss_cls=0.0747, loss_bbox=0.6041, matched_ious=0.5425, d_time=0.01(0.01), f_time=1.24(1.37), b_time=1.24(1.39)  Time cost: 32:44/56:26 [16:51:41/12:49:38]  Acc_iter 43900       Data time: 0.01(0.01)  Forward time: 1.24(1.37)  Batch time: 1.24(1.39)
2025-09-02 14:37:22,216   INFO  Train:   12/20 ( 60%) [1467/3862 ( 38%)]  Loss: 1.224 (1.12)  LR: 8.167e-04  Grad: 10.2420  max=0.4316(module.vfe.pfn_layers.0.linear.weight)  min: -0.4317(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4581, loss_cls=0.0755, loss_bbox=0.5592, matched_ious=0.5467, d_time=0.01(0.01), f_time=2.15(1.37), b_time=2.16(1.39)  Time cost: 33:53/55:18 [16:52:51/12:48:45]  Acc_iter 43950       Data time: 0.01(0.01)  Forward time: 2.15(1.37)  Batch time: 2.16(1.39)
2025-09-02 14:38:32,102   INFO  Train:   12/20 ( 60%) [1517/3862 ( 39%)]  Loss: 0.8582 (1.12)  LR: 8.154e-04  Grad: 10.2936  max=0.4169(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4318(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4564, loss_cls=0.0761, loss_bbox=0.5875, matched_ious=0.5492, d_time=0.01(0.01), f_time=1.37(1.37), b_time=1.38(1.39)  Time cost: 35:03/54:09 [16:54:01/12:47:49]  Acc_iter 44000       Data time: 0.01(0.01)  Forward time: 1.37(1.37)  Batch time: 1.38(1.39)
2025-09-02 14:39:41,964   INFO  Train:   12/20 ( 60%) [1567/3862 ( 41%)]  Loss: 1.496 (1.12)  LR: 8.141e-04  Grad: 10.3221  max=0.4220(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4345(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4627, loss_cls=0.0764, loss_bbox=0.5655, matched_ious=0.5494, d_time=0.01(0.01), f_time=2.19(1.37), b_time=2.20(1.39)  Time cost: 36:13/53:01 [16:55:11/12:46:52]  Acc_iter 44050       Data time: 0.01(0.01)  Forward time: 2.19(1.37)  Batch time: 2.20(1.39)
2025-09-02 14:40:50,570   INFO  Train:   12/20 ( 60%) [1617/3862 ( 42%)]  Loss: 1.130 (1.12)  LR: 8.128e-04  Grad: 10.3683  max=0.4156(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4842(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4530, loss_cls=0.0756, loss_bbox=0.5599, matched_ious=0.5502, d_time=0.00(0.01), f_time=1.30(1.37), b_time=1.30(1.39)  Time cost: 37:22/51:51 [16:56:19/12:45:28]  Acc_iter 44100       Data time: 0.00(0.01)  Forward time: 1.30(1.37)  Batch time: 1.30(1.39)
2025-09-02 14:41:59,042   INFO  Train:   12/20 ( 60%) [1667/3862 ( 43%)]  Loss: 1.012 (1.12)  LR: 8.114e-04  Grad: 10.4502  max=0.9606(module.vfe.pfn_layers.0.linear.weight)  min: -0.4343(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4657, loss_cls=0.0768, loss_bbox=0.5731, matched_ious=0.5526, d_time=0.00(0.01), f_time=1.39(1.37), b_time=1.40(1.39)  Time cost: 38:30/50:40 [16:57:28/12:44:02]  Acc_iter 44150       Data time: 0.00(0.01)  Forward time: 1.39(1.37)  Batch time: 1.40(1.39)
2025-09-02 14:43:07,583   INFO  Train:   12/20 ( 60%) [1717/3862 ( 44%)]  Loss: 1.281 (1.12)  LR: 8.101e-04  Grad: 10.4690  max=0.4270(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5729(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4741, loss_cls=0.0788, loss_bbox=0.6079, matched_ious=0.5408, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.38(1.39)  Time cost: 39:39/49:30 [16:58:36/12:42:39]  Acc_iter 44200       Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.38(1.39)
2025-09-02 14:44:17,870   INFO  Train:   12/20 ( 60%) [1767/3862 ( 46%)]  Loss: 1.114 (1.12)  LR: 8.088e-04  Grad: 10.5028  max=0.4290(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4397(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4686, loss_cls=0.0749, loss_bbox=0.5950, matched_ious=0.5477, d_time=0.01(0.01), f_time=1.32(1.37), b_time=1.33(1.39)  Time cost: 40:49/48:22 [16:59:46/12:41:49]  Acc_iter 44250       Data time: 0.01(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.39)
2025-09-02 14:45:27,113   INFO  Train:   12/20 ( 60%) [1817/3862 ( 47%)]  Loss: 1.240 (1.12)  LR: 8.074e-04  Grad: 10.5683  max=0.6244(module.vfe.pfn_layers.0.linear.weight)  min: -0.4391(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4466, loss_cls=0.0757, loss_bbox=0.5818, matched_ious=0.5374, d_time=0.01(0.01), f_time=1.36(1.37), b_time=1.36(1.39)  Time cost: 41:58/47:13 [17:00:56/12:40:39]  Acc_iter 44300       Data time: 0.01(0.01)  Forward time: 1.36(1.37)  Batch time: 1.36(1.39)
2025-09-02 14:46:35,525   INFO  Train:   12/20 ( 60%) [1867/3862 ( 48%)]  Loss: 1.601 (1.12)  LR: 8.061e-04  Grad: 10.7130  max=1.4620(module.vfe.pfn_layers.0.linear.weight)  min: -0.7312(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4781, loss_cls=0.0779, loss_bbox=0.6009, matched_ious=0.5444, d_time=0.01(0.01), f_time=1.50(1.37), b_time=1.50(1.39)  Time cost: 43:07/46:03 [17:02:04/12:39:15]  Acc_iter 44350       Data time: 0.01(0.01)  Forward time: 1.50(1.37)  Batch time: 1.50(1.39)
2025-09-02 14:47:43,520   INFO  Train:   12/20 ( 60%) [1917/3862 ( 50%)]  Loss: 1.025 (1.12)  LR: 8.048e-04  Grad: 10.6722  max=0.4192(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1195(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4481, loss_cls=0.0748, loss_bbox=0.5598, matched_ious=0.5493, d_time=0.01(0.01), f_time=1.26(1.37), b_time=1.27(1.38)  Time cost: 44:15/44:52 [17:03:12/12:37:44]  Acc_iter 44400       Data time: 0.01(0.01)  Forward time: 1.26(1.37)  Batch time: 1.27(1.38)
2025-09-02 14:48:52,171   INFO  Train:   12/20 ( 60%) [1967/3862 ( 51%)]  Loss: 1.111 (1.12)  LR: 8.034e-04  Grad: 10.7113  max=0.6597(module.vfe.pfn_layers.0.linear.weight)  min: -0.4462(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4563, loss_cls=0.0761, loss_bbox=0.5932, matched_ious=0.5396, d_time=0.00(0.01), f_time=1.40(1.37), b_time=1.40(1.38)  Time cost: 45:23/43:42 [17:04:21/12:36:25]  Acc_iter 44450       Data time: 0.00(0.01)  Forward time: 1.40(1.37)  Batch time: 1.40(1.38)
2025-09-02 14:50:02,126   INFO  Train:   12/20 ( 60%) [2017/3862 ( 52%)]  Loss: 1.145 (1.12)  LR: 8.021e-04  Grad: 10.7599  max=0.4893(module.vfe.pfn_layers.0.linear.weight)  min: -0.4443(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4516, loss_cls=0.0742, loss_bbox=0.5974, matched_ious=0.5443, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.31(1.38)  Time cost: 46:33/42:34 [17:05:31/12:35:28]  Acc_iter 44500       Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.31(1.38)
2025-09-02 14:51:10,753   INFO  Train:   12/20 ( 60%) [2067/3862 ( 54%)]  Loss: 1.029 (1.12)  LR: 8.007e-04  Grad: 10.7665  max=0.4263(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4460(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4656, loss_cls=0.0778, loss_bbox=0.5975, matched_ious=0.5474, d_time=0.01(0.01), f_time=1.21(1.37), b_time=1.22(1.38)  Time cost: 47:42/41:24 [17:06:39/12:34:10]  Acc_iter 44550       Data time: 0.01(0.01)  Forward time: 1.21(1.37)  Batch time: 1.22(1.38)
2025-09-02 14:52:19,380   INFO  Train:   12/20 ( 60%) [2117/3862 ( 55%)]  Loss: 1.436 (1.12)  LR: 7.994e-04  Grad: 10.8895  max=0.7508(module.vfe.pfn_layers.0.linear.weight)  min: -0.9374(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4716, loss_cls=0.0786, loss_bbox=0.5841, matched_ious=0.5475, d_time=0.00(0.01), f_time=1.44(1.37), b_time=1.44(1.38)  Time cost: 48:51/40:14 [17:07:48/12:32:51]  Acc_iter 44600       Data time: 0.00(0.01)  Forward time: 1.44(1.37)  Batch time: 1.44(1.38)
2025-09-02 14:53:28,221   INFO  Train:   12/20 ( 60%) [2167/3862 ( 56%)]  Loss: 0.9268 (1.12)  LR: 7.980e-04  Grad: 10.9600  max=1.2381(module.vfe.pfn_layers.0.linear.weight)  min: -0.4598(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4476, loss_cls=0.0724, loss_bbox=0.5813, matched_ious=0.5474, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 49:59/39:05 [17:08:57/12:31:37]  Acc_iter 44650       Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 14:54:36,651   INFO  Train:   12/20 ( 60%) [2217/3862 ( 57%)]  Loss: 1.263 (1.12)  LR: 7.966e-04  Grad: 10.9645  max=0.9011(module.vfe.pfn_layers.0.linear.weight)  min: -0.4589(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4609, loss_cls=0.0748, loss_bbox=0.6052, matched_ious=0.5433, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.37(1.38)  Time cost: 51:08/37:55 [17:10:05/12:30:17]  Acc_iter 44700       Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.37(1.38)
2025-09-02 14:55:45,960   INFO  Train:   12/20 ( 60%) [2267/3862 ( 59%)]  Loss: 0.9049 (1.12)  LR: 7.953e-04  Grad: 11.1104  max=0.4398(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2512(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4486, loss_cls=0.0745, loss_bbox=0.5701, matched_ious=0.5508, d_time=0.00(0.01), f_time=1.42(1.37), b_time=1.42(1.38)  Time cost: 52:17/36:46 [17:11:15/12:29:09]  Acc_iter 44750       Data time: 0.00(0.01)  Forward time: 1.42(1.37)  Batch time: 1.42(1.38)
2025-09-02 14:56:55,177   INFO  Train:   12/20 ( 60%) [2317/3862 ( 60%)]  Loss: 0.9087 (1.12)  LR: 7.939e-04  Grad: 10.9916  max=0.4341(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4600(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4411, loss_cls=0.0732, loss_bbox=0.5538, matched_ious=0.5465, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.33(1.38)  Time cost: 53:26/35:37 [17:12:24/12:28:01]  Acc_iter 44800       Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.33(1.38)
2025-09-02 14:58:05,469   INFO  Train:   12/20 ( 60%) [2367/3862 ( 61%)]  Loss: 1.453 (1.12)  LR: 7.925e-04  Grad: 11.0399  max=0.4335(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7563(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4542, loss_cls=0.0751, loss_bbox=0.5718, matched_ious=0.5444, d_time=0.00(0.01), f_time=1.30(1.37), b_time=1.31(1.38)  Time cost: 54:37/34:29 [17:13:34/12:27:07]  Acc_iter 44850       Data time: 0.00(0.01)  Forward time: 1.30(1.37)  Batch time: 1.31(1.38)
2025-09-02 14:59:14,147   INFO  Train:   12/20 ( 60%) [2417/3862 ( 63%)]  Loss: 1.329 (1.12)  LR: 7.912e-04  Grad: 11.1438  max=1.1062(module.vfe.pfn_layers.0.linear.weight)  min: -0.4686(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4581, loss_cls=0.0761, loss_bbox=0.5788, matched_ious=0.5464, d_time=0.01(0.01), f_time=1.35(1.37), b_time=1.35(1.38)  Time cost: 55:45/33:19 [17:14:43/12:25:51]  Acc_iter 44900       Data time: 0.01(0.01)  Forward time: 1.35(1.37)  Batch time: 1.35(1.38)
2025-09-02 15:00:21,893   INFO  Train:   12/20 ( 60%) [2467/3862 ( 64%)]  Loss: 0.9094 (1.12)  LR: 7.898e-04  Grad: 11.1485  max=0.4402(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4688(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4573, loss_cls=0.0760, loss_bbox=0.5690, matched_ious=0.5488, d_time=0.00(0.01), f_time=1.35(1.37), b_time=1.35(1.38)  Time cost: 56:53/32:09 [17:15:50/12:24:23]  Acc_iter 44950       Data time: 0.00(0.01)  Forward time: 1.35(1.37)  Batch time: 1.35(1.38)
2025-09-02 15:01:31,637   INFO  Train:   12/20 ( 60%) [2517/3862 ( 65%)]  Loss: 1.054 (1.12)  LR: 7.884e-04  Grad: 11.2067  max=0.7451(module.vfe.pfn_layers.0.linear.weight)  min: -0.4689(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4482, loss_cls=0.0753, loss_bbox=0.5497, matched_ious=0.5492, d_time=0.40(0.01), f_time=1.29(1.37), b_time=1.69(1.38)  Time cost: 58:03/31:00 [17:17:00/12:23:21]  Acc_iter 45000       Data time: 0.40(0.01)  Forward time: 1.29(1.37)  Batch time: 1.69(1.38)
2025-09-02 15:02:40,821   INFO  Train:   12/20 ( 60%) [2567/3862 ( 66%)]  Loss: 0.6867 (1.12)  LR: 7.870e-04  Grad: 11.2138  max=0.4430(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4678(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4522, loss_cls=0.0746, loss_bbox=0.5748, matched_ious=0.5485, d_time=0.00(0.01), f_time=1.27(1.37), b_time=1.28(1.38)  Time cost: 59:12/29:51 [17:18:09/12:22:12]  Acc_iter 45050       Data time: 0.00(0.01)  Forward time: 1.27(1.37)  Batch time: 1.28(1.38)
2025-09-02 15:03:51,818   INFO  Train:   12/20 ( 60%) [2617/3862 ( 68%)]  Loss: 1.056 (1.12)  LR: 7.856e-04  Grad: 11.9280  max=3.3144(module.vfe.pfn_layers.0.linear.weight)  min: -1.1309(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4597, loss_cls=0.0767, loss_bbox=0.5763, matched_ious=0.5458, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 1:00:23/28:43 [17:19:20/12:21:25]  Acc_iter 45100       Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 15:05:00,441   INFO  Train:   12/20 ( 60%) [2667/3862 ( 69%)]  Loss: 1.113 (1.12)  LR: 7.842e-04  Grad: 11.4709  max=0.4482(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.8225(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4443, loss_cls=0.0745, loss_bbox=0.5838, matched_ious=0.5458, d_time=0.01(0.01), f_time=1.28(1.37), b_time=1.28(1.38)  Time cost: 1:01:32/27:33 [17:20:29/12:20:09]  Acc_iter 45150       Data time: 0.01(0.01)  Forward time: 1.28(1.37)  Batch time: 1.28(1.38)
2025-09-02 15:06:08,459   INFO  Train:   12/20 ( 60%) [2717/3862 ( 70%)]  Loss: 1.016 (1.12)  LR: 7.828e-04  Grad: 11.3688  max=0.4483(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6893(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4387, loss_cls=0.0718, loss_bbox=0.5568, matched_ious=0.5497, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 1:02:40/26:24 [17:21:37/12:18:46]  Acc_iter 45200       Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 15:07:16,667   INFO  Train:   12/20 ( 60%) [2767/3862 ( 72%)]  Loss: 1.201 (1.12)  LR: 7.814e-04  Grad: 11.6771  max=0.4689(module.vfe.pfn_layers.0.linear.weight)  min: -2.0010(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4536, loss_cls=0.0731, loss_bbox=0.5649, matched_ious=0.5489, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.33(1.38)  Time cost: 1:03:48/25:14 [17:22:45/12:17:26]  Acc_iter 45250       Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.38)
2025-09-02 15:08:25,756   INFO  Train:   12/20 ( 60%) [2817/3862 ( 73%)]  Loss: 1.019 (1.12)  LR: 7.800e-04  Grad: 11.4920  max=0.4541(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8423(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4559, loss_cls=0.0766, loss_bbox=0.6145, matched_ious=0.5431, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 1:04:57/24:05 [17:23:54/12:16:16]  Acc_iter 45300       Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 15:09:34,416   INFO  Train:   12/20 ( 60%) [2867/3862 ( 74%)]  Loss: 0.8886 (1.12)  LR: 7.786e-04  Grad: 11.4772  max=0.5556(module.vfe.pfn_layers.0.linear.weight)  min: -0.4906(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4471, loss_cls=0.0733, loss_bbox=0.5657, matched_ious=0.5504, d_time=0.00(0.01), f_time=1.45(1.37), b_time=1.45(1.38)  Time cost: 1:06:06/22:55 [17:25:03/12:15:01]  Acc_iter 45350       Data time: 0.00(0.01)  Forward time: 1.45(1.37)  Batch time: 1.45(1.38)
2025-09-02 15:10:43,387   INFO  Train:   12/20 ( 60%) [2917/3862 ( 76%)]  Loss: 0.7466 (1.12)  LR: 7.772e-04  Grad: 11.5873  max=0.4533(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8833(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4567, loss_cls=0.0776, loss_bbox=0.5967, matched_ious=0.5443, d_time=0.00(0.01), f_time=1.26(1.37), b_time=1.26(1.38)  Time cost: 1:07:15/21:46 [17:26:12/12:13:50]  Acc_iter 45400       Data time: 0.00(0.01)  Forward time: 1.26(1.37)  Batch time: 1.26(1.38)
2025-09-02 15:11:51,804   INFO  Train:   12/20 ( 60%) [2967/3862 ( 77%)]  Loss: 0.9395 (1.11)  LR: 7.758e-04  Grad: 11.6490  max=0.4683(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2083(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4443, loss_cls=0.0744, loss_bbox=0.5414, matched_ious=0.5513, d_time=0.00(0.01), f_time=1.22(1.37), b_time=1.23(1.38)  Time cost: 1:08:23/20:37 [17:27:20/12:12:33]  Acc_iter 45450       Data time: 0.00(0.01)  Forward time: 1.22(1.37)  Batch time: 1.23(1.38)
2025-09-02 15:13:00,209   INFO  Train:   12/20 ( 60%) [3017/3862 ( 78%)]  Loss: 1.161 (1.11)  LR: 7.744e-04  Grad: 11.6932  max=0.7105(module.vfe.pfn_layers.0.linear.weight)  min: -0.6108(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4545, loss_cls=0.0753, loss_bbox=0.5557, matched_ious=0.5480, d_time=0.00(0.01), f_time=1.42(1.37), b_time=1.43(1.38)  Time cost: 1:09:31/19:28 [17:28:29/12:11:17]  Acc_iter 45500       Data time: 0.00(0.01)  Forward time: 1.42(1.37)  Batch time: 1.43(1.38)
2025-09-02 15:14:09,198   INFO  Train:   12/20 ( 60%) [3067/3862 ( 79%)]  Loss: 1.015 (1.11)  LR: 7.730e-04  Grad: 11.6490  max=0.4600(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4984(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4504, loss_cls=0.0748, loss_bbox=0.5609, matched_ious=0.5439, d_time=0.01(0.01), f_time=1.32(1.37), b_time=1.33(1.38)  Time cost: 1:10:40/18:18 [17:29:38/12:10:06]  Acc_iter 45550       Data time: 0.01(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.38)
2025-09-02 15:15:18,015   INFO  Train:   12/20 ( 60%) [3117/3862 ( 81%)]  Loss: 1.207 (1.11)  LR: 7.715e-04  Grad: 11.7429  max=0.4612(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5049(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4558, loss_cls=0.0744, loss_bbox=0.5641, matched_ious=0.5476, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.32(1.38)  Time cost: 1:11:49/17:09 [17:30:47/12:08:54]  Acc_iter 45600       Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.32(1.38)
2025-09-02 15:16:26,316   INFO  Train:   12/20 ( 60%) [3167/3862 ( 82%)]  Loss: 1.040 (1.11)  LR: 7.701e-04  Grad: 11.7743  max=0.4588(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5070(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4206, loss_cls=0.0719, loss_bbox=0.5410, matched_ious=0.5473, d_time=0.01(0.01), f_time=1.39(1.37), b_time=1.40(1.38)  Time cost: 1:12:58/16:00 [17:31:55/12:07:37]  Acc_iter 45650       Data time: 0.01(0.01)  Forward time: 1.39(1.37)  Batch time: 1.40(1.38)
2025-09-02 15:17:34,631   INFO  Train:   12/20 ( 60%) [3217/3862 ( 83%)]  Loss: 1.187 (1.11)  LR: 7.687e-04  Grad: 11.8179  max=0.4631(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5075(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4443, loss_cls=0.0765, loss_bbox=0.5407, matched_ious=0.5467, d_time=0.01(0.01), f_time=1.35(1.37), b_time=1.36(1.38)  Time cost: 1:14:06/14:51 [17:33:03/12:06:20]  Acc_iter 45700       Data time: 0.01(0.01)  Forward time: 1.35(1.37)  Batch time: 1.36(1.38)
2025-09-02 15:18:43,570   INFO  Train:   12/20 ( 60%) [3267/3862 ( 85%)]  Loss: 1.023 (1.11)  LR: 7.673e-04  Grad: 17.4897  max=8.3085(module.vfe.pfn_layers.0.linear.weight)  min: -9.4606(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4533, loss_cls=0.0747, loss_bbox=0.5474, matched_ious=0.5510, d_time=0.00(0.01), f_time=1.40(1.37), b_time=1.40(1.38)  Time cost: 1:15:15/13:42 [17:34:12/12:05:10]  Acc_iter 45750       Data time: 0.00(0.01)  Forward time: 1.40(1.37)  Batch time: 1.40(1.38)
2025-09-02 15:19:50,447   INFO  Train:   12/20 ( 60%) [3317/3862 ( 86%)]  Loss: 1.342 (1.11)  LR: 7.658e-04  Grad: 11.9194  max=0.5021(module.vfe.pfn_layers.0.linear.weight)  min: -0.5108(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4625, loss_cls=0.0750, loss_bbox=0.5715, matched_ious=0.5456, d_time=0.00(0.01), f_time=1.36(1.37), b_time=1.37(1.38)  Time cost: 1:16:22/12:32 [17:35:19/12:03:40]  Acc_iter 45800       Data time: 0.00(0.01)  Forward time: 1.36(1.37)  Batch time: 1.37(1.38)
2025-09-02 15:21:00,563   INFO  Train:   12/20 ( 60%) [3367/3862 ( 87%)]  Loss: 1.099 (1.11)  LR: 7.644e-04  Grad: 12.0944  max=0.4634(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.8154(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4532, loss_cls=0.0749, loss_bbox=0.5764, matched_ious=0.5511, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.30(1.38)  Time cost: 1:17:32/11:23 [17:36:29/12:02:40]  Acc_iter 45850       Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.30(1.38)
2025-09-02 15:22:09,896   INFO  Train:   12/20 ( 60%) [3417/3862 ( 88%)]  Loss: 1.118 (1.11)  LR: 7.629e-04  Grad: 11.9946  max=0.4742(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5088(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4355, loss_cls=0.0716, loss_bbox=0.5347, matched_ious=0.5483, d_time=0.01(0.01), f_time=1.49(1.37), b_time=1.49(1.38)  Time cost: 1:18:41/10:14 [17:37:38/12:01:34]  Acc_iter 45900       Data time: 0.01(0.01)  Forward time: 1.49(1.37)  Batch time: 1.49(1.38)
2025-09-02 15:23:17,614   INFO  Train:   12/20 ( 60%) [3467/3862 ( 90%)]  Loss: 0.9796 (1.11)  LR: 7.615e-04  Grad: 12.0456  max=0.4803(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5076(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4462, loss_cls=0.0742, loss_bbox=0.5724, matched_ious=0.5470, d_time=0.01(0.01), f_time=1.46(1.37), b_time=1.47(1.38)  Time cost: 1:19:49/09:05 [17:38:46/12:00:13]  Acc_iter 45950       Data time: 0.01(0.01)  Forward time: 1.46(1.37)  Batch time: 1.47(1.38)
2025-09-02 15:24:25,427   INFO  Train:   12/20 ( 60%) [3517/3862 ( 91%)]  Loss: 1.075 (1.11)  LR: 7.601e-04  Grad: 12.0671  max=0.4634(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5209(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4602, loss_cls=0.0767, loss_bbox=0.5685, matched_ious=0.5498, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.34(1.38)  Time cost: 1:20:57/07:56 [17:39:54/11:58:53]  Acc_iter 46000       Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.34(1.38)
2025-09-02 15:25:32,537   INFO  Train:   12/20 ( 60%) [3567/3862 ( 92%)]  Loss: 1.211 (1.11)  LR: 7.586e-04  Grad: 12.1101  max=0.4763(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6565(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4534, loss_cls=0.0743, loss_bbox=0.5753, matched_ious=0.5535, d_time=0.01(0.01), f_time=1.36(1.37), b_time=1.37(1.38)  Time cost: 1:22:04/06:47 [17:41:01/11:57:27]  Acc_iter 46050       Data time: 0.01(0.01)  Forward time: 1.36(1.37)  Batch time: 1.37(1.38)
2025-09-02 15:26:42,438   INFO  Train:   12/20 ( 60%) [3617/3862 ( 94%)]  Loss: 1.115 (1.11)  LR: 7.572e-04  Grad: 12.1811  max=0.4820(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5179(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4562, loss_cls=0.0729, loss_bbox=0.5754, matched_ious=0.5503, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.33(1.38)  Time cost: 1:23:14/05:38 [17:42:11/11:56:25]  Acc_iter 46100       Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.33(1.38)
2025-09-02 15:27:52,325   INFO  Train:   12/20 ( 60%) [3667/3862 ( 95%)]  Loss: 1.146 (1.11)  LR: 7.557e-04  Grad: 12.2215  max=0.4838(module.vfe.pfn_layers.0.linear.weight)  min: -0.5227(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4608, loss_cls=0.0726, loss_bbox=0.5683, matched_ious=0.5528, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.38(1.38)  Time cost: 1:24:24/04:29 [17:43:21/11:55:24]  Acc_iter 46150       Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.38(1.38)
2025-09-02 15:29:00,624   INFO  Train:   12/20 ( 60%) [3717/3862 ( 96%)]  Loss: 1.473 (1.11)  LR: 7.542e-04  Grad: 12.3685  max=1.5877(module.vfe.pfn_layers.0.linear.weight)  min: -0.5273(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4401, loss_cls=0.0721, loss_bbox=0.5722, matched_ious=0.5497, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.37(1.38)  Time cost: 1:25:32/03:20 [17:44:29/11:54:09]  Acc_iter 46200       Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.37(1.38)
2025-09-02 15:30:09,710   INFO  Train:   12/20 ( 60%) [3767/3862 ( 98%)]  Loss: 0.8441 (1.11)  LR: 7.528e-04  Grad: 12.2993  max=0.5016(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5210(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4554, loss_cls=0.0734, loss_bbox=0.5718, matched_ious=0.5511, d_time=0.01(0.01), f_time=1.37(1.37), b_time=1.38(1.38)  Time cost: 1:26:41/02:11 [17:45:38/11:53:00]  Acc_iter 46250       Data time: 0.01(0.01)  Forward time: 1.37(1.37)  Batch time: 1.38(1.38)
2025-09-02 15:31:17,964   INFO  Train:   12/20 ( 60%) [3817/3862 ( 99%)]  Loss: 0.9754 (1.11)  LR: 7.513e-04  Grad: 12.3301  max=0.4845(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5229(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4390, loss_cls=0.0721, loss_bbox=0.5685, matched_ious=0.5504, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.29(1.38)  Time cost: 1:27:49/01:02 [17:46:47/11:51:45]  Acc_iter 46300       Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.29(1.38)
2025-09-02 15:32:17,939   INFO  Train:   12/20 ( 60%) [3861/3862 (100%)]  Loss: 0.9011 (1.11)  LR: 7.500e-04  Grad: 12.4158  max=0.6979(module.vfe.pfn_layers.0.linear.weight)  min: -0.5209(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4399, loss_cls=0.0707, loss_bbox=0.5606, matched_ious=0.5484, d_time=0.00(0.01), f_time=1.29(1.37), b_time=1.30(1.38)  Time cost: 1:28:49/00:01 [17:47:46/11:50:38]  Acc_iter 46344       Data time: 0.00(0.01)  Forward time: 1.29(1.37)  Batch time: 1.30(1.38)

                                               [Aepochs:  60%|██████    | 12/20 [17:47:47<11:51:41, 5337.69s/it]epochs:  60%|██████    | 12/20 [17:47:47<11:51:41, 5337.75s/it]epochs:  60%|██████    | 12/20 [17:47:47<11:51:42, 5337.75s/it]epochs:  60%|██████    | 12/20 [17:47:47<11:51:42, 5337.75s/it]epochs:  60%|██████    | 12/20 [17:47:47<11:51:42, 5337.75s/it]epochs:  60%|██████    | 12/20 [17:47:47<11:51:42, 5337.75s/it]epochs:  60%|██████    | 12/20 [17:47:47<11:51:42, 5337.77s/it]epochs:  60%|██████    | 12/20 [17:47:48<11:51:41, 5337.75s/it]
train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-02 15:32:26,264   INFO  Train:   13/20 ( 65%) [   0/3862 (  0%)]  Loss: 1.067 (1.07)  LR: 7.500e-04  Grad: 12.4304  max=0.6775(module.vfe.pfn_layers.0.linear.weight)  min: -0.5300(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4650, loss_cls=0.0765, loss_bbox=0.5256, matched_ious=0.5634, d_time=3.19(3.19), f_time=3.33(3.33), b_time=6.52(6.52)  Time cost: 00:06/6:36:01 [17:47:55/52:48:09]  Acc_iter 46345       Data time: 3.19(3.19)  Forward time: 3.33(3.33)  Batch time: 6.52(6.52)
2025-09-02 15:32:34,037   INFO  Train:   13/20 ( 65%) [   5/3862 (  0%)]  Loss: 0.8278 (1.10)  LR: 7.499e-04  Grad: 12.4446  max=0.8261(module.vfe.pfn_layers.0.linear.weight)  min: -0.5190(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4021, loss_cls=0.0654, loss_bbox=0.6402, matched_ious=0.5533, d_time=0.01(0.54), f_time=2.35(1.84), b_time=2.35(2.39)  Time cost: 00:13/2:29:12 [17:48:03/19:55:02]  Acc_iter 46350       Data time: 0.01(0.54)  Forward time: 2.35(1.84)  Batch time: 2.35(2.39)
2025-09-02 15:33:43,755   INFO  Train:   13/20 ( 65%) [  55/3862 (  1%)]  Loss: 1.041 (1.09)  LR: 7.484e-04  Grad: 12.4530  max=0.5135(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5198(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4415, loss_cls=0.0730, loss_bbox=0.5756, matched_ious=0.5530, d_time=0.00(0.07), f_time=1.33(1.43), b_time=1.33(1.50)  Time cost: 01:23/1:34:46 [17:49:12/12:47:47]  Acc_iter 46400       Data time: 0.00(0.07)  Forward time: 1.33(1.43)  Batch time: 1.33(1.50)
2025-09-02 15:34:54,683   INFO  Train:   13/20 ( 65%) [ 105/3862 (  3%)]  Loss: 0.9458 (1.06)  LR: 7.469e-04  Grad: 12.5306  max=0.5101(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5264(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4267, loss_cls=0.0699, loss_bbox=0.5363, matched_ious=0.5560, d_time=0.00(0.04), f_time=1.31(1.42), b_time=1.31(1.46)  Time cost: 02:34/1:31:18 [17:50:23/12:28:17]  Acc_iter 46450       Data time: 0.00(0.04)  Forward time: 1.31(1.42)  Batch time: 1.31(1.46)
2025-09-02 15:36:03,395   INFO  Train:   13/20 ( 65%) [ 155/3862 (  4%)]  Loss: 1.064 (1.07)  LR: 7.454e-04  Grad: 12.8107  max=0.5048(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.4304(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4552, loss_cls=0.0749, loss_bbox=0.5561, matched_ious=0.5526, d_time=0.02(0.03), f_time=1.29(1.41), b_time=1.31(1.43)  Time cost: 03:43/1:28:25 [17:51:32/12:13:20]  Acc_iter 46500       Data time: 0.02(0.03)  Forward time: 1.29(1.41)  Batch time: 1.31(1.43)
2025-09-02 15:37:12,360   INFO  Train:   13/20 ( 65%) [ 205/3862 (  5%)]  Loss: 0.9305 (1.07)  LR: 7.440e-04  Grad: 12.6378  max=0.5160(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5280(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4428, loss_cls=0.0748, loss_bbox=0.5633, matched_ious=0.5523, d_time=0.01(0.02), f_time=1.36(1.40), b_time=1.37(1.42)  Time cost: 04:52/1:26:28 [17:52:41/12:05:41]  Acc_iter 46550       Data time: 0.01(0.02)  Forward time: 1.36(1.40)  Batch time: 1.37(1.42)
2025-09-02 15:38:21,853   INFO  Train:   13/20 ( 65%) [ 255/3862 (  7%)]  Loss: 1.031 (1.08)  LR: 7.425e-04  Grad: 12.6654  max=0.5104(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6118(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4564, loss_cls=0.0747, loss_bbox=0.5729, matched_ious=0.5476, d_time=0.00(0.02), f_time=1.30(1.39), b_time=1.30(1.41)  Time cost: 06:01/1:24:56 [17:53:50/12:01:37]  Acc_iter 46600       Data time: 0.00(0.02)  Forward time: 1.30(1.39)  Batch time: 1.30(1.41)
2025-09-02 15:39:32,525   INFO  Train:   13/20 ( 65%) [ 305/3862 (  8%)]  Loss: 1.160 (1.08)  LR: 7.410e-04  Grad: 12.6818  max=0.5092(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5360(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4375, loss_cls=0.0724, loss_bbox=0.5494, matched_ious=0.5501, d_time=0.00(0.02), f_time=1.37(1.40), b_time=1.38(1.41)  Time cost: 07:12/1:23:46 [17:55:01/12:00:28]  Acc_iter 46650       Data time: 0.00(0.02)  Forward time: 1.37(1.40)  Batch time: 1.38(1.41)
2025-09-02 15:40:41,036   INFO  Train:   13/20 ( 65%) [ 355/3862 (  9%)]  Loss: 0.9581 (1.07)  LR: 7.395e-04  Grad: 12.6854  max=0.5045(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5358(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4436, loss_cls=0.0724, loss_bbox=0.5444, matched_ious=0.5516, d_time=0.01(0.02), f_time=1.34(1.39), b_time=1.35(1.41)  Time cost: 08:20/1:22:14 [17:56:10/11:56:14]  Acc_iter 46700       Data time: 0.01(0.02)  Forward time: 1.34(1.39)  Batch time: 1.35(1.41)
2025-09-02 15:41:51,004   INFO  Train:   13/20 ( 65%) [ 405/3862 ( 10%)]  Loss: 1.114 (1.08)  LR: 7.380e-04  Grad: 12.7346  max=0.5101(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5363(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4533, loss_cls=0.0746, loss_bbox=0.5912, matched_ious=0.5533, d_time=0.00(0.02), f_time=1.54(1.39), b_time=1.55(1.41)  Time cost: 09:30/1:21:01 [17:57:20/11:54:34]  Acc_iter 46750       Data time: 0.00(0.02)  Forward time: 1.54(1.39)  Batch time: 1.55(1.41)
2025-09-02 15:42:59,825   INFO  Train:   13/20 ( 65%) [ 455/3862 ( 12%)]  Loss: 0.9999 (1.07)  LR: 7.365e-04  Grad: 12.7917  max=0.5064(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5429(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4219, loss_cls=0.0705, loss_bbox=0.5328, matched_ious=0.5544, d_time=0.00(0.02), f_time=1.36(1.39), b_time=1.37(1.40)  Time cost: 10:39/1:19:39 [17:58:28/11:51:45]  Acc_iter 46800       Data time: 0.00(0.02)  Forward time: 1.36(1.39)  Batch time: 1.37(1.40)
2025-09-02 15:44:08,399   INFO  Train:   13/20 ( 65%) [ 505/3862 ( 13%)]  Loss: 1.085 (1.07)  LR: 7.350e-04  Grad: 12.8389  max=0.4992(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5456(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4355, loss_cls=0.0737, loss_bbox=0.5702, matched_ious=0.5495, d_time=0.00(0.01), f_time=1.36(1.39), b_time=1.36(1.40)  Time cost: 11:48/1:18:19 [17:59:37/11:49:00]  Acc_iter 46850       Data time: 0.00(0.01)  Forward time: 1.36(1.39)  Batch time: 1.36(1.40)
2025-09-02 15:45:18,645   INFO  Train:   13/20 ( 65%) [ 555/3862 ( 14%)]  Loss: 1.195 (1.08)  LR: 7.335e-04  Grad: 12.8939  max=0.4966(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5487(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4460, loss_cls=0.0716, loss_bbox=0.5804, matched_ious=0.5546, d_time=0.01(0.01), f_time=1.39(1.39), b_time=1.40(1.40)  Time cost: 12:58/1:17:10 [18:00:47/11:48:04]  Acc_iter 46900       Data time: 0.01(0.01)  Forward time: 1.39(1.39)  Batch time: 1.40(1.40)
2025-09-02 15:46:26,744   INFO  Train:   13/20 ( 65%) [ 605/3862 ( 16%)]  Loss: 0.9796 (1.08)  LR: 7.320e-04  Grad: 12.9329  max=0.4961(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5553(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4495, loss_cls=0.0751, loss_bbox=0.5780, matched_ious=0.5472, d_time=0.00(0.01), f_time=1.27(1.38), b_time=1.27(1.40)  Time cost: 14:06/1:15:50 [18:01:55/11:45:19]  Acc_iter 46950       Data time: 0.00(0.01)  Forward time: 1.27(1.38)  Batch time: 1.27(1.40)
2025-09-02 15:47:36,504   INFO  Train:   13/20 ( 65%) [ 655/3862 ( 17%)]  Loss: 0.8907 (1.08)  LR: 7.305e-04  Grad: 13.7516  max=2.5704(module.vfe.pfn_layers.0.linear.weight)  min: -3.2208(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4315, loss_cls=0.0721, loss_bbox=0.5557, matched_ious=0.5524, d_time=0.03(0.01), f_time=1.34(1.38), b_time=1.37(1.40)  Time cost: 15:16/1:14:40 [18:03:05/11:44:05]  Acc_iter 47000       Data time: 0.03(0.01)  Forward time: 1.34(1.38)  Batch time: 1.37(1.40)
2025-09-02 15:48:45,795   INFO  Train:   13/20 ( 65%) [ 705/3862 ( 18%)]  Loss: 0.9132 (1.08)  LR: 7.290e-04  Grad: 12.9979  max=0.5210(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5537(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4569, loss_cls=0.0748, loss_bbox=0.5839, matched_ious=0.5512, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.34(1.40)  Time cost: 16:25/1:13:27 [18:04:14/11:42:31]  Acc_iter 47050       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.34(1.40)
2025-09-02 15:49:56,215   INFO  Train:   13/20 ( 65%) [ 755/3862 ( 20%)]  Loss: 0.7614 (1.08)  LR: 7.275e-04  Grad: 13.0523  max=0.5192(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5519(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4545, loss_cls=0.0758, loss_bbox=0.5539, matched_ious=0.5471, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.32(1.40)  Time cost: 17:36/1:12:20 [18:05:25/11:41:46]  Acc_iter 47100       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.32(1.40)
2025-09-02 15:51:05,768   INFO  Train:   13/20 ( 65%) [ 805/3862 ( 21%)]  Loss: 0.8846 (1.08)  LR: 7.260e-04  Grad: 13.0790  max=0.5173(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5581(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4408, loss_cls=0.0736, loss_bbox=0.5700, matched_ious=0.5534, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.33(1.40)  Time cost: 18:45/1:11:09 [18:06:34/11:40:25]  Acc_iter 47150       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.33(1.40)
2025-09-02 15:52:15,200   INFO  Train:   13/20 ( 65%) [ 855/3862 ( 22%)]  Loss: 0.7793 (1.08)  LR: 7.245e-04  Grad: 13.1395  max=0.5089(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5598(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4338, loss_cls=0.0722, loss_bbox=0.5467, matched_ious=0.5578, d_time=0.00(0.01), f_time=1.24(1.38), b_time=1.25(1.40)  Time cost: 19:55/1:09:58 [18:07:44/11:39:01]  Acc_iter 47200       Data time: 0.00(0.01)  Forward time: 1.24(1.38)  Batch time: 1.25(1.40)
2025-09-02 15:53:24,302   INFO  Train:   13/20 ( 65%) [ 905/3862 ( 23%)]  Loss: 1.180 (1.08)  LR: 7.230e-04  Grad: 13.1845  max=0.5211(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5627(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4351, loss_cls=0.0730, loss_bbox=0.5659, matched_ious=0.5470, d_time=0.01(0.01), f_time=1.39(1.38), b_time=1.40(1.40)  Time cost: 21:04/1:08:46 [18:08:53/11:37:28]  Acc_iter 47250       Data time: 0.01(0.01)  Forward time: 1.39(1.38)  Batch time: 1.40(1.40)
2025-09-02 15:54:33,163   INFO  Train:   13/20 ( 65%) [ 955/3862 ( 25%)]  Loss: 1.291 (1.08)  LR: 7.215e-04  Grad: 13.2587  max=0.6393(module.vfe.pfn_layers.0.linear.weight)  min: -0.5577(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4432, loss_cls=0.0736, loss_bbox=0.6076, matched_ious=0.5463, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.31(1.39)  Time cost: 22:13/1:07:33 [18:10:02/11:35:49]  Acc_iter 47300       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.31(1.39)
2025-09-02 15:55:42,026   INFO  Train:   13/20 ( 65%) [1005/3862 ( 26%)]  Loss: 0.9262 (1.08)  LR: 7.199e-04  Grad: 13.2794  max=0.5296(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5907(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4424, loss_cls=0.0725, loss_bbox=0.5559, matched_ious=0.5528, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.34(1.39)  Time cost: 23:21/1:06:21 [18:11:11/11:34:14]  Acc_iter 47350       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.34(1.39)
2025-09-02 15:56:51,332   INFO  Train:   13/20 ( 65%) [1055/3862 ( 27%)]  Loss: 1.136 (1.08)  LR: 7.184e-04  Grad: 13.3398  max=0.5200(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5598(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4478, loss_cls=0.0730, loss_bbox=0.5817, matched_ious=0.5533, d_time=0.00(0.01), f_time=1.28(1.38), b_time=1.29(1.39)  Time cost: 24:31/1:05:10 [18:12:20/11:32:54]  Acc_iter 47400       Data time: 0.00(0.01)  Forward time: 1.28(1.38)  Batch time: 1.29(1.39)
2025-09-02 15:58:00,223   INFO  Train:   13/20 ( 65%) [1105/3862 ( 29%)]  Loss: 1.128 (1.08)  LR: 7.169e-04  Grad: 13.3964  max=0.5228(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8629(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4342, loss_cls=0.0713, loss_bbox=0.5497, matched_ious=0.5527, d_time=0.00(0.01), f_time=1.39(1.38), b_time=1.40(1.39)  Time cost: 25:40/1:03:59 [18:13:29/11:31:24]  Acc_iter 47450       Data time: 0.00(0.01)  Forward time: 1.39(1.38)  Batch time: 1.40(1.39)
2025-09-02 15:59:10,713   INFO  Train:   13/20 ( 65%) [1155/3862 ( 30%)]  Loss: 1.041 (1.08)  LR: 7.154e-04  Grad: 13.4313  max=0.5313(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5649(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4514, loss_cls=0.0738, loss_bbox=0.5637, matched_ious=0.5489, d_time=0.01(0.01), f_time=1.25(1.38), b_time=1.26(1.39)  Time cost: 26:50/1:02:51 [18:14:39/11:30:36]  Acc_iter 47500       Data time: 0.01(0.01)  Forward time: 1.25(1.38)  Batch time: 1.26(1.39)
2025-09-02 16:00:19,130   INFO  Train:   13/20 ( 65%) [1205/3862 ( 31%)]  Loss: 1.078 (1.08)  LR: 7.138e-04  Grad: 13.5149  max=0.5311(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5616(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4332, loss_cls=0.0730, loss_bbox=0.5414, matched_ious=0.5494, d_time=0.00(0.01), f_time=1.40(1.38), b_time=1.41(1.39)  Time cost: 27:59/1:01:39 [18:15:48/11:28:56]  Acc_iter 47550       Data time: 0.00(0.01)  Forward time: 1.40(1.38)  Batch time: 1.41(1.39)
2025-09-02 16:01:26,901   INFO  Train:   13/20 ( 65%) [1255/3862 ( 32%)]  Loss: 1.135 (1.08)  LR: 7.123e-04  Grad: 13.5827  max=1.1135(module.vfe.pfn_layers.0.linear.weight)  min: -0.5610(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4458, loss_cls=0.0735, loss_bbox=0.5674, matched_ious=0.5461, d_time=0.02(0.01), f_time=1.39(1.38), b_time=1.41(1.39)  Time cost: 29:06/1:00:25 [18:16:55/11:27:03]  Acc_iter 47600       Data time: 0.02(0.01)  Forward time: 1.39(1.38)  Batch time: 1.41(1.39)
2025-09-02 16:02:36,841   INFO  Train:   13/20 ( 65%) [1305/3862 ( 34%)]  Loss: 1.162 (1.08)  LR: 7.108e-04  Grad: 13.5975  max=0.5304(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5619(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4597, loss_cls=0.0755, loss_bbox=0.5933, matched_ious=0.5501, d_time=0.00(0.01), f_time=1.35(1.38), b_time=1.36(1.39)  Time cost: 30:16/59:16 [18:18:05/11:26:03]  Acc_iter 47650       Data time: 0.00(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.39)
2025-09-02 16:03:45,671   INFO  Train:   13/20 ( 65%) [1355/3862 ( 35%)]  Loss: 1.479 (1.08)  LR: 7.092e-04  Grad: 13.6105  max=0.5449(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5631(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4149, loss_cls=0.0691, loss_bbox=0.5087, matched_ious=0.5590, d_time=0.01(0.01), f_time=1.42(1.38), b_time=1.43(1.39)  Time cost: 31:25/58:06 [18:19:14/11:24:37]  Acc_iter 47700       Data time: 0.01(0.01)  Forward time: 1.42(1.38)  Batch time: 1.43(1.39)
2025-09-02 16:04:56,099   INFO  Train:   13/20 ( 65%) [1405/3862 ( 36%)]  Loss: 1.104 (1.08)  LR: 7.077e-04  Grad: 13.6554  max=0.5353(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5628(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4317, loss_cls=0.0702, loss_bbox=0.5755, matched_ious=0.5480, d_time=0.00(0.01), f_time=1.42(1.38), b_time=1.43(1.39)  Time cost: 32:35/56:58 [18:20:25/11:23:47]  Acc_iter 47750       Data time: 0.00(0.01)  Forward time: 1.42(1.38)  Batch time: 1.43(1.39)
2025-09-02 16:06:05,402   INFO  Train:   13/20 ( 65%) [1455/3862 ( 38%)]  Loss: 0.8039 (1.08)  LR: 7.061e-04  Grad: 13.6824  max=0.5472(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5592(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4383, loss_cls=0.0717, loss_bbox=0.5615, matched_ious=0.5545, d_time=0.00(0.01), f_time=1.45(1.38), b_time=1.45(1.39)  Time cost: 33:45/55:48 [18:21:34/11:22:32]  Acc_iter 47800       Data time: 0.00(0.01)  Forward time: 1.45(1.38)  Batch time: 1.45(1.39)
2025-09-02 16:07:13,605   INFO  Train:   13/20 ( 65%) [1505/3862 ( 39%)]  Loss: 1.136 (1.08)  LR: 7.046e-04  Grad: 13.8053  max=0.5382(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9213(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4409, loss_cls=0.0724, loss_bbox=0.6020, matched_ious=0.5526, d_time=0.00(0.01), f_time=1.40(1.38), b_time=1.40(1.39)  Time cost: 34:53/54:36 [18:22:42/11:20:56]  Acc_iter 47850       Data time: 0.00(0.01)  Forward time: 1.40(1.38)  Batch time: 1.40(1.39)
2025-09-02 16:08:22,453   INFO  Train:   13/20 ( 65%) [1555/3862 ( 40%)]  Loss: 1.083 (1.08)  LR: 7.031e-04  Grad: 13.7737  max=0.5363(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5637(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4173, loss_cls=0.0683, loss_bbox=0.5416, matched_ious=0.5591, d_time=0.00(0.01), f_time=1.42(1.38), b_time=1.43(1.39)  Time cost: 36:02/53:25 [18:23:51/11:19:34]  Acc_iter 47900       Data time: 0.00(0.01)  Forward time: 1.42(1.38)  Batch time: 1.43(1.39)
2025-09-02 16:09:31,658   INFO  Train:   13/20 ( 65%) [1605/3862 ( 42%)]  Loss: 1.028 (1.08)  LR: 7.015e-04  Grad: 14.0077  max=0.5502(module.dense_head.decoder.self_attn.in_proj_weight)  min: -2.2035(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4507, loss_cls=0.0753, loss_bbox=0.5579, matched_ious=0.5533, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.33(1.39)  Time cost: 37:11/52:16 [18:25:00/11:18:20]  Acc_iter 47950       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.33(1.39)
2025-09-02 16:10:42,540   INFO  Train:   13/20 ( 65%) [1655/3862 ( 43%)]  Loss: 1.176 (1.08)  LR: 7.000e-04  Grad: 13.8375  max=0.5582(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5724(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4449, loss_cls=0.0732, loss_bbox=0.5584, matched_ious=0.5524, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.33(1.39)  Time cost: 38:22/51:08 [18:26:11/11:17:35]  Acc_iter 48000       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.33(1.39)
2025-09-02 16:11:49,969   INFO  Train:   13/20 ( 65%) [1705/3862 ( 44%)]  Loss: 0.9022 (1.08)  LR: 6.984e-04  Grad: 13.8776  max=0.5645(module.vfe.pfn_layers.0.linear.weight)  min: -0.5745(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4312, loss_cls=0.0713, loss_bbox=0.5595, matched_ious=0.5505, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.34(1.39)  Time cost: 39:29/49:56 [18:27:19/11:15:50]  Acc_iter 48050       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.34(1.39)
2025-09-02 16:12:58,875   INFO  Train:   13/20 ( 65%) [1755/3862 ( 45%)]  Loss: 0.9374 (1.08)  LR: 6.968e-04  Grad: 13.9392  max=0.5488(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6487(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4186, loss_cls=0.0698, loss_bbox=0.5601, matched_ious=0.5540, d_time=0.01(0.01), f_time=1.30(1.38), b_time=1.31(1.39)  Time cost: 40:38/48:46 [18:28:27/11:14:31]  Acc_iter 48100       Data time: 0.01(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.39)
2025-09-02 16:14:06,781   INFO  Train:   13/20 ( 65%) [1805/3862 ( 47%)]  Loss: 1.135 (1.08)  LR: 6.953e-04  Grad: 13.9933  max=0.5591(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5737(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4415, loss_cls=0.0719, loss_bbox=0.5646, matched_ious=0.5578, d_time=0.01(0.01), f_time=1.31(1.38), b_time=1.33(1.39)  Time cost: 41:46/47:35 [18:29:35/11:12:57]  Acc_iter 48150       Data time: 0.01(0.01)  Forward time: 1.31(1.38)  Batch time: 1.33(1.39)
2025-09-02 16:15:15,998   INFO  Train:   13/20 ( 65%) [1855/3862 ( 48%)]  Loss: 1.042 (1.08)  LR: 6.937e-04  Grad: 13.9655  max=0.5691(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5799(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4302, loss_cls=0.0689, loss_bbox=0.5400, matched_ious=0.5562, d_time=0.00(0.01), f_time=1.42(1.38), b_time=1.43(1.39)  Time cost: 42:55/46:25 [18:30:45/11:11:45]  Acc_iter 48200       Data time: 0.00(0.01)  Forward time: 1.42(1.38)  Batch time: 1.43(1.39)
2025-09-02 16:16:26,305   INFO  Train:   13/20 ( 65%) [1905/3862 ( 49%)]  Loss: 0.8733 (1.08)  LR: 6.922e-04  Grad: 14.0408  max=0.5586(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5791(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4407, loss_cls=0.0734, loss_bbox=0.5821, matched_ious=0.5558, d_time=0.00(0.01), f_time=2.16(1.38), b_time=2.16(1.39)  Time cost: 44:06/45:17 [18:31:55/11:10:49]  Acc_iter 48250       Data time: 0.00(0.01)  Forward time: 2.16(1.38)  Batch time: 2.16(1.39)
2025-09-02 16:17:35,089   INFO  Train:   13/20 ( 65%) [1955/3862 ( 51%)]  Loss: 0.8805 (1.08)  LR: 6.906e-04  Grad: 14.0716  max=0.5663(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5843(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4310, loss_cls=0.0705, loss_bbox=0.5627, matched_ious=0.5514, d_time=0.00(0.01), f_time=1.41(1.38), b_time=1.42(1.39)  Time cost: 45:14/44:06 [18:33:04/11:09:30]  Acc_iter 48300       Data time: 0.00(0.01)  Forward time: 1.41(1.38)  Batch time: 1.42(1.39)
2025-09-02 16:18:44,000   INFO  Train:   13/20 ( 65%) [2005/3862 ( 52%)]  Loss: 1.364 (1.07)  LR: 6.890e-04  Grad: 14.1536  max=0.5676(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5815(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4248, loss_cls=0.0706, loss_bbox=0.5357, matched_ious=0.5574, d_time=0.01(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 46:23/42:57 [18:34:13/11:08:14]  Acc_iter 48350       Data time: 0.01(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 16:19:51,442   INFO  Train:   13/20 ( 65%) [2055/3862 ( 53%)]  Loss: 1.088 (1.07)  LR: 6.875e-04  Grad: 14.3355  max=0.5617(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.9038(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4368, loss_cls=0.0711, loss_bbox=0.5688, matched_ious=0.5497, d_time=0.00(0.01), f_time=1.38(1.38), b_time=1.38(1.39)  Time cost: 47:31/41:46 [18:35:20/11:06:37]  Acc_iter 48400       Data time: 0.00(0.01)  Forward time: 1.38(1.38)  Batch time: 1.38(1.39)
2025-09-02 16:21:00,700   INFO  Train:   13/20 ( 65%) [2105/3862 ( 55%)]  Loss: 1.034 (1.07)  LR: 6.859e-04  Grad: 14.1971  max=0.5801(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5898(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4225, loss_cls=0.0712, loss_bbox=0.5202, matched_ious=0.5528, d_time=0.00(0.01), f_time=1.44(1.38), b_time=1.44(1.39)  Time cost: 48:40/40:36 [18:36:29/11:05:27]  Acc_iter 48450       Data time: 0.00(0.01)  Forward time: 1.44(1.38)  Batch time: 1.44(1.39)
2025-09-02 16:22:09,210   INFO  Train:   13/20 ( 65%) [2155/3862 ( 56%)]  Loss: 0.9824 (1.07)  LR: 6.843e-04  Grad: 14.2068  max=0.5845(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5938(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4163, loss_cls=0.0689, loss_bbox=0.5557, matched_ious=0.5560, d_time=0.01(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 49:49/39:26 [18:37:38/11:04:06]  Acc_iter 48500       Data time: 0.01(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 16:23:19,752   INFO  Train:   13/20 ( 65%) [2205/3862 ( 57%)]  Loss: 0.7940 (1.07)  LR: 6.827e-04  Grad: 14.2884  max=0.5750(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5913(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4375, loss_cls=0.0718, loss_bbox=0.5549, matched_ious=0.5509, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 50:59/38:18 [18:38:48/11:03:13]  Acc_iter 48550       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 16:24:28,181   INFO  Train:   13/20 ( 65%) [2255/3862 ( 58%)]  Loss: 1.015 (1.07)  LR: 6.812e-04  Grad: 14.2955  max=0.5735(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5913(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4230, loss_cls=0.0708, loss_bbox=0.5554, matched_ious=0.5571, d_time=0.01(0.01), f_time=1.44(1.38), b_time=1.45(1.39)  Time cost: 52:08/37:08 [18:39:57/11:01:52]  Acc_iter 48600       Data time: 0.01(0.01)  Forward time: 1.44(1.38)  Batch time: 1.45(1.39)
2025-09-02 16:25:36,423   INFO  Train:   13/20 ( 65%) [2305/3862 ( 60%)]  Loss: 0.9459 (1.07)  LR: 6.796e-04  Grad: 14.3298  max=0.5850(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5949(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4202, loss_cls=0.0686, loss_bbox=0.5388, matched_ious=0.5590, d_time=0.01(0.01), f_time=1.34(1.38), b_time=1.34(1.39)  Time cost: 53:16/35:58 [18:41:05/11:00:29]  Acc_iter 48650       Data time: 0.01(0.01)  Forward time: 1.34(1.38)  Batch time: 1.34(1.39)
2025-09-02 16:26:45,709   INFO  Train:   13/20 ( 65%) [2355/3862 ( 61%)]  Loss: 0.8842 (1.07)  LR: 6.780e-04  Grad: 14.4093  max=0.5823(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6005(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4575, loss_cls=0.0741, loss_bbox=0.5721, matched_ious=0.5498, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 54:25/34:48 [18:42:14/10:59:20]  Acc_iter 48700       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 16:27:55,951   INFO  Train:   13/20 ( 65%) [2405/3862 ( 62%)]  Loss: 0.9384 (1.07)  LR: 6.764e-04  Grad: 14.5002  max=0.5718(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5982(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4302, loss_cls=0.0711, loss_bbox=0.5264, matched_ious=0.5538, d_time=0.01(0.01), f_time=1.30(1.38), b_time=1.31(1.39)  Time cost: 55:35/33:40 [18:43:25/10:58:21]  Acc_iter 48750       Data time: 0.01(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.39)
2025-09-02 16:29:05,655   INFO  Train:   13/20 ( 65%) [2455/3862 ( 64%)]  Loss: 1.148 (1.07)  LR: 6.748e-04  Grad: 14.5434  max=0.5948(module.vfe.pfn_layers.0.linear.weight)  min: -0.6353(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4512, loss_cls=0.0724, loss_bbox=0.5798, matched_ious=0.5554, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.32(1.39)  Time cost: 56:45/32:30 [18:44:34/10:57:16]  Acc_iter 48800       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.32(1.39)
2025-09-02 16:30:13,316   INFO  Train:   13/20 ( 65%) [2505/3862 ( 65%)]  Loss: 1.025 (1.07)  LR: 6.732e-04  Grad: 14.5487  max=0.7278(module.vfe.pfn_layers.0.linear.weight)  min: -0.5996(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4297, loss_cls=0.0713, loss_bbox=0.5444, matched_ious=0.5549, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.34(1.39)  Time cost: 57:53/31:20 [18:45:42/10:55:48]  Acc_iter 48850       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.34(1.39)
2025-09-02 16:31:22,267   INFO  Train:   13/20 ( 65%) [2555/3862 ( 66%)]  Loss: 1.309 (1.07)  LR: 6.716e-04  Grad: 14.6023  max=0.6036(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6384(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4229, loss_cls=0.0681, loss_bbox=0.5244, matched_ious=0.5585, d_time=0.00(0.01), f_time=1.29(1.38), b_time=1.30(1.39)  Time cost: 59:02/30:11 [18:46:51/10:54:35]  Acc_iter 48900       Data time: 0.00(0.01)  Forward time: 1.29(1.38)  Batch time: 1.30(1.39)
2025-09-02 16:32:31,651   INFO  Train:   13/20 ( 65%) [2605/3862 ( 67%)]  Loss: 0.8999 (1.07)  LR: 6.700e-04  Grad: 6.3210  max=0.4883(module.vfe.pfn_layers.0.linear.weight)  min: -1.3395(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4377, loss_cls=0.0716, loss_bbox=0.5609, matched_ious=0.5502, d_time=0.01(0.01), f_time=1.42(1.38), b_time=1.43(1.39)  Time cost: 1:00:11/29:02 [18:48:00/10:53:27]  Acc_iter 48950       Data time: 0.01(0.01)  Forward time: 1.42(1.38)  Batch time: 1.43(1.39)
2025-09-02 16:33:39,700   INFO  Train:   13/20 ( 65%) [2655/3862 ( 69%)]  Loss: 0.9675 (1.07)  LR: 6.685e-04  Grad: 6.2051  max=0.4552(module.vfe.pfn_layers.0.linear.weight)  min: -0.3346(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4333, loss_cls=0.0721, loss_bbox=0.5223, matched_ious=0.5592, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.31(1.39)  Time cost: 1:01:19/27:52 [18:49:08/10:52:04]  Acc_iter 49000       Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.31(1.39)
2025-09-02 16:34:49,063   INFO  Train:   13/20 ( 65%) [2705/3862 ( 70%)]  Loss: 1.217 (1.07)  LR: 6.669e-04  Grad: 6.2333  max=0.2833(module.vfe.pfn_layers.0.linear.weight)  min: -0.4398(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4329, loss_cls=0.0701, loss_bbox=0.5682, matched_ious=0.5528, d_time=0.00(0.01), f_time=1.42(1.37), b_time=1.43(1.39)  Time cost: 1:02:28/26:42 [18:50:18/10:50:56]  Acc_iter 49050       Data time: 0.00(0.01)  Forward time: 1.42(1.37)  Batch time: 1.43(1.39)
2025-09-02 16:35:57,328   INFO  Train:   13/20 ( 65%) [2755/3862 ( 71%)]  Loss: 1.298 (1.07)  LR: 6.653e-04  Grad: 6.2481  max=0.2651(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3022(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4359, loss_cls=0.0716, loss_bbox=0.5578, matched_ious=0.5543, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.38(1.39)  Time cost: 1:03:37/25:33 [18:51:26/10:49:36]  Acc_iter 49100       Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.38(1.39)
2025-09-02 16:37:05,002   INFO  Train:   13/20 ( 65%) [2805/3862 ( 73%)]  Loss: 0.9523 (1.07)  LR: 6.637e-04  Grad: 6.2830  max=0.2659(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2578(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4352, loss_cls=0.0719, loss_bbox=0.5482, matched_ious=0.5573, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 1:04:44/24:23 [18:52:34/10:48:11]  Acc_iter 49150       Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 16:38:12,383   INFO  Train:   13/20 ( 65%) [2855/3862 ( 74%)]  Loss: 1.127 (1.07)  LR: 6.621e-04  Grad: 6.3735  max=0.7280(module.vfe.pfn_layers.0.linear.weight)  min: -0.3625(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4399, loss_cls=0.0732, loss_bbox=0.5248, matched_ious=0.5568, d_time=0.01(0.01), f_time=1.38(1.37), b_time=1.39(1.38)  Time cost: 1:05:52/23:13 [18:53:41/10:46:44]  Acc_iter 49200       Data time: 0.01(0.01)  Forward time: 1.38(1.37)  Batch time: 1.39(1.38)
2025-09-02 16:39:23,023   INFO  Train:   13/20 ( 65%) [2905/3862 ( 75%)]  Loss: 1.215 (1.07)  LR: 6.605e-04  Grad: 6.3811  max=0.2537(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5546(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4362, loss_cls=0.0712, loss_bbox=0.5566, matched_ious=0.5534, d_time=0.00(0.01), f_time=1.40(1.37), b_time=1.40(1.38)  Time cost: 1:07:02/22:04 [18:54:52/10:45:49]  Acc_iter 49250       Data time: 0.00(0.01)  Forward time: 1.40(1.37)  Batch time: 1.40(1.38)
2025-09-02 16:40:32,219   INFO  Train:   13/20 ( 65%) [2955/3862 ( 77%)]  Loss: 0.8403 (1.07)  LR: 6.588e-04  Grad: 6.4463  max=0.5049(module.vfe.pfn_layers.0.linear.weight)  min: -0.2767(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4287, loss_cls=0.0709, loss_bbox=0.5481, matched_ious=0.5547, d_time=0.02(0.01), f_time=2.16(1.37), b_time=2.18(1.38)  Time cost: 1:08:12/20:55 [18:56:01/10:44:39]  Acc_iter 49300       Data time: 0.02(0.01)  Forward time: 2.16(1.37)  Batch time: 2.18(1.38)
2025-09-02 16:41:40,463   INFO  Train:   13/20 ( 65%) [3005/3862 ( 78%)]  Loss: 0.9175 (1.07)  LR: 6.572e-04  Grad: 6.4031  max=0.2674(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2690(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4378, loss_cls=0.0718, loss_bbox=0.5079, matched_ious=0.5574, d_time=0.00(0.01), f_time=1.37(1.37), b_time=1.37(1.38)  Time cost: 1:09:20/19:46 [18:57:09/10:43:21]  Acc_iter 49350       Data time: 0.00(0.01)  Forward time: 1.37(1.37)  Batch time: 1.37(1.38)
2025-09-02 16:42:48,525   INFO  Train:   13/20 ( 65%) [3055/3862 ( 79%)]  Loss: 1.043 (1.07)  LR: 6.556e-04  Grad: 6.4595  max=0.2712(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3332(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4263, loss_cls=0.0707, loss_bbox=0.5256, matched_ious=0.5576, d_time=0.00(0.01), f_time=1.27(1.37), b_time=1.27(1.38)  Time cost: 1:10:28/18:36 [18:58:17/10:42:02]  Acc_iter 49400       Data time: 0.00(0.01)  Forward time: 1.27(1.37)  Batch time: 1.27(1.38)
2025-09-02 16:43:55,961   INFO  Train:   13/20 ( 65%) [3105/3862 ( 80%)]  Loss: 0.9505 (1.07)  LR: 6.540e-04  Grad: 6.5323  max=0.3380(module.vfe.pfn_layers.0.linear.weight)  min: -0.2718(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4226, loss_cls=0.0706, loss_bbox=0.5475, matched_ious=0.5564, d_time=0.00(0.01), f_time=1.50(1.37), b_time=1.50(1.38)  Time cost: 1:11:35/17:26 [18:59:25/10:40:37]  Acc_iter 49450       Data time: 0.00(0.01)  Forward time: 1.50(1.37)  Batch time: 1.50(1.38)
2025-09-02 16:45:04,198   INFO  Train:   13/20 ( 65%) [3155/3862 ( 82%)]  Loss: 0.8797 (1.07)  LR: 6.524e-04  Grad: 6.6280  max=0.6912(module.vfe.pfn_layers.0.linear.weight)  min: -0.5766(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4405, loss_cls=0.0734, loss_bbox=0.5466, matched_ious=0.5548, d_time=0.01(0.01), f_time=1.29(1.37), b_time=1.30(1.38)  Time cost: 1:12:44/16:17 [19:00:33/10:39:19]  Acc_iter 49500       Data time: 0.01(0.01)  Forward time: 1.29(1.37)  Batch time: 1.30(1.38)
2025-09-02 16:46:13,613   INFO  Train:   13/20 ( 65%) [3205/3862 ( 83%)]  Loss: 1.048 (1.07)  LR: 6.508e-04  Grad: 6.6161  max=0.2685(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4403(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4366, loss_cls=0.0721, loss_bbox=0.5382, matched_ious=0.5522, d_time=0.00(0.01), f_time=1.34(1.37), b_time=1.35(1.38)  Time cost: 1:13:53/15:08 [19:01:42/10:38:13]  Acc_iter 49550       Data time: 0.00(0.01)  Forward time: 1.34(1.37)  Batch time: 1.35(1.38)
2025-09-02 16:47:24,177   INFO  Train:   13/20 ( 65%) [3255/3862 ( 84%)]  Loss: 0.8882 (1.06)  LR: 6.492e-04  Grad: 6.6732  max=0.5548(module.vfe.pfn_layers.0.linear.weight)  min: -0.2731(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4397, loss_cls=0.0725, loss_bbox=0.5341, matched_ious=0.5568, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 1:15:04/13:59 [19:02:53/10:37:16]  Acc_iter 49600       Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 16:48:32,468   INFO  Train:   13/20 ( 65%) [3305/3862 ( 86%)]  Loss: 0.8837 (1.07)  LR: 6.476e-04  Grad: 6.9544  max=1.7134(module.vfe.pfn_layers.0.linear.weight)  min: -0.3716(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4328, loss_cls=0.0713, loss_bbox=0.5646, matched_ious=0.5619, d_time=0.00(0.01), f_time=1.23(1.37), b_time=1.24(1.38)  Time cost: 1:16:12/12:50 [19:04:01/10:35:59]  Acc_iter 49650       Data time: 0.00(0.01)  Forward time: 1.23(1.37)  Batch time: 1.24(1.38)
2025-09-02 16:49:39,537   INFO  Train:   13/20 ( 65%) [3355/3862 ( 87%)]  Loss: 1.294 (1.06)  LR: 6.459e-04  Grad: 6.7440  max=0.2710(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4060(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4129, loss_cls=0.0679, loss_bbox=0.5464, matched_ious=0.5594, d_time=0.01(0.01), f_time=1.38(1.37), b_time=1.39(1.38)  Time cost: 1:17:19/11:40 [19:05:08/10:34:33]  Acc_iter 49700       Data time: 0.01(0.01)  Forward time: 1.38(1.37)  Batch time: 1.39(1.38)
2025-09-02 16:50:48,546   INFO  Train:   13/20 ( 65%) [3405/3862 ( 88%)]  Loss: 1.034 (1.06)  LR: 6.443e-04  Grad: 6.7586  max=0.2785(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3222(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4272, loss_cls=0.0706, loss_bbox=0.5363, matched_ious=0.5574, d_time=0.00(0.01), f_time=1.43(1.37), b_time=1.43(1.38)  Time cost: 1:18:28/10:31 [19:06:17/10:33:23]  Acc_iter 49750       Data time: 0.00(0.01)  Forward time: 1.43(1.37)  Batch time: 1.43(1.38)
2025-09-02 16:51:58,790   INFO  Train:   13/20 ( 65%) [3455/3862 ( 89%)]  Loss: 1.121 (1.06)  LR: 6.427e-04  Grad: 6.7890  max=0.2832(module.vfe.pfn_layers.0.linear.weight)  min: -0.3321(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4100, loss_cls=0.0682, loss_bbox=0.5050, matched_ious=0.5615, d_time=0.01(0.01), f_time=1.31(1.37), b_time=1.32(1.38)  Time cost: 1:19:38/09:22 [19:07:27/10:32:23]  Acc_iter 49800       Data time: 0.01(0.01)  Forward time: 1.31(1.37)  Batch time: 1.32(1.38)
2025-09-02 16:53:08,069   INFO  Train:   13/20 ( 65%) [3505/3862 ( 91%)]  Loss: 1.067 (1.06)  LR: 6.411e-04  Grad: 6.8277  max=0.2802(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2808(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4285, loss_cls=0.0696, loss_bbox=0.5292, matched_ious=0.5587, d_time=0.01(0.01), f_time=1.35(1.37), b_time=1.36(1.38)  Time cost: 1:20:47/08:13 [19:08:37/10:31:15]  Acc_iter 49850       Data time: 0.01(0.01)  Forward time: 1.35(1.37)  Batch time: 1.36(1.38)
2025-09-02 16:54:17,779   INFO  Train:   13/20 ( 65%) [3555/3862 ( 92%)]  Loss: 0.9447 (1.06)  LR: 6.394e-04  Grad: 6.9075  max=0.4069(module.vfe.pfn_layers.0.linear.weight)  min: -0.4015(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4275, loss_cls=0.0709, loss_bbox=0.5123, matched_ious=0.5597, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.34(1.38)  Time cost: 1:21:57/07:04 [19:09:46/10:30:10]  Acc_iter 49900       Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.34(1.38)
2025-09-02 16:55:25,486   INFO  Train:   13/20 ( 65%) [3605/3862 ( 93%)]  Loss: 1.169 (1.06)  LR: 6.378e-04  Grad: 6.9073  max=0.2811(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4100(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4450, loss_cls=0.0729, loss_bbox=0.5519, matched_ious=0.5563, d_time=0.00(0.01), f_time=1.30(1.37), b_time=1.30(1.38)  Time cost: 1:23:05/05:55 [19:10:54/10:28:50]  Acc_iter 49950       Data time: 0.00(0.01)  Forward time: 1.30(1.37)  Batch time: 1.30(1.38)
2025-09-02 16:56:33,141   INFO  Train:   13/20 ( 65%) [3655/3862 ( 95%)]  Loss: 0.9834 (1.06)  LR: 6.362e-04  Grad: 6.9808  max=0.2741(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4664(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4235, loss_cls=0.0692, loss_bbox=0.5286, matched_ious=0.5612, d_time=0.00(0.01), f_time=1.31(1.37), b_time=1.31(1.38)  Time cost: 1:24:13/04:46 [19:12:02/10:27:30]  Acc_iter 50000       Data time: 0.00(0.01)  Forward time: 1.31(1.37)  Batch time: 1.31(1.38)
2025-09-02 16:57:43,258   INFO  Train:   13/20 ( 65%) [3705/3862 ( 96%)]  Loss: 0.9609 (1.06)  LR: 6.345e-04  Grad: 7.0527  max=0.6276(module.vfe.pfn_layers.0.linear.weight)  min: -0.5683(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4216, loss_cls=0.0692, loss_bbox=0.5412, matched_ious=0.5589, d_time=0.00(0.01), f_time=1.45(1.37), b_time=1.45(1.38)  Time cost: 1:25:23/03:37 [19:13:12/10:26:28]  Acc_iter 50050       Data time: 0.00(0.01)  Forward time: 1.45(1.37)  Batch time: 1.45(1.38)
2025-09-02 16:58:52,560   INFO  Train:   13/20 ( 65%) [3755/3862 ( 97%)]  Loss: 1.093 (1.06)  LR: 6.329e-04  Grad: 7.0928  max=0.2801(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4297(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4319, loss_cls=0.0708, loss_bbox=0.5524, matched_ious=0.5503, d_time=0.01(0.01), f_time=1.33(1.37), b_time=1.34(1.38)  Time cost: 1:26:32/02:27 [19:14:21/10:25:20]  Acc_iter 50100       Data time: 0.01(0.01)  Forward time: 1.33(1.37)  Batch time: 1.34(1.38)
2025-09-02 17:00:00,964   INFO  Train:   13/20 ( 65%) [3805/3862 ( 99%)]  Loss: 1.129 (1.06)  LR: 6.313e-04  Grad: 7.1117  max=0.2746(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3857(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4159, loss_cls=0.0685, loss_bbox=0.5393, matched_ious=0.5591, d_time=0.00(0.01), f_time=1.32(1.37), b_time=1.33(1.38)  Time cost: 1:27:40/01:18 [19:15:30/10:24:06]  Acc_iter 50150       Data time: 0.00(0.01)  Forward time: 1.32(1.37)  Batch time: 1.33(1.38)
2025-09-02 17:01:08,591   INFO  Train:   13/20 ( 65%) [3855/3862 (100%)]  Loss: 0.8597 (1.06)  LR: 6.296e-04  Grad: 7.1433  max=0.3506(module.vfe.pfn_layers.0.linear.weight)  min: -0.3009(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4313, loss_cls=0.0721, loss_bbox=0.5532, matched_ious=0.5630, d_time=0.00(0.01), f_time=1.38(1.37), b_time=1.39(1.38)  Time cost: 1:28:48/00:09 [19:16:37/10:22:46]  Acc_iter 50200       Data time: 0.00(0.01)  Forward time: 1.38(1.37)  Batch time: 1.39(1.38)
2025-09-02 17:01:16,398   INFO  Train:   13/20 ( 65%) [3861/3862 (100%)]  Loss: 1.034 (1.06)  LR: 6.294e-04  Grad: 7.1679  max=0.2861(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3007(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4079, loss_cls=0.0682, loss_bbox=0.4608, matched_ious=0.5559, d_time=0.00(0.01), f_time=1.30(1.37), b_time=1.31(1.38)  Time cost: 1:28:56/00:01 [19:16:45/10:22:35]  Acc_iter 50206       Data time: 0.00(0.01)  Forward time: 1.30(1.37)  Batch time: 1.31(1.38)

                                               [Aepochs:  65%|██████▌   | 13/20 [19:16:46<10:22:46, 5338.01s/it]epochs:  65%|██████▌   | 13/20 [19:16:46<10:22:46, 5338.02s/it]epochs:  65%|██████▌   | 13/20 [19:16:46<10:22:46, 5338.01s/it]epochs:  65%|██████▌   | 13/20 [19:16:46<10:22:46, 5338.01s/it]epochs:  65%|██████▌   | 13/20 [19:16:46<10:22:46, 5338.01s/it]epochs:  65%|██████▌   | 13/20 [19:16:46<10:22:46, 5338.03s/it]epochs:  65%|██████▌   | 13/20 [19:16:46<10:22:46, 5338.01s/it]epochs:  65%|██████▌   | 13/20 [19:16:46<10:22:46, 5338.00s/it]
train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-02 17:01:25,304   INFO  Train:   14/20 ( 70%) [   0/3862 (  0%)]  Loss: 0.9604 (0.960)  LR: 6.294e-04  Grad: 7.1826  max=0.2849(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4200(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3615, loss_cls=0.0662, loss_bbox=0.5327, matched_ious=0.5714, d_time=3.35(3.35), f_time=3.63(3.63), b_time=6.97(6.97)  Time cost: 00:06/7:17:20 [19:16:54/51:01:21]  Acc_iter 50207       Data time: 3.35(3.35)  Forward time: 3.63(3.63)  Batch time: 6.97(6.97)
2025-09-02 17:02:23,898   INFO  Train:   14/20 ( 70%) [  43/3862 (  1%)]  Loss: 0.8171 (1.04)  LR: 6.280e-04  Grad: 7.1972  max=0.3905(module.vfe.pfn_layers.0.linear.weight)  min: -0.2996(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4184, loss_cls=0.0679, loss_bbox=0.5572, matched_ious=0.5591, d_time=0.00(0.08), f_time=1.31(1.41), b_time=1.31(1.49)  Time cost: 01:05/1:34:35 [19:17:52/11:08:31]  Acc_iter 50250       Data time: 0.00(0.08)  Forward time: 1.31(1.41)  Batch time: 1.31(1.49)
2025-09-02 17:03:34,679   INFO  Train:   14/20 ( 70%) [  93/3862 (  2%)]  Loss: 1.122 (1.03)  LR: 6.264e-04  Grad: 7.2955  max=0.2807(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8174(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4115, loss_cls=0.0667, loss_bbox=0.5358, matched_ious=0.5552, d_time=0.01(0.04), f_time=1.41(1.41), b_time=1.42(1.45)  Time cost: 02:16/1:30:59 [19:19:03/10:50:24]  Acc_iter 50300       Data time: 0.01(0.04)  Forward time: 1.41(1.41)  Batch time: 1.42(1.45)
2025-09-02 17:04:44,699   INFO  Train:   14/20 ( 70%) [ 143/3862 (  4%)]  Loss: 1.201 (1.03)  LR: 6.247e-04  Grad: 7.2675  max=0.3993(module.vfe.pfn_layers.0.linear.weight)  min: -0.3040(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4104, loss_cls=0.0685, loss_bbox=0.5434, matched_ious=0.5566, d_time=0.00(0.03), f_time=1.31(1.40), b_time=1.31(1.43)  Time cost: 03:26/1:28:45 [19:20:13/10:41:44]  Acc_iter 50350       Data time: 0.00(0.03)  Forward time: 1.31(1.40)  Batch time: 1.31(1.43)
2025-09-02 17:05:54,513   INFO  Train:   14/20 ( 70%) [ 193/3862 (  5%)]  Loss: 0.9246 (1.03)  LR: 6.231e-04  Grad: 7.3193  max=0.2855(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3103(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4196, loss_cls=0.0685, loss_bbox=0.5426, matched_ious=0.5584, d_time=0.01(0.02), f_time=1.40(1.40), b_time=1.42(1.42)  Time cost: 04:36/1:26:59 [19:21:23/10:36:26]  Acc_iter 50400       Data time: 0.01(0.02)  Forward time: 1.40(1.40)  Batch time: 1.42(1.42)
2025-09-02 17:07:03,763   INFO  Train:   14/20 ( 70%) [ 243/3862 (  6%)]  Loss: 1.041 (1.02)  LR: 6.214e-04  Grad: 7.3154  max=0.2861(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3133(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4263, loss_cls=0.0694, loss_bbox=0.5150, matched_ious=0.5597, d_time=0.01(0.02), f_time=1.41(1.39), b_time=1.42(1.42)  Time cost: 05:45/1:25:20 [19:22:32/10:31:48]  Acc_iter 50450       Data time: 0.01(0.02)  Forward time: 1.41(1.39)  Batch time: 1.42(1.42)
2025-09-02 17:08:11,578   INFO  Train:   14/20 ( 70%) [ 293/3862 (  8%)]  Loss: 1.126 (1.02)  LR: 6.198e-04  Grad: 7.3909  max=0.3411(module.vfe.pfn_layers.0.linear.weight)  min: -0.6816(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4091, loss_cls=0.0698, loss_bbox=0.5054, matched_ious=0.5602, d_time=0.00(0.02), f_time=1.28(1.39), b_time=1.28(1.41)  Time cost: 06:53/1:23:34 [19:23:40/10:26:11]  Acc_iter 50500       Data time: 0.00(0.02)  Forward time: 1.28(1.39)  Batch time: 1.28(1.41)
2025-09-02 17:09:20,739   INFO  Train:   14/20 ( 70%) [ 343/3862 (  9%)]  Loss: 1.049 (1.01)  LR: 6.181e-04  Grad: 7.4633  max=0.4588(module.vfe.pfn_layers.0.linear.weight)  min: -0.3922(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4081, loss_cls=0.0674, loss_bbox=0.5290, matched_ious=0.5610, d_time=0.00(0.02), f_time=1.41(1.39), b_time=1.41(1.40)  Time cost: 08:02/1:22:13 [19:24:49/10:23:36]  Acc_iter 50550       Data time: 0.00(0.02)  Forward time: 1.41(1.39)  Batch time: 1.41(1.40)
2025-09-02 17:10:30,551   INFO  Train:   14/20 ( 70%) [ 393/3862 ( 10%)]  Loss: 0.8045 (1.02)  LR: 6.165e-04  Grad: 7.4441  max=0.2906(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3816(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4262, loss_cls=0.0690, loss_bbox=0.5327, matched_ious=0.5618, d_time=0.01(0.02), f_time=1.31(1.39), b_time=1.32(1.40)  Time cost: 09:12/1:21:00 [19:25:59/10:22:07]  Acc_iter 50600       Data time: 0.01(0.02)  Forward time: 1.31(1.39)  Batch time: 1.32(1.40)
2025-09-02 17:11:41,023   INFO  Train:   14/20 ( 70%) [ 443/3862 ( 11%)]  Loss: 0.8196 (1.01)  LR: 6.149e-04  Grad: 7.5112  max=0.4820(module.vfe.pfn_layers.0.linear.weight)  min: -0.3124(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4111, loss_cls=0.0687, loss_bbox=0.5217, matched_ious=0.5520, d_time=0.01(0.01), f_time=1.36(1.39), b_time=1.37(1.40)  Time cost: 10:22/1:19:53 [19:27:10/10:21:22]  Acc_iter 50650       Data time: 0.01(0.01)  Forward time: 1.36(1.39)  Batch time: 1.37(1.40)
2025-09-02 17:12:49,708   INFO  Train:   14/20 ( 70%) [ 493/3862 ( 13%)]  Loss: 1.057 (1.02)  LR: 6.132e-04  Grad: 7.5585  max=0.5381(module.vfe.pfn_layers.0.linear.weight)  min: -0.3095(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4216, loss_cls=0.0689, loss_bbox=0.5323, matched_ious=0.5541, d_time=0.01(0.01), f_time=1.29(1.39), b_time=1.30(1.40)  Time cost: 11:31/1:18:33 [19:28:18/10:18:55]  Acc_iter 50700       Data time: 0.01(0.01)  Forward time: 1.29(1.39)  Batch time: 1.30(1.40)
2025-09-02 17:13:58,993   INFO  Train:   14/20 ( 70%) [ 543/3862 ( 14%)]  Loss: 0.8895 (1.02)  LR: 6.116e-04  Grad: 7.5522  max=0.2953(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3163(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4219, loss_cls=0.0706, loss_bbox=0.5393, matched_ious=0.5613, d_time=0.00(0.01), f_time=1.29(1.38), b_time=1.30(1.40)  Time cost: 12:40/1:17:19 [19:29:28/10:17:13]  Acc_iter 50750       Data time: 0.00(0.01)  Forward time: 1.29(1.38)  Batch time: 1.30(1.40)
2025-09-02 17:15:07,828   INFO  Train:   14/20 ( 70%) [ 593/3862 ( 15%)]  Loss: 1.125 (1.02)  LR: 6.099e-04  Grad: 7.6003  max=0.3364(module.vfe.pfn_layers.0.linear.weight)  min: -0.3140(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4121, loss_cls=0.0691, loss_bbox=0.5425, matched_ious=0.5545, d_time=0.01(0.01), f_time=1.43(1.38), b_time=1.44(1.40)  Time cost: 13:49/1:16:04 [19:30:36/10:15:15]  Acc_iter 50800       Data time: 0.01(0.01)  Forward time: 1.43(1.38)  Batch time: 1.44(1.40)
2025-09-02 17:16:19,482   INFO  Train:   14/20 ( 70%) [ 643/3862 ( 17%)]  Loss: 0.9822 (1.02)  LR: 6.082e-04  Grad: 7.6591  max=0.2956(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6037(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4232, loss_cls=0.0709, loss_bbox=0.5136, matched_ious=0.5658, d_time=0.00(0.01), f_time=1.44(1.39), b_time=1.44(1.40)  Time cost: 15:00/1:15:03 [19:31:48/10:15:21]  Acc_iter 50850       Data time: 0.00(0.01)  Forward time: 1.44(1.39)  Batch time: 1.44(1.40)
2025-09-02 17:17:29,413   INFO  Train:   14/20 ( 70%) [ 693/3862 ( 18%)]  Loss: 1.039 (1.02)  LR: 6.066e-04  Grad: 7.6616  max=0.3359(module.vfe.pfn_layers.0.linear.weight)  min: -0.6041(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4123, loss_cls=0.0661, loss_bbox=0.5288, matched_ious=0.5577, d_time=0.00(0.01), f_time=1.36(1.39), b_time=1.36(1.40)  Time cost: 16:10/1:13:53 [19:32:58/10:14:10]  Acc_iter 50900       Data time: 0.00(0.01)  Forward time: 1.36(1.39)  Batch time: 1.36(1.40)
2025-09-02 17:18:37,795   INFO  Train:   14/20 ( 70%) [ 743/3862 ( 19%)]  Loss: 1.028 (1.02)  LR: 6.049e-04  Grad: 7.6877  max=0.3388(module.vfe.pfn_layers.0.linear.weight)  min: -0.3162(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4154, loss_cls=0.0684, loss_bbox=0.5363, matched_ious=0.5622, d_time=0.00(0.01), f_time=1.33(1.39), b_time=1.33(1.40)  Time cost: 17:19/1:12:36 [19:34:06/10:12:05]  Acc_iter 50950       Data time: 0.00(0.01)  Forward time: 1.33(1.39)  Batch time: 1.33(1.40)
2025-09-02 17:19:47,155   INFO  Train:   14/20 ( 70%) [ 793/3862 ( 21%)]  Loss: 1.104 (1.01)  LR: 6.033e-04  Grad: 7.7369  max=0.2990(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3557(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4138, loss_cls=0.0672, loss_bbox=0.5116, matched_ious=0.5605, d_time=0.01(0.01), f_time=1.58(1.38), b_time=1.58(1.40)  Time cost: 18:28/1:11:25 [19:35:16/10:10:39]  Acc_iter 51000       Data time: 0.01(0.01)  Forward time: 1.58(1.38)  Batch time: 1.58(1.40)
2025-09-02 17:20:54,326   INFO  Train:   14/20 ( 70%) [ 843/3862 ( 22%)]  Loss: 1.075 (1.02)  LR: 6.016e-04  Grad: 7.7780  max=0.3149(module.vfe.pfn_layers.0.linear.weight)  min: -0.3268(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4218, loss_cls=0.0681, loss_bbox=0.5675, matched_ious=0.5491, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.33(1.39)  Time cost: 19:35/1:10:05 [19:36:23/10:08:07]  Acc_iter 51050       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.33(1.39)
2025-09-02 17:22:03,543   INFO  Train:   14/20 ( 70%) [ 893/3862 ( 23%)]  Loss: 1.316 (1.02)  LR: 6.000e-04  Grad: 7.8036  max=0.3029(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3232(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4348, loss_cls=0.0689, loss_bbox=0.5615, matched_ious=0.5597, d_time=0.00(0.01), f_time=1.29(1.38), b_time=1.29(1.39)  Time cost: 20:45/1:08:54 [19:37:32/10:06:45]  Acc_iter 51100       Data time: 0.00(0.01)  Forward time: 1.29(1.38)  Batch time: 1.29(1.39)
2025-09-02 17:23:13,585   INFO  Train:   14/20 ( 70%) [ 943/3862 ( 24%)]  Loss: 1.080 (1.02)  LR: 5.983e-04  Grad: 7.8775  max=0.3166(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3309(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4232, loss_cls=0.0713, loss_bbox=0.5395, matched_ious=0.5596, d_time=0.00(0.01), f_time=1.28(1.38), b_time=1.29(1.39)  Time cost: 21:55/1:07:46 [19:38:42/10:05:47]  Acc_iter 51150       Data time: 0.00(0.01)  Forward time: 1.28(1.38)  Batch time: 1.29(1.39)
2025-09-02 17:24:23,940   INFO  Train:   14/20 ( 70%) [ 993/3862 ( 26%)]  Loss: 1.112 (1.02)  LR: 5.966e-04  Grad: 7.8816  max=0.3294(module.vfe.pfn_layers.0.linear.weight)  min: -0.3261(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4235, loss_cls=0.0695, loss_bbox=0.5386, matched_ious=0.5567, d_time=0.01(0.01), f_time=1.34(1.38), b_time=1.35(1.39)  Time cost: 23:05/1:06:38 [19:39:53/10:04:55]  Acc_iter 51200       Data time: 0.01(0.01)  Forward time: 1.34(1.38)  Batch time: 1.35(1.39)
2025-09-02 17:25:31,968   INFO  Train:   14/20 ( 70%) [1043/3862 ( 27%)]  Loss: 1.075 (1.02)  LR: 5.950e-04  Grad: 9.5971  max=3.5464(module.vfe.pfn_layers.0.linear.weight)  min: -3.6188(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4233, loss_cls=0.0681, loss_bbox=0.5093, matched_ious=0.5563, d_time=0.01(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 24:13/1:05:24 [19:41:01/10:03:04]  Acc_iter 51250       Data time: 0.01(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 17:26:41,391   INFO  Train:   14/20 ( 70%) [1093/3862 ( 28%)]  Loss: 0.9797 (1.02)  LR: 5.933e-04  Grad: 8.0263  max=1.0046(module.vfe.pfn_layers.0.linear.weight)  min: -0.3274(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4177, loss_cls=0.0694, loss_bbox=0.5348, matched_ious=0.5555, d_time=0.00(0.01), f_time=1.42(1.38), b_time=1.43(1.39)  Time cost: 25:22/1:04:14 [19:42:10/10:01:50]  Acc_iter 51300       Data time: 0.00(0.01)  Forward time: 1.42(1.38)  Batch time: 1.43(1.39)
2025-09-02 17:27:49,911   INFO  Train:   14/20 ( 70%) [1143/3862 ( 30%)]  Loss: 1.318 (1.02)  LR: 5.916e-04  Grad: 8.0041  max=0.3181(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3211(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4198, loss_cls=0.0692, loss_bbox=0.5198, matched_ious=0.5578, d_time=0.00(0.01), f_time=1.38(1.38), b_time=1.38(1.39)  Time cost: 26:31/1:03:02 [19:43:18/10:00:16]  Acc_iter 51350       Data time: 0.00(0.01)  Forward time: 1.38(1.38)  Batch time: 1.38(1.39)
2025-09-02 17:28:59,826   INFO  Train:   14/20 ( 70%) [1193/3862 ( 31%)]  Loss: 0.9309 (1.02)  LR: 5.900e-04  Grad: 8.0784  max=0.3189(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5343(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4130, loss_cls=0.0684, loss_bbox=0.5344, matched_ious=0.5564, d_time=0.00(0.01), f_time=1.39(1.38), b_time=1.39(1.39)  Time cost: 27:41/1:01:53 [19:44:28/9:59:14]  Acc_iter 51400       Data time: 0.00(0.01)  Forward time: 1.39(1.38)  Batch time: 1.39(1.39)
2025-09-02 17:30:11,026   INFO  Train:   14/20 ( 70%) [1243/3862 ( 32%)]  Loss: 0.9880 (1.02)  LR: 5.883e-04  Grad: 8.1088  max=0.3351(module.vfe.pfn_layers.0.linear.weight)  min: -0.3193(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4075, loss_cls=0.0654, loss_bbox=0.5251, matched_ious=0.5549, d_time=0.01(0.01), f_time=1.28(1.38), b_time=1.29(1.39)  Time cost: 28:52/1:00:47 [19:45:40/9:58:39]  Acc_iter 51450       Data time: 0.01(0.01)  Forward time: 1.28(1.38)  Batch time: 1.29(1.39)
2025-09-02 17:31:19,777   INFO  Train:   14/20 ( 70%) [1293/3862 ( 33%)]  Loss: 1.125 (1.02)  LR: 5.866e-04  Grad: 8.1718  max=0.3879(module.vfe.pfn_layers.0.linear.weight)  min: -0.6292(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4117, loss_cls=0.0688, loss_bbox=0.5284, matched_ious=0.5653, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.37(1.39)  Time cost: 30:01/59:36 [19:46:48/9:57:11]  Acc_iter 51500       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.37(1.39)
2025-09-02 17:32:28,602   INFO  Train:   14/20 ( 70%) [1343/3862 ( 35%)]  Loss: 0.9302 (1.02)  LR: 5.850e-04  Grad: 8.1783  max=0.3212(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3229(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4197, loss_cls=0.0686, loss_bbox=0.4989, matched_ious=0.5626, d_time=0.00(0.01), f_time=1.47(1.38), b_time=1.47(1.39)  Time cost: 31:10/58:25 [19:47:57/9:55:47]  Acc_iter 51550       Data time: 0.00(0.01)  Forward time: 1.47(1.38)  Batch time: 1.47(1.39)
2025-09-02 17:33:37,560   INFO  Train:   14/20 ( 70%) [1393/3862 ( 36%)]  Loss: 0.7410 (1.02)  LR: 5.833e-04  Grad: 8.2146  max=0.3277(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3247(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4195, loss_cls=0.0710, loss_bbox=0.5327, matched_ious=0.5600, d_time=0.01(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 32:19/57:14 [19:49:06/9:54:26]  Acc_iter 51600       Data time: 0.01(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 17:34:46,319   INFO  Train:   14/20 ( 70%) [1443/3862 ( 37%)]  Loss: 1.220 (1.02)  LR: 5.816e-04  Grad: 8.2547  max=0.3269(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3317(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4102, loss_cls=0.0687, loss_bbox=0.4993, matched_ious=0.5630, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.35(1.39)  Time cost: 33:27/56:03 [19:50:15/9:53:03]  Acc_iter 51650       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.35(1.39)
2025-09-02 17:35:56,234   INFO  Train:   14/20 ( 70%) [1493/3862 ( 39%)]  Loss: 1.099 (1.02)  LR: 5.800e-04  Grad: 8.2915  max=0.3242(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3286(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4234, loss_cls=0.0717, loss_bbox=0.5207, matched_ious=0.5562, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 34:37/54:54 [19:51:25/9:52:00]  Acc_iter 51700       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 17:37:05,223   INFO  Train:   14/20 ( 70%) [1543/3862 ( 40%)]  Loss: 1.059 (1.02)  LR: 5.783e-04  Grad: 8.2874  max=0.3350(module.vfe.pfn_layers.0.linear.weight)  min: -0.3212(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4282, loss_cls=0.0703, loss_bbox=0.5509, matched_ious=0.5531, d_time=0.00(0.01), f_time=1.42(1.38), b_time=1.42(1.39)  Time cost: 35:46/53:44 [19:52:34/9:50:41]  Acc_iter 51750       Data time: 0.00(0.01)  Forward time: 1.42(1.38)  Batch time: 1.42(1.39)
2025-09-02 17:38:13,512   INFO  Train:   14/20 ( 70%) [1593/3862 ( 41%)]  Loss: 1.260 (1.02)  LR: 5.766e-04  Grad: 8.3407  max=0.3386(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4641(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4202, loss_cls=0.0694, loss_bbox=0.5303, matched_ious=0.5589, d_time=0.00(0.01), f_time=1.23(1.38), b_time=1.24(1.39)  Time cost: 36:55/52:32 [19:53:42/9:49:12]  Acc_iter 51800       Data time: 0.00(0.01)  Forward time: 1.23(1.38)  Batch time: 1.24(1.39)
2025-09-02 17:39:22,167   INFO  Train:   14/20 ( 70%) [1643/3862 ( 43%)]  Loss: 1.362 (1.02)  LR: 5.749e-04  Grad: 8.3635  max=0.3420(module.vfe.pfn_layers.0.linear.weight)  min: -0.3320(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4203, loss_cls=0.0702, loss_bbox=0.5575, matched_ious=0.5526, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.37(1.39)  Time cost: 38:03/51:22 [19:54:51/9:47:50]  Acc_iter 51850       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.37(1.39)
2025-09-02 17:40:30,615   INFO  Train:   14/20 ( 70%) [1693/3862 ( 44%)]  Loss: 1.063 (1.02)  LR: 5.733e-04  Grad: 8.3594  max=0.3451(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3282(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4122, loss_cls=0.0698, loss_bbox=0.5329, matched_ious=0.5587, d_time=0.03(0.01), f_time=1.42(1.38), b_time=1.45(1.39)  Time cost: 39:12/50:11 [19:55:59/9:46:25]  Acc_iter 51900       Data time: 0.03(0.01)  Forward time: 1.42(1.38)  Batch time: 1.45(1.39)
2025-09-02 17:41:40,736   INFO  Train:   14/20 ( 70%) [1743/3862 ( 45%)]  Loss: 1.028 (1.02)  LR: 5.716e-04  Grad: 8.4263  max=0.3395(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4017(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4006, loss_cls=0.0655, loss_bbox=0.5033, matched_ious=0.5611, d_time=0.00(0.01), f_time=1.38(1.38), b_time=1.39(1.39)  Time cost: 40:22/49:03 [19:57:09/9:45:26]  Acc_iter 51950       Data time: 0.00(0.01)  Forward time: 1.38(1.38)  Batch time: 1.39(1.39)
2025-09-02 17:42:49,895   INFO  Train:   14/20 ( 70%) [1793/3862 ( 46%)]  Loss: 0.8336 (1.02)  LR: 5.699e-04  Grad: 8.4576  max=0.3406(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3386(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3995, loss_cls=0.0674, loss_bbox=0.5130, matched_ious=0.5615, d_time=0.00(0.01), f_time=1.28(1.38), b_time=1.29(1.39)  Time cost: 41:31/47:53 [19:58:18/9:44:13]  Acc_iter 52000       Data time: 0.00(0.01)  Forward time: 1.28(1.38)  Batch time: 1.29(1.39)
2025-09-02 17:43:57,850   INFO  Train:   14/20 ( 70%) [1843/3862 ( 48%)]  Loss: 1.118 (1.02)  LR: 5.682e-04  Grad: 8.5025  max=0.3329(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3456(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3943, loss_cls=0.0654, loss_bbox=0.5423, matched_ious=0.5617, d_time=0.00(0.01), f_time=1.51(1.38), b_time=1.51(1.39)  Time cost: 42:39/46:42 [19:59:26/9:42:43]  Acc_iter 52050       Data time: 0.00(0.01)  Forward time: 1.51(1.38)  Batch time: 1.51(1.39)
2025-09-02 17:45:07,126   INFO  Train:   14/20 ( 70%) [1893/3862 ( 49%)]  Loss: 0.8665 (1.01)  LR: 5.665e-04  Grad: 8.5491  max=0.3433(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3414(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4187, loss_cls=0.0696, loss_bbox=0.5089, matched_ious=0.5636, d_time=0.01(0.01), f_time=1.35(1.38), b_time=1.36(1.39)  Time cost: 43:48/45:32 [20:00:36/9:41:32]  Acc_iter 52100       Data time: 0.01(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.39)
2025-09-02 17:46:16,520   INFO  Train:   14/20 ( 70%) [1943/3862 ( 50%)]  Loss: 0.8105 (1.01)  LR: 5.649e-04  Grad: 8.6284  max=0.6231(module.vfe.pfn_layers.0.linear.weight)  min: -0.5850(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4111, loss_cls=0.0674, loss_bbox=0.5104, matched_ious=0.5612, d_time=0.00(0.01), f_time=1.41(1.38), b_time=1.42(1.39)  Time cost: 44:58/44:23 [20:01:45/9:40:22]  Acc_iter 52150       Data time: 0.00(0.01)  Forward time: 1.41(1.38)  Batch time: 1.42(1.39)
2025-09-02 17:47:26,456   INFO  Train:   14/20 ( 70%) [1993/3862 ( 52%)]  Loss: 0.9298 (1.01)  LR: 5.632e-04  Grad: 8.6006  max=0.3549(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3377(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4172, loss_cls=0.0682, loss_bbox=0.5427, matched_ious=0.5626, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.30(1.39)  Time cost: 46:07/43:14 [20:02:55/9:39:20]  Acc_iter 52200       Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.30(1.39)
2025-09-02 17:48:35,693   INFO  Train:   14/20 ( 70%) [2043/3862 ( 53%)]  Loss: 1.035 (1.01)  LR: 5.615e-04  Grad: 8.6656  max=0.3670(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3418(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4083, loss_cls=0.0662, loss_bbox=0.5330, matched_ious=0.5616, d_time=0.01(0.01), f_time=1.35(1.38), b_time=1.36(1.39)  Time cost: 47:17/42:04 [20:04:04/9:38:08]  Acc_iter 52250       Data time: 0.01(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.39)
2025-09-02 17:49:44,122   INFO  Train:   14/20 ( 70%) [2093/3862 ( 54%)]  Loss: 1.094 (1.01)  LR: 5.598e-04  Grad: 8.6969  max=0.3594(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3404(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4117, loss_cls=0.0671, loss_bbox=0.5234, matched_ious=0.5638, d_time=0.00(0.01), f_time=1.39(1.38), b_time=1.40(1.39)  Time cost: 48:25/40:54 [20:05:13/9:36:47]  Acc_iter 52300       Data time: 0.00(0.01)  Forward time: 1.39(1.38)  Batch time: 1.40(1.39)
2025-09-02 17:50:53,712   INFO  Train:   14/20 ( 70%) [2143/3862 ( 55%)]  Loss: 0.8865 (1.01)  LR: 5.581e-04  Grad: 8.8538  max=1.5663(module.vfe.pfn_layers.0.linear.weight)  min: -0.6850(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4133, loss_cls=0.0682, loss_bbox=0.5168, matched_ious=0.5667, d_time=0.00(0.01), f_time=1.43(1.38), b_time=1.43(1.39)  Time cost: 49:35/39:45 [20:06:22/9:35:40]  Acc_iter 52350       Data time: 0.00(0.01)  Forward time: 1.43(1.38)  Batch time: 1.43(1.39)
2025-09-02 17:52:02,835   INFO  Train:   14/20 ( 70%) [2193/3862 ( 57%)]  Loss: 1.380 (1.02)  LR: 5.565e-04  Grad: 8.7618  max=0.3674(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4470(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4379, loss_cls=0.0700, loss_bbox=0.5637, matched_ious=0.5556, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.31(1.39)  Time cost: 50:44/38:35 [20:07:31/9:34:28]  Acc_iter 52400       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.31(1.39)
2025-09-02 17:53:11,369   INFO  Train:   14/20 ( 70%) [2243/3862 ( 58%)]  Loss: 0.7827 (1.01)  LR: 5.548e-04  Grad: 8.8021  max=0.3651(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3451(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3953, loss_cls=0.0669, loss_bbox=0.5036, matched_ious=0.5627, d_time=0.01(0.01), f_time=1.35(1.38), b_time=1.36(1.39)  Time cost: 51:52/37:25 [20:08:40/9:33:09]  Acc_iter 52450       Data time: 0.01(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.39)
2025-09-02 17:54:20,479   INFO  Train:   14/20 ( 70%) [2293/3862 ( 59%)]  Loss: 1.011 (1.01)  LR: 5.531e-04  Grad: 8.8160  max=0.3665(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3732(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4124, loss_cls=0.0667, loss_bbox=0.5038, matched_ious=0.5635, d_time=0.01(0.01), f_time=1.44(1.38), b_time=1.45(1.39)  Time cost: 53:01/36:16 [20:09:49/9:31:57]  Acc_iter 52500       Data time: 0.01(0.01)  Forward time: 1.44(1.38)  Batch time: 1.45(1.39)
2025-09-02 17:55:30,743   INFO  Train:   14/20 ( 70%) [2343/3862 ( 61%)]  Loss: 0.9614 (1.01)  LR: 5.514e-04  Grad: 8.8230  max=0.3738(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3546(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4084, loss_cls=0.0668, loss_bbox=0.5230, matched_ious=0.5644, d_time=0.00(0.01), f_time=1.35(1.38), b_time=1.36(1.39)  Time cost: 54:12/35:07 [20:10:59/9:30:58]  Acc_iter 52550       Data time: 0.00(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.39)
2025-09-02 17:56:39,796   INFO  Train:   14/20 ( 70%) [2393/3862 ( 62%)]  Loss: 0.8383 (1.01)  LR: 5.497e-04  Grad: 8.8787  max=0.5883(module.vfe.pfn_layers.0.linear.weight)  min: -0.3543(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4187, loss_cls=0.0664, loss_bbox=0.5176, matched_ious=0.5623, d_time=0.01(0.01), f_time=1.36(1.38), b_time=1.37(1.39)  Time cost: 55:21/33:57 [20:12:08/9:29:45]  Acc_iter 52600       Data time: 0.01(0.01)  Forward time: 1.36(1.38)  Batch time: 1.37(1.39)
2025-09-02 17:57:49,746   INFO  Train:   14/20 ( 70%) [2443/3862 ( 63%)]  Loss: 0.8380 (1.01)  LR: 5.480e-04  Grad: 8.8798  max=0.3738(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3587(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4186, loss_cls=0.0698, loss_bbox=0.5234, matched_ious=0.5570, d_time=0.01(0.01), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 56:31/32:48 [20:13:18/9:28:41]  Acc_iter 52650       Data time: 0.01(0.01)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 17:58:57,948   INFO  Train:   14/20 ( 70%) [2493/3862 ( 65%)]  Loss: 0.9172 (1.01)  LR: 5.463e-04  Grad: 8.9671  max=0.3576(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3611(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4205, loss_cls=0.0674, loss_bbox=0.5536, matched_ious=0.5575, d_time=0.01(0.01), f_time=1.31(1.38), b_time=1.33(1.39)  Time cost: 57:39/31:38 [20:14:27/9:27:20]  Acc_iter 52700       Data time: 0.01(0.01)  Forward time: 1.31(1.38)  Batch time: 1.33(1.39)
2025-09-02 18:00:07,161   INFO  Train:   14/20 ( 70%) [2543/3862 ( 66%)]  Loss: 0.7765 (1.01)  LR: 5.447e-04  Grad: 8.9699  max=0.3698(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3605(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4324, loss_cls=0.0709, loss_bbox=0.5270, matched_ious=0.5555, d_time=0.01(0.01), f_time=1.32(1.38), b_time=1.33(1.39)  Time cost: 58:48/30:29 [20:15:36/9:26:10]  Acc_iter 52750       Data time: 0.01(0.01)  Forward time: 1.32(1.38)  Batch time: 1.33(1.39)
2025-09-02 18:01:17,866   INFO  Train:   14/20 ( 70%) [2593/3862 ( 67%)]  Loss: 0.5584 (1.01)  LR: 5.430e-04  Grad: 9.0142  max=0.3675(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3614(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4242, loss_cls=0.0682, loss_bbox=0.5194, matched_ious=0.5662, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.37(1.39)  Time cost: 59:59/29:20 [20:16:46/9:25:13]  Acc_iter 52800       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.37(1.39)
2025-09-02 18:02:26,986   INFO  Train:   14/20 ( 70%) [2643/3862 ( 68%)]  Loss: 1.284 (1.01)  LR: 5.413e-04  Grad: 9.0500  max=0.3674(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3618(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4137, loss_cls=0.0676, loss_bbox=0.5349, matched_ious=0.5609, d_time=0.00(0.01), f_time=1.26(1.38), b_time=1.26(1.39)  Time cost: 1:01:08/28:11 [20:17:56/9:24:01]  Acc_iter 52850       Data time: 0.00(0.01)  Forward time: 1.26(1.38)  Batch time: 1.26(1.39)
2025-09-02 18:03:35,386   INFO  Train:   14/20 ( 70%) [2693/3862 ( 70%)]  Loss: 1.207 (1.01)  LR: 5.396e-04  Grad: 9.0842  max=0.3587(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3618(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4170, loss_cls=0.0677, loss_bbox=0.5262, matched_ious=0.5589, d_time=0.00(0.01), f_time=1.24(1.38), b_time=1.24(1.39)  Time cost: 1:02:16/27:01 [20:19:04/9:22:43]  Acc_iter 52900       Data time: 0.00(0.01)  Forward time: 1.24(1.38)  Batch time: 1.24(1.39)
2025-09-02 18:04:45,619   INFO  Train:   14/20 ( 70%) [2743/3862 ( 71%)]  Loss: 0.7590 (1.01)  LR: 5.379e-04  Grad: 9.1017  max=0.4074(module.vfe.pfn_layers.0.linear.weight)  min: -0.3643(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3953, loss_cls=0.0666, loss_bbox=0.5248, matched_ious=0.5606, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.30(1.39)  Time cost: 1:03:27/25:52 [20:20:14/9:21:42]  Acc_iter 52950       Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.30(1.39)
2025-09-02 18:05:53,510   INFO  Train:   14/20 ( 70%) [2793/3862 ( 72%)]  Loss: 1.071 (1.01)  LR: 5.362e-04  Grad: 9.2024  max=0.3583(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6360(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4036, loss_cls=0.0643, loss_bbox=0.5236, matched_ious=0.5605, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.31(1.39)  Time cost: 1:04:35/24:42 [20:21:22/9:20:19]  Acc_iter 53000       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.31(1.39)
2025-09-02 18:07:03,737   INFO  Train:   14/20 ( 70%) [2843/3862 ( 74%)]  Loss: 1.027 (1.01)  LR: 5.345e-04  Grad: 9.2082  max=0.5116(module.vfe.pfn_layers.0.linear.weight)  min: -0.3604(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3996, loss_cls=0.0661, loss_bbox=0.5161, matched_ious=0.5665, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.31(1.39)  Time cost: 1:05:45/23:33 [20:22:32/9:19:18]  Acc_iter 53050       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.31(1.39)
2025-09-02 18:08:11,821   INFO  Train:   14/20 ( 70%) [2893/3862 ( 75%)]  Loss: 0.9650 (1.01)  LR: 5.328e-04  Grad: 9.2667  max=0.3683(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3633(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4033, loss_cls=0.0645, loss_bbox=0.5089, matched_ious=0.5642, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 1:06:53/22:23 [20:23:40/9:17:58]  Acc_iter 53100       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 18:09:21,021   INFO  Train:   14/20 ( 70%) [2943/3862 ( 76%)]  Loss: 1.181 (1.01)  LR: 5.311e-04  Grad: 9.2734  max=0.3768(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3666(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4030, loss_cls=0.0674, loss_bbox=0.5083, matched_ious=0.5637, d_time=0.02(0.01), f_time=1.32(1.38), b_time=1.34(1.39)  Time cost: 1:08:02/21:14 [20:24:50/9:16:47]  Acc_iter 53150       Data time: 0.02(0.01)  Forward time: 1.32(1.38)  Batch time: 1.34(1.39)
2025-09-02 18:10:31,218   INFO  Train:   14/20 ( 70%) [2993/3862 ( 77%)]  Loss: 1.021 (1.01)  LR: 5.294e-04  Grad: 9.3475  max=0.6011(module.vfe.pfn_layers.0.linear.weight)  min: -0.5322(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4133, loss_cls=0.0688, loss_bbox=0.5283, matched_ious=0.5586, d_time=0.03(0.01), f_time=1.43(1.38), b_time=1.46(1.39)  Time cost: 1:09:12/20:05 [20:26:00/9:15:45]  Acc_iter 53200       Data time: 0.03(0.01)  Forward time: 1.43(1.38)  Batch time: 1.46(1.39)
2025-09-02 18:11:40,841   INFO  Train:   14/20 ( 70%) [3043/3862 ( 79%)]  Loss: 1.248 (1.01)  LR: 5.277e-04  Grad: 9.3906  max=0.4242(module.vfe.pfn_layers.0.linear.weight)  min: -0.5691(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4291, loss_cls=0.0701, loss_bbox=0.5460, matched_ious=0.5573, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.31(1.39)  Time cost: 1:10:22/18:56 [20:27:09/9:14:37]  Acc_iter 53250       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.31(1.39)
2025-09-02 18:12:49,728   INFO  Train:   14/20 ( 70%) [3093/3862 ( 80%)]  Loss: 0.8514 (1.01)  LR: 5.261e-04  Grad: 9.4839  max=1.5296(module.vfe.pfn_layers.0.linear.weight)  min: -0.3723(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4011, loss_cls=0.0683, loss_bbox=0.5015, matched_ious=0.5652, d_time=0.00(0.01), f_time=1.39(1.38), b_time=1.40(1.39)  Time cost: 1:11:31/17:46 [20:28:18/9:13:24]  Acc_iter 53300       Data time: 0.00(0.01)  Forward time: 1.39(1.38)  Batch time: 1.40(1.39)
2025-09-02 18:13:57,917   INFO  Train:   14/20 ( 70%) [3143/3862 ( 81%)]  Loss: 0.8480 (1.01)  LR: 5.244e-04  Grad: 9.3854  max=0.3845(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3737(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4104, loss_cls=0.0691, loss_bbox=0.5373, matched_ious=0.5609, d_time=0.00(0.01), f_time=1.28(1.38), b_time=1.29(1.39)  Time cost: 1:12:39/16:36 [20:29:26/9:12:06]  Acc_iter 53350       Data time: 0.00(0.01)  Forward time: 1.28(1.38)  Batch time: 1.29(1.39)
2025-09-02 18:15:06,640   INFO  Train:   14/20 ( 70%) [3193/3862 ( 83%)]  Loss: 1.482 (1.01)  LR: 5.227e-04  Grad: 9.4409  max=0.5108(module.vfe.pfn_layers.0.linear.weight)  min: -0.3750(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4096, loss_cls=0.0675, loss_bbox=0.5186, matched_ious=0.5629, d_time=0.00(0.01), f_time=1.48(1.38), b_time=1.48(1.39)  Time cost: 1:13:48/15:27 [20:30:35/9:10:52]  Acc_iter 53400       Data time: 0.00(0.01)  Forward time: 1.48(1.38)  Batch time: 1.48(1.39)
2025-09-02 18:16:16,260   INFO  Train:   14/20 ( 70%) [3243/3862 ( 84%)]  Loss: 1.273 (1.01)  LR: 5.210e-04  Grad: 9.4517  max=0.3860(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3777(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4137, loss_cls=0.0664, loss_bbox=0.5358, matched_ious=0.5626, d_time=0.01(0.01), f_time=1.28(1.38), b_time=1.29(1.39)  Time cost: 1:14:57/14:18 [20:31:45/9:09:45]  Acc_iter 53450       Data time: 0.01(0.01)  Forward time: 1.28(1.38)  Batch time: 1.29(1.39)
2025-09-02 18:17:25,278   INFO  Train:   14/20 ( 70%) [3293/3862 ( 85%)]  Loss: 1.094 (1.01)  LR: 5.193e-04  Grad: 9.5182  max=0.4016(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5948(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4124, loss_cls=0.0680, loss_bbox=0.5036, matched_ious=0.5637, d_time=0.01(0.01), f_time=1.41(1.38), b_time=1.42(1.39)  Time cost: 1:16:06/13:08 [20:32:54/9:08:34]  Acc_iter 53500       Data time: 0.01(0.01)  Forward time: 1.41(1.38)  Batch time: 1.42(1.39)
2025-09-02 18:18:34,706   INFO  Train:   14/20 ( 70%) [3343/3862 ( 87%)]  Loss: 1.097 (1.01)  LR: 5.176e-04  Grad: 9.4530  max=0.3977(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3797(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4052, loss_cls=0.0664, loss_bbox=0.4948, matched_ious=0.5663, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.30(1.39)  Time cost: 1:17:16/11:59 [20:34:03/9:07:25]  Acc_iter 53550       Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.30(1.39)
2025-09-02 18:19:42,843   INFO  Train:   14/20 ( 70%) [3393/3862 ( 88%)]  Loss: 1.017 (1.01)  LR: 5.159e-04  Grad: 9.5347  max=0.3842(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3818(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3934, loss_cls=0.0633, loss_bbox=0.5124, matched_ious=0.5637, d_time=0.01(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 1:18:24/10:50 [20:35:11/9:06:08]  Acc_iter 53600       Data time: 0.01(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 18:20:51,236   INFO  Train:   14/20 ( 70%) [3443/3862 ( 89%)]  Loss: 0.9774 (1.01)  LR: 5.142e-04  Grad: 9.6140  max=0.4564(module.vfe.pfn_layers.0.linear.weight)  min: -0.5066(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4007, loss_cls=0.0663, loss_bbox=0.5067, matched_ious=0.5614, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.32(1.39)  Time cost: 1:19:32/09:40 [20:36:20/9:04:52]  Acc_iter 53650       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.32(1.39)
2025-09-02 18:22:02,447   INFO  Train:   14/20 ( 70%) [3493/3862 ( 90%)]  Loss: 1.128 (1.01)  LR: 5.125e-04  Grad: 9.5965  max=0.3948(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3821(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4007, loss_cls=0.0644, loss_bbox=0.5373, matched_ious=0.5675, d_time=0.00(0.01), f_time=1.46(1.38), b_time=1.47(1.39)  Time cost: 1:20:43/08:31 [20:37:31/9:03:56]  Acc_iter 53700       Data time: 0.00(0.01)  Forward time: 1.46(1.38)  Batch time: 1.47(1.39)
2025-09-02 18:23:11,621   INFO  Train:   14/20 ( 70%) [3543/3862 ( 92%)]  Loss: 0.8953 (1.01)  LR: 5.108e-04  Grad: 9.6518  max=0.3980(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3829(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4071, loss_cls=0.0652, loss_bbox=0.5090, matched_ious=0.5604, d_time=0.00(0.01), f_time=1.50(1.38), b_time=1.50(1.39)  Time cost: 1:21:53/07:22 [20:38:40/9:02:45]  Acc_iter 53750       Data time: 0.00(0.01)  Forward time: 1.50(1.38)  Batch time: 1.50(1.39)
2025-09-02 18:24:21,125   INFO  Train:   14/20 ( 70%) [3593/3862 ( 93%)]  Loss: 0.8981 (1.01)  LR: 5.091e-04  Grad: 9.7093  max=0.3899(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3853(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4003, loss_cls=0.0661, loss_bbox=0.5165, matched_ious=0.5605, d_time=0.51(0.01), f_time=1.31(1.38), b_time=1.82(1.39)  Time cost: 1:23:02/06:12 [20:39:50/9:01:37]  Acc_iter 53800       Data time: 0.51(0.01)  Forward time: 1.31(1.38)  Batch time: 1.82(1.39)
2025-09-02 18:25:30,002   INFO  Train:   14/20 ( 70%) [3643/3862 ( 94%)]  Loss: 0.8472 (1.01)  LR: 5.074e-04  Grad: 9.8156  max=0.8483(module.vfe.pfn_layers.0.linear.weight)  min: -0.3887(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4106, loss_cls=0.0647, loss_bbox=0.5333, matched_ious=0.5639, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.31(1.39)  Time cost: 1:24:11/05:03 [20:40:59/9:00:25]  Acc_iter 53850       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.31(1.39)
2025-09-02 18:26:38,945   INFO  Train:   14/20 ( 70%) [3693/3862 ( 96%)]  Loss: 1.387 (1.01)  LR: 5.057e-04  Grad: 9.7842  max=0.6558(module.vfe.pfn_layers.0.linear.weight)  min: -0.4701(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3981, loss_cls=0.0661, loss_bbox=0.5190, matched_ious=0.5613, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.31(1.39)  Time cost: 1:25:20/03:54 [20:42:08/8:59:14]  Acc_iter 53900       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.31(1.39)
2025-09-02 18:27:48,709   INFO  Train:   14/20 ( 70%) [3743/3862 ( 97%)]  Loss: 1.063 (1.01)  LR: 5.040e-04  Grad: 9.7966  max=0.4275(module.vfe.pfn_layers.0.linear.weight)  min: -0.4385(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4193, loss_cls=0.0675, loss_bbox=0.5577, matched_ious=0.5551, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.35(1.39)  Time cost: 1:26:30/02:44 [20:43:17/8:58:07]  Acc_iter 53950       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.35(1.39)
2025-09-02 18:28:57,204   INFO  Train:   14/20 ( 70%) [3793/3862 ( 98%)]  Loss: 0.9018 (1.01)  LR: 5.023e-04  Grad: 9.8121  max=0.3983(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3901(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4050, loss_cls=0.0641, loss_bbox=0.4958, matched_ious=0.5661, d_time=0.00(0.01), f_time=1.39(1.38), b_time=1.39(1.39)  Time cost: 1:27:38/01:35 [20:44:26/8:56:53]  Acc_iter 54000       Data time: 0.00(0.01)  Forward time: 1.39(1.38)  Batch time: 1.39(1.39)
2025-09-02 18:30:05,349   INFO  Train:   14/20 ( 70%) [3843/3862 (100%)]  Loss: 0.8659 (1.01)  LR: 5.006e-04  Grad: 9.9386  max=0.4706(module.vfe.pfn_layers.0.linear.weight)  min: -1.2035(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4023, loss_cls=0.0648, loss_bbox=0.5337, matched_ious=0.5599, d_time=0.01(0.01), f_time=1.19(1.38), b_time=1.21(1.39)  Time cost: 1:28:46/00:26 [20:45:34/8:55:37]  Acc_iter 54050       Data time: 0.01(0.01)  Forward time: 1.19(1.38)  Batch time: 1.21(1.39)
2025-09-02 18:30:29,937   INFO  Train:   14/20 ( 70%) [3861/3862 (100%)]  Loss: 0.7498 (1.01)  LR: 5.000e-04  Grad: 9.9130  max=0.5997(module.vfe.pfn_layers.0.linear.weight)  min: -0.7937(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4029, loss_cls=0.0654, loss_bbox=0.4941, matched_ious=0.5589, d_time=0.00(0.01), f_time=1.27(1.38), b_time=1.27(1.39)  Time cost: 1:29:11/00:01 [20:45:59/8:55:09]  Acc_iter 54068       Data time: 0.00(0.01)  Forward time: 1.27(1.38)  Batch time: 1.27(1.39)

                                               [Aepochs:  70%|███████   | 14/20 [20:45:59<8:54:15, 5342.66s/it] epochs:  70%|███████   | 14/20 [20:45:59<8:54:16, 5342.67s/it] epochs:  70%|███████   | 14/20 [20:45:59<8:54:16, 5342.67s/it] epochs:  70%|███████   | 14/20 [20:46:00<8:54:16, 5342.67s/it] epochs:  70%|███████   | 14/20 [20:46:00<8:54:16, 5342.68s/it] epochs:  70%|███████   | 14/20 [20:46:00<8:54:16, 5342.67s/it] epochs:  70%|███████   | 14/20 [20:46:00<8:54:16, 5342.67s/it] epochs:  70%|███████   | 14/20 [20:46:00<8:54:16, 5342.68s/it] 
train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-02 18:30:38,102   INFO  Train:   15/20 ( 75%) [   0/3862 (  0%)]  Loss: 0.8512 (0.851)  LR: 5.000e-04  Grad: 9.8610  max=0.3909(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4609(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3936, loss_cls=0.0716, loss_bbox=0.3860, matched_ious=0.5781, d_time=3.17(3.17), f_time=3.15(3.15), b_time=6.32(6.32)  Time cost: 00:06/6:32:26 [20:46:07/39:14:41]  Acc_iter 54069       Data time: 3.17(3.17)  Forward time: 3.15(3.15)  Batch time: 6.32(6.32)
2025-09-02 18:31:21,456   INFO  Train:   15/20 ( 75%) [  31/3862 (  1%)]  Loss: 1.169 (1.03)  LR: 4.990e-04  Grad: 9.8348  max=0.3934(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3908(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4188, loss_cls=0.0677, loss_bbox=0.5491, matched_ious=0.5617, d_time=0.00(0.11), f_time=1.28(1.45), b_time=1.28(1.55)  Time cost: 00:49/1:38:40 [20:46:50/9:56:02]  Acc_iter 54100       Data time: 0.00(0.11)  Forward time: 1.28(1.45)  Batch time: 1.28(1.55)
2025-09-02 18:32:31,982   INFO  Train:   15/20 ( 75%) [  81/3862 (  2%)]  Loss: 0.8278 (1.01)  LR: 4.973e-04  Grad: 9.9245  max=0.3917(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6195(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4027, loss_cls=0.0664, loss_bbox=0.5337, matched_ious=0.5625, d_time=0.00(0.05), f_time=1.46(1.42), b_time=1.46(1.47)  Time cost: 01:59/1:32:12 [20:48:01/9:23:06]  Acc_iter 54150       Data time: 0.00(0.05)  Forward time: 1.46(1.42)  Batch time: 1.46(1.47)
2025-09-02 18:33:42,142   INFO  Train:   15/20 ( 75%) [ 131/3862 (  3%)]  Loss: 0.9450 (1.01)  LR: 4.956e-04  Grad: 9.9419  max=0.3968(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3946(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4152, loss_cls=0.0677, loss_bbox=0.5276, matched_ious=0.5618, d_time=0.00(0.04), f_time=1.28(1.41), b_time=1.28(1.44)  Time cost: 03:10/1:29:34 [20:49:11/9:13:09]  Acc_iter 54200       Data time: 0.00(0.04)  Forward time: 1.28(1.41)  Batch time: 1.28(1.44)
2025-09-02 18:34:52,131   INFO  Train:   15/20 ( 75%) [ 181/3862 (  5%)]  Loss: 0.7776 (1.00)  LR: 4.939e-04  Grad: 10.0046  max=0.3893(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3949(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4085, loss_cls=0.0662, loss_bbox=0.5106, matched_ious=0.5619, d_time=0.00(0.03), f_time=1.34(1.40), b_time=1.34(1.43)  Time cost: 04:20/1:27:41 [20:50:21/9:07:40]  Acc_iter 54250       Data time: 0.00(0.03)  Forward time: 1.34(1.40)  Batch time: 1.34(1.43)
2025-09-02 18:36:00,774   INFO  Train:   15/20 ( 75%) [ 231/3862 (  6%)]  Loss: 0.8934 (0.997)  LR: 4.922e-04  Grad: 10.0047  max=0.3904(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3967(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3975, loss_cls=0.0660, loss_bbox=0.5044, matched_ious=0.5679, d_time=0.00(0.02), f_time=1.45(1.39), b_time=1.45(1.42)  Time cost: 05:28/1:25:45 [20:51:29/9:01:50]  Acc_iter 54300       Data time: 0.00(0.02)  Forward time: 1.45(1.39)  Batch time: 1.45(1.42)
2025-09-02 18:37:09,847   INFO  Train:   15/20 ( 75%) [ 281/3862 (  7%)]  Loss: 0.9822 (1.00)  LR: 4.905e-04  Grad: 10.0899  max=0.5653(module.vfe.pfn_layers.0.linear.weight)  min: -0.4019(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4191, loss_cls=0.0691, loss_bbox=0.5353, matched_ious=0.5610, d_time=0.48(0.02), f_time=1.45(1.39), b_time=1.93(1.41)  Time cost: 06:37/1:24:12 [20:52:38/8:58:14]  Acc_iter 54350       Data time: 0.48(0.02)  Forward time: 1.45(1.39)  Batch time: 1.93(1.41)
2025-09-02 18:38:20,076   INFO  Train:   15/20 ( 75%) [ 331/3862 (  9%)]  Loss: 1.270 (1.00)  LR: 4.888e-04  Grad: 10.1267  max=0.4085(module.vfe.pfn_layers.0.linear.weight)  min: -0.3996(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3964, loss_cls=0.0642, loss_bbox=0.5420, matched_ious=0.5622, d_time=0.03(0.02), f_time=1.33(1.39), b_time=1.36(1.41)  Time cost: 07:48/1:22:58 [20:53:49/8:56:42]  Acc_iter 54400       Data time: 0.03(0.02)  Forward time: 1.33(1.39)  Batch time: 1.36(1.41)
2025-09-02 18:39:29,809   INFO  Train:   15/20 ( 75%) [ 381/3862 ( 10%)]  Loss: 0.8014 (1.00)  LR: 4.871e-04  Grad: 10.1453  max=0.3849(module.vfe.pfn_layers.0.linear.weight)  min: -0.4022(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4070, loss_cls=0.0667, loss_bbox=0.5155, matched_ious=0.5652, d_time=0.01(0.02), f_time=1.53(1.39), b_time=1.54(1.41)  Time cost: 08:57/1:21:40 [20:54:58/8:54:46]  Acc_iter 54450       Data time: 0.01(0.02)  Forward time: 1.53(1.39)  Batch time: 1.54(1.41)
2025-09-02 18:40:38,425   INFO  Train:   15/20 ( 75%) [ 431/3862 ( 11%)]  Loss: 0.8468 (0.998)  LR: 4.854e-04  Grad: 10.2177  max=0.7476(module.vfe.pfn_layers.0.linear.weight)  min: -0.4019(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3979, loss_cls=0.0643, loss_bbox=0.5164, matched_ious=0.5619, d_time=0.03(0.02), f_time=1.35(1.39), b_time=1.38(1.40)  Time cost: 10:06/1:20:16 [20:56:07/8:52:02]  Acc_iter 54500       Data time: 0.03(0.02)  Forward time: 1.35(1.39)  Batch time: 1.38(1.40)
2025-09-02 18:41:47,176   INFO  Train:   15/20 ( 75%) [ 481/3862 ( 12%)]  Loss: 0.6843 (0.994)  LR: 4.837e-04  Grad: 10.2569  max=0.3808(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8949(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3976, loss_cls=0.0667, loss_bbox=0.4981, matched_ious=0.5688, d_time=0.00(0.02), f_time=1.36(1.38), b_time=1.36(1.40)  Time cost: 11:15/1:18:56 [20:57:16/8:49:44]  Acc_iter 54550       Data time: 0.00(0.02)  Forward time: 1.36(1.38)  Batch time: 1.36(1.40)
2025-09-02 18:42:55,589   INFO  Train:   15/20 ( 75%) [ 531/3862 ( 14%)]  Loss: 0.7855 (0.994)  LR: 4.820e-04  Grad: 10.2597  max=0.4536(module.vfe.pfn_layers.0.linear.weight)  min: -0.4031(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4052, loss_cls=0.0666, loss_bbox=0.5227, matched_ious=0.5664, d_time=0.00(0.02), f_time=1.44(1.38), b_time=1.44(1.40)  Time cost: 12:23/1:17:35 [20:58:24/8:47:25]  Acc_iter 54600       Data time: 0.00(0.02)  Forward time: 1.44(1.38)  Batch time: 1.44(1.40)
2025-09-02 18:44:05,761   INFO  Train:   15/20 ( 75%) [ 581/3862 ( 15%)]  Loss: 0.9349 (0.995)  LR: 4.803e-04  Grad: 10.2845  max=0.4001(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4051(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4104, loss_cls=0.0653, loss_bbox=0.5304, matched_ious=0.5637, d_time=0.00(0.02), f_time=1.37(1.38), b_time=1.37(1.40)  Time cost: 13:33/1:16:27 [20:59:34/8:46:27]  Acc_iter 54650       Data time: 0.00(0.02)  Forward time: 1.37(1.38)  Batch time: 1.37(1.40)
2025-09-02 18:45:16,130   INFO  Train:   15/20 ( 75%) [ 631/3862 ( 16%)]  Loss: 0.9063 (0.997)  LR: 4.786e-04  Grad: 10.3494  max=0.4026(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6455(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4156, loss_cls=0.0678, loss_bbox=0.5299, matched_ious=0.5614, d_time=0.46(0.02), f_time=1.31(1.38), b_time=1.76(1.40)  Time cost: 14:44/1:15:19 [21:00:45/8:45:33]  Acc_iter 54700       Data time: 0.46(0.02)  Forward time: 1.31(1.38)  Batch time: 1.76(1.40)
2025-09-02 18:46:24,524   INFO  Train:   15/20 ( 75%) [ 681/3862 ( 18%)]  Loss: 1.072 (0.994)  LR: 4.769e-04  Grad: 10.3303  max=0.4026(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4092(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4021, loss_cls=0.0671, loss_bbox=0.4992, matched_ious=0.5613, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.32(1.40)  Time cost: 15:52/1:14:02 [21:01:53/8:43:32]  Acc_iter 54750       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.32(1.40)
2025-09-02 18:47:33,465   INFO  Train:   15/20 ( 75%) [ 731/3862 ( 19%)]  Loss: 0.8488 (0.994)  LR: 4.752e-04  Grad: 10.4171  max=0.6620(module.vfe.pfn_layers.0.linear.weight)  min: -0.4096(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4039, loss_cls=0.0662, loss_bbox=0.5224, matched_ious=0.5635, d_time=0.00(0.01), f_time=1.28(1.38), b_time=1.28(1.40)  Time cost: 17:01/1:12:49 [21:03:02/8:41:55]  Acc_iter 54800       Data time: 0.00(0.01)  Forward time: 1.28(1.38)  Batch time: 1.28(1.40)
2025-09-02 18:48:42,302   INFO  Train:   15/20 ( 75%) [ 781/3862 ( 20%)]  Loss: 0.9371 (0.995)  LR: 4.735e-04  Grad: 10.4182  max=0.5905(module.vfe.pfn_layers.0.linear.weight)  min: -0.4087(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4082, loss_cls=0.0666, loss_bbox=0.5250, matched_ious=0.5645, d_time=0.01(0.01), f_time=1.30(1.38), b_time=1.31(1.39)  Time cost: 18:10/1:11:35 [21:04:11/8:40:18]  Acc_iter 54850       Data time: 0.01(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.39)
2025-09-02 18:49:51,034   INFO  Train:   15/20 ( 75%) [ 831/3862 ( 22%)]  Loss: 0.9144 (0.993)  LR: 4.719e-04  Grad: 10.4510  max=0.4009(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4145(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3908, loss_cls=0.0616, loss_bbox=0.5225, matched_ious=0.5673, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.31(1.39)  Time cost: 19:19/1:10:22 [21:05:20/8:38:42]  Acc_iter 54900       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.31(1.39)
2025-09-02 18:51:00,838   INFO  Train:   15/20 ( 75%) [ 881/3862 ( 23%)]  Loss: 0.8717 (0.992)  LR: 4.702e-04  Grad: 10.5305  max=0.4059(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4193(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3940, loss_cls=0.0642, loss_bbox=0.5017, matched_ious=0.5715, d_time=0.01(0.01), f_time=1.42(1.38), b_time=1.43(1.39)  Time cost: 20:28/1:09:13 [21:06:29/8:37:36]  Acc_iter 54950       Data time: 0.01(0.01)  Forward time: 1.42(1.38)  Batch time: 1.43(1.39)
2025-09-02 18:52:10,133   INFO  Train:   15/20 ( 75%) [ 931/3862 ( 24%)]  Loss: 1.164 (0.992)  LR: 4.685e-04  Grad: 10.6171  max=0.9718(module.vfe.pfn_layers.0.linear.weight)  min: -0.6534(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4112, loss_cls=0.0672, loss_bbox=0.5218, matched_ious=0.5653, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 21:38/1:08:02 [21:07:39/8:36:18]  Acc_iter 55000       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 18:53:19,041   INFO  Train:   15/20 ( 75%) [ 981/3862 ( 25%)]  Loss: 1.131 (0.994)  LR: 4.668e-04  Grad: 10.6253  max=0.7129(module.vfe.pfn_layers.0.linear.weight)  min: -0.5812(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3994, loss_cls=0.0664, loss_bbox=0.5571, matched_ious=0.5599, d_time=0.01(0.01), f_time=1.45(1.38), b_time=1.46(1.39)  Time cost: 22:47/1:06:50 [21:08:48/8:34:51]  Acc_iter 55050       Data time: 0.01(0.01)  Forward time: 1.45(1.38)  Batch time: 1.46(1.39)
2025-09-02 18:54:26,883   INFO  Train:   15/20 ( 75%) [1031/3862 ( 27%)]  Loss: 0.9161 (0.992)  LR: 4.651e-04  Grad: 10.6458  max=0.3986(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6939(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3916, loss_cls=0.0635, loss_bbox=0.5060, matched_ious=0.5606, d_time=0.01(0.01), f_time=1.23(1.38), b_time=1.24(1.39)  Time cost: 23:54/1:05:36 [21:09:55/8:33:04]  Acc_iter 55100       Data time: 0.01(0.01)  Forward time: 1.23(1.38)  Batch time: 1.24(1.39)
2025-09-02 18:55:34,893   INFO  Train:   15/20 ( 75%) [1081/3862 ( 28%)]  Loss: 0.7624 (0.991)  LR: 4.634e-04  Grad: 10.6044  max=0.4087(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4270(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3995, loss_cls=0.0658, loss_bbox=0.5064, matched_ious=0.5635, d_time=0.00(0.01), f_time=1.26(1.38), b_time=1.27(1.39)  Time cost: 25:02/1:04:22 [21:11:03/8:31:24]  Acc_iter 55150       Data time: 0.00(0.01)  Forward time: 1.26(1.38)  Batch time: 1.27(1.39)
2025-09-02 18:56:45,074   INFO  Train:   15/20 ( 75%) [1131/3862 ( 29%)]  Loss: 0.7995 (0.989)  LR: 4.617e-04  Grad: 10.6457  max=0.4138(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4279(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3769, loss_cls=0.0621, loss_bbox=0.5075, matched_ious=0.5632, d_time=0.03(0.01), f_time=1.30(1.38), b_time=1.33(1.39)  Time cost: 26:13/1:03:15 [21:12:14/8:30:29]  Acc_iter 55200       Data time: 0.03(0.01)  Forward time: 1.30(1.38)  Batch time: 1.33(1.39)
2025-09-02 18:57:53,886   INFO  Train:   15/20 ( 75%) [1181/3862 ( 31%)]  Loss: 0.8614 (0.989)  LR: 4.600e-04  Grad: 10.7167  max=0.6920(module.vfe.pfn_layers.0.linear.weight)  min: -0.5249(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4052, loss_cls=0.0644, loss_bbox=0.5242, matched_ious=0.5612, d_time=0.00(0.01), f_time=1.27(1.38), b_time=1.27(1.39)  Time cost: 27:21/1:02:04 [21:13:22/8:29:07]  Acc_iter 55250       Data time: 0.00(0.01)  Forward time: 1.27(1.38)  Batch time: 1.27(1.39)
2025-09-02 18:59:02,273   INFO  Train:   15/20 ( 75%) [1231/3862 ( 32%)]  Loss: 0.8133 (0.989)  LR: 4.583e-04  Grad: 10.7243  max=0.5522(module.vfe.pfn_layers.0.linear.weight)  min: -0.4295(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3930, loss_cls=0.0643, loss_bbox=0.5186, matched_ious=0.5648, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.32(1.39)  Time cost: 28:30/1:00:52 [21:14:31/8:27:38]  Acc_iter 55300       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.32(1.39)
2025-09-02 19:00:10,841   INFO  Train:   15/20 ( 75%) [1281/3862 ( 33%)]  Loss: 0.8681 (0.987)  LR: 4.566e-04  Grad: 10.7813  max=0.4599(module.vfe.pfn_layers.0.linear.weight)  min: -0.5037(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3905, loss_cls=0.0645, loss_bbox=0.4830, matched_ious=0.5685, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.34(1.39)  Time cost: 29:38/59:41 [21:15:39/8:26:14]  Acc_iter 55350       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.34(1.39)
2025-09-02 19:01:19,845   INFO  Train:   15/20 ( 75%) [1331/3862 ( 34%)]  Loss: 1.006 (0.986)  LR: 4.550e-04  Grad: 10.7747  max=0.4281(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4311(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4029, loss_cls=0.0656, loss_bbox=0.4969, matched_ious=0.5650, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 30:47/58:31 [21:16:48/8:24:59]  Acc_iter 55400       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 19:02:28,487   INFO  Train:   15/20 ( 75%) [1381/3862 ( 36%)]  Loss: 0.9938 (0.985)  LR: 4.533e-04  Grad: 10.7829  max=0.4329(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6042(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3993, loss_cls=0.0648, loss_bbox=0.4949, matched_ious=0.5696, d_time=0.00(0.01), f_time=1.40(1.38), b_time=1.41(1.39)  Time cost: 31:56/57:20 [21:17:57/8:23:38]  Acc_iter 55450       Data time: 0.00(0.01)  Forward time: 1.40(1.38)  Batch time: 1.41(1.39)
2025-09-02 19:03:39,549   INFO  Train:   15/20 ( 75%) [1431/3862 ( 37%)]  Loss: 0.8775 (0.985)  LR: 4.516e-04  Grad: 10.8400  max=0.4206(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4901(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4012, loss_cls=0.0654, loss_bbox=0.5142, matched_ious=0.5641, d_time=0.01(0.01), f_time=1.32(1.38), b_time=1.33(1.39)  Time cost: 33:07/56:14 [21:19:08/8:22:55]  Acc_iter 55500       Data time: 0.01(0.01)  Forward time: 1.32(1.38)  Batch time: 1.33(1.39)
2025-09-02 19:04:49,570   INFO  Train:   15/20 ( 75%) [1481/3862 ( 38%)]  Loss: 1.007 (0.985)  LR: 4.499e-04  Grad: 10.8129  max=0.4315(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4392(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4097, loss_cls=0.0659, loss_bbox=0.5207, matched_ious=0.5680, d_time=0.01(0.01), f_time=1.25(1.38), b_time=1.26(1.39)  Time cost: 34:17/55:05 [21:20:18/8:21:55]  Acc_iter 55550       Data time: 0.01(0.01)  Forward time: 1.25(1.38)  Batch time: 1.26(1.39)
2025-09-02 19:05:57,212   INFO  Train:   15/20 ( 75%) [1531/3862 ( 40%)]  Loss: 1.005 (0.985)  LR: 4.482e-04  Grad: 10.9067  max=0.4196(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5841(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4095, loss_cls=0.0681, loss_bbox=0.5058, matched_ious=0.5630, d_time=0.00(0.01), f_time=1.38(1.38), b_time=1.39(1.39)  Time cost: 35:25/53:53 [21:21:26/8:20:20]  Acc_iter 55600       Data time: 0.00(0.01)  Forward time: 1.38(1.38)  Batch time: 1.39(1.39)
2025-09-02 19:07:06,733   INFO  Train:   15/20 ( 75%) [1581/3862 ( 41%)]  Loss: 1.229 (0.986)  LR: 4.465e-04  Grad: 10.9103  max=0.4313(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5614(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4065, loss_cls=0.0671, loss_bbox=0.5250, matched_ious=0.5660, d_time=0.01(0.01), f_time=1.40(1.38), b_time=1.41(1.39)  Time cost: 36:34/52:44 [21:22:35/8:19:13]  Acc_iter 55650       Data time: 0.01(0.01)  Forward time: 1.40(1.38)  Batch time: 1.41(1.39)
2025-09-02 19:08:15,378   INFO  Train:   15/20 ( 75%) [1631/3862 ( 42%)]  Loss: 0.8541 (0.985)  LR: 4.448e-04  Grad: 10.9420  max=0.4337(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4468(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3882, loss_cls=0.0642, loss_bbox=0.5102, matched_ious=0.5707, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.32(1.39)  Time cost: 37:43/51:34 [21:23:44/8:17:54]  Acc_iter 55700       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.32(1.39)
2025-09-02 19:09:25,329   INFO  Train:   15/20 ( 75%) [1681/3862 ( 44%)]  Loss: 1.117 (0.985)  LR: 4.432e-04  Grad: 10.9831  max=0.4434(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4474(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4016, loss_cls=0.0657, loss_bbox=0.5062, matched_ious=0.5658, d_time=0.00(0.01), f_time=1.52(1.38), b_time=1.53(1.39)  Time cost: 38:53/50:25 [21:24:54/8:16:53]  Acc_iter 55750       Data time: 0.00(0.01)  Forward time: 1.52(1.38)  Batch time: 1.53(1.39)
2025-09-02 19:10:35,460   INFO  Train:   15/20 ( 75%) [1731/3862 ( 45%)]  Loss: 0.7953 (0.983)  LR: 4.415e-04  Grad: 11.0074  max=0.4453(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4519(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3777, loss_cls=0.0621, loss_bbox=0.4975, matched_ious=0.5640, d_time=0.01(0.01), f_time=1.29(1.38), b_time=1.30(1.39)  Time cost: 40:03/49:17 [21:26:04/8:15:53]  Acc_iter 55800       Data time: 0.01(0.01)  Forward time: 1.29(1.38)  Batch time: 1.30(1.39)
2025-09-02 19:11:44,864   INFO  Train:   15/20 ( 75%) [1781/3862 ( 46%)]  Loss: 0.9865 (0.983)  LR: 4.398e-04  Grad: 11.0749  max=0.5401(module.vfe.pfn_layers.0.linear.weight)  min: -0.4502(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3985, loss_cls=0.0665, loss_bbox=0.5050, matched_ious=0.5644, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.37(1.39)  Time cost: 41:12/48:07 [21:27:13/8:14:44]  Acc_iter 55850       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.37(1.39)
2025-09-02 19:12:54,570   INFO  Train:   15/20 ( 75%) [1831/3862 ( 47%)]  Loss: 0.8744 (0.983)  LR: 4.381e-04  Grad: 11.0848  max=0.4567(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4536(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3985, loss_cls=0.0665, loss_bbox=0.5068, matched_ious=0.5688, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.33(1.39)  Time cost: 42:22/46:58 [21:28:23/8:13:38]  Acc_iter 55900       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.33(1.39)
2025-09-02 19:14:02,301   INFO  Train:   15/20 ( 75%) [1881/3862 ( 49%)]  Loss: 0.8757 (0.983)  LR: 4.364e-04  Grad: 11.1034  max=0.4567(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4559(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3984, loss_cls=0.0664, loss_bbox=0.5299, matched_ious=0.5627, d_time=0.01(0.01), f_time=1.26(1.38), b_time=1.26(1.39)  Time cost: 43:30/45:47 [21:29:31/8:12:10]  Acc_iter 55950       Data time: 0.01(0.01)  Forward time: 1.26(1.38)  Batch time: 1.26(1.39)
2025-09-02 19:15:11,527   INFO  Train:   15/20 ( 75%) [1931/3862 ( 50%)]  Loss: 1.015 (0.983)  LR: 4.347e-04  Grad: 11.1291  max=0.4582(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4614(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3998, loss_cls=0.0660, loss_bbox=0.5218, matched_ious=0.5660, d_time=0.00(0.01), f_time=1.25(1.38), b_time=1.26(1.39)  Time cost: 44:39/44:38 [21:30:40/8:10:59]  Acc_iter 56000       Data time: 0.00(0.01)  Forward time: 1.25(1.38)  Batch time: 1.26(1.39)
2025-09-02 19:16:21,468   INFO  Train:   15/20 ( 75%) [1981/3862 ( 51%)]  Loss: 0.8570 (0.983)  LR: 4.331e-04  Grad: 11.1978  max=0.4650(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7098(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3992, loss_cls=0.0645, loss_bbox=0.5286, matched_ious=0.5667, d_time=0.00(0.01), f_time=1.28(1.38), b_time=1.28(1.39)  Time cost: 45:49/43:29 [21:31:50/8:09:56]  Acc_iter 56050       Data time: 0.00(0.01)  Forward time: 1.28(1.38)  Batch time: 1.28(1.39)
2025-09-02 19:17:31,650   INFO  Train:   15/20 ( 75%) [2031/3862 ( 53%)]  Loss: 1.205 (0.983)  LR: 4.314e-04  Grad: 11.1455  max=0.5767(module.vfe.pfn_layers.0.linear.weight)  min: -0.4601(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3972, loss_cls=0.0649, loss_bbox=0.5232, matched_ious=0.5637, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.31(1.39)  Time cost: 46:59/42:20 [21:33:00/8:08:55]  Acc_iter 56100       Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.39)
2025-09-02 19:18:40,570   INFO  Train:   15/20 ( 75%) [2081/3862 ( 54%)]  Loss: 0.9316 (0.983)  LR: 4.297e-04  Grad: 11.1767  max=0.4555(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4620(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3889, loss_cls=0.0639, loss_bbox=0.5105, matched_ious=0.5748, d_time=0.01(0.01), f_time=1.27(1.38), b_time=1.28(1.39)  Time cost: 48:08/41:10 [21:34:09/8:07:41]  Acc_iter 56150       Data time: 0.01(0.01)  Forward time: 1.27(1.38)  Batch time: 1.28(1.39)
2025-09-02 19:19:49,554   INFO  Train:   15/20 ( 75%) [2131/3862 ( 55%)]  Loss: 0.6234 (0.982)  LR: 4.280e-04  Grad: 11.2443  max=0.4687(module.vfe.pfn_layers.0.linear.weight)  min: -0.7593(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3980, loss_cls=0.0652, loss_bbox=0.4992, matched_ious=0.5662, d_time=0.01(0.01), f_time=1.40(1.38), b_time=1.41(1.39)  Time cost: 49:17/40:01 [21:35:18/8:06:28]  Acc_iter 56200       Data time: 0.01(0.01)  Forward time: 1.40(1.38)  Batch time: 1.41(1.39)
2025-09-02 19:20:58,615   INFO  Train:   15/20 ( 75%) [2181/3862 ( 56%)]  Loss: 0.8839 (0.982)  LR: 4.264e-04  Grad: 11.2055  max=0.4568(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4657(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4004, loss_cls=0.0669, loss_bbox=0.5082, matched_ious=0.5688, d_time=0.01(0.01), f_time=1.27(1.38), b_time=1.28(1.39)  Time cost: 50:26/38:51 [21:36:27/8:05:16]  Acc_iter 56250       Data time: 0.01(0.01)  Forward time: 1.27(1.38)  Batch time: 1.28(1.39)
2025-09-02 19:22:09,523   INFO  Train:   15/20 ( 75%) [2231/3862 ( 58%)]  Loss: 1.099 (0.982)  LR: 4.247e-04  Grad: 11.2577  max=0.4433(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4678(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4101, loss_cls=0.0668, loss_bbox=0.5119, matched_ious=0.5653, d_time=0.00(0.01), f_time=1.29(1.38), b_time=1.30(1.39)  Time cost: 51:37/37:43 [21:37:38/8:04:21]  Acc_iter 56300       Data time: 0.00(0.01)  Forward time: 1.29(1.38)  Batch time: 1.30(1.39)
2025-09-02 19:23:18,012   INFO  Train:   15/20 ( 75%) [2281/3862 ( 59%)]  Loss: 0.7549 (0.982)  LR: 4.230e-04  Grad: 11.2714  max=0.4522(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4687(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4029, loss_cls=0.0677, loss_bbox=0.5171, matched_ious=0.5611, d_time=0.03(0.01), f_time=1.31(1.38), b_time=1.34(1.39)  Time cost: 52:46/36:33 [21:38:47/8:03:03]  Acc_iter 56350       Data time: 0.03(0.01)  Forward time: 1.31(1.38)  Batch time: 1.34(1.39)
2025-09-02 19:24:27,029   INFO  Train:   15/20 ( 75%) [2331/3862 ( 60%)]  Loss: 0.9048 (0.982)  LR: 4.213e-04  Grad: 11.3347  max=0.4474(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4728(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3899, loss_cls=0.0625, loss_bbox=0.5098, matched_ious=0.5706, d_time=0.00(0.01), f_time=1.27(1.38), b_time=1.28(1.39)  Time cost: 53:55/35:23 [21:39:56/8:01:51]  Acc_iter 56400       Data time: 0.00(0.01)  Forward time: 1.27(1.38)  Batch time: 1.28(1.39)
2025-09-02 19:25:35,841   INFO  Train:   15/20 ( 75%) [2381/3862 ( 62%)]  Loss: 1.060 (0.981)  LR: 4.197e-04  Grad: 11.3608  max=0.4602(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4745(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3897, loss_cls=0.0624, loss_bbox=0.5053, matched_ious=0.5677, d_time=0.01(0.01), f_time=1.28(1.38), b_time=1.29(1.39)  Time cost: 55:03/34:14 [21:41:04/8:00:37]  Acc_iter 56450       Data time: 0.01(0.01)  Forward time: 1.28(1.38)  Batch time: 1.29(1.39)
2025-09-02 19:26:43,524   INFO  Train:   15/20 ( 75%) [2431/3862 ( 63%)]  Loss: 0.8065 (0.981)  LR: 4.180e-04  Grad: 11.3877  max=0.4652(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4766(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3980, loss_cls=0.0651, loss_bbox=0.4939, matched_ious=0.5702, d_time=0.01(0.01), f_time=1.42(1.38), b_time=1.43(1.39)  Time cost: 56:11/33:03 [21:42:12/7:59:13]  Acc_iter 56500       Data time: 0.01(0.01)  Forward time: 1.42(1.38)  Batch time: 1.43(1.39)
2025-09-02 19:27:54,319   INFO  Train:   15/20 ( 75%) [2481/3862 ( 64%)]  Loss: 0.9534 (0.980)  LR: 4.163e-04  Grad: 11.3981  max=0.4497(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4781(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3907, loss_cls=0.0647, loss_bbox=0.4866, matched_ious=0.5692, d_time=0.00(0.01), f_time=1.39(1.38), b_time=1.39(1.39)  Time cost: 57:22/31:55 [21:43:23/7:58:16]  Acc_iter 56550       Data time: 0.00(0.01)  Forward time: 1.39(1.38)  Batch time: 1.39(1.39)
2025-09-02 19:29:03,448   INFO  Train:   15/20 ( 75%) [2531/3862 ( 66%)]  Loss: 0.6182 (0.980)  LR: 4.146e-04  Grad: 11.4773  max=0.4447(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6623(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3995, loss_cls=0.0657, loss_bbox=0.5292, matched_ious=0.5653, d_time=0.00(0.01), f_time=1.35(1.38), b_time=1.36(1.39)  Time cost: 58:31/30:45 [21:44:32/7:57:05]  Acc_iter 56600       Data time: 0.00(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.39)
2025-09-02 19:30:12,435   INFO  Train:   15/20 ( 75%) [2581/3862 ( 67%)]  Loss: 0.9404 (0.980)  LR: 4.130e-04  Grad: 11.4907  max=0.4487(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4834(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3879, loss_cls=0.0642, loss_bbox=0.5033, matched_ious=0.5651, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.37(1.39)  Time cost: 59:40/29:36 [21:45:41/7:55:53]  Acc_iter 56650       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.37(1.39)
2025-09-02 19:31:22,450   INFO  Train:   15/20 ( 75%) [2631/3862 ( 68%)]  Loss: 0.7948 (0.979)  LR: 4.113e-04  Grad: 11.6699  max=1.8266(module.vfe.pfn_layers.0.linear.weight)  min: -0.4840(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3982, loss_cls=0.0648, loss_bbox=0.4907, matched_ious=0.5691, d_time=0.00(0.01), f_time=1.29(1.38), b_time=1.30(1.39)  Time cost: 1:00:50/28:27 [21:46:51/7:54:49]  Acc_iter 56700       Data time: 0.00(0.01)  Forward time: 1.29(1.38)  Batch time: 1.30(1.39)
2025-09-02 19:32:30,451   INFO  Train:   15/20 ( 75%) [2681/3862 ( 69%)]  Loss: 0.9258 (0.978)  LR: 4.096e-04  Grad: 11.5510  max=0.4578(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4865(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3786, loss_cls=0.0628, loss_bbox=0.4672, matched_ious=0.5709, d_time=0.00(0.01), f_time=1.41(1.38), b_time=1.41(1.39)  Time cost: 1:01:58/27:17 [21:47:59/7:53:29]  Acc_iter 56750       Data time: 0.00(0.01)  Forward time: 1.41(1.38)  Batch time: 1.41(1.39)
2025-09-02 19:33:42,097   INFO  Train:   15/20 ( 75%) [2731/3862 ( 71%)]  Loss: 0.7541 (0.977)  LR: 4.080e-04  Grad: 11.6206  max=0.4996(module.vfe.pfn_layers.0.linear.weight)  min: -0.4900(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3904, loss_cls=0.0631, loss_bbox=0.4779, matched_ious=0.5701, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 1:03:10/26:09 [21:49:11/7:52:37]  Acc_iter 56800       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 19:34:49,829   INFO  Train:   15/20 ( 75%) [2781/3862 ( 72%)]  Loss: 1.509 (0.977)  LR: 4.063e-04  Grad: 11.6262  max=0.4328(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4900(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4035, loss_cls=0.0669, loss_bbox=0.5109, matched_ious=0.5648, d_time=0.01(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 1:04:17/24:59 [21:50:18/7:51:16]  Acc_iter 56850       Data time: 0.01(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 19:35:59,672   INFO  Train:   15/20 ( 75%) [2831/3862 ( 73%)]  Loss: 0.8781 (0.977)  LR: 4.046e-04  Grad: 11.6601  max=0.4287(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4948(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3954, loss_cls=0.0652, loss_bbox=0.4937, matched_ious=0.5696, d_time=0.00(0.01), f_time=1.47(1.38), b_time=1.48(1.39)  Time cost: 1:05:27/23:49 [21:51:28/7:50:10]  Acc_iter 56900       Data time: 0.00(0.01)  Forward time: 1.47(1.38)  Batch time: 1.48(1.39)
2025-09-02 19:37:08,504   INFO  Train:   15/20 ( 75%) [2881/3862 ( 75%)]  Loss: 0.9561 (0.977)  LR: 4.030e-04  Grad: 11.7036  max=0.4582(module.vfe.pfn_layers.0.linear.weight)  min: -0.4960(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3973, loss_cls=0.0646, loss_bbox=0.4912, matched_ious=0.5735, d_time=0.01(0.01), f_time=1.38(1.38), b_time=1.39(1.39)  Time cost: 1:06:36/22:40 [21:52:37/7:48:57]  Acc_iter 56950       Data time: 0.01(0.01)  Forward time: 1.38(1.38)  Batch time: 1.39(1.39)
2025-09-02 19:38:16,377   INFO  Train:   15/20 ( 75%) [2931/3862 ( 76%)]  Loss: 0.9916 (0.976)  LR: 4.013e-04  Grad: 11.7593  max=0.6351(module.vfe.pfn_layers.0.linear.weight)  min: -0.4967(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3890, loss_cls=0.0634, loss_bbox=0.4955, matched_ious=0.5631, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.35(1.39)  Time cost: 1:07:44/21:30 [21:53:45/7:47:38]  Acc_iter 57000       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.35(1.39)
2025-09-02 19:39:24,757   INFO  Train:   15/20 ( 75%) [2981/3862 ( 77%)]  Loss: 0.8261 (0.976)  LR: 3.997e-04  Grad: 11.7513  max=0.4398(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5037(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4033, loss_cls=0.0647, loss_bbox=0.5038, matched_ious=0.5639, d_time=0.00(0.01), f_time=1.29(1.38), b_time=1.30(1.39)  Time cost: 1:08:52/20:20 [21:54:53/7:46:22]  Acc_iter 57050       Data time: 0.00(0.01)  Forward time: 1.29(1.38)  Batch time: 1.30(1.39)
2025-09-02 19:40:35,697   INFO  Train:   15/20 ( 75%) [3031/3862 ( 78%)]  Loss: 1.060 (0.976)  LR: 3.980e-04  Grad: 11.8033  max=0.4519(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5036(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3838, loss_cls=0.0628, loss_bbox=0.5042, matched_ious=0.5642, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 1:10:03/19:12 [21:56:04/7:45:24]  Acc_iter 57100       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 19:41:43,595   INFO  Train:   15/20 ( 75%) [3081/3862 ( 80%)]  Loss: 1.124 (0.976)  LR: 3.963e-04  Grad: 11.8701  max=0.7505(module.vfe.pfn_layers.0.linear.weight)  min: -0.5050(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3973, loss_cls=0.0656, loss_bbox=0.5109, matched_ious=0.5685, d_time=0.00(0.01), f_time=1.41(1.38), b_time=1.42(1.39)  Time cost: 1:11:11/18:02 [21:57:12/7:44:05]  Acc_iter 57150       Data time: 0.00(0.01)  Forward time: 1.41(1.38)  Batch time: 1.42(1.39)
2025-09-02 19:42:52,934   INFO  Train:   15/20 ( 75%) [3131/3862 ( 81%)]  Loss: 1.143 (0.975)  LR: 3.947e-04  Grad: 11.8794  max=1.0760(module.vfe.pfn_layers.0.linear.weight)  min: -0.5076(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3945, loss_cls=0.0647, loss_bbox=0.5096, matched_ious=0.5660, d_time=0.01(0.01), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 1:12:20/16:53 [21:58:22/7:42:56]  Acc_iter 57200       Data time: 0.01(0.01)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 19:44:01,279   INFO  Train:   15/20 ( 75%) [3181/3862 ( 82%)]  Loss: 1.004 (0.975)  LR: 3.930e-04  Grad: 11.9218  max=1.0657(module.vfe.pfn_layers.0.linear.weight)  min: -0.5080(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3902, loss_cls=0.0636, loss_bbox=0.4874, matched_ious=0.5724, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.31(1.39)  Time cost: 1:13:29/15:43 [21:59:30/7:41:41]  Acc_iter 57250       Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.39)
2025-09-02 19:45:10,042   INFO  Train:   15/20 ( 75%) [3231/3862 ( 84%)]  Loss: 0.9562 (0.974)  LR: 3.914e-04  Grad: 11.9103  max=0.5618(module.vfe.pfn_layers.0.linear.weight)  min: -0.5073(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3745, loss_cls=0.0616, loss_bbox=0.4970, matched_ious=0.5749, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 1:14:38/14:34 [22:00:39/7:40:28]  Acc_iter 57300       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 19:46:20,792   INFO  Train:   15/20 ( 75%) [3281/3862 ( 85%)]  Loss: 0.8784 (0.973)  LR: 3.897e-04  Grad: 11.9442  max=0.4673(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5094(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3724, loss_cls=0.0625, loss_bbox=0.4761, matched_ious=0.5715, d_time=0.01(0.01), f_time=1.38(1.38), b_time=1.39(1.39)  Time cost: 1:15:48/13:25 [22:01:49/7:39:28]  Acc_iter 57350       Data time: 0.01(0.01)  Forward time: 1.38(1.38)  Batch time: 1.39(1.39)
2025-09-02 19:47:28,456   INFO  Train:   15/20 ( 75%) [3331/3862 ( 86%)]  Loss: 0.8999 (0.973)  LR: 3.881e-04  Grad: 11.9609  max=0.4698(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5109(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3850, loss_cls=0.0626, loss_bbox=0.4959, matched_ious=0.5681, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 1:16:56/12:15 [22:02:57/7:38:09]  Acc_iter 57400       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 19:48:39,606   INFO  Train:   15/20 ( 75%) [3381/3862 ( 88%)]  Loss: 1.120 (0.973)  LR: 3.864e-04  Grad: 11.9986  max=0.4857(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5159(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3918, loss_cls=0.0634, loss_bbox=0.5075, matched_ious=0.5674, d_time=0.00(0.01), f_time=1.42(1.38), b_time=1.42(1.39)  Time cost: 1:18:07/11:06 [22:04:08/7:37:11]  Acc_iter 57450       Data time: 0.00(0.01)  Forward time: 1.42(1.38)  Batch time: 1.42(1.39)
2025-09-02 19:49:46,451   INFO  Train:   15/20 ( 75%) [3431/3862 ( 89%)]  Loss: 0.7574 (0.972)  LR: 3.848e-04  Grad: 12.0263  max=0.4791(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5161(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3891, loss_cls=0.0654, loss_bbox=0.5020, matched_ious=0.5622, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 1:19:14/09:57 [22:05:15/7:35:47]  Acc_iter 57500       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 19:50:55,412   INFO  Train:   15/20 ( 75%) [3481/3862 ( 90%)]  Loss: 1.188 (0.972)  LR: 3.831e-04  Grad: 12.0700  max=0.4881(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5186(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3926, loss_cls=0.0651, loss_bbox=0.4936, matched_ious=0.5704, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 1:20:23/08:47 [22:06:24/7:34:36]  Acc_iter 57550       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 19:52:05,911   INFO  Train:   15/20 ( 75%) [3531/3862 ( 91%)]  Loss: 1.045 (0.972)  LR: 3.815e-04  Grad: 12.1002  max=0.4811(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5217(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3850, loss_cls=0.0641, loss_bbox=0.4964, matched_ious=0.5673, d_time=0.00(0.01), f_time=1.41(1.38), b_time=1.41(1.39)  Time cost: 1:21:33/07:38 [22:07:34/7:33:34]  Acc_iter 57600       Data time: 0.00(0.01)  Forward time: 1.41(1.38)  Batch time: 1.41(1.39)
2025-09-02 19:53:13,316   INFO  Train:   15/20 ( 75%) [3581/3862 ( 93%)]  Loss: 0.8951 (0.971)  LR: 3.798e-04  Grad: 12.1846  max=0.9846(module.vfe.pfn_layers.0.linear.weight)  min: -0.5247(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3891, loss_cls=0.0633, loss_bbox=0.4897, matched_ious=0.5699, d_time=0.01(0.01), f_time=1.23(1.38), b_time=1.24(1.39)  Time cost: 1:22:41/06:29 [22:08:42/7:32:14]  Acc_iter 57650       Data time: 0.01(0.01)  Forward time: 1.23(1.38)  Batch time: 1.24(1.39)
2025-09-02 19:54:23,016   INFO  Train:   15/20 ( 75%) [3631/3862 ( 94%)]  Loss: 0.7754 (0.971)  LR: 3.782e-04  Grad: 12.1674  max=0.4904(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5260(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3904, loss_cls=0.0633, loss_bbox=0.4850, matched_ious=0.5736, d_time=0.00(0.01), f_time=1.44(1.38), b_time=1.44(1.39)  Time cost: 1:23:51/05:19 [22:09:52/7:31:07]  Acc_iter 57700       Data time: 0.00(0.01)  Forward time: 1.44(1.38)  Batch time: 1.44(1.39)
2025-09-02 19:55:32,305   INFO  Train:   15/20 ( 75%) [3681/3862 ( 95%)]  Loss: 1.135 (0.970)  LR: 3.765e-04  Grad: 12.2358  max=0.5024(module.vfe.pfn_layers.0.linear.weight)  min: -0.5286(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3872, loss_cls=0.0624, loss_bbox=0.4870, matched_ious=0.5723, d_time=0.00(0.01), f_time=1.35(1.38), b_time=1.36(1.39)  Time cost: 1:25:00/04:10 [22:11:01/7:29:58]  Acc_iter 57750       Data time: 0.00(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.39)
2025-09-02 19:56:41,182   INFO  Train:   15/20 ( 75%) [3731/3862 ( 97%)]  Loss: 1.189 (0.971)  LR: 3.749e-04  Grad: 12.1777  max=0.4872(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5285(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3981, loss_cls=0.0640, loss_bbox=0.5170, matched_ious=0.5692, d_time=0.00(0.01), f_time=1.41(1.38), b_time=1.41(1.39)  Time cost: 1:26:09/03:01 [22:12:10/7:28:47]  Acc_iter 57800       Data time: 0.00(0.01)  Forward time: 1.41(1.38)  Batch time: 1.41(1.39)
2025-09-02 19:57:49,742   INFO  Train:   15/20 ( 75%) [3781/3862 ( 98%)]  Loss: 1.121 (0.971)  LR: 3.733e-04  Grad: 12.2385  max=0.4964(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5320(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3989, loss_cls=0.0637, loss_bbox=0.5092, matched_ious=0.5698, d_time=0.01(0.01), f_time=1.35(1.38), b_time=1.36(1.38)  Time cost: 1:27:17/01:52 [22:13:18/7:27:34]  Acc_iter 57850       Data time: 0.01(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.38)
2025-09-02 19:58:59,100   INFO  Train:   15/20 ( 75%) [3831/3862 ( 99%)]  Loss: 0.8023 (0.971)  LR: 3.716e-04  Grad: 12.2658  max=0.5089(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6411(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3975, loss_cls=0.0637, loss_bbox=0.5098, matched_ious=0.5681, d_time=0.01(0.01), f_time=1.33(1.38), b_time=1.35(1.38)  Time cost: 1:28:27/00:42 [22:14:28/7:26:26]  Acc_iter 57900       Data time: 0.01(0.01)  Forward time: 1.33(1.38)  Batch time: 1.35(1.38)
2025-09-02 19:59:39,177   INFO  Train:   15/20 ( 75%) [3861/3862 (100%)]  Loss: 0.8553 (0.970)  LR: 3.706e-04  Grad: 12.2846  max=0.4924(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5370(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3893, loss_cls=0.0648, loss_bbox=0.4948, matched_ious=0.5682, d_time=0.00(0.01), f_time=1.33(1.37), b_time=1.33(1.38)  Time cost: 1:29:07/00:01 [22:15:08/7:25:37]  Acc_iter 57930       Data time: 0.00(0.01)  Forward time: 1.33(1.37)  Batch time: 1.33(1.38)

                                               [Aepochs:  75%|███████▌  | 15/20 [22:15:08<7:25:22, 5344.58s/it]epochs:  75%|███████▌  | 15/20 [22:15:08<7:25:22, 5344.59s/it]epochs:  75%|███████▌  | 15/20 [22:15:09<7:25:23, 5344.64s/it]epochs:  75%|███████▌  | 15/20 [22:15:09<7:25:23, 5344.65s/it]epochs:  75%|███████▌  | 15/20 [22:15:09<7:25:23, 5344.66s/it]epochs:  75%|███████▌  | 15/20 [22:15:09<7:25:23, 5344.65s/it]epochs:  75%|███████▌  | 15/20 [22:15:09<7:25:23, 5344.65s/it]epochs:  75%|███████▌  | 15/20 [22:15:09<7:25:23, 5344.65s/it]
train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-02 19:59:47,465   INFO  Train:   16/20 ( 80%) [   0/3862 (  0%)]  Loss: 0.8079 (0.808)  LR: 3.706e-04  Grad: 12.2906  max=0.4928(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5368(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.2955, loss_cls=0.0479, loss_bbox=0.4644, matched_ious=0.5805, d_time=3.23(3.23), f_time=3.25(3.25), b_time=6.49(6.49)  Time cost: 00:06/6:37:01 [22:15:16/33:05:08]  Acc_iter 57931       Data time: 3.23(3.23)  Forward time: 3.25(3.25)  Batch time: 6.49(6.49)
2025-09-02 20:00:13,697   INFO  Train:   16/20 ( 80%) [  19/3862 (  0%)]  Loss: 0.8939 (0.942)  LR: 3.700e-04  Grad: 12.3038  max=0.4973(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5344(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3892, loss_cls=0.0640, loss_bbox=0.4955, matched_ious=0.5702, d_time=0.00(0.17), f_time=1.35(1.47), b_time=1.35(1.64)  Time cost: 00:32/1:43:46 [22:15:42/8:40:53]  Acc_iter 57950       Data time: 0.00(0.17)  Forward time: 1.35(1.47)  Batch time: 1.35(1.64)
2025-09-02 20:01:23,701   INFO  Train:   16/20 ( 80%) [  69/3862 (  2%)]  Loss: 1.010 (0.988)  LR: 3.683e-04  Grad: 12.3828  max=0.4705(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5389(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4083, loss_cls=0.0664, loss_bbox=0.5324, matched_ious=0.5550, d_time=0.00(0.05), f_time=1.37(1.42), b_time=1.38(1.47)  Time cost: 01:42/1:32:28 [22:16:52/7:49:08]  Acc_iter 58000       Data time: 0.00(0.05)  Forward time: 1.37(1.42)  Batch time: 1.38(1.47)
2025-09-02 20:02:34,338   INFO  Train:   16/20 ( 80%) [ 119/3862 (  3%)]  Loss: 0.7997 (0.979)  LR: 3.667e-04  Grad: 12.4400  max=0.4788(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5403(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3879, loss_cls=0.0647, loss_bbox=0.5124, matched_ious=0.5638, d_time=0.00(0.03), f_time=1.31(1.41), b_time=1.31(1.44)  Time cost: 02:53/1:29:57 [22:18:03/7:41:13]  Acc_iter 58050       Data time: 0.00(0.03)  Forward time: 1.31(1.41)  Batch time: 1.31(1.44)
2025-09-02 20:03:42,824   INFO  Train:   16/20 ( 80%) [ 169/3862 (  4%)]  Loss: 1.085 (0.967)  LR: 3.651e-04  Grad: 12.4419  max=0.4826(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5449(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3864, loss_cls=0.0633, loss_bbox=0.4889, matched_ious=0.5762, d_time=0.00(0.03), f_time=1.42(1.40), b_time=1.42(1.42)  Time cost: 04:01/1:27:26 [22:19:11/7:33:14]  Acc_iter 58100       Data time: 0.00(0.03)  Forward time: 1.42(1.40)  Batch time: 1.42(1.42)
2025-09-02 20:04:53,842   INFO  Train:   16/20 ( 80%) [ 219/3862 (  6%)]  Loss: 0.9300 (0.959)  LR: 3.634e-04  Grad: 12.4530  max=0.4944(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5438(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3890, loss_cls=0.0636, loss_bbox=0.4814, matched_ious=0.5717, d_time=0.00(0.02), f_time=1.38(1.40), b_time=1.39(1.42)  Time cost: 05:12/1:26:15 [22:20:22/7:32:01]  Acc_iter 58150       Data time: 0.00(0.02)  Forward time: 1.38(1.40)  Batch time: 1.39(1.42)
2025-09-02 20:06:02,873   INFO  Train:   16/20 ( 80%) [ 269/3862 (  7%)]  Loss: 0.9666 (0.954)  LR: 3.618e-04  Grad: 12.4708  max=0.4958(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5428(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3788, loss_cls=0.0614, loss_bbox=0.4903, matched_ious=0.5753, d_time=0.00(0.02), f_time=1.42(1.40), b_time=1.42(1.41)  Time cost: 06:21/1:24:37 [22:21:31/7:28:29]  Acc_iter 58200       Data time: 0.00(0.02)  Forward time: 1.42(1.40)  Batch time: 1.42(1.41)
2025-09-02 20:07:11,211   INFO  Train:   16/20 ( 80%) [ 319/3862 (  8%)]  Loss: 1.015 (0.953)  LR: 3.602e-04  Grad: 12.5138  max=0.5002(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5447(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3815, loss_cls=0.0609, loss_bbox=0.5052, matched_ious=0.5726, d_time=0.00(0.02), f_time=1.26(1.39), b_time=1.26(1.41)  Time cost: 07:29/1:23:01 [22:22:40/7:25:01]  Acc_iter 58250       Data time: 0.00(0.02)  Forward time: 1.26(1.39)  Batch time: 1.26(1.41)
2025-09-02 20:08:19,752   INFO  Train:   16/20 ( 80%) [ 369/3862 ( 10%)]  Loss: 0.9414 (0.955)  LR: 3.586e-04  Grad: 12.5216  max=0.4993(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5498(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3796, loss_cls=0.0621, loss_bbox=0.5266, matched_ious=0.5701, d_time=0.00(0.02), f_time=1.29(1.38), b_time=1.29(1.40)  Time cost: 08:38/1:21:34 [22:23:48/7:22:20]  Acc_iter 58300       Data time: 0.00(0.02)  Forward time: 1.29(1.38)  Batch time: 1.29(1.40)
2025-09-02 20:09:28,920   INFO  Train:   16/20 ( 80%) [ 419/3862 ( 11%)]  Loss: 0.9281 (0.957)  LR: 3.569e-04  Grad: 12.5565  max=0.5046(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5548(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3874, loss_cls=0.0641, loss_bbox=0.5163, matched_ious=0.5684, d_time=0.00(0.02), f_time=1.35(1.38), b_time=1.35(1.40)  Time cost: 09:47/1:20:17 [22:24:57/7:20:30]  Acc_iter 58350       Data time: 0.00(0.02)  Forward time: 1.35(1.38)  Batch time: 1.35(1.40)
2025-09-02 20:10:37,841   INFO  Train:   16/20 ( 80%) [ 469/3862 ( 12%)]  Loss: 0.8612 (0.959)  LR: 3.553e-04  Grad: 12.4651  max=1.3465(module.vfe.pfn_layers.0.linear.weight)  min: -0.5465(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3938, loss_cls=0.0652, loss_bbox=0.5173, matched_ious=0.5670, d_time=0.00(0.02), f_time=2.22(1.38), b_time=2.22(1.40)  Time cost: 10:56/1:18:59 [22:26:06/7:18:39]  Acc_iter 58400       Data time: 0.00(0.02)  Forward time: 2.22(1.38)  Batch time: 2.22(1.40)
2025-09-02 20:11:46,535   INFO  Train:   16/20 ( 80%) [ 519/3862 ( 13%)]  Loss: 0.9989 (0.955)  LR: 3.537e-04  Grad: 12.4048  max=0.4698(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5502(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3788, loss_cls=0.0631, loss_bbox=0.4732, matched_ious=0.5750, d_time=0.01(0.02), f_time=1.33(1.38), b_time=1.34(1.40)  Time cost: 12:05/1:17:42 [22:27:15/7:16:47]  Acc_iter 58450       Data time: 0.01(0.02)  Forward time: 1.33(1.38)  Batch time: 1.34(1.40)
2025-09-02 20:12:56,312   INFO  Train:   16/20 ( 80%) [ 569/3862 ( 15%)]  Loss: 1.177 (0.955)  LR: 3.521e-04  Grad: 12.3994  max=0.4826(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5489(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3883, loss_cls=0.0635, loss_bbox=0.5061, matched_ious=0.5724, d_time=0.00(0.02), f_time=1.52(1.38), b_time=1.53(1.40)  Time cost: 13:15/1:16:32 [22:28:25/7:15:39]  Acc_iter 58500       Data time: 0.00(0.02)  Forward time: 1.52(1.38)  Batch time: 1.53(1.40)
2025-09-02 20:14:05,703   INFO  Train:   16/20 ( 80%) [ 619/3862 ( 16%)]  Loss: 0.9597 (0.953)  LR: 3.505e-04  Grad: 12.4386  max=0.6034(module.vfe.pfn_layers.0.linear.weight)  min: -0.5502(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3899, loss_cls=0.0655, loss_bbox=0.4748, matched_ious=0.5715, d_time=0.01(0.01), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 14:24/1:15:21 [22:29:34/7:14:19]  Acc_iter 58550       Data time: 0.01(0.01)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 20:15:16,070   INFO  Train:   16/20 ( 80%) [ 669/3862 ( 17%)]  Loss: 0.7302 (0.953)  LR: 3.488e-04  Grad: 12.4235  max=0.4873(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5535(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3855, loss_cls=0.0626, loss_bbox=0.5042, matched_ious=0.5681, d_time=0.00(0.01), f_time=1.38(1.38), b_time=1.38(1.40)  Time cost: 15:34/1:14:14 [22:30:45/7:13:27]  Acc_iter 58600       Data time: 0.00(0.01)  Forward time: 1.38(1.38)  Batch time: 1.38(1.40)
2025-09-02 20:16:24,124   INFO  Train:   16/20 ( 80%) [ 719/3862 ( 19%)]  Loss: 0.9381 (0.953)  LR: 3.472e-04  Grad: 12.4708  max=0.4806(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5534(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3834, loss_cls=0.0625, loss_bbox=0.5104, matched_ious=0.5731, d_time=0.00(0.01), f_time=1.50(1.38), b_time=1.50(1.39)  Time cost: 16:42/1:12:57 [22:31:53/7:11:33]  Acc_iter 58650       Data time: 0.00(0.01)  Forward time: 1.50(1.38)  Batch time: 1.50(1.39)
2025-09-02 20:17:34,489   INFO  Train:   16/20 ( 80%) [ 769/3862 ( 20%)]  Loss: 1.108 (0.951)  LR: 3.456e-04  Grad: 12.5482  max=0.4695(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5554(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3910, loss_cls=0.0633, loss_bbox=0.4671, matched_ious=0.5756, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.33(1.39)  Time cost: 17:53/1:11:50 [22:33:03/7:10:41]  Acc_iter 58700       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.33(1.39)
2025-09-02 20:18:43,246   INFO  Train:   16/20 ( 80%) [ 819/3862 ( 21%)]  Loss: 0.7358 (0.953)  LR: 3.440e-04  Grad: 12.5469  max=0.5812(module.vfe.pfn_layers.0.linear.weight)  min: -0.5592(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3882, loss_cls=0.0658, loss_bbox=0.5284, matched_ious=0.5695, d_time=0.01(0.01), f_time=1.40(1.38), b_time=1.41(1.39)  Time cost: 19:01/1:10:37 [22:34:12/7:09:10]  Acc_iter 58750       Data time: 0.01(0.01)  Forward time: 1.40(1.38)  Batch time: 1.41(1.39)
2025-09-02 20:19:52,384   INFO  Train:   16/20 ( 80%) [ 869/3862 ( 23%)]  Loss: 0.9804 (0.950)  LR: 3.424e-04  Grad: 12.5617  max=0.4924(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6886(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3662, loss_cls=0.0615, loss_bbox=0.4697, matched_ious=0.5738, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.30(1.39)  Time cost: 20:11/1:09:26 [22:35:21/7:07:50]  Acc_iter 58800       Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.30(1.39)
2025-09-02 20:21:01,494   INFO  Train:   16/20 ( 80%) [ 919/3862 ( 24%)]  Loss: 1.128 (0.950)  LR: 3.408e-04  Grad: 12.5930  max=0.5132(module.vfe.pfn_layers.0.linear.weight)  min: -0.5608(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3828, loss_cls=0.0641, loss_bbox=0.4997, matched_ious=0.5738, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.32(1.39)  Time cost: 21:20/1:08:15 [22:36:30/7:06:31]  Acc_iter 58850       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.32(1.39)
2025-09-02 20:22:09,478   INFO  Train:   16/20 ( 80%) [ 969/3862 ( 25%)]  Loss: 0.9165 (0.949)  LR: 3.392e-04  Grad: 12.6372  max=0.4988(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5649(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3816, loss_cls=0.0636, loss_bbox=0.4888, matched_ious=0.5731, d_time=0.02(0.01), f_time=1.32(1.38), b_time=1.34(1.39)  Time cost: 22:28/1:07:00 [22:37:38/7:04:51]  Acc_iter 58900       Data time: 0.02(0.01)  Forward time: 1.32(1.38)  Batch time: 1.34(1.39)
2025-09-02 20:23:18,676   INFO  Train:   16/20 ( 80%) [1019/3862 ( 26%)]  Loss: 1.282 (0.949)  LR: 3.376e-04  Grad: 12.6439  max=0.5068(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5639(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3946, loss_cls=0.0651, loss_bbox=0.4980, matched_ious=0.5672, d_time=0.01(0.01), f_time=1.44(1.38), b_time=1.45(1.39)  Time cost: 23:37/1:05:50 [22:38:47/7:03:36]  Acc_iter 58950       Data time: 0.01(0.01)  Forward time: 1.44(1.38)  Batch time: 1.45(1.39)
2025-09-02 20:24:28,877   INFO  Train:   16/20 ( 80%) [1069/3862 ( 28%)]  Loss: 1.457 (0.949)  LR: 3.360e-04  Grad: 12.6446  max=0.5106(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5663(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3879, loss_cls=0.0620, loss_bbox=0.5006, matched_ious=0.5660, d_time=0.01(0.01), f_time=1.47(1.38), b_time=1.48(1.39)  Time cost: 24:47/1:04:42 [22:39:57/7:02:39]  Acc_iter 59000       Data time: 0.01(0.01)  Forward time: 1.47(1.38)  Batch time: 1.48(1.39)
2025-09-02 20:25:38,417   INFO  Train:   16/20 ( 80%) [1119/3862 ( 29%)]  Loss: 1.209 (0.949)  LR: 3.344e-04  Grad: 12.6161  max=0.5268(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5676(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3776, loss_cls=0.0630, loss_bbox=0.5025, matched_ious=0.5727, d_time=0.01(0.01), f_time=1.29(1.38), b_time=1.31(1.39)  Time cost: 25:57/1:03:33 [22:41:07/7:01:30]  Acc_iter 59050       Data time: 0.01(0.01)  Forward time: 1.29(1.38)  Batch time: 1.31(1.39)
2025-09-02 20:26:48,020   INFO  Train:   16/20 ( 80%) [1169/3862 ( 30%)]  Loss: 0.9269 (0.948)  LR: 3.328e-04  Grad: 12.6784  max=0.7268(module.vfe.pfn_layers.0.linear.weight)  min: -0.5712(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3848, loss_cls=0.0623, loss_bbox=0.4798, matched_ious=0.5725, d_time=0.01(0.01), f_time=1.40(1.38), b_time=1.40(1.39)  Time cost: 27:06/1:02:24 [22:42:17/7:00:22]  Acc_iter 59100       Data time: 0.01(0.01)  Forward time: 1.40(1.38)  Batch time: 1.40(1.39)
2025-09-02 20:27:55,742   INFO  Train:   16/20 ( 80%) [1219/3862 ( 32%)]  Loss: 0.8415 (0.948)  LR: 3.312e-04  Grad: 12.6952  max=0.5177(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5711(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3925, loss_cls=0.0644, loss_bbox=0.4997, matched_ious=0.5650, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.31(1.39)  Time cost: 28:14/1:01:10 [22:43:24/6:58:46]  Acc_iter 59150       Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.39)
2025-09-02 20:29:04,291   INFO  Train:   16/20 ( 80%) [1269/3862 ( 33%)]  Loss: 1.028 (0.948)  LR: 3.296e-04  Grad: 12.7039  max=0.5266(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5759(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3813, loss_cls=0.0619, loss_bbox=0.5029, matched_ious=0.5721, d_time=0.00(0.01), f_time=1.41(1.38), b_time=1.41(1.39)  Time cost: 29:22/59:59 [22:44:33/6:57:24]  Acc_iter 59200       Data time: 0.00(0.01)  Forward time: 1.41(1.38)  Batch time: 1.41(1.39)
2025-09-02 20:30:14,870   INFO  Train:   16/20 ( 80%) [1319/3862 ( 34%)]  Loss: 0.8743 (0.948)  LR: 3.280e-04  Grad: 12.7514  max=0.5244(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5761(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3882, loss_cls=0.0632, loss_bbox=0.4908, matched_ious=0.5727, d_time=0.00(0.01), f_time=1.38(1.38), b_time=1.38(1.39)  Time cost: 30:33/58:52 [22:45:43/6:56:30]  Acc_iter 59250       Data time: 0.00(0.01)  Forward time: 1.38(1.38)  Batch time: 1.38(1.39)
2025-09-02 20:31:23,723   INFO  Train:   16/20 ( 80%) [1369/3862 ( 35%)]  Loss: 0.7041 (0.947)  LR: 3.264e-04  Grad: 12.8013  max=0.5104(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5770(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3739, loss_cls=0.0616, loss_bbox=0.4719, matched_ious=0.5751, d_time=0.00(0.01), f_time=1.26(1.38), b_time=1.27(1.39)  Time cost: 31:42/57:41 [22:46:52/6:55:13]  Acc_iter 59300       Data time: 0.00(0.01)  Forward time: 1.26(1.38)  Batch time: 1.27(1.39)
2025-09-02 20:32:33,189   INFO  Train:   16/20 ( 80%) [1419/3862 ( 37%)]  Loss: 0.8110 (0.947)  LR: 3.248e-04  Grad: 12.8093  max=0.5158(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5814(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3801, loss_cls=0.0620, loss_bbox=0.5062, matched_ious=0.5728, d_time=0.00(0.01), f_time=1.52(1.38), b_time=1.52(1.39)  Time cost: 32:51/56:32 [22:48:02/6:54:04]  Acc_iter 59350       Data time: 0.00(0.01)  Forward time: 1.52(1.38)  Batch time: 1.52(1.39)
2025-09-02 20:33:41,301   INFO  Train:   16/20 ( 80%) [1469/3862 ( 38%)]  Loss: 1.082 (0.946)  LR: 3.232e-04  Grad: 12.8790  max=0.5144(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5811(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3861, loss_cls=0.0613, loss_bbox=0.4885, matched_ious=0.5744, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.34(1.39)  Time cost: 33:59/55:20 [22:49:10/6:52:38]  Acc_iter 59400       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.34(1.39)
2025-09-02 20:34:50,868   INFO  Train:   16/20 ( 80%) [1519/3862 ( 39%)]  Loss: 0.8400 (0.947)  LR: 3.216e-04  Grad: 12.9238  max=0.5133(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5815(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3771, loss_cls=0.0630, loss_bbox=0.5346, matched_ious=0.5638, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.31(1.39)  Time cost: 35:09/54:11 [22:50:19/6:51:31]  Acc_iter 59450       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.31(1.39)
2025-09-02 20:36:01,052   INFO  Train:   16/20 ( 80%) [1569/3862 ( 41%)]  Loss: 0.8486 (0.947)  LR: 3.201e-04  Grad: 12.9873  max=0.5188(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5811(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3808, loss_cls=0.0635, loss_bbox=0.5024, matched_ious=0.5702, d_time=0.01(0.01), f_time=1.46(1.38), b_time=1.47(1.39)  Time cost: 36:19/53:03 [22:51:30/6:50:31]  Acc_iter 59500       Data time: 0.01(0.01)  Forward time: 1.46(1.38)  Batch time: 1.47(1.39)
2025-09-02 20:37:10,041   INFO  Train:   16/20 ( 80%) [1619/3862 ( 42%)]  Loss: 0.8995 (0.946)  LR: 3.185e-04  Grad: 12.9460  max=0.5471(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5846(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3730, loss_cls=0.0646, loss_bbox=0.4748, matched_ious=0.5716, d_time=0.00(0.01), f_time=1.40(1.38), b_time=1.41(1.39)  Time cost: 37:28/51:53 [22:52:39/6:49:17]  Acc_iter 59550       Data time: 0.00(0.01)  Forward time: 1.40(1.38)  Batch time: 1.41(1.39)
2025-09-02 20:38:20,839   INFO  Train:   16/20 ( 80%) [1669/3862 ( 43%)]  Loss: 0.9183 (0.946)  LR: 3.169e-04  Grad: 12.9835  max=0.5232(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5876(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3765, loss_cls=0.0622, loss_bbox=0.5113, matched_ious=0.5691, d_time=0.01(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 38:39/50:45 [22:53:49/6:48:22]  Acc_iter 59600       Data time: 0.01(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 20:39:29,217   INFO  Train:   16/20 ( 80%) [1719/3862 ( 45%)]  Loss: 1.053 (0.947)  LR: 3.153e-04  Grad: 12.9558  max=0.5403(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5890(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3916, loss_cls=0.0648, loss_bbox=0.5156, matched_ious=0.5685, d_time=0.00(0.01), f_time=1.44(1.38), b_time=1.45(1.39)  Time cost: 39:47/49:35 [22:54:58/6:47:01]  Acc_iter 59650       Data time: 0.00(0.01)  Forward time: 1.44(1.38)  Batch time: 1.45(1.39)
2025-09-02 20:40:37,542   INFO  Train:   16/20 ( 80%) [1769/3862 ( 46%)]  Loss: 0.9769 (0.947)  LR: 3.137e-04  Grad: 13.0025  max=0.5392(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5915(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3817, loss_cls=0.0618, loss_bbox=0.5034, matched_ious=0.5730, d_time=0.02(0.01), f_time=1.36(1.38), b_time=1.38(1.39)  Time cost: 40:56/48:24 [22:56:06/6:45:41]  Acc_iter 59700       Data time: 0.02(0.01)  Forward time: 1.36(1.38)  Batch time: 1.38(1.39)
2025-09-02 20:41:46,764   INFO  Train:   16/20 ( 80%) [1819/3862 ( 47%)]  Loss: 1.023 (0.946)  LR: 3.122e-04  Grad: 13.0327  max=0.5434(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5907(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3742, loss_cls=0.0606, loss_bbox=0.4918, matched_ious=0.5713, d_time=0.01(0.01), f_time=1.43(1.38), b_time=1.44(1.39)  Time cost: 42:05/47:14 [22:57:15/6:44:30]  Acc_iter 59750       Data time: 0.01(0.01)  Forward time: 1.43(1.38)  Batch time: 1.44(1.39)
2025-09-02 20:42:56,362   INFO  Train:   16/20 ( 80%) [1869/3862 ( 48%)]  Loss: 0.8312 (0.946)  LR: 3.106e-04  Grad: 13.0901  max=0.5340(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5946(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3727, loss_cls=0.0601, loss_bbox=0.5050, matched_ious=0.5667, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 43:15/46:05 [22:58:25/6:43:23]  Acc_iter 59800       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 20:44:06,464   INFO  Train:   16/20 ( 80%) [1919/3862 ( 50%)]  Loss: 1.306 (0.946)  LR: 3.090e-04  Grad: 13.0821  max=0.5211(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5963(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3808, loss_cls=0.0625, loss_bbox=0.5059, matched_ious=0.5684, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.32(1.39)  Time cost: 44:25/44:57 [22:59:35/6:42:20]  Acc_iter 59850       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.32(1.39)
2025-09-02 20:45:14,789   INFO  Train:   16/20 ( 80%) [1969/3862 ( 51%)]  Loss: 0.9211 (0.946)  LR: 3.075e-04  Grad: 13.1256  max=0.5289(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5985(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3707, loss_cls=0.0594, loss_bbox=0.4878, matched_ious=0.5771, d_time=0.00(0.01), f_time=1.22(1.38), b_time=1.22(1.39)  Time cost: 45:33/43:46 [23:00:43/6:41:01]  Acc_iter 59900       Data time: 0.00(0.01)  Forward time: 1.22(1.38)  Batch time: 1.22(1.39)
2025-09-02 20:46:23,755   INFO  Train:   16/20 ( 80%) [2019/3862 ( 52%)]  Loss: 0.9219 (0.945)  LR: 3.059e-04  Grad: 13.1709  max=0.5334(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.3988(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3842, loss_cls=0.0609, loss_bbox=0.4832, matched_ious=0.5710, d_time=0.00(0.01), f_time=1.43(1.38), b_time=1.44(1.39)  Time cost: 46:42/42:36 [23:01:52/6:39:48]  Acc_iter 59950       Data time: 0.00(0.01)  Forward time: 1.43(1.38)  Batch time: 1.44(1.39)
2025-09-02 20:47:32,490   INFO  Train:   16/20 ( 80%) [2069/3862 ( 54%)]  Loss: 0.8391 (0.945)  LR: 3.044e-04  Grad: 13.1439  max=0.5329(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5987(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3792, loss_cls=0.0625, loss_bbox=0.4896, matched_ious=0.5747, d_time=0.00(0.01), f_time=1.38(1.38), b_time=1.39(1.39)  Time cost: 47:51/41:26 [23:03:01/6:38:34]  Acc_iter 60000       Data time: 0.00(0.01)  Forward time: 1.38(1.38)  Batch time: 1.39(1.39)
2025-09-02 20:48:41,436   INFO  Train:   16/20 ( 80%) [2119/3862 ( 55%)]  Loss: 0.8321 (0.945)  LR: 3.028e-04  Grad: 13.1359  max=0.5420(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6010(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3891, loss_cls=0.0625, loss_bbox=0.4914, matched_ious=0.5764, d_time=0.00(0.01), f_time=1.40(1.38), b_time=1.41(1.39)  Time cost: 49:00/40:17 [23:04:10/6:37:21]  Acc_iter 60050       Data time: 0.00(0.01)  Forward time: 1.40(1.38)  Batch time: 1.41(1.39)
2025-09-02 20:49:53,731   INFO  Train:   16/20 ( 80%) [2169/3862 ( 56%)]  Loss: 1.105 (0.944)  LR: 3.012e-04  Grad: 13.1734  max=0.5348(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6030(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3749, loss_cls=0.0594, loss_bbox=0.4838, matched_ious=0.5727, d_time=0.01(0.01), f_time=2.19(1.38), b_time=2.21(1.39)  Time cost: 50:12/39:10 [23:05:22/6:36:35]  Acc_iter 60100       Data time: 0.01(0.01)  Forward time: 2.19(1.38)  Batch time: 2.21(1.39)
2025-09-02 20:51:01,709   INFO  Train:   16/20 ( 80%) [2219/3862 ( 57%)]  Loss: 0.8900 (0.944)  LR: 2.997e-04  Grad: 13.1978  max=0.5372(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6067(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3713, loss_cls=0.0615, loss_bbox=0.4879, matched_ious=0.5726, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.37(1.39)  Time cost: 51:20/37:59 [23:06:30/6:35:15]  Acc_iter 60150       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.37(1.39)
2025-09-02 20:52:10,350   INFO  Train:   16/20 ( 80%) [2269/3862 ( 59%)]  Loss: 1.018 (0.943)  LR: 2.981e-04  Grad: 13.1987  max=0.5488(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6071(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3662, loss_cls=0.0592, loss_bbox=0.4823, matched_ious=0.5701, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.33(1.39)  Time cost: 52:29/36:49 [23:07:39/6:34:00]  Acc_iter 60200       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.33(1.39)
2025-09-02 20:53:18,219   INFO  Train:   16/20 ( 80%) [2319/3862 ( 60%)]  Loss: 0.8347 (0.942)  LR: 2.966e-04  Grad: 13.2608  max=0.5294(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6076(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3706, loss_cls=0.0609, loss_bbox=0.4736, matched_ious=0.5722, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.37(1.39)  Time cost: 53:36/35:39 [23:08:47/6:32:39]  Acc_iter 60250       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.37(1.39)
2025-09-02 20:54:28,316   INFO  Train:   16/20 ( 80%) [2369/3862 ( 61%)]  Loss: 0.8927 (0.942)  LR: 2.950e-04  Grad: 13.3183  max=0.5166(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6064(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3860, loss_cls=0.0622, loss_bbox=0.4991, matched_ious=0.5706, d_time=0.00(0.01), f_time=1.34(1.38), b_time=1.35(1.39)  Time cost: 54:47/34:30 [23:09:57/6:31:35]  Acc_iter 60300       Data time: 0.00(0.01)  Forward time: 1.34(1.38)  Batch time: 1.35(1.39)
2025-09-02 20:55:39,724   INFO  Train:   16/20 ( 80%) [2419/3862 ( 63%)]  Loss: 0.7374 (0.942)  LR: 2.935e-04  Grad: 13.2993  max=0.5081(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6094(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3694, loss_cls=0.0587, loss_bbox=0.4882, matched_ious=0.5698, d_time=0.41(0.01), f_time=1.31(1.38), b_time=1.71(1.39)  Time cost: 55:58/33:22 [23:11:08/6:30:41]  Acc_iter 60350       Data time: 0.41(0.01)  Forward time: 1.31(1.38)  Batch time: 1.71(1.39)
2025-09-02 20:56:48,078   INFO  Train:   16/20 ( 80%) [2469/3862 ( 64%)]  Loss: 1.075 (0.941)  LR: 2.919e-04  Grad: 13.3373  max=0.5294(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6136(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3802, loss_cls=0.0619, loss_bbox=0.4889, matched_ious=0.5742, d_time=0.01(0.01), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 57:06/32:12 [23:12:17/6:29:24]  Acc_iter 60400       Data time: 0.01(0.01)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 20:57:56,536   INFO  Train:   16/20 ( 80%) [2519/3862 ( 65%)]  Loss: 0.9371 (0.941)  LR: 2.904e-04  Grad: 13.3152  max=0.5319(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6130(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3748, loss_cls=0.0612, loss_bbox=0.4978, matched_ious=0.5711, d_time=0.00(0.01), f_time=1.39(1.38), b_time=1.40(1.39)  Time cost: 58:15/31:02 [23:13:25/6:28:08]  Acc_iter 60450       Data time: 0.00(0.01)  Forward time: 1.39(1.38)  Batch time: 1.40(1.39)
2025-09-02 20:59:04,306   INFO  Train:   16/20 ( 80%) [2569/3862 ( 67%)]  Loss: 0.7355 (0.941)  LR: 2.889e-04  Grad: 13.3295  max=0.5285(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6121(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3771, loss_cls=0.0625, loss_bbox=0.4914, matched_ious=0.5711, d_time=0.00(0.01), f_time=1.28(1.38), b_time=1.29(1.39)  Time cost: 59:23/29:52 [23:14:33/6:26:49]  Acc_iter 60500       Data time: 0.00(0.01)  Forward time: 1.28(1.38)  Batch time: 1.29(1.39)
2025-09-02 21:00:13,705   INFO  Train:   16/20 ( 80%) [2619/3862 ( 68%)]  Loss: 0.8552 (0.941)  LR: 2.873e-04  Grad: 13.3811  max=0.5000(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6146(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3878, loss_cls=0.0627, loss_bbox=0.5109, matched_ious=0.5661, d_time=0.00(0.01), f_time=1.35(1.38), b_time=1.35(1.39)  Time cost: 1:00:32/28:43 [23:15:42/6:25:40]  Acc_iter 60550       Data time: 0.00(0.01)  Forward time: 1.35(1.38)  Batch time: 1.35(1.39)
2025-09-02 21:01:22,259   INFO  Train:   16/20 ( 80%) [2669/3862 ( 69%)]  Loss: 1.010 (0.941)  LR: 2.858e-04  Grad: 13.3891  max=0.5088(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6171(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3844, loss_cls=0.0637, loss_bbox=0.4841, matched_ious=0.5729, d_time=0.00(0.01), f_time=1.26(1.38), b_time=1.26(1.39)  Time cost: 1:01:40/27:33 [23:16:51/6:24:26]  Acc_iter 60600       Data time: 0.00(0.01)  Forward time: 1.26(1.38)  Batch time: 1.26(1.39)
2025-09-02 21:02:32,499   INFO  Train:   16/20 ( 80%) [2719/3862 ( 70%)]  Loss: 1.162 (0.941)  LR: 2.843e-04  Grad: 13.4245  max=0.5028(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6174(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3856, loss_cls=0.0603, loss_bbox=0.5020, matched_ious=0.5716, d_time=0.00(0.01), f_time=1.40(1.38), b_time=1.40(1.39)  Time cost: 1:02:51/26:24 [23:18:01/6:23:22]  Acc_iter 60650       Data time: 0.00(0.01)  Forward time: 1.40(1.38)  Batch time: 1.40(1.39)
2025-09-02 21:03:41,596   INFO  Train:   16/20 ( 80%) [2769/3862 ( 72%)]  Loss: 1.036 (0.942)  LR: 2.827e-04  Grad: 13.4489  max=0.4910(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6184(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3880, loss_cls=0.0618, loss_bbox=0.5109, matched_ious=0.5673, d_time=0.00(0.01), f_time=1.41(1.38), b_time=1.41(1.39)  Time cost: 1:04:00/25:15 [23:19:10/6:22:12]  Acc_iter 60700       Data time: 0.00(0.01)  Forward time: 1.41(1.38)  Batch time: 1.41(1.39)
2025-09-02 21:04:49,702   INFO  Train:   16/20 ( 80%) [2819/3862 ( 73%)]  Loss: 1.001 (0.942)  LR: 2.812e-04  Grad: 13.4635  max=0.4994(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6214(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3771, loss_cls=0.0620, loss_bbox=0.5091, matched_ious=0.5712, d_time=0.00(0.01), f_time=1.25(1.38), b_time=1.25(1.39)  Time cost: 1:05:08/24:05 [23:20:18/6:20:55]  Acc_iter 60750       Data time: 0.00(0.01)  Forward time: 1.25(1.38)  Batch time: 1.25(1.39)
2025-09-02 21:05:59,665   INFO  Train:   16/20 ( 80%) [2869/3862 ( 74%)]  Loss: 0.9511 (0.941)  LR: 2.797e-04  Grad: 13.5127  max=0.5078(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6250(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3784, loss_cls=0.0609, loss_bbox=0.4687, matched_ious=0.5739, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.37(1.39)  Time cost: 1:06:18/22:56 [23:21:28/6:19:50]  Acc_iter 60800       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.37(1.39)
2025-09-02 21:07:07,247   INFO  Train:   16/20 ( 80%) [2919/3862 ( 76%)]  Loss: 1.049 (0.941)  LR: 2.782e-04  Grad: 13.5428  max=0.8918(module.vfe.pfn_layers.0.linear.weight)  min: -0.6235(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3765, loss_cls=0.0620, loss_bbox=0.4970, matched_ious=0.5696, d_time=0.00(0.01), f_time=1.33(1.38), b_time=1.34(1.39)  Time cost: 1:07:25/21:46 [23:22:36/6:18:31]  Acc_iter 60850       Data time: 0.00(0.01)  Forward time: 1.33(1.38)  Batch time: 1.34(1.39)
2025-09-02 21:08:18,413   INFO  Train:   16/20 ( 80%) [2969/3862 ( 77%)]  Loss: 0.8995 (0.940)  LR: 2.767e-04  Grad: 13.6008  max=0.5249(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6263(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3722, loss_cls=0.0600, loss_bbox=0.4619, matched_ious=0.5755, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.32(1.39)  Time cost: 1:08:37/20:37 [23:23:47/6:17:32]  Acc_iter 60900       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.32(1.39)
2025-09-02 21:09:27,809   INFO  Train:   16/20 ( 80%) [3019/3862 ( 78%)]  Loss: 0.6824 (0.940)  LR: 2.751e-04  Grad: 13.5827  max=0.5203(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6287(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3681, loss_cls=0.0598, loss_bbox=0.4813, matched_ious=0.5751, d_time=0.01(0.01), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 1:09:46/19:28 [23:24:56/6:16:23]  Acc_iter 60950       Data time: 0.01(0.01)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 21:10:36,384   INFO  Train:   16/20 ( 80%) [3069/3862 ( 79%)]  Loss: 1.011 (0.940)  LR: 2.736e-04  Grad: 13.6021  max=0.5310(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6272(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3784, loss_cls=0.0621, loss_bbox=0.5024, matched_ious=0.5729, d_time=0.00(0.01), f_time=1.38(1.38), b_time=1.38(1.39)  Time cost: 1:10:55/18:19 [23:26:05/6:15:10]  Acc_iter 61000       Data time: 0.00(0.01)  Forward time: 1.38(1.38)  Batch time: 1.38(1.39)
2025-09-02 21:11:43,860   INFO  Train:   16/20 ( 80%) [3119/3862 ( 81%)]  Loss: 0.9346 (0.939)  LR: 2.721e-04  Grad: 13.6028  max=0.6829(module.vfe.pfn_layers.0.linear.weight)  min: -0.8429(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3723, loss_cls=0.0613, loss_bbox=0.4648, matched_ious=0.5736, d_time=0.00(0.01), f_time=1.31(1.38), b_time=1.31(1.39)  Time cost: 1:12:02/17:09 [23:27:12/6:13:51]  Acc_iter 61050       Data time: 0.00(0.01)  Forward time: 1.31(1.38)  Batch time: 1.31(1.39)
2025-09-02 21:12:52,996   INFO  Train:   16/20 ( 80%) [3169/3862 ( 82%)]  Loss: 0.7450 (0.939)  LR: 2.706e-04  Grad: 13.6126  max=0.5144(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6295(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3805, loss_cls=0.0624, loss_bbox=0.5125, matched_ious=0.5706, d_time=0.02(0.01), f_time=1.26(1.38), b_time=1.28(1.39)  Time cost: 1:13:11/16:00 [23:28:22/6:12:41]  Acc_iter 61100       Data time: 0.02(0.01)  Forward time: 1.26(1.38)  Batch time: 1.28(1.39)
2025-09-02 21:14:03,500   INFO  Train:   16/20 ( 80%) [3219/3862 ( 83%)]  Loss: 0.9578 (0.939)  LR: 2.691e-04  Grad: 13.6708  max=0.5201(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6316(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3724, loss_cls=0.0604, loss_bbox=0.4732, matched_ious=0.5743, d_time=0.00(0.01), f_time=1.36(1.38), b_time=1.36(1.39)  Time cost: 1:14:22/14:51 [23:29:32/6:11:38]  Acc_iter 61150       Data time: 0.00(0.01)  Forward time: 1.36(1.38)  Batch time: 1.36(1.39)
2025-09-02 21:15:12,636   INFO  Train:   16/20 ( 80%) [3269/3862 ( 85%)]  Loss: 0.9620 (0.938)  LR: 2.676e-04  Grad: 13.7101  max=0.5154(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6329(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3586, loss_cls=0.0583, loss_bbox=0.4555, matched_ious=0.5756, d_time=0.00(0.01), f_time=1.39(1.38), b_time=1.39(1.39)  Time cost: 1:15:31/13:41 [23:30:41/6:10:28]  Acc_iter 61200       Data time: 0.00(0.01)  Forward time: 1.39(1.38)  Batch time: 1.39(1.39)
2025-09-02 21:16:20,845   INFO  Train:   16/20 ( 80%) [3319/3862 ( 86%)]  Loss: 0.9621 (0.937)  LR: 2.661e-04  Grad: 13.7267  max=0.5167(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6360(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3753, loss_cls=0.0600, loss_bbox=0.4675, matched_ious=0.5773, d_time=0.00(0.01), f_time=1.35(1.38), b_time=1.36(1.39)  Time cost: 1:16:39/12:32 [23:31:49/6:09:14]  Acc_iter 61250       Data time: 0.00(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.39)
2025-09-02 21:17:29,327   INFO  Train:   16/20 ( 80%) [3369/3862 ( 87%)]  Loss: 1.115 (0.937)  LR: 2.646e-04  Grad: 13.7506  max=0.5077(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6403(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3729, loss_cls=0.0613, loss_bbox=0.4702, matched_ious=0.5766, d_time=0.00(0.01), f_time=1.40(1.38), b_time=1.40(1.39)  Time cost: 1:17:48/11:22 [23:32:58/6:08:01]  Acc_iter 61300       Data time: 0.00(0.01)  Forward time: 1.40(1.38)  Batch time: 1.40(1.39)
2025-09-02 21:18:39,862   INFO  Train:   16/20 ( 80%) [3419/3862 ( 89%)]  Loss: 0.7829 (0.936)  LR: 2.631e-04  Grad: 13.7544  max=0.5030(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6416(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3724, loss_cls=0.0598, loss_bbox=0.4756, matched_ious=0.5721, d_time=0.00(0.01), f_time=1.37(1.38), b_time=1.38(1.39)  Time cost: 1:18:58/10:13 [23:34:08/6:06:57]  Acc_iter 61350       Data time: 0.00(0.01)  Forward time: 1.37(1.38)  Batch time: 1.38(1.39)
2025-09-02 21:19:50,256   INFO  Train:   16/20 ( 80%) [3469/3862 ( 90%)]  Loss: 1.038 (0.936)  LR: 2.616e-04  Grad: 13.7564  max=0.5192(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6440(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3678, loss_cls=0.0598, loss_bbox=0.4793, matched_ious=0.5732, d_time=0.00(0.01), f_time=1.41(1.38), b_time=1.41(1.39)  Time cost: 1:20:08/09:04 [23:35:19/6:05:53]  Acc_iter 61400       Data time: 0.00(0.01)  Forward time: 1.41(1.38)  Batch time: 1.41(1.39)
2025-09-02 21:20:59,787   INFO  Train:   16/20 ( 80%) [3519/3862 ( 91%)]  Loss: 0.8555 (0.936)  LR: 2.601e-04  Grad: 13.7685  max=0.5133(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6465(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3730, loss_cls=0.0618, loss_bbox=0.4685, matched_ious=0.5760, d_time=0.01(0.01), f_time=1.32(1.38), b_time=1.33(1.39)  Time cost: 1:21:18/07:55 [23:36:28/6:04:45]  Acc_iter 61450       Data time: 0.01(0.01)  Forward time: 1.32(1.38)  Batch time: 1.33(1.39)
2025-09-02 21:22:07,610   INFO  Train:   16/20 ( 80%) [3569/3862 ( 92%)]  Loss: 1.021 (0.935)  LR: 2.587e-04  Grad: 13.8322  max=0.5122(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6510(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3734, loss_cls=0.0618, loss_bbox=0.4748, matched_ious=0.5754, d_time=0.00(0.01), f_time=1.38(1.38), b_time=1.38(1.39)  Time cost: 1:22:26/06:45 [23:37:36/6:03:29]  Acc_iter 61500       Data time: 0.00(0.01)  Forward time: 1.38(1.38)  Batch time: 1.38(1.39)
2025-09-02 21:23:16,585   INFO  Train:   16/20 ( 80%) [3619/3862 ( 94%)]  Loss: 0.7311 (0.935)  LR: 2.572e-04  Grad: 13.8603  max=0.5270(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6556(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3571, loss_cls=0.0597, loss_bbox=0.4701, matched_ious=0.5737, d_time=0.00(0.01), f_time=1.32(1.38), b_time=1.32(1.39)  Time cost: 1:23:35/05:36 [23:38:45/6:02:18]  Acc_iter 61550       Data time: 0.00(0.01)  Forward time: 1.32(1.38)  Batch time: 1.32(1.39)
2025-09-02 21:24:26,431   INFO  Train:   16/20 ( 80%) [3669/3862 ( 95%)]  Loss: 0.7098 (0.935)  LR: 2.557e-04  Grad: 13.8862  max=0.6468(module.vfe.pfn_layers.0.linear.weight)  min: -0.6560(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3862, loss_cls=0.0607, loss_bbox=0.5081, matched_ious=0.5762, d_time=0.00(0.01), f_time=1.30(1.38), b_time=1.31(1.39)  Time cost: 1:24:45/04:27 [23:39:55/6:01:12]  Acc_iter 61600       Data time: 0.00(0.01)  Forward time: 1.30(1.38)  Batch time: 1.31(1.39)
2025-09-02 21:25:34,368   INFO  Train:   16/20 ( 80%) [3719/3862 ( 96%)]  Loss: 1.169 (0.934)  LR: 2.542e-04  Grad: 13.8907  max=0.5204(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6574(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3645, loss_cls=0.0584, loss_bbox=0.4784, matched_ious=0.5755, d_time=0.00(0.01), f_time=1.35(1.38), b_time=1.36(1.39)  Time cost: 1:25:53/03:18 [23:41:03/5:59:57]  Acc_iter 61650       Data time: 0.00(0.01)  Forward time: 1.35(1.38)  Batch time: 1.36(1.39)
2025-09-02 21:26:46,120   INFO  Train:   16/20 ( 80%) [3769/3862 ( 98%)]  Loss: 0.6825 (0.934)  LR: 2.527e-04  Grad: 13.9071  max=0.5475(module.vfe.pfn_layers.0.linear.weight)  min: -0.6587(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3663, loss_cls=0.0599, loss_bbox=0.4660, matched_ious=0.5764, d_time=0.00(0.01), f_time=1.43(1.38), b_time=1.43(1.39)  Time cost: 1:27:04/02:08 [23:42:15/5:58:58]  Acc_iter 61700       Data time: 0.00(0.01)  Forward time: 1.43(1.38)  Batch time: 1.43(1.39)
2025-09-02 21:27:53,737   INFO  Train:   16/20 ( 80%) [3819/3862 ( 99%)]  Loss: 0.9062 (0.934)  LR: 2.513e-04  Grad: 13.9260  max=0.5270(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6600(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3739, loss_cls=0.0622, loss_bbox=0.4843, matched_ious=0.5783, d_time=0.00(0.01), f_time=1.41(1.38), b_time=1.41(1.39)  Time cost: 1:28:12/00:59 [23:43:22/5:57:42]  Acc_iter 61750       Data time: 0.00(0.01)  Forward time: 1.41(1.38)  Batch time: 1.41(1.39)
2025-09-02 21:28:50,255   INFO  Train:   16/20 ( 80%) [3861/3862 (100%)]  Loss: 0.9048 (0.934)  LR: 2.500e-04  Grad: 13.9656  max=0.5247(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6589(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3787, loss_cls=0.0619, loss_bbox=0.4899, matched_ious=0.5746, d_time=0.00(0.01), f_time=1.22(1.38), b_time=1.23(1.39)  Time cost: 1:29:08/00:01 [23:44:19/5:56:37]  Acc_iter 61792       Data time: 0.00(0.01)  Forward time: 1.22(1.38)  Batch time: 1.23(1.39)

                                               [Aepochs:  80%|████████  | 16/20 [23:44:20<5:56:26, 5346.60s/it]epochs:  80%|████████  | 16/20 [23:44:20<5:56:26, 5346.60s/it]epochs:  80%|████████  | 16/20 [23:44:20<5:56:26, 5346.60s/it]epochs:  80%|████████  | 16/20 [23:44:20<5:56:26, 5346.63s/it]epochs:  80%|████████  | 16/20 [23:44:20<5:56:26, 5346.60s/it]epochs:  80%|████████  | 16/20 [23:44:20<5:56:26, 5346.60s/it]epochs:  80%|████████  | 16/20 [23:44:20<5:56:26, 5346.63s/it]epochs:  80%|████████  | 16/20 [23:44:20<5:56:26, 5346.59s/it]2025-09-02 21:28:51,436   INFO  Disable augmentations: ['gt_sampling']

train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-02 21:28:57,222   INFO  Train:   17/20 ( 85%) [   0/3862 (  0%)]  Loss: 1.209 (1.21)  LR: 2.500e-04  Grad: 14.0939  max=0.5279(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6588(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6505, loss_cls=0.1488, loss_bbox=0.4101, matched_ious=0.6324, d_time=2.18(2.18), f_time=1.95(1.95), b_time=4.13(4.13)  Time cost: 00:04/4:26:28 [23:44:26/17:45:54]  Acc_iter 61793       Data time: 2.18(2.18)  Forward time: 1.95(1.95)  Batch time: 4.13(4.13)
2025-09-02 21:29:06,724   INFO  Train:   17/20 ( 85%) [   7/3862 (  0%)]  Loss: 0.9698 (1.06)  LR: 2.498e-04  Grad: 14.0057  max=0.5226(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6577(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4339, loss_cls=0.0918, loss_bbox=0.5073, matched_ious=0.5588, d_time=0.00(0.30), f_time=1.29(1.43), b_time=1.29(1.73)  Time cost: 00:13/1:50:02 [23:44:35/7:20:46]  Acc_iter 61800       Data time: 0.00(0.30)  Forward time: 1.29(1.43)  Batch time: 1.29(1.73)
2025-09-02 21:30:11,005   INFO  Train:   17/20 ( 85%) [  57/3862 (  1%)]  Loss: 1.014 (0.911)  LR: 2.483e-04  Grad: 14.1014  max=0.7583(module.vfe.pfn_layers.0.linear.weight)  min: -0.9607(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3972, loss_cls=0.0712, loss_bbox=0.4195, matched_ious=0.5747, d_time=0.00(0.05), f_time=1.34(1.30), b_time=1.34(1.35)  Time cost: 01:17/1:25:16 [23:45:40/5:44:54]  Acc_iter 61850       Data time: 0.00(0.05)  Forward time: 1.34(1.30)  Batch time: 1.34(1.35)
2025-09-02 21:31:14,872   INFO  Train:   17/20 ( 85%) [ 107/3862 (  3%)]  Loss: 0.9254 (0.873)  LR: 2.469e-04  Grad: 14.0585  max=0.4909(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6635(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3720, loss_cls=0.0662, loss_bbox=0.3904, matched_ious=0.5854, d_time=0.01(0.03), f_time=1.30(1.29), b_time=1.30(1.31)  Time cost: 02:21/1:22:12 [23:46:43/5:35:49]  Acc_iter 61900       Data time: 0.01(0.03)  Forward time: 1.30(1.29)  Batch time: 1.30(1.31)
2025-09-02 21:32:19,340   INFO  Train:   17/20 ( 85%) [ 157/3862 (  4%)]  Loss: 0.7694 (0.862)  LR: 2.454e-04  Grad: 14.0370  max=0.5026(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6586(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3860, loss_cls=0.0672, loss_bbox=0.3844, matched_ious=0.5859, d_time=0.00(0.02), f_time=1.22(1.29), b_time=1.22(1.31)  Time cost: 03:26/1:20:38 [23:47:48/5:32:47]  Acc_iter 61950       Data time: 0.00(0.02)  Forward time: 1.22(1.29)  Batch time: 1.22(1.31)
2025-09-02 21:33:24,130   INFO  Train:   17/20 ( 85%) [ 207/3862 (  5%)]  Loss: 0.8963 (0.876)  LR: 2.440e-04  Grad: 14.0916  max=0.4958(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6665(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3961, loss_cls=0.0711, loss_bbox=0.4541, matched_ious=0.5810, d_time=0.03(0.02), f_time=1.47(1.29), b_time=1.49(1.30)  Time cost: 04:31/1:19:23 [23:48:53/5:31:05]  Acc_iter 62000       Data time: 0.03(0.02)  Forward time: 1.47(1.29)  Batch time: 1.49(1.30)
2025-09-02 21:34:28,722   INFO  Train:   17/20 ( 85%) [ 257/3862 (  7%)]  Loss: 0.6764 (0.867)  LR: 2.425e-04  Grad: 14.1412  max=0.4909(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6701(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3634, loss_cls=0.0666, loss_bbox=0.3982, matched_ious=0.5787, d_time=0.01(0.02), f_time=1.21(1.29), b_time=1.22(1.30)  Time cost: 05:35/1:18:10 [23:49:57/5:29:26]  Acc_iter 62050       Data time: 0.01(0.02)  Forward time: 1.21(1.29)  Batch time: 1.22(1.30)
2025-09-02 21:35:34,244   INFO  Train:   17/20 ( 85%) [ 307/3862 (  8%)]  Loss: 0.9164 (0.867)  LR: 2.411e-04  Grad: 14.7094  max=1.6250(module.vfe.pfn_layers.0.linear.weight)  min: -3.3009(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3786, loss_cls=0.0662, loss_bbox=0.4239, matched_ious=0.5831, d_time=0.00(0.01), f_time=1.40(1.29), b_time=1.41(1.30)  Time cost: 06:41/1:17:11 [23:51:03/5:28:43]  Acc_iter 62100       Data time: 0.00(0.01)  Forward time: 1.40(1.29)  Batch time: 1.41(1.30)
2025-09-02 21:36:38,605   INFO  Train:   17/20 ( 85%) [ 357/3862 (  9%)]  Loss: 0.8273 (0.863)  LR: 2.396e-04  Grad: 14.2296  max=0.4924(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6772(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3737, loss_cls=0.0656, loss_bbox=0.3971, matched_ious=0.5894, d_time=0.00(0.01), f_time=1.43(1.29), b_time=1.44(1.30)  Time cost: 07:45/1:15:58 [23:52:07/5:27:06]  Acc_iter 62150       Data time: 0.00(0.01)  Forward time: 1.43(1.29)  Batch time: 1.44(1.30)
2025-09-02 21:37:42,395   INFO  Train:   17/20 ( 85%) [ 407/3862 ( 11%)]  Loss: 0.9609 (0.855)  LR: 2.382e-04  Grad: 14.2716  max=0.7677(module.vfe.pfn_layers.0.linear.weight)  min: -0.8570(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3581, loss_cls=0.0617, loss_bbox=0.3791, matched_ious=0.5825, d_time=0.01(0.01), f_time=1.27(1.29), b_time=1.27(1.30)  Time cost: 08:49/1:14:42 [23:53:11/5:25:15]  Acc_iter 62200       Data time: 0.01(0.01)  Forward time: 1.27(1.29)  Batch time: 1.27(1.30)
2025-09-02 21:38:47,412   INFO  Train:   17/20 ( 85%) [ 457/3862 ( 12%)]  Loss: 1.114 (0.858)  LR: 2.367e-04  Grad: 14.2426  max=0.5292(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6853(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3854, loss_cls=0.0693, loss_bbox=0.4299, matched_ious=0.5777, d_time=0.00(0.01), f_time=1.31(1.29), b_time=1.31(1.30)  Time cost: 09:54/1:13:38 [23:54:16/5:24:15]  Acc_iter 62250       Data time: 0.00(0.01)  Forward time: 1.31(1.29)  Batch time: 1.31(1.30)
2025-09-02 21:39:52,940   INFO  Train:   17/20 ( 85%) [ 507/3862 ( 13%)]  Loss: 0.9193 (0.856)  LR: 2.353e-04  Grad: 14.2531  max=0.5262(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6892(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3653, loss_cls=0.0608, loss_bbox=0.4084, matched_ious=0.5852, d_time=0.00(0.01), f_time=1.25(1.29), b_time=1.25(1.30)  Time cost: 10:59/1:12:38 [23:55:22/5:23:29]  Acc_iter 62300       Data time: 0.00(0.01)  Forward time: 1.25(1.29)  Batch time: 1.25(1.30)
2025-09-02 21:40:58,091   INFO  Train:   17/20 ( 85%) [ 557/3862 ( 14%)]  Loss: 0.7428 (0.856)  LR: 2.338e-04  Grad: 14.2774  max=0.5158(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6949(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3759, loss_cls=0.0678, loss_bbox=0.4088, matched_ious=0.5785, d_time=0.00(0.01), f_time=1.14(1.29), b_time=1.15(1.30)  Time cost: 12:05/1:11:34 [23:56:27/5:22:29]  Acc_iter 62350       Data time: 0.00(0.01)  Forward time: 1.14(1.29)  Batch time: 1.15(1.30)
2025-09-02 21:42:03,284   INFO  Train:   17/20 ( 85%) [ 607/3862 ( 16%)]  Loss: 0.9755 (0.859)  LR: 2.324e-04  Grad: 14.2999  max=0.5147(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6965(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3861, loss_cls=0.0665, loss_bbox=0.4387, matched_ious=0.5779, d_time=0.00(0.01), f_time=1.30(1.29), b_time=1.30(1.30)  Time cost: 13:10/1:10:30 [23:57:32/5:21:29]  Acc_iter 62400       Data time: 0.00(0.01)  Forward time: 1.30(1.29)  Batch time: 1.30(1.30)
2025-09-02 21:43:07,765   INFO  Train:   17/20 ( 85%) [ 657/3862 ( 17%)]  Loss: 0.7056 (0.860)  LR: 2.310e-04  Grad: 14.3583  max=0.5065(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6968(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3773, loss_cls=0.0663, loss_bbox=0.4388, matched_ious=0.5730, d_time=0.01(0.01), f_time=1.28(1.29), b_time=1.30(1.30)  Time cost: 14:14/1:09:23 [23:58:36/5:20:13]  Acc_iter 62450       Data time: 0.01(0.01)  Forward time: 1.28(1.29)  Batch time: 1.30(1.30)
2025-09-02 21:44:12,757   INFO  Train:   17/20 ( 85%) [ 707/3862 ( 18%)]  Loss: 0.7121 (0.859)  LR: 2.295e-04  Grad: 14.3790  max=0.5135(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6994(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3677, loss_cls=0.0678, loss_bbox=0.4043, matched_ious=0.5815, d_time=0.02(0.01), f_time=1.52(1.29), b_time=1.54(1.30)  Time cost: 15:19/1:08:18 [23:59:41/5:19:09]  Acc_iter 62500       Data time: 0.02(0.01)  Forward time: 1.52(1.29)  Batch time: 1.54(1.30)
2025-09-02 21:45:18,528   INFO  Train:   17/20 ( 85%) [ 757/3862 ( 20%)]  Loss: 0.8537 (0.859)  LR: 2.281e-04  Grad: 14.4102  max=0.5213(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6989(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3757, loss_cls=0.0654, loss_bbox=0.4155, matched_ious=0.5827, d_time=0.00(0.01), f_time=1.33(1.29), b_time=1.34(1.30)  Time cost: 16:25/1:07:16 [24:00:47/5:18:20]  Acc_iter 62550       Data time: 0.00(0.01)  Forward time: 1.33(1.29)  Batch time: 1.34(1.30)
2025-09-02 21:46:22,296   INFO  Train:   17/20 ( 85%) [ 807/3862 ( 21%)]  Loss: 0.8825 (0.859)  LR: 2.267e-04  Grad: 14.4192  max=0.5155(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7017(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3716, loss_cls=0.0656, loss_bbox=0.4203, matched_ious=0.5783, d_time=0.00(0.01), f_time=1.19(1.29), b_time=1.20(1.30)  Time cost: 17:29/1:06:07 [24:01:51/5:16:52]  Acc_iter 62600       Data time: 0.00(0.01)  Forward time: 1.19(1.29)  Batch time: 1.20(1.30)
2025-09-02 21:47:27,062   INFO  Train:   17/20 ( 85%) [ 857/3862 ( 22%)]  Loss: 0.5996 (0.859)  LR: 2.253e-04  Grad: 14.4542  max=0.5193(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7047(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3805, loss_cls=0.0671, loss_bbox=0.4091, matched_ious=0.5826, d_time=0.00(0.01), f_time=1.23(1.29), b_time=1.24(1.30)  Time cost: 18:34/1:05:01 [24:02:56/5:15:45]  Acc_iter 62650       Data time: 0.00(0.01)  Forward time: 1.23(1.29)  Batch time: 1.24(1.30)
2025-09-02 21:48:31,925   INFO  Train:   17/20 ( 85%) [ 907/3862 ( 23%)]  Loss: 0.9585 (0.858)  LR: 2.239e-04  Grad: 14.4850  max=0.5134(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7058(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3748, loss_cls=0.0676, loss_bbox=0.4006, matched_ious=0.5821, d_time=0.01(0.01), f_time=1.43(1.29), b_time=1.44(1.30)  Time cost: 19:38/1:03:56 [24:04:00/5:14:39]  Acc_iter 62700       Data time: 0.01(0.01)  Forward time: 1.43(1.29)  Batch time: 1.44(1.30)
2025-09-02 21:49:36,680   INFO  Train:   17/20 ( 85%) [ 957/3862 ( 25%)]  Loss: 0.9701 (0.856)  LR: 2.225e-04  Grad: 14.5209  max=0.5286(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7056(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3817, loss_cls=0.0643, loss_bbox=0.3803, matched_ious=0.5915, d_time=0.00(0.01), f_time=1.36(1.29), b_time=1.36(1.30)  Time cost: 20:43/1:02:51 [24:05:05/5:13:31]  Acc_iter 62750       Data time: 0.00(0.01)  Forward time: 1.36(1.29)  Batch time: 1.36(1.30)
2025-09-02 21:50:41,423   INFO  Train:   17/20 ( 85%) [1007/3862 ( 26%)]  Loss: 0.6108 (0.856)  LR: 2.211e-04  Grad: 14.5477  max=0.5227(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7103(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3806, loss_cls=0.0667, loss_bbox=0.4070, matched_ious=0.5828, d_time=0.00(0.01), f_time=1.41(1.29), b_time=1.42(1.30)  Time cost: 21:48/1:01:45 [24:06:10/5:12:24]  Acc_iter 62800       Data time: 0.00(0.01)  Forward time: 1.41(1.29)  Batch time: 1.42(1.30)
2025-09-02 21:51:46,791   INFO  Train:   17/20 ( 85%) [1057/3862 ( 27%)]  Loss: 0.7663 (0.855)  LR: 2.196e-04  Grad: 14.5392  max=0.5388(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7103(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3768, loss_cls=0.0656, loss_bbox=0.4016, matched_ious=0.5942, d_time=0.00(0.01), f_time=1.27(1.29), b_time=1.28(1.30)  Time cost: 22:53/1:00:42 [24:07:15/5:11:26]  Acc_iter 62850       Data time: 0.00(0.01)  Forward time: 1.27(1.29)  Batch time: 1.28(1.30)
2025-09-02 21:52:51,416   INFO  Train:   17/20 ( 85%) [1107/3862 ( 29%)]  Loss: 0.8727 (0.855)  LR: 2.182e-04  Grad: 14.5457  max=0.5282(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7114(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3751, loss_cls=0.0642, loss_bbox=0.4159, matched_ious=0.5800, d_time=0.01(0.01), f_time=1.30(1.29), b_time=1.31(1.30)  Time cost: 23:58/59:36 [24:08:20/5:10:17]  Acc_iter 62900       Data time: 0.01(0.01)  Forward time: 1.30(1.29)  Batch time: 1.31(1.30)
2025-09-02 21:53:56,838   INFO  Train:   17/20 ( 85%) [1157/3862 ( 30%)]  Loss: 0.7416 (0.852)  LR: 2.168e-04  Grad: 14.5960  max=0.5222(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7225(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3481, loss_cls=0.0621, loss_bbox=0.3709, matched_ious=0.5839, d_time=0.00(0.01), f_time=1.30(1.29), b_time=1.30(1.30)  Time cost: 25:03/58:32 [24:09:25/5:09:18]  Acc_iter 62950       Data time: 0.00(0.01)  Forward time: 1.30(1.29)  Batch time: 1.30(1.30)
2025-09-02 21:55:02,258   INFO  Train:   17/20 ( 85%) [1207/3862 ( 31%)]  Loss: 0.8936 (0.852)  LR: 2.155e-04  Grad: 14.6174  max=0.5214(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7240(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3715, loss_cls=0.0657, loss_bbox=0.4077, matched_ious=0.5795, d_time=0.00(0.01), f_time=1.45(1.29), b_time=1.46(1.30)  Time cost: 26:09/57:28 [24:10:31/5:08:19]  Acc_iter 63000       Data time: 0.00(0.01)  Forward time: 1.45(1.29)  Batch time: 1.46(1.30)
2025-09-02 21:56:07,184   INFO  Train:   17/20 ( 85%) [1257/3862 ( 33%)]  Loss: 0.8560 (0.851)  LR: 2.141e-04  Grad: 14.6406  max=0.5345(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7254(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3582, loss_cls=0.0637, loss_bbox=0.4053, matched_ious=0.5856, d_time=0.00(0.01), f_time=1.27(1.29), b_time=1.27(1.30)  Time cost: 27:14/56:23 [24:11:36/5:07:14]  Acc_iter 63050       Data time: 0.00(0.01)  Forward time: 1.27(1.29)  Batch time: 1.27(1.30)
2025-09-02 21:57:11,576   INFO  Train:   17/20 ( 85%) [1307/3862 ( 34%)]  Loss: 1.050 (0.849)  LR: 2.127e-04  Grad: 14.6623  max=0.5538(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7279(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3448, loss_cls=0.0631, loss_bbox=0.3985, matched_ious=0.5928, d_time=0.00(0.01), f_time=1.28(1.29), b_time=1.29(1.30)  Time cost: 28:18/55:17 [24:12:40/5:06:03]  Acc_iter 63100       Data time: 0.00(0.01)  Forward time: 1.28(1.29)  Batch time: 1.29(1.30)
2025-09-02 21:58:15,201   INFO  Train:   17/20 ( 85%) [1357/3862 ( 35%)]  Loss: 0.9746 (0.851)  LR: 2.113e-04  Grad: 14.6870  max=0.5489(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7300(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3993, loss_cls=0.0681, loss_bbox=0.4408, matched_ious=0.5861, d_time=0.00(0.01), f_time=1.26(1.29), b_time=1.26(1.30)  Time cost: 29:22/54:10 [24:13:44/5:04:44]  Acc_iter 63150       Data time: 0.00(0.01)  Forward time: 1.26(1.29)  Batch time: 1.26(1.30)
2025-09-02 21:59:21,053   INFO  Train:   17/20 ( 85%) [1407/3862 ( 36%)]  Loss: 0.7507 (0.850)  LR: 2.099e-04  Grad: 14.7217  max=0.6461(module.vfe.pfn_layers.0.linear.weight)  min: -0.7366(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3546, loss_cls=0.0627, loss_bbox=0.3945, matched_ious=0.5787, d_time=0.02(0.01), f_time=1.25(1.29), b_time=1.27(1.30)  Time cost: 30:28/53:07 [24:14:50/5:03:49]  Acc_iter 63200       Data time: 0.02(0.01)  Forward time: 1.25(1.29)  Batch time: 1.27(1.30)
2025-09-02 22:00:25,774   INFO  Train:   17/20 ( 85%) [1457/3862 ( 38%)]  Loss: 0.8979 (0.849)  LR: 2.085e-04  Grad: 14.7627  max=0.5406(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7386(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3723, loss_cls=0.0664, loss_bbox=0.3883, matched_ious=0.5941, d_time=0.00(0.01), f_time=1.24(1.29), b_time=1.24(1.30)  Time cost: 31:32/52:02 [24:15:54/5:02:42]  Acc_iter 63250       Data time: 0.00(0.01)  Forward time: 1.24(1.29)  Batch time: 1.24(1.30)
2025-09-02 22:01:30,956   INFO  Train:   17/20 ( 85%) [1507/3862 ( 39%)]  Loss: 0.4614 (0.849)  LR: 2.072e-04  Grad: 14.7764  max=0.5413(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7382(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3780, loss_cls=0.0644, loss_bbox=0.4120, matched_ious=0.5886, d_time=0.00(0.01), f_time=1.46(1.29), b_time=1.47(1.30)  Time cost: 32:37/50:57 [24:17:00/5:01:40]  Acc_iter 63300       Data time: 0.00(0.01)  Forward time: 1.46(1.29)  Batch time: 1.47(1.30)
2025-09-02 22:02:35,293   INFO  Train:   17/20 ( 85%) [1557/3862 ( 40%)]  Loss: 0.9825 (0.849)  LR: 2.058e-04  Grad: 14.7958  max=0.5417(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7352(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3627, loss_cls=0.0646, loss_bbox=0.4139, matched_ious=0.5847, d_time=0.00(0.01), f_time=1.26(1.29), b_time=1.26(1.30)  Time cost: 33:42/49:51 [24:18:04/5:00:30]  Acc_iter 63350       Data time: 0.00(0.01)  Forward time: 1.26(1.29)  Batch time: 1.26(1.30)
2025-09-02 22:03:40,854   INFO  Train:   17/20 ( 85%) [1607/3862 ( 42%)]  Loss: 0.8183 (0.849)  LR: 2.044e-04  Grad: 14.8248  max=0.5405(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7334(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3601, loss_cls=0.0684, loss_bbox=0.4146, matched_ious=0.5790, d_time=0.01(0.01), f_time=1.32(1.29), b_time=1.33(1.30)  Time cost: 34:47/48:47 [24:19:09/4:59:31]  Acc_iter 63400       Data time: 0.01(0.01)  Forward time: 1.32(1.29)  Batch time: 1.33(1.30)
2025-09-02 22:04:44,933   INFO  Train:   17/20 ( 85%) [1657/3862 ( 43%)]  Loss: 0.7969 (0.849)  LR: 2.030e-04  Grad: 14.8195  max=0.5585(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7347(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3775, loss_cls=0.0668, loss_bbox=0.3921, matched_ious=0.5788, d_time=0.01(0.01), f_time=1.38(1.29), b_time=1.39(1.30)  Time cost: 35:51/47:41 [24:20:13/4:58:19]  Acc_iter 63450       Data time: 0.01(0.01)  Forward time: 1.38(1.29)  Batch time: 1.39(1.30)
2025-09-02 22:05:49,642   INFO  Train:   17/20 ( 85%) [1707/3862 ( 44%)]  Loss: 0.8549 (0.848)  LR: 2.017e-04  Grad: 14.8673  max=0.5572(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7348(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3595, loss_cls=0.0645, loss_bbox=0.3960, matched_ious=0.5848, d_time=0.01(0.01), f_time=1.36(1.29), b_time=1.37(1.30)  Time cost: 36:56/46:36 [24:21:18/4:57:12]  Acc_iter 63500       Data time: 0.01(0.01)  Forward time: 1.36(1.29)  Batch time: 1.37(1.30)
2025-09-02 22:06:55,044   INFO  Train:   17/20 ( 85%) [1757/3862 ( 45%)]  Loss: 0.7981 (0.846)  LR: 2.003e-04  Grad: 14.8698  max=0.5567(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7414(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3622, loss_cls=0.0602, loss_bbox=0.3769, matched_ious=0.5914, d_time=0.00(0.01), f_time=1.34(1.29), b_time=1.35(1.30)  Time cost: 38:02/45:32 [24:22:24/4:56:12]  Acc_iter 63550       Data time: 0.00(0.01)  Forward time: 1.34(1.29)  Batch time: 1.35(1.30)
2025-09-02 22:08:00,229   INFO  Train:   17/20 ( 85%) [1807/3862 ( 47%)]  Loss: 0.7884 (0.845)  LR: 1.990e-04  Grad: 14.9141  max=0.5465(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7588(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3360, loss_cls=0.0595, loss_bbox=0.3892, matched_ious=0.5950, d_time=0.01(0.01), f_time=1.31(1.29), b_time=1.32(1.30)  Time cost: 39:07/44:27 [24:23:29/4:55:09]  Acc_iter 63600       Data time: 0.01(0.01)  Forward time: 1.31(1.29)  Batch time: 1.32(1.30)
2025-09-02 22:09:03,965   INFO  Train:   17/20 ( 85%) [1857/3862 ( 48%)]  Loss: 0.7054 (0.844)  LR: 1.976e-04  Grad: 14.8719  max=0.5628(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7402(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3901, loss_cls=0.0665, loss_bbox=0.3773, matched_ious=0.5976, d_time=0.00(0.01), f_time=1.32(1.29), b_time=1.32(1.30)  Time cost: 40:10/43:21 [24:24:33/4:53:55]  Acc_iter 63650       Data time: 0.00(0.01)  Forward time: 1.32(1.29)  Batch time: 1.32(1.30)
2025-09-02 22:10:08,605   INFO  Train:   17/20 ( 85%) [1907/3862 ( 49%)]  Loss: 0.9444 (0.844)  LR: 1.963e-04  Grad: 14.9213  max=0.5541(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7438(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3657, loss_cls=0.0662, loss_bbox=0.3926, matched_ious=0.5877, d_time=0.00(0.01), f_time=1.47(1.29), b_time=1.48(1.30)  Time cost: 41:15/42:16 [24:25:37/4:52:49]  Acc_iter 63700       Data time: 0.00(0.01)  Forward time: 1.47(1.29)  Batch time: 1.48(1.30)
2025-09-02 22:11:13,808   INFO  Train:   17/20 ( 85%) [1957/3862 ( 51%)]  Loss: 0.8441 (0.843)  LR: 1.949e-04  Grad: 14.9591  max=0.5576(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7462(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3831, loss_cls=0.0682, loss_bbox=0.3767, matched_ious=0.6007, d_time=0.00(0.01), f_time=1.32(1.29), b_time=1.32(1.30)  Time cost: 42:20/41:12 [24:26:42/4:51:46]  Acc_iter 63750       Data time: 0.00(0.01)  Forward time: 1.32(1.29)  Batch time: 1.32(1.30)
2025-09-02 22:12:18,850   INFO  Train:   17/20 ( 85%) [2007/3862 ( 52%)]  Loss: 0.9976 (0.843)  LR: 1.936e-04  Grad: 14.9396  max=0.5531(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7426(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3638, loss_cls=0.0658, loss_bbox=0.3866, matched_ious=0.5878, d_time=0.00(0.01), f_time=1.22(1.29), b_time=1.22(1.30)  Time cost: 43:25/40:07 [24:27:47/4:50:42]  Acc_iter 63800       Data time: 0.00(0.01)  Forward time: 1.22(1.29)  Batch time: 1.22(1.30)
2025-09-02 22:13:24,156   INFO  Train:   17/20 ( 85%) [2057/3862 ( 53%)]  Loss: 0.7894 (0.842)  LR: 1.923e-04  Grad: 14.9685  max=0.5668(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7481(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3626, loss_cls=0.0635, loss_bbox=0.3975, matched_ious=0.5824, d_time=0.01(0.01), f_time=1.36(1.29), b_time=1.37(1.30)  Time cost: 44:31/39:02 [24:28:53/4:49:40]  Acc_iter 63850       Data time: 0.01(0.01)  Forward time: 1.36(1.29)  Batch time: 1.37(1.30)
2025-09-02 22:14:29,071   INFO  Train:   17/20 ( 85%) [2107/3862 ( 55%)]  Loss: 0.8922 (0.843)  LR: 1.909e-04  Grad: 14.9885  max=0.5712(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7534(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3693, loss_cls=0.0645, loss_bbox=0.4250, matched_ious=0.5770, d_time=0.00(0.01), f_time=1.26(1.29), b_time=1.26(1.30)  Time cost: 45:36/37:57 [24:29:58/4:48:35]  Acc_iter 63900       Data time: 0.00(0.01)  Forward time: 1.26(1.29)  Batch time: 1.26(1.30)
2025-09-02 22:15:34,131   INFO  Train:   17/20 ( 85%) [2157/3862 ( 56%)]  Loss: 1.118 (0.843)  LR: 1.896e-04  Grad: 15.0398  max=0.5910(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7533(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3789, loss_cls=0.0654, loss_bbox=0.4340, matched_ious=0.5880, d_time=0.01(0.01), f_time=1.50(1.29), b_time=1.51(1.30)  Time cost: 46:41/36:53 [24:31:03/4:47:31]  Acc_iter 63950       Data time: 0.01(0.01)  Forward time: 1.50(1.29)  Batch time: 1.51(1.30)
2025-09-02 22:16:38,803   INFO  Train:   17/20 ( 85%) [2207/3862 ( 57%)]  Loss: 0.9712 (0.844)  LR: 1.883e-04  Grad: 15.1050  max=1.3342(module.vfe.pfn_layers.0.linear.weight)  min: -0.7561(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3857, loss_cls=0.0688, loss_bbox=0.4089, matched_ious=0.5861, d_time=0.00(0.01), f_time=1.29(1.29), b_time=1.29(1.30)  Time cost: 47:45/35:48 [24:32:07/4:46:25]  Acc_iter 64000       Data time: 0.00(0.01)  Forward time: 1.29(1.29)  Batch time: 1.29(1.30)
2025-09-02 22:17:43,459   INFO  Train:   17/20 ( 85%) [2257/3862 ( 58%)]  Loss: 0.8523 (0.842)  LR: 1.869e-04  Grad: 15.0858  max=0.9558(module.vfe.pfn_layers.0.linear.weight)  min: -0.7591(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3377, loss_cls=0.0603, loss_bbox=0.3789, matched_ious=0.5897, d_time=0.01(0.01), f_time=1.23(1.29), b_time=1.24(1.30)  Time cost: 48:50/34:42 [24:33:12/4:45:19]  Acc_iter 64050       Data time: 0.01(0.01)  Forward time: 1.23(1.29)  Batch time: 1.24(1.30)
2025-09-02 22:18:48,620   INFO  Train:   17/20 ( 85%) [2307/3862 ( 60%)]  Loss: 0.6499 (0.843)  LR: 1.856e-04  Grad: 15.0795  max=0.5889(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7673(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3621, loss_cls=0.0646, loss_bbox=0.4465, matched_ious=0.5860, d_time=0.01(0.01), f_time=1.27(1.29), b_time=1.28(1.30)  Time cost: 49:55/33:38 [24:34:17/4:44:15]  Acc_iter 64100       Data time: 0.01(0.01)  Forward time: 1.27(1.29)  Batch time: 1.28(1.30)
2025-09-02 22:19:53,770   INFO  Train:   17/20 ( 85%) [2357/3862 ( 61%)]  Loss: 0.7617 (0.842)  LR: 1.843e-04  Grad: 15.0585  max=0.6031(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7672(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3508, loss_cls=0.0604, loss_bbox=0.3698, matched_ious=0.5840, d_time=0.00(0.01), f_time=1.27(1.29), b_time=1.28(1.30)  Time cost: 51:00/32:33 [24:35:22/4:43:12]  Acc_iter 64150       Data time: 0.00(0.01)  Forward time: 1.27(1.29)  Batch time: 1.28(1.30)
2025-09-02 22:20:58,448   INFO  Train:   17/20 ( 85%) [2407/3862 ( 62%)]  Loss: 0.8471 (0.840)  LR: 1.830e-04  Grad: 15.0939  max=0.5928(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7660(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3452, loss_cls=0.0610, loss_bbox=0.3669, matched_ious=0.5855, d_time=0.00(0.01), f_time=1.27(1.29), b_time=1.27(1.30)  Time cost: 52:05/31:28 [24:36:27/4:42:06]  Acc_iter 64200       Data time: 0.00(0.01)  Forward time: 1.27(1.29)  Batch time: 1.27(1.30)
2025-09-02 22:22:03,665   INFO  Train:   17/20 ( 85%) [2457/3862 ( 64%)]  Loss: 0.5589 (0.840)  LR: 1.817e-04  Grad: 15.1165  max=0.5989(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7675(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3495, loss_cls=0.0608, loss_bbox=0.3915, matched_ious=0.5893, d_time=0.02(0.01), f_time=1.29(1.29), b_time=1.31(1.30)  Time cost: 53:10/30:23 [24:37:32/4:41:03]  Acc_iter 64250       Data time: 0.02(0.01)  Forward time: 1.29(1.29)  Batch time: 1.31(1.30)
2025-09-02 22:23:07,918   INFO  Train:   17/20 ( 85%) [2507/3862 ( 65%)]  Loss: 0.8279 (0.839)  LR: 1.804e-04  Grad: 15.1381  max=0.5888(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7651(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3538, loss_cls=0.0599, loss_bbox=0.3941, matched_ious=0.5827, d_time=0.00(0.01), f_time=1.24(1.29), b_time=1.24(1.30)  Time cost: 54:14/29:18 [24:38:36/4:39:54]  Acc_iter 64300       Data time: 0.00(0.01)  Forward time: 1.24(1.29)  Batch time: 1.24(1.30)
2025-09-02 22:24:12,542   INFO  Train:   17/20 ( 85%) [2557/3862 ( 66%)]  Loss: 0.9155 (0.838)  LR: 1.791e-04  Grad: 15.1845  max=0.7829(module.vfe.pfn_layers.0.linear.weight)  min: -0.7658(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3618, loss_cls=0.0616, loss_bbox=0.3913, matched_ious=0.5903, d_time=0.01(0.01), f_time=1.61(1.29), b_time=1.62(1.30)  Time cost: 55:19/28:13 [24:39:41/4:38:48]  Acc_iter 64350       Data time: 0.01(0.01)  Forward time: 1.61(1.29)  Batch time: 1.62(1.30)
2025-09-02 22:25:17,213   INFO  Train:   17/20 ( 85%) [2607/3862 ( 68%)]  Loss: 0.7873 (0.838)  LR: 1.778e-04  Grad: 15.1777  max=0.5756(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7746(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3485, loss_cls=0.0612, loss_bbox=0.3819, matched_ious=0.5809, d_time=0.00(0.01), f_time=1.32(1.29), b_time=1.32(1.30)  Time cost: 56:24/27:08 [24:40:46/4:37:42]  Acc_iter 64400       Data time: 0.00(0.01)  Forward time: 1.32(1.29)  Batch time: 1.32(1.30)
2025-09-02 22:26:22,705   INFO  Train:   17/20 ( 85%) [2657/3862 ( 69%)]  Loss: 1.029 (0.837)  LR: 1.765e-04  Grad: 15.1938  max=0.5748(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7750(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3508, loss_cls=0.0611, loss_bbox=0.4131, matched_ious=0.5861, d_time=0.00(0.01), f_time=1.14(1.29), b_time=1.14(1.30)  Time cost: 57:29/26:03 [24:41:51/4:36:40]  Acc_iter 64450       Data time: 0.00(0.01)  Forward time: 1.14(1.29)  Batch time: 1.14(1.30)
2025-09-02 22:27:28,511   INFO  Train:   17/20 ( 85%) [2707/3862 ( 70%)]  Loss: 0.9459 (0.836)  LR: 1.752e-04  Grad: 15.2115  max=0.5660(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7753(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3382, loss_cls=0.0574, loss_bbox=0.3830, matched_ious=0.5922, d_time=0.00(0.01), f_time=1.54(1.29), b_time=1.55(1.30)  Time cost: 58:35/24:59 [24:42:57/4:35:40]  Acc_iter 64500       Data time: 0.00(0.01)  Forward time: 1.54(1.29)  Batch time: 1.55(1.30)
2025-09-02 22:28:32,776   INFO  Train:   17/20 ( 85%) [2757/3862 ( 71%)]  Loss: 0.6926 (0.836)  LR: 1.739e-04  Grad: 15.2743  max=0.5815(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7794(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3467, loss_cls=0.0619, loss_bbox=0.3951, matched_ious=0.5828, d_time=0.01(0.01), f_time=1.24(1.29), b_time=1.25(1.30)  Time cost: 59:39/23:54 [24:44:01/4:34:32]  Acc_iter 64550       Data time: 0.01(0.01)  Forward time: 1.24(1.29)  Batch time: 1.25(1.30)
2025-09-02 22:29:38,523   INFO  Train:   17/20 ( 85%) [2807/3862 ( 73%)]  Loss: 0.6433 (0.836)  LR: 1.726e-04  Grad: 15.3044  max=0.5846(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7824(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3634, loss_cls=0.0634, loss_bbox=0.4288, matched_ious=0.5762, d_time=0.02(0.01), f_time=1.24(1.29), b_time=1.26(1.30)  Time cost: 1:00:45/22:49 [24:45:07/4:33:31]  Acc_iter 64600       Data time: 0.02(0.01)  Forward time: 1.24(1.29)  Batch time: 1.26(1.30)
2025-09-02 22:30:42,775   INFO  Train:   17/20 ( 85%) [2857/3862 ( 74%)]  Loss: 0.8366 (0.835)  LR: 1.713e-04  Grad: 15.2901  max=0.5838(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7858(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3489, loss_cls=0.0607, loss_bbox=0.3811, matched_ious=0.5887, d_time=0.00(0.01), f_time=1.12(1.29), b_time=1.12(1.30)  Time cost: 1:01:49/21:44 [24:46:11/4:32:23]  Acc_iter 64650       Data time: 0.00(0.01)  Forward time: 1.12(1.29)  Batch time: 1.12(1.30)
2025-09-02 22:31:48,083   INFO  Train:   17/20 ( 85%) [2907/3862 ( 75%)]  Loss: 0.5527 (0.835)  LR: 1.701e-04  Grad: 15.2880  max=0.5954(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7844(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3497, loss_cls=0.0618, loss_bbox=0.4025, matched_ious=0.5849, d_time=0.00(0.01), f_time=1.35(1.29), b_time=1.35(1.30)  Time cost: 1:02:55/20:39 [24:47:17/4:31:20]  Acc_iter 64700       Data time: 0.00(0.01)  Forward time: 1.35(1.29)  Batch time: 1.35(1.30)
2025-09-02 22:32:52,471   INFO  Train:   17/20 ( 85%) [2957/3862 ( 77%)]  Loss: 0.8162 (0.835)  LR: 1.688e-04  Grad: 15.3259  max=0.5958(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7876(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3616, loss_cls=0.0605, loss_bbox=0.3925, matched_ious=0.5906, d_time=0.01(0.01), f_time=1.23(1.29), b_time=1.24(1.30)  Time cost: 1:03:59/19:34 [24:48:21/4:30:13]  Acc_iter 64750       Data time: 0.01(0.01)  Forward time: 1.23(1.29)  Batch time: 1.24(1.30)
2025-09-02 22:33:57,167   INFO  Train:   17/20 ( 85%) [3007/3862 ( 78%)]  Loss: 0.8644 (0.834)  LR: 1.675e-04  Grad: 15.3610  max=0.5909(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7904(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3549, loss_cls=0.0620, loss_bbox=0.4062, matched_ious=0.5904, d_time=0.02(0.01), f_time=1.30(1.29), b_time=1.32(1.30)  Time cost: 1:05:04/18:29 [24:49:26/4:29:07]  Acc_iter 64800       Data time: 0.02(0.01)  Forward time: 1.30(1.29)  Batch time: 1.32(1.30)
2025-09-02 22:35:01,902   INFO  Train:   17/20 ( 85%) [3057/3862 ( 79%)]  Loss: 0.7208 (0.833)  LR: 1.663e-04  Grad: 15.3518  max=0.5910(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7916(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3431, loss_cls=0.0592, loss_bbox=0.3724, matched_ious=0.5979, d_time=0.00(0.01), f_time=1.25(1.29), b_time=1.25(1.30)  Time cost: 1:06:08/17:24 [24:50:30/4:28:01]  Acc_iter 64850       Data time: 0.00(0.01)  Forward time: 1.25(1.29)  Batch time: 1.25(1.30)
2025-09-02 22:36:06,368   INFO  Train:   17/20 ( 85%) [3107/3862 ( 80%)]  Loss: 0.5909 (0.834)  LR: 1.650e-04  Grad: 15.4121  max=0.5866(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7980(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3706, loss_cls=0.0661, loss_bbox=0.4105, matched_ious=0.5937, d_time=0.00(0.01), f_time=1.30(1.29), b_time=1.30(1.30)  Time cost: 1:07:13/16:19 [24:51:35/4:26:55]  Acc_iter 64900       Data time: 0.00(0.01)  Forward time: 1.30(1.29)  Batch time: 1.30(1.30)
2025-09-02 22:37:11,555   INFO  Train:   17/20 ( 85%) [3157/3862 ( 82%)]  Loss: 0.9096 (0.833)  LR: 1.637e-04  Grad: 15.4545  max=0.5800(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8033(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3579, loss_cls=0.0603, loss_bbox=0.4066, matched_ious=0.5963, d_time=0.00(0.01), f_time=1.29(1.29), b_time=1.29(1.30)  Time cost: 1:08:18/15:14 [24:52:40/4:25:51]  Acc_iter 64950       Data time: 0.00(0.01)  Forward time: 1.29(1.29)  Batch time: 1.29(1.30)
2025-09-02 22:38:15,396   INFO  Train:   17/20 ( 85%) [3207/3862 ( 83%)]  Loss: 0.8612 (0.833)  LR: 1.625e-04  Grad: 15.4896  max=0.5675(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7951(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3622, loss_cls=0.0648, loss_bbox=0.3965, matched_ious=0.5813, d_time=0.00(0.01), f_time=1.30(1.29), b_time=1.31(1.30)  Time cost: 1:09:22/14:09 [24:53:44/4:24:42]  Acc_iter 65000       Data time: 0.00(0.01)  Forward time: 1.30(1.29)  Batch time: 1.31(1.30)
2025-09-02 22:39:20,275   INFO  Train:   17/20 ( 85%) [3257/3862 ( 84%)]  Loss: 0.7874 (0.833)  LR: 1.612e-04  Grad: 15.5012  max=0.5834(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7960(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3675, loss_cls=0.0625, loss_bbox=0.3919, matched_ious=0.5860, d_time=0.00(0.01), f_time=1.29(1.29), b_time=1.30(1.30)  Time cost: 1:10:27/13:04 [24:54:49/4:23:37]  Acc_iter 65050       Data time: 0.00(0.01)  Forward time: 1.29(1.29)  Batch time: 1.30(1.30)
2025-09-02 22:40:24,920   INFO  Train:   17/20 ( 85%) [3307/3862 ( 86%)]  Loss: 0.6148 (0.832)  LR: 1.600e-04  Grad: 15.5258  max=0.5848(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7962(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3549, loss_cls=0.0619, loss_bbox=0.3728, matched_ious=0.5851, d_time=0.00(0.01), f_time=1.28(1.29), b_time=1.28(1.30)  Time cost: 1:11:31/12:00 [24:55:53/4:22:32]  Acc_iter 65100       Data time: 0.00(0.01)  Forward time: 1.28(1.29)  Batch time: 1.28(1.30)
2025-09-02 22:41:30,237   INFO  Train:   17/20 ( 85%) [3357/3862 ( 87%)]  Loss: 0.9870 (0.832)  LR: 1.588e-04  Grad: 15.5407  max=0.5771(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7971(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3351, loss_cls=0.0599, loss_bbox=0.4044, matched_ious=0.5916, d_time=0.00(0.01), f_time=1.32(1.29), b_time=1.33(1.30)  Time cost: 1:12:37/10:55 [24:56:59/4:21:28]  Acc_iter 65150       Data time: 0.00(0.01)  Forward time: 1.32(1.29)  Batch time: 1.33(1.30)
2025-09-02 22:42:35,354   INFO  Train:   17/20 ( 85%) [3407/3862 ( 88%)]  Loss: 0.5750 (0.832)  LR: 1.575e-04  Grad: 15.5518  max=0.5774(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7964(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3524, loss_cls=0.0601, loss_bbox=0.3921, matched_ious=0.5862, d_time=0.00(0.01), f_time=1.31(1.29), b_time=1.31(1.30)  Time cost: 1:13:42/09:50 [24:58:04/4:20:24]  Acc_iter 65200       Data time: 0.00(0.01)  Forward time: 1.31(1.29)  Batch time: 1.31(1.30)
2025-09-02 22:43:39,919   INFO  Train:   17/20 ( 85%) [3457/3862 ( 90%)]  Loss: 0.7636 (0.831)  LR: 1.563e-04  Grad: 15.5918  max=0.5747(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7995(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3592, loss_cls=0.0619, loss_bbox=0.3991, matched_ious=0.5872, d_time=0.00(0.01), f_time=1.27(1.29), b_time=1.27(1.30)  Time cost: 1:14:46/08:45 [24:59:08/4:19:18]  Acc_iter 65250       Data time: 0.00(0.01)  Forward time: 1.27(1.29)  Batch time: 1.27(1.30)
2025-09-02 22:44:44,874   INFO  Train:   17/20 ( 85%) [3507/3862 ( 91%)]  Loss: 0.9218 (0.830)  LR: 1.551e-04  Grad: 15.6120  max=0.5789(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8004(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3257, loss_cls=0.0595, loss_bbox=0.3566, matched_ious=0.5884, d_time=0.00(0.01), f_time=1.57(1.29), b_time=1.58(1.30)  Time cost: 1:15:51/07:40 [25:00:13/4:18:14]  Acc_iter 65300       Data time: 0.00(0.01)  Forward time: 1.57(1.29)  Batch time: 1.58(1.30)
2025-09-02 22:45:49,605   INFO  Train:   17/20 ( 85%) [3557/3862 ( 92%)]  Loss: 1.049 (0.830)  LR: 1.538e-04  Grad: 15.6528  max=0.6099(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8092(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3505, loss_cls=0.0626, loss_bbox=0.4032, matched_ious=0.5786, d_time=0.01(0.01), f_time=1.31(1.29), b_time=1.32(1.30)  Time cost: 1:16:56/06:35 [25:01:18/4:17:08]  Acc_iter 65350       Data time: 0.01(0.01)  Forward time: 1.31(1.29)  Batch time: 1.32(1.30)
2025-09-02 22:46:53,936   INFO  Train:   17/20 ( 85%) [3607/3862 ( 93%)]  Loss: 0.7027 (0.830)  LR: 1.526e-04  Grad: 15.6434  max=0.6211(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8088(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3623, loss_cls=0.0608, loss_bbox=0.4061, matched_ious=0.5905, d_time=0.01(0.01), f_time=1.18(1.29), b_time=1.19(1.30)  Time cost: 1:18:00/05:30 [25:02:22/4:16:02]  Acc_iter 65400       Data time: 0.01(0.01)  Forward time: 1.18(1.29)  Batch time: 1.19(1.30)
2025-09-02 22:47:59,535   INFO  Train:   17/20 ( 85%) [3657/3862 ( 95%)]  Loss: 0.7381 (0.830)  LR: 1.514e-04  Grad: 15.6715  max=0.6042(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8103(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3499, loss_cls=0.0585, loss_bbox=0.3974, matched_ious=0.5834, d_time=0.01(0.01), f_time=1.46(1.29), b_time=1.47(1.30)  Time cost: 1:19:06/04:26 [25:03:28/4:14:59]  Acc_iter 65450       Data time: 0.01(0.01)  Forward time: 1.46(1.29)  Batch time: 1.47(1.30)
2025-09-02 22:49:04,959   INFO  Train:   17/20 ( 85%) [3707/3862 ( 96%)]  Loss: 0.8061 (0.829)  LR: 1.502e-04  Grad: 15.7302  max=0.5898(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8107(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3637, loss_cls=0.0622, loss_bbox=0.3927, matched_ious=0.5989, d_time=0.00(0.01), f_time=1.19(1.29), b_time=1.19(1.30)  Time cost: 1:20:11/03:21 [25:04:34/4:13:56]  Acc_iter 65500       Data time: 0.00(0.01)  Forward time: 1.19(1.29)  Batch time: 1.19(1.30)
2025-09-02 22:50:09,520   INFO  Train:   17/20 ( 85%) [3757/3862 ( 97%)]  Loss: 1.122 (0.829)  LR: 1.490e-04  Grad: 15.6986  max=0.5864(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8130(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3435, loss_cls=0.0623, loss_bbox=0.3662, matched_ious=0.5912, d_time=0.00(0.01), f_time=1.24(1.29), b_time=1.24(1.30)  Time cost: 1:21:16/02:16 [25:05:38/4:12:50]  Acc_iter 65550       Data time: 0.00(0.01)  Forward time: 1.24(1.29)  Batch time: 1.24(1.30)
2025-09-02 22:51:14,033   INFO  Train:   17/20 ( 85%) [3807/3862 ( 99%)]  Loss: 0.8090 (0.827)  LR: 1.478e-04  Grad: 15.7182  max=0.6118(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8144(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3245, loss_cls=0.0572, loss_bbox=0.3546, matched_ious=0.6007, d_time=0.00(0.01), f_time=1.33(1.29), b_time=1.33(1.30)  Time cost: 1:22:21/01:11 [25:06:43/4:11:44]  Acc_iter 65600       Data time: 0.00(0.01)  Forward time: 1.33(1.29)  Batch time: 1.33(1.30)
2025-09-02 22:52:17,515   INFO  Train:   17/20 ( 85%) [3857/3862 (100%)]  Loss: 0.8490 (0.827)  LR: 1.466e-04  Grad: 15.7519  max=0.5985(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8192(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3397, loss_cls=0.0621, loss_bbox=0.3809, matched_ious=0.5913, d_time=0.00(0.01), f_time=1.25(1.29), b_time=1.26(1.30)  Time cost: 1:23:24/00:06 [25:07:46/4:10:35]  Acc_iter 65650       Data time: 0.00(0.01)  Forward time: 1.25(1.29)  Batch time: 1.26(1.30)
2025-09-02 22:52:22,642   INFO  Train:   17/20 ( 85%) [3861/3862 (100%)]  Loss: 0.4625 (0.827)  LR: 1.465e-04  Grad: 15.7378  max=0.5968(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8192(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3002, loss_cls=0.0429, loss_bbox=0.3526, matched_ious=0.6113, d_time=0.00(0.01), f_time=1.27(1.29), b_time=1.28(1.30)  Time cost: 1:23:29/00:01 [25:07:51/4:10:30]  Acc_iter 65654       Data time: 0.00(0.01)  Forward time: 1.27(1.29)  Batch time: 1.28(1.30)

                                               [Aepochs:  85%|████████▌ | 17/20 [25:07:52<4:22:18, 5246.01s/it]epochs:  85%|████████▌ | 17/20 [25:07:52<4:22:18, 5246.03s/it]epochs:  85%|████████▌ | 17/20 [25:07:52<4:22:18, 5246.02s/it]epochs:  85%|████████▌ | 17/20 [25:07:52<4:22:18, 5246.01s/it]epochs:  85%|████████▌ | 17/20 [25:07:52<4:22:18, 5246.03s/it]epochs:  85%|████████▌ | 17/20 [25:07:52<4:22:18, 5246.02s/it]epochs:  85%|████████▌ | 17/20 [25:07:52<4:22:18, 5246.01s/it]epochs:  85%|████████▌ | 17/20 [25:07:52<4:22:18, 5246.02s/it]
train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-02 22:52:29,447   INFO  Train:   18/20 ( 90%) [   0/3862 (  0%)]  Loss: 0.9577 (0.958)  LR: 1.465e-04  Grad: 15.7397  max=0.5976(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8188(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4570, loss_cls=0.0808, loss_bbox=0.4198, matched_ious=0.5772, d_time=1.74(1.74), f_time=3.47(3.47), b_time=5.21(5.21)  Time cost: 00:04/5:21:05 [25:07:58/16:03:17]  Acc_iter 65655       Data time: 1.74(1.74)  Forward time: 3.47(3.47)  Batch time: 5.21(5.21)
2025-09-02 22:53:29,528   INFO  Train:   18/20 ( 90%) [  45/3862 (  1%)]  Loss: 0.8826 (0.772)  LR: 1.454e-04  Grad: 15.7425  max=0.6078(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8159(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3305, loss_cls=0.0575, loss_bbox=0.3803, matched_ious=0.5844, d_time=0.00(0.04), f_time=1.25(1.37), b_time=1.26(1.42)  Time cost: 01:05/1:29:59 [25:08:58/4:32:05]  Acc_iter 65700       Data time: 0.00(0.04)  Forward time: 1.25(1.37)  Batch time: 1.26(1.42)
2025-09-02 22:54:34,929   INFO  Train:   18/20 ( 90%) [  95/3862 (  2%)]  Loss: 0.6444 (0.783)  LR: 1.442e-04  Grad: 15.7787  max=0.6085(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8229(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3430, loss_cls=0.0591, loss_bbox=0.3916, matched_ious=0.5972, d_time=0.00(0.02), f_time=1.26(1.34), b_time=1.26(1.36)  Time cost: 02:10/1:25:19 [25:10:03/4:20:17]  Acc_iter 65750       Data time: 0.00(0.02)  Forward time: 1.26(1.34)  Batch time: 1.26(1.36)
2025-09-02 22:55:39,899   INFO  Train:   18/20 ( 90%) [ 145/3862 (  4%)]  Loss: 1.082 (0.807)  LR: 1.430e-04  Grad: 15.8196  max=0.6010(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8249(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3649, loss_cls=0.0647, loss_bbox=0.4223, matched_ious=0.5855, d_time=0.00(0.02), f_time=1.23(1.32), b_time=1.24(1.34)  Time cost: 03:15/1:22:55 [25:11:08/4:15:15]  Acc_iter 65800       Data time: 0.00(0.02)  Forward time: 1.23(1.32)  Batch time: 1.24(1.34)
2025-09-02 22:56:44,350   INFO  Train:   18/20 ( 90%) [ 195/3862 (  5%)]  Loss: 0.4590 (0.814)  LR: 1.418e-04  Grad: 15.8298  max=0.6068(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8246(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3606, loss_cls=0.0638, loss_bbox=0.4110, matched_ious=0.5994, d_time=0.00(0.01), f_time=1.19(1.31), b_time=1.20(1.33)  Time cost: 04:19/1:21:02 [25:12:13/4:11:44]  Acc_iter 65850       Data time: 0.00(0.01)  Forward time: 1.19(1.31)  Batch time: 1.20(1.33)
2025-09-02 22:57:49,826   INFO  Train:   18/20 ( 90%) [ 245/3862 (  6%)]  Loss: 0.8313 (0.810)  LR: 1.406e-04  Grad: 15.8214  max=0.5980(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8224(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3571, loss_cls=0.0644, loss_bbox=0.3720, matched_ious=0.6011, d_time=0.00(0.01), f_time=1.35(1.31), b_time=1.35(1.32)  Time cost: 05:25/1:19:43 [25:13:18/4:09:59]  Acc_iter 65900       Data time: 0.00(0.01)  Forward time: 1.35(1.31)  Batch time: 1.35(1.32)
2025-09-02 22:58:55,015   INFO  Train:   18/20 ( 90%) [ 295/3862 (  8%)]  Loss: 0.8025 (0.814)  LR: 1.395e-04  Grad: 15.8456  max=0.5948(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8236(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3588, loss_cls=0.0658, loss_bbox=0.4077, matched_ious=0.5906, d_time=0.00(0.01), f_time=1.33(1.31), b_time=1.34(1.32)  Time cost: 06:30/1:18:26 [25:14:24/4:08:17]  Acc_iter 65950       Data time: 0.00(0.01)  Forward time: 1.33(1.31)  Batch time: 1.34(1.32)
2025-09-02 22:59:59,214   INFO  Train:   18/20 ( 90%) [ 345/3862 (  9%)]  Loss: 0.9020 (0.814)  LR: 1.383e-04  Grad: 15.8474  max=0.6188(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8302(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3579, loss_cls=0.0640, loss_bbox=0.3910, matched_ious=0.5889, d_time=0.00(0.01), f_time=1.27(1.30), b_time=1.27(1.31)  Time cost: 07:34/1:17:02 [25:15:28/4:06:14]  Acc_iter 66000       Data time: 0.00(0.01)  Forward time: 1.27(1.30)  Batch time: 1.27(1.31)
2025-09-02 23:01:04,732   INFO  Train:   18/20 ( 90%) [ 395/3862 ( 10%)]  Loss: 0.8505 (0.813)  LR: 1.371e-04  Grad: 15.8752  max=0.6151(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8288(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3521, loss_cls=0.0601, loss_bbox=0.3969, matched_ious=0.5975, d_time=0.01(0.01), f_time=1.28(1.30), b_time=1.29(1.31)  Time cost: 08:40/1:15:55 [25:16:33/4:05:03]  Acc_iter 66050       Data time: 0.01(0.01)  Forward time: 1.28(1.30)  Batch time: 1.29(1.31)
2025-09-02 23:02:09,745   INFO  Train:   18/20 ( 90%) [ 445/3862 ( 12%)]  Loss: 0.7487 (0.814)  LR: 1.360e-04  Grad: 15.8963  max=0.6123(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8310(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3623, loss_cls=0.0633, loss_bbox=0.3918, matched_ious=0.5858, d_time=0.00(0.01), f_time=1.26(1.30), b_time=1.27(1.31)  Time cost: 09:45/1:14:44 [25:17:38/4:03:40]  Acc_iter 66100       Data time: 0.00(0.01)  Forward time: 1.26(1.30)  Batch time: 1.27(1.31)
2025-09-02 23:03:14,493   INFO  Train:   18/20 ( 90%) [ 495/3862 ( 13%)]  Loss: 0.5971 (0.809)  LR: 1.348e-04  Grad: 15.9269  max=0.6230(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8318(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3385, loss_cls=0.0594, loss_bbox=0.3714, matched_ious=0.5973, d_time=0.00(0.01), f_time=1.14(1.30), b_time=1.14(1.31)  Time cost: 10:50/1:13:32 [25:18:43/4:02:15]  Acc_iter 66150       Data time: 0.00(0.01)  Forward time: 1.14(1.30)  Batch time: 1.14(1.31)
2025-09-02 23:04:19,370   INFO  Train:   18/20 ( 90%) [ 545/3862 ( 14%)]  Loss: 1.055 (0.810)  LR: 1.336e-04  Grad: 15.9259  max=0.6228(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8327(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3531, loss_cls=0.0639, loss_bbox=0.4034, matched_ious=0.5949, d_time=0.00(0.01), f_time=1.58(1.30), b_time=1.58(1.31)  Time cost: 11:54/1:12:23 [25:19:48/4:00:56]  Acc_iter 66200       Data time: 0.00(0.01)  Forward time: 1.58(1.30)  Batch time: 1.58(1.31)
2025-09-02 23:05:24,309   INFO  Train:   18/20 ( 90%) [ 595/3862 ( 15%)]  Loss: 0.6327 (0.809)  LR: 1.325e-04  Grad: 15.9580  max=0.6010(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8331(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3495, loss_cls=0.0615, loss_bbox=0.3882, matched_ious=0.5902, d_time=0.01(0.01), f_time=1.29(1.30), b_time=1.30(1.31)  Time cost: 12:59/1:11:14 [25:20:53/3:59:41]  Acc_iter 66250       Data time: 0.01(0.01)  Forward time: 1.29(1.30)  Batch time: 1.30(1.31)
2025-09-02 23:06:29,495   INFO  Train:   18/20 ( 90%) [ 645/3862 ( 17%)]  Loss: 0.9485 (0.807)  LR: 1.313e-04  Grad: 15.9799  max=0.6032(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8336(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3375, loss_cls=0.0609, loss_bbox=0.3839, matched_ious=0.5891, d_time=0.01(0.01), f_time=1.31(1.30), b_time=1.31(1.31)  Time cost: 14:05/1:10:08 [25:21:58/3:58:31]  Acc_iter 66300       Data time: 0.01(0.01)  Forward time: 1.31(1.30)  Batch time: 1.31(1.31)
2025-09-02 23:07:33,745   INFO  Train:   18/20 ( 90%) [ 695/3862 ( 18%)]  Loss: 0.6329 (0.810)  LR: 1.302e-04  Grad: 16.0107  max=0.6165(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8328(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3715, loss_cls=0.0661, loss_bbox=0.4104, matched_ious=0.5881, d_time=0.00(0.01), f_time=1.26(1.30), b_time=1.26(1.31)  Time cost: 15:09/1:08:57 [25:23:02/3:57:08]  Acc_iter 66350       Data time: 0.00(0.01)  Forward time: 1.26(1.30)  Batch time: 1.26(1.31)
2025-09-02 23:08:38,223   INFO  Train:   18/20 ( 90%) [ 745/3862 ( 19%)]  Loss: 0.5336 (0.810)  LR: 1.291e-04  Grad: 16.0118  max=0.6190(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8361(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3636, loss_cls=0.0646, loss_bbox=0.3797, matched_ious=0.5835, d_time=0.00(0.01), f_time=1.19(1.30), b_time=1.19(1.31)  Time cost: 16:13/1:07:48 [25:24:07/3:55:50]  Acc_iter 66400       Data time: 0.00(0.01)  Forward time: 1.19(1.30)  Batch time: 1.19(1.31)
2025-09-02 23:09:42,664   INFO  Train:   18/20 ( 90%) [ 795/3862 ( 21%)]  Loss: 0.6444 (0.808)  LR: 1.279e-04  Grad: 16.0882  max=0.6165(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8461(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3478, loss_cls=0.0572, loss_bbox=0.3788, matched_ious=0.5930, d_time=0.00(0.01), f_time=1.29(1.30), b_time=1.30(1.30)  Time cost: 17:18/1:06:40 [25:25:11/3:54:34]  Acc_iter 66450       Data time: 0.00(0.01)  Forward time: 1.29(1.30)  Batch time: 1.30(1.30)
2025-09-02 23:10:47,730   INFO  Train:   18/20 ( 90%) [ 845/3862 ( 22%)]  Loss: 0.7038 (0.808)  LR: 1.268e-04  Grad: 16.1163  max=0.6157(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8425(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3503, loss_cls=0.0610, loss_bbox=0.3891, matched_ious=0.5930, d_time=0.00(0.01), f_time=1.28(1.30), b_time=1.28(1.30)  Time cost: 18:23/1:05:34 [25:26:16/3:53:27]  Acc_iter 66500       Data time: 0.00(0.01)  Forward time: 1.28(1.30)  Batch time: 1.28(1.30)
2025-09-02 23:11:53,135   INFO  Train:   18/20 ( 90%) [ 895/3862 ( 23%)]  Loss: 1.013 (0.807)  LR: 1.257e-04  Grad: 16.0981  max=0.6242(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8428(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3511, loss_cls=0.0626, loss_bbox=0.3750, matched_ious=0.5911, d_time=0.00(0.01), f_time=1.31(1.30), b_time=1.32(1.30)  Time cost: 19:28/1:04:29 [25:27:22/3:52:24]  Acc_iter 66550       Data time: 0.00(0.01)  Forward time: 1.31(1.30)  Batch time: 1.32(1.30)
2025-09-02 23:12:58,514   INFO  Train:   18/20 ( 90%) [ 945/3862 ( 24%)]  Loss: 0.6908 (0.805)  LR: 1.245e-04  Grad: 16.1359  max=0.6221(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8449(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3434, loss_cls=0.0616, loss_bbox=0.3771, matched_ious=0.5949, d_time=0.00(0.01), f_time=1.31(1.30), b_time=1.31(1.30)  Time cost: 20:34/1:03:25 [25:28:27/3:51:21]  Acc_iter 66600       Data time: 0.00(0.01)  Forward time: 1.31(1.30)  Batch time: 1.31(1.30)
2025-09-02 23:14:03,337   INFO  Train:   18/20 ( 90%) [ 995/3862 ( 26%)]  Loss: 0.9700 (0.804)  LR: 1.234e-04  Grad: 16.1816  max=0.6079(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8460(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3331, loss_cls=0.0565, loss_bbox=0.3838, matched_ious=0.5801, d_time=0.00(0.01), f_time=1.40(1.30), b_time=1.40(1.30)  Time cost: 21:38/1:02:18 [25:29:32/3:50:11]  Acc_iter 66650       Data time: 0.00(0.01)  Forward time: 1.40(1.30)  Batch time: 1.40(1.30)
2025-09-02 23:15:08,306   INFO  Train:   18/20 ( 90%) [1045/3862 ( 27%)]  Loss: 0.9076 (0.803)  LR: 1.223e-04  Grad: 16.1896  max=0.6095(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8470(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3528, loss_cls=0.0623, loss_bbox=0.3623, matched_ious=0.5899, d_time=0.00(0.01), f_time=1.38(1.30), b_time=1.38(1.30)  Time cost: 22:43/1:01:13 [25:30:37/3:49:04]  Acc_iter 66700       Data time: 0.00(0.01)  Forward time: 1.38(1.30)  Batch time: 1.38(1.30)
2025-09-02 23:16:12,930   INFO  Train:   18/20 ( 90%) [1095/3862 ( 28%)]  Loss: 0.8485 (0.804)  LR: 1.212e-04  Grad: 16.2255  max=0.6245(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8609(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3606, loss_cls=0.0638, loss_bbox=0.4150, matched_ious=0.5912, d_time=0.00(0.01), f_time=1.17(1.30), b_time=1.17(1.30)  Time cost: 23:48/1:00:06 [25:31:41/3:47:53]  Acc_iter 66750       Data time: 0.00(0.01)  Forward time: 1.17(1.30)  Batch time: 1.17(1.30)
2025-09-02 23:17:18,130   INFO  Train:   18/20 ( 90%) [1145/3862 ( 30%)]  Loss: 0.8264 (0.805)  LR: 1.201e-04  Grad: 16.2309  max=0.6137(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8666(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3645, loss_cls=0.0650, loss_bbox=0.3939, matched_ious=0.5876, d_time=0.01(0.01), f_time=1.22(1.30), b_time=1.22(1.30)  Time cost: 24:53/59:01 [25:32:47/3:46:48]  Acc_iter 66800       Data time: 0.01(0.01)  Forward time: 1.22(1.30)  Batch time: 1.22(1.30)
2025-09-02 23:18:22,055   INFO  Train:   18/20 ( 90%) [1195/3862 ( 31%)]  Loss: 0.5573 (0.805)  LR: 1.190e-04  Grad: 16.2528  max=0.6167(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8712(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3413, loss_cls=0.0586, loss_bbox=0.4067, matched_ious=0.5885, d_time=0.01(0.01), f_time=1.29(1.29), b_time=1.30(1.30)  Time cost: 25:57/57:53 [25:33:51/3:45:32]  Acc_iter 66850       Data time: 0.01(0.01)  Forward time: 1.29(1.29)  Batch time: 1.30(1.30)
2025-09-02 23:19:26,236   INFO  Train:   18/20 ( 90%) [1245/3862 ( 32%)]  Loss: 0.7048 (0.804)  LR: 1.179e-04  Grad: 16.2990  max=0.6133(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8673(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3510, loss_cls=0.0590, loss_bbox=0.3742, matched_ious=0.5965, d_time=0.00(0.01), f_time=1.29(1.29), b_time=1.30(1.30)  Time cost: 27:01/56:46 [25:34:55/3:44:19]  Acc_iter 66900       Data time: 0.00(0.01)  Forward time: 1.29(1.29)  Batch time: 1.30(1.30)
2025-09-02 23:20:31,697   INFO  Train:   18/20 ( 90%) [1295/3862 ( 34%)]  Loss: 0.6966 (0.804)  LR: 1.168e-04  Grad: 16.3044  max=0.6122(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8670(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3560, loss_cls=0.0627, loss_bbox=0.3860, matched_ious=0.5912, d_time=0.00(0.01), f_time=1.25(1.29), b_time=1.26(1.30)  Time cost: 28:07/55:41 [25:36:00/3:43:17]  Acc_iter 66950       Data time: 0.00(0.01)  Forward time: 1.25(1.29)  Batch time: 1.26(1.30)
2025-09-02 23:21:36,632   INFO  Train:   18/20 ( 90%) [1345/3862 ( 35%)]  Loss: 0.8855 (0.805)  LR: 1.157e-04  Grad: 16.3241  max=0.5974(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8639(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3543, loss_cls=0.0618, loss_bbox=0.4106, matched_ious=0.5903, d_time=0.01(0.01), f_time=1.43(1.29), b_time=1.43(1.30)  Time cost: 29:12/54:36 [25:37:05/3:42:11]  Acc_iter 67000       Data time: 0.01(0.01)  Forward time: 1.43(1.29)  Batch time: 1.43(1.30)
2025-09-02 23:22:41,197   INFO  Train:   18/20 ( 90%) [1395/3862 ( 36%)]  Loss: 0.9614 (0.805)  LR: 1.147e-04  Grad: 16.3631  max=0.6064(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8648(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3366, loss_cls=0.0615, loss_bbox=0.3938, matched_ious=0.5904, d_time=0.01(0.01), f_time=1.28(1.29), b_time=1.29(1.30)  Time cost: 30:16/53:30 [25:38:10/3:41:02]  Acc_iter 67050       Data time: 0.01(0.01)  Forward time: 1.28(1.29)  Batch time: 1.29(1.30)
2025-09-02 23:23:46,686   INFO  Train:   18/20 ( 90%) [1445/3862 ( 37%)]  Loss: 0.8010 (0.804)  LR: 1.136e-04  Grad: 16.3808  max=0.6169(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8673(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3470, loss_cls=0.0613, loss_bbox=0.3701, matched_ious=0.5971, d_time=0.00(0.01), f_time=1.36(1.29), b_time=1.37(1.30)  Time cost: 31:22/52:26 [25:39:15/3:40:00]  Acc_iter 67100       Data time: 0.00(0.01)  Forward time: 1.36(1.29)  Batch time: 1.37(1.30)
2025-09-02 23:24:51,381   INFO  Train:   18/20 ( 90%) [1495/3862 ( 39%)]  Loss: 0.8828 (0.803)  LR: 1.125e-04  Grad: 16.4121  max=0.6256(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8702(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3344, loss_cls=0.0588, loss_bbox=0.3957, matched_ious=0.5873, d_time=0.01(0.01), f_time=1.35(1.29), b_time=1.36(1.30)  Time cost: 32:26/51:20 [25:40:20/3:38:52]  Acc_iter 67150       Data time: 0.01(0.01)  Forward time: 1.35(1.29)  Batch time: 1.36(1.30)
2025-09-02 23:25:56,323   INFO  Train:   18/20 ( 90%) [1545/3862 ( 40%)]  Loss: 0.7792 (0.805)  LR: 1.114e-04  Grad: 16.4015  max=0.6361(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8680(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3753, loss_cls=0.0658, loss_bbox=0.4056, matched_ious=0.5937, d_time=0.00(0.01), f_time=1.18(1.29), b_time=1.19(1.30)  Time cost: 33:31/50:15 [25:41:25/3:37:46]  Acc_iter 67200       Data time: 0.00(0.01)  Forward time: 1.18(1.29)  Batch time: 1.19(1.30)
2025-09-02 23:27:00,528   INFO  Train:   18/20 ( 90%) [1595/3862 ( 41%)]  Loss: 0.5969 (0.804)  LR: 1.104e-04  Grad: 16.4138  max=0.6277(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8735(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3258, loss_cls=0.0567, loss_bbox=0.4012, matched_ious=0.5916, d_time=0.00(0.01), f_time=1.28(1.29), b_time=1.29(1.30)  Time cost: 34:36/49:08 [25:42:29/3:36:36]  Acc_iter 67250       Data time: 0.00(0.01)  Forward time: 1.28(1.29)  Batch time: 1.29(1.30)
2025-09-02 23:28:06,018   INFO  Train:   18/20 ( 90%) [1645/3862 ( 43%)]  Loss: 0.8509 (0.804)  LR: 1.093e-04  Grad: 16.4332  max=0.6258(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8777(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3464, loss_cls=0.0609, loss_bbox=0.4074, matched_ious=0.5849, d_time=0.00(0.01), f_time=1.35(1.29), b_time=1.35(1.30)  Time cost: 35:41/48:04 [25:43:35/3:35:33]  Acc_iter 67300       Data time: 0.00(0.01)  Forward time: 1.35(1.29)  Batch time: 1.35(1.30)
2025-09-02 23:29:10,356   INFO  Train:   18/20 ( 90%) [1695/3862 ( 44%)]  Loss: 0.7101 (0.804)  LR: 1.083e-04  Grad: 16.4471  max=0.6373(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8771(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3456, loss_cls=0.0589, loss_bbox=0.3946, matched_ious=0.5924, d_time=0.00(0.01), f_time=1.32(1.29), b_time=1.32(1.30)  Time cost: 36:45/46:58 [25:44:39/3:34:24]  Acc_iter 67350       Data time: 0.00(0.01)  Forward time: 1.32(1.29)  Batch time: 1.32(1.30)
2025-09-02 23:30:15,567   INFO  Train:   18/20 ( 90%) [1745/3862 ( 45%)]  Loss: 0.6875 (0.803)  LR: 1.072e-04  Grad: 16.5274  max=0.6189(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8770(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3398, loss_cls=0.0602, loss_bbox=0.3625, matched_ious=0.5920, d_time=0.00(0.01), f_time=1.15(1.29), b_time=1.16(1.30)  Time cost: 37:51/45:53 [25:45:44/3:33:20]  Acc_iter 67400       Data time: 0.00(0.01)  Forward time: 1.15(1.29)  Batch time: 1.16(1.30)
2025-09-02 23:31:20,534   INFO  Train:   18/20 ( 90%) [1795/3862 ( 46%)]  Loss: 0.5727 (0.803)  LR: 1.062e-04  Grad: 16.5204  max=0.6238(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8793(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3361, loss_cls=0.0599, loss_bbox=0.4174, matched_ious=0.5939, d_time=0.00(0.01), f_time=1.27(1.29), b_time=1.27(1.30)  Time cost: 38:56/44:48 [25:46:49/3:32:15]  Acc_iter 67450       Data time: 0.00(0.01)  Forward time: 1.27(1.29)  Batch time: 1.27(1.30)
2025-09-02 23:32:26,279   INFO  Train:   18/20 ( 90%) [1845/3862 ( 48%)]  Loss: 0.8097 (0.804)  LR: 1.051e-04  Grad: 16.5289  max=0.6223(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8813(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3708, loss_cls=0.0666, loss_bbox=0.3893, matched_ious=0.5923, d_time=0.03(0.01), f_time=1.78(1.29), b_time=1.81(1.30)  Time cost: 40:01/43:44 [25:47:55/3:31:13]  Acc_iter 67500       Data time: 0.03(0.01)  Forward time: 1.78(1.29)  Batch time: 1.81(1.30)
2025-09-02 23:33:31,741   INFO  Train:   18/20 ( 90%) [1895/3862 ( 49%)]  Loss: 0.7222 (0.803)  LR: 1.041e-04  Grad: 16.5925  max=0.8083(module.vfe.pfn_layers.0.linear.weight)  min: -0.8834(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3362, loss_cls=0.0577, loss_bbox=0.3717, matched_ious=0.5981, d_time=0.01(0.01), f_time=1.21(1.29), b_time=1.22(1.30)  Time cost: 41:07/42:39 [25:49:00/3:30:10]  Acc_iter 67550       Data time: 0.01(0.01)  Forward time: 1.21(1.29)  Batch time: 1.22(1.30)
2025-09-02 23:34:36,601   INFO  Train:   18/20 ( 90%) [1945/3862 ( 50%)]  Loss: 0.8185 (0.802)  LR: 1.030e-04  Grad: 16.5586  max=0.6253(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8835(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3330, loss_cls=0.0607, loss_bbox=0.3671, matched_ious=0.5806, d_time=0.01(0.01), f_time=1.25(1.29), b_time=1.26(1.30)  Time cost: 42:12/41:34 [25:50:05/3:29:04]  Acc_iter 67600       Data time: 0.01(0.01)  Forward time: 1.25(1.29)  Batch time: 1.26(1.30)
2025-09-02 23:35:40,269   INFO  Train:   18/20 ( 90%) [1995/3862 ( 52%)]  Loss: 0.5715 (0.802)  LR: 1.020e-04  Grad: 16.6056  max=0.6228(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8817(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3444, loss_cls=0.0587, loss_bbox=0.4166, matched_ious=0.6046, d_time=0.00(0.01), f_time=1.48(1.29), b_time=1.48(1.30)  Time cost: 43:15/40:28 [25:51:09/3:27:53]  Acc_iter 67650       Data time: 0.00(0.01)  Forward time: 1.48(1.29)  Batch time: 1.48(1.30)
2025-09-02 23:36:44,909   INFO  Train:   18/20 ( 90%) [2045/3862 ( 53%)]  Loss: 1.202 (0.802)  LR: 1.010e-04  Grad: 16.6178  max=0.6161(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8897(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3281, loss_cls=0.0600, loss_bbox=0.3840, matched_ious=0.5930, d_time=0.01(0.01), f_time=1.35(1.29), b_time=1.36(1.30)  Time cost: 44:20/39:22 [25:52:13/3:26:46]  Acc_iter 67700       Data time: 0.01(0.01)  Forward time: 1.35(1.29)  Batch time: 1.36(1.30)
2025-09-02 23:37:49,922   INFO  Train:   18/20 ( 90%) [2095/3862 ( 54%)]  Loss: 0.7927 (0.803)  LR: 9.997e-05  Grad: 16.6638  max=0.6208(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8902(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3662, loss_cls=0.0638, loss_bbox=0.4259, matched_ious=0.5731, d_time=0.00(0.01), f_time=1.27(1.29), b_time=1.28(1.30)  Time cost: 45:25/38:17 [25:53:18/3:25:41]  Acc_iter 67750       Data time: 0.00(0.01)  Forward time: 1.27(1.29)  Batch time: 1.28(1.30)
2025-09-02 23:38:55,123   INFO  Train:   18/20 ( 90%) [2145/3862 ( 56%)]  Loss: 1.043 (0.802)  LR: 9.896e-05  Grad: 16.7009  max=0.6417(module.vfe.pfn_layers.0.linear.weight)  min: -0.8927(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3472, loss_cls=0.0604, loss_bbox=0.3735, matched_ious=0.5922, d_time=0.01(0.01), f_time=1.17(1.29), b_time=1.18(1.30)  Time cost: 46:30/37:12 [25:54:24/3:24:37]  Acc_iter 67800       Data time: 0.01(0.01)  Forward time: 1.17(1.29)  Batch time: 1.18(1.30)
2025-09-02 23:40:00,046   INFO  Train:   18/20 ( 90%) [2195/3862 ( 57%)]  Loss: 0.9785 (0.802)  LR: 9.795e-05  Grad: 16.6997  max=0.6333(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8927(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3576, loss_cls=0.0629, loss_bbox=0.3888, matched_ious=0.6002, d_time=0.01(0.01), f_time=1.55(1.29), b_time=1.56(1.30)  Time cost: 47:35/36:07 [25:55:29/3:23:31]  Acc_iter 67850       Data time: 0.01(0.01)  Forward time: 1.55(1.29)  Batch time: 1.56(1.30)
2025-09-02 23:41:05,663   INFO  Train:   18/20 ( 90%) [2245/3862 ( 58%)]  Loss: 0.6395 (0.801)  LR: 9.694e-05  Grad: 16.7362  max=0.6222(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8952(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3223, loss_cls=0.0590, loss_bbox=0.3537, matched_ious=0.5896, d_time=0.01(0.01), f_time=1.60(1.29), b_time=1.61(1.30)  Time cost: 48:41/35:03 [25:56:34/3:22:29]  Acc_iter 67900       Data time: 0.01(0.01)  Forward time: 1.60(1.29)  Batch time: 1.61(1.30)
2025-09-02 23:42:10,864   INFO  Train:   18/20 ( 90%) [2295/3862 ( 59%)]  Loss: 0.7300 (0.802)  LR: 9.594e-05  Grad: 16.6910  max=0.6493(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8938(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3564, loss_cls=0.0621, loss_bbox=0.4276, matched_ious=0.5829, d_time=0.01(0.01), f_time=1.39(1.29), b_time=1.40(1.30)  Time cost: 49:46/33:58 [25:57:39/3:21:24]  Acc_iter 67950       Data time: 0.01(0.01)  Forward time: 1.39(1.29)  Batch time: 1.40(1.30)
2025-09-02 23:43:15,748   INFO  Train:   18/20 ( 90%) [2345/3862 ( 61%)]  Loss: 0.6365 (0.802)  LR: 9.495e-05  Grad: 16.7517  max=0.6374(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8962(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3498, loss_cls=0.0612, loss_bbox=0.3825, matched_ious=0.5925, d_time=0.01(0.01), f_time=1.19(1.29), b_time=1.20(1.30)  Time cost: 50:51/32:53 [25:58:44/3:20:19]  Acc_iter 68000       Data time: 0.01(0.01)  Forward time: 1.19(1.29)  Batch time: 1.20(1.30)
2025-09-02 23:44:20,547   INFO  Train:   18/20 ( 90%) [2395/3862 ( 62%)]  Loss: 0.8509 (0.802)  LR: 9.396e-05  Grad: 16.7363  max=0.6405(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8979(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3424, loss_cls=0.0600, loss_bbox=0.3980, matched_ious=0.5911, d_time=0.00(0.01), f_time=1.28(1.29), b_time=1.28(1.30)  Time cost: 51:56/31:47 [25:59:49/3:19:13]  Acc_iter 68050       Data time: 0.00(0.01)  Forward time: 1.28(1.29)  Batch time: 1.28(1.30)
2025-09-02 23:45:24,692   INFO  Train:   18/20 ( 90%) [2445/3862 ( 63%)]  Loss: 1.110 (0.802)  LR: 9.297e-05  Grad: 16.7996  max=0.6474(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8940(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3601, loss_cls=0.0626, loss_bbox=0.3898, matched_ious=0.5882, d_time=0.00(0.01), f_time=1.28(1.29), b_time=1.28(1.30)  Time cost: 53:00/30:42 [26:00:53/3:18:04]  Acc_iter 68100       Data time: 0.00(0.01)  Forward time: 1.28(1.29)  Batch time: 1.28(1.30)
2025-09-02 23:46:29,055   INFO  Train:   18/20 ( 90%) [2495/3862 ( 65%)]  Loss: 0.6013 (0.802)  LR: 9.199e-05  Grad: 16.7894  max=0.6415(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8967(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3556, loss_cls=0.0598, loss_bbox=0.3756, matched_ious=0.5922, d_time=0.01(0.01), f_time=1.28(1.29), b_time=1.29(1.30)  Time cost: 54:04/29:36 [26:01:58/3:16:57]  Acc_iter 68150       Data time: 0.01(0.01)  Forward time: 1.28(1.29)  Batch time: 1.29(1.30)
2025-09-02 23:47:34,252   INFO  Train:   18/20 ( 90%) [2545/3862 ( 66%)]  Loss: 0.9066 (0.802)  LR: 9.101e-05  Grad: 16.8035  max=0.6403(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8930(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3544, loss_cls=0.0603, loss_bbox=0.3955, matched_ious=0.5957, d_time=0.01(0.01), f_time=1.15(1.29), b_time=1.15(1.30)  Time cost: 55:09/28:32 [26:03:03/3:15:53]  Acc_iter 68200       Data time: 0.01(0.01)  Forward time: 1.15(1.29)  Batch time: 1.15(1.30)
2025-09-02 23:48:38,488   INFO  Train:   18/20 ( 90%) [2595/3862 ( 67%)]  Loss: 0.8838 (0.801)  LR: 9.004e-05  Grad: 16.8271  max=0.6290(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8940(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3491, loss_cls=0.0583, loss_bbox=0.3630, matched_ious=0.5980, d_time=0.00(0.01), f_time=1.33(1.29), b_time=1.33(1.30)  Time cost: 56:14/27:26 [26:04:07/3:14:45]  Acc_iter 68250       Data time: 0.00(0.01)  Forward time: 1.33(1.29)  Batch time: 1.33(1.30)
2025-09-02 23:49:43,586   INFO  Train:   18/20 ( 90%) [2645/3862 ( 68%)]  Loss: 0.7214 (0.800)  LR: 8.907e-05  Grad: 16.8802  max=0.6224(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.8967(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3184, loss_cls=0.0557, loss_bbox=0.3703, matched_ious=0.5923, d_time=0.01(0.01), f_time=1.21(1.29), b_time=1.21(1.30)  Time cost: 57:19/26:21 [26:05:12/3:13:41]  Acc_iter 68300       Data time: 0.01(0.01)  Forward time: 1.21(1.29)  Batch time: 1.21(1.30)
2025-09-02 23:50:48,711   INFO  Train:   18/20 ( 90%) [2695/3862 ( 70%)]  Loss: 0.8589 (0.800)  LR: 8.811e-05  Grad: 16.9489  max=0.6322(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9002(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3510, loss_cls=0.0619, loss_bbox=0.3915, matched_ious=0.5910, d_time=0.01(0.01), f_time=1.27(1.29), b_time=1.28(1.30)  Time cost: 58:24/25:16 [26:06:17/3:12:36]  Acc_iter 68350       Data time: 0.01(0.01)  Forward time: 1.27(1.29)  Batch time: 1.28(1.30)
2025-09-02 23:51:54,421   INFO  Train:   18/20 ( 90%) [2745/3862 ( 71%)]  Loss: 0.5980 (0.799)  LR: 8.715e-05  Grad: 16.9185  max=0.6282(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9005(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3172, loss_cls=0.0563, loss_bbox=0.3488, matched_ious=0.5925, d_time=0.00(0.01), f_time=1.27(1.29), b_time=1.28(1.30)  Time cost: 59:29/24:12 [26:07:23/3:11:33]  Acc_iter 68400       Data time: 0.00(0.01)  Forward time: 1.27(1.29)  Batch time: 1.28(1.30)
2025-09-02 23:53:00,147   INFO  Train:   18/20 ( 90%) [2795/3862 ( 72%)]  Loss: 0.6193 (0.798)  LR: 8.620e-05  Grad: 16.9688  max=0.6337(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9015(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3114, loss_cls=0.0548, loss_bbox=0.3903, matched_ious=0.5890, d_time=0.01(0.01), f_time=1.20(1.29), b_time=1.21(1.30)  Time cost: 1:00:35/23:07 [26:08:29/3:10:31]  Acc_iter 68450       Data time: 0.01(0.01)  Forward time: 1.20(1.29)  Batch time: 1.21(1.30)
2025-09-02 23:54:05,394   INFO  Train:   18/20 ( 90%) [2845/3862 ( 74%)]  Loss: 0.9535 (0.798)  LR: 8.525e-05  Grad: 16.9477  max=0.6274(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9030(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3505, loss_cls=0.0634, loss_bbox=0.3846, matched_ious=0.5937, d_time=0.01(0.01), f_time=1.21(1.29), b_time=1.22(1.30)  Time cost: 1:01:40/22:02 [26:09:34/3:09:26]  Acc_iter 68500       Data time: 0.01(0.01)  Forward time: 1.21(1.29)  Batch time: 1.22(1.30)
2025-09-02 23:55:10,795   INFO  Train:   18/20 ( 90%) [2895/3862 ( 75%)]  Loss: 0.8820 (0.798)  LR: 8.430e-05  Grad: 16.9740  max=0.6296(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9081(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3307, loss_cls=0.0582, loss_bbox=0.3732, matched_ious=0.5904, d_time=0.01(0.01), f_time=1.42(1.29), b_time=1.43(1.30)  Time cost: 1:02:46/20:57 [26:10:39/3:08:22]  Acc_iter 68550       Data time: 0.01(0.01)  Forward time: 1.42(1.29)  Batch time: 1.43(1.30)
2025-09-02 23:56:15,856   INFO  Train:   18/20 ( 90%) [2945/3862 ( 76%)]  Loss: 1.108 (0.798)  LR: 8.336e-05  Grad: 16.9858  max=0.6388(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9119(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3400, loss_cls=0.0576, loss_bbox=0.3991, matched_ious=0.5932, d_time=0.01(0.01), f_time=1.28(1.29), b_time=1.29(1.30)  Time cost: 1:03:51/19:52 [26:11:44/3:07:17]  Acc_iter 68600       Data time: 0.01(0.01)  Forward time: 1.28(1.29)  Batch time: 1.29(1.30)
2025-09-02 23:57:20,709   INFO  Train:   18/20 ( 90%) [2995/3862 ( 78%)]  Loss: 0.7530 (0.797)  LR: 8.243e-05  Grad: 17.0183  max=0.6296(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9103(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3512, loss_cls=0.0642, loss_bbox=0.3746, matched_ious=0.5919, d_time=0.01(0.01), f_time=1.25(1.29), b_time=1.26(1.30)  Time cost: 1:04:56/18:47 [26:12:49/3:06:12]  Acc_iter 68650       Data time: 0.01(0.01)  Forward time: 1.25(1.29)  Batch time: 1.26(1.30)
2025-09-02 23:58:25,038   INFO  Train:   18/20 ( 90%) [3045/3862 ( 79%)]  Loss: 0.5419 (0.797)  LR: 8.150e-05  Grad: 17.0494  max=0.6350(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9096(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3362, loss_cls=0.0585, loss_bbox=0.3814, matched_ious=0.5996, d_time=0.01(0.01), f_time=1.23(1.29), b_time=1.23(1.30)  Time cost: 1:06:00/17:42 [26:13:54/3:05:05]  Acc_iter 68700       Data time: 0.01(0.01)  Forward time: 1.23(1.29)  Batch time: 1.23(1.30)
2025-09-02 23:59:29,073   INFO  Train:   18/20 ( 90%) [3095/3862 ( 80%)]  Loss: 0.8689 (0.796)  LR: 8.057e-05  Grad: 17.0519  max=0.6420(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9141(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3344, loss_cls=0.0568, loss_bbox=0.3642, matched_ious=0.5902, d_time=0.01(0.01), f_time=1.20(1.29), b_time=1.21(1.30)  Time cost: 1:07:04/16:37 [26:14:58/3:03:57]  Acc_iter 68750       Data time: 0.01(0.01)  Forward time: 1.20(1.29)  Batch time: 1.21(1.30)
2025-09-03 00:00:34,856   INFO  Train:   18/20 ( 90%) [3145/3862 ( 81%)]  Loss: 1.114 (0.797)  LR: 7.965e-05  Grad: 17.0758  max=0.6458(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9111(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3609, loss_cls=0.0606, loss_bbox=0.4095, matched_ious=0.5889, d_time=0.00(0.01), f_time=1.23(1.29), b_time=1.23(1.30)  Time cost: 1:08:10/15:32 [26:16:03/3:02:54]  Acc_iter 68800       Data time: 0.00(0.01)  Forward time: 1.23(1.29)  Batch time: 1.23(1.30)
2025-09-03 00:01:40,635   INFO  Train:   18/20 ( 90%) [3195/3862 ( 83%)]  Loss: 0.9773 (0.797)  LR: 7.874e-05  Grad: 17.0732  max=0.6545(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9129(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3431, loss_cls=0.0615, loss_bbox=0.3945, matched_ious=0.5904, d_time=0.01(0.01), f_time=1.28(1.29), b_time=1.29(1.30)  Time cost: 1:09:16/14:27 [26:17:09/3:01:51]  Acc_iter 68850       Data time: 0.01(0.01)  Forward time: 1.28(1.29)  Batch time: 1.29(1.30)
2025-09-03 00:02:45,744   INFO  Train:   18/20 ( 90%) [3245/3862 ( 84%)]  Loss: 0.4803 (0.797)  LR: 7.783e-05  Grad: 17.0947  max=0.6587(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9124(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3309, loss_cls=0.0578, loss_bbox=0.3851, matched_ious=0.5905, d_time=0.00(0.01), f_time=1.23(1.29), b_time=1.24(1.30)  Time cost: 1:10:21/13:22 [26:18:14/3:00:47]  Acc_iter 68900       Data time: 0.00(0.01)  Forward time: 1.23(1.29)  Batch time: 1.24(1.30)
2025-09-03 00:03:50,234   INFO  Train:   18/20 ( 90%) [3295/3862 ( 85%)]  Loss: 0.5141 (0.796)  LR: 7.692e-05  Grad: 17.1337  max=0.6535(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9181(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3279, loss_cls=0.0564, loss_bbox=0.3790, matched_ious=0.5944, d_time=0.01(0.01), f_time=1.18(1.29), b_time=1.19(1.30)  Time cost: 1:11:25/12:17 [26:19:19/2:59:40]  Acc_iter 68950       Data time: 0.01(0.01)  Forward time: 1.18(1.29)  Batch time: 1.19(1.30)
2025-09-03 00:04:55,262   INFO  Train:   18/20 ( 90%) [3345/3862 ( 87%)]  Loss: 0.7158 (0.796)  LR: 7.602e-05  Grad: 17.1528  max=0.6417(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9185(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3472, loss_cls=0.0628, loss_bbox=0.3735, matched_ious=0.5983, d_time=0.00(0.01), f_time=1.33(1.29), b_time=1.34(1.30)  Time cost: 1:12:30/11:12 [26:20:24/2:58:35]  Acc_iter 69000       Data time: 0.00(0.01)  Forward time: 1.33(1.29)  Batch time: 1.34(1.30)
2025-09-03 00:06:00,261   INFO  Train:   18/20 ( 90%) [3395/3862 ( 88%)]  Loss: 0.5301 (0.796)  LR: 7.513e-05  Grad: 17.1431  max=0.6549(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9204(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3270, loss_cls=0.0579, loss_bbox=0.3972, matched_ious=0.5845, d_time=0.00(0.01), f_time=1.30(1.29), b_time=1.30(1.30)  Time cost: 1:13:35/10:07 [26:21:29/2:57:30]  Acc_iter 69050       Data time: 0.00(0.01)  Forward time: 1.30(1.29)  Batch time: 1.30(1.30)
2025-09-03 00:07:04,725   INFO  Train:   18/20 ( 90%) [3445/3862 ( 89%)]  Loss: 0.5946 (0.795)  LR: 7.424e-05  Grad: 17.2003  max=0.6485(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9256(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3334, loss_cls=0.0578, loss_bbox=0.3768, matched_ious=0.5867, d_time=0.00(0.01), f_time=1.30(1.29), b_time=1.31(1.30)  Time cost: 1:14:40/09:02 [26:22:33/2:56:24]  Acc_iter 69100       Data time: 0.00(0.01)  Forward time: 1.30(1.29)  Batch time: 1.31(1.30)
2025-09-03 00:08:09,364   INFO  Train:   18/20 ( 90%) [3495/3862 ( 90%)]  Loss: 0.7877 (0.795)  LR: 7.335e-05  Grad: 17.2106  max=0.6409(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9299(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3333, loss_cls=0.0572, loss_bbox=0.3646, matched_ious=0.6005, d_time=0.00(0.01), f_time=1.43(1.29), b_time=1.43(1.30)  Time cost: 1:15:44/07:57 [26:23:38/2:55:18]  Acc_iter 69150       Data time: 0.00(0.01)  Forward time: 1.43(1.29)  Batch time: 1.43(1.30)
2025-09-03 00:09:14,272   INFO  Train:   18/20 ( 90%) [3545/3862 ( 92%)]  Loss: 1.020 (0.794)  LR: 7.247e-05  Grad: 17.2485  max=0.6752(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9330(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3209, loss_cls=0.0572, loss_bbox=0.3671, matched_ious=0.6006, d_time=0.01(0.01), f_time=1.27(1.29), b_time=1.28(1.30)  Time cost: 1:16:49/06:52 [26:24:43/2:54:13]  Acc_iter 69200       Data time: 0.01(0.01)  Forward time: 1.27(1.29)  Batch time: 1.28(1.30)
2025-09-03 00:10:18,828   INFO  Train:   18/20 ( 90%) [3595/3862 ( 93%)]  Loss: 0.7472 (0.794)  LR: 7.159e-05  Grad: 17.2411  max=0.6829(module.vfe.pfn_layers.0.linear.weight)  min: -0.9338(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3489, loss_cls=0.0579, loss_bbox=0.3919, matched_ious=0.5881, d_time=0.00(0.01), f_time=1.19(1.29), b_time=1.20(1.30)  Time cost: 1:17:54/05:47 [26:25:47/2:53:07]  Acc_iter 69250       Data time: 0.00(0.01)  Forward time: 1.19(1.29)  Batch time: 1.20(1.30)
2025-09-03 00:11:23,250   INFO  Train:   18/20 ( 90%) [3645/3862 ( 94%)]  Loss: 0.7759 (0.794)  LR: 7.072e-05  Grad: 17.2666  max=0.6711(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9370(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3576, loss_cls=0.0617, loss_bbox=0.3769, matched_ious=0.5976, d_time=0.01(0.01), f_time=1.30(1.29), b_time=1.31(1.30)  Time cost: 1:18:58/04:42 [26:26:52/2:52:01]  Acc_iter 69300       Data time: 0.01(0.01)  Forward time: 1.30(1.29)  Batch time: 1.31(1.30)
2025-09-03 00:12:28,279   INFO  Train:   18/20 ( 90%) [3695/3862 ( 96%)]  Loss: 0.7475 (0.794)  LR: 6.985e-05  Grad: 17.2850  max=0.6812(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9378(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3407, loss_cls=0.0579, loss_bbox=0.3904, matched_ious=0.6009, d_time=0.00(0.01), f_time=1.24(1.29), b_time=1.24(1.30)  Time cost: 1:20:03/03:37 [26:27:57/2:50:56]  Acc_iter 69350       Data time: 0.00(0.01)  Forward time: 1.24(1.29)  Batch time: 1.24(1.30)
2025-09-03 00:13:32,461   INFO  Train:   18/20 ( 90%) [3745/3862 ( 97%)]  Loss: 0.9645 (0.794)  LR: 6.899e-05  Grad: 17.3148  max=0.6715(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9399(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3476, loss_cls=0.0617, loss_bbox=0.3806, matched_ious=0.5947, d_time=0.00(0.01), f_time=1.23(1.29), b_time=1.24(1.30)  Time cost: 1:21:07/02:32 [26:29:01/2:49:49]  Acc_iter 69400       Data time: 0.00(0.01)  Forward time: 1.23(1.29)  Batch time: 1.24(1.30)
2025-09-03 00:14:37,796   INFO  Train:   18/20 ( 90%) [3795/3862 ( 98%)]  Loss: 0.8624 (0.794)  LR: 6.814e-05  Grad: 17.3333  max=0.6769(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9487(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3411, loss_cls=0.0606, loss_bbox=0.3844, matched_ious=0.5932, d_time=0.00(0.01), f_time=1.31(1.29), b_time=1.31(1.30)  Time cost: 1:22:13/01:27 [26:30:06/2:48:45]  Acc_iter 69450       Data time: 0.00(0.01)  Forward time: 1.31(1.29)  Batch time: 1.31(1.30)
2025-09-03 00:15:42,154   INFO  Train:   18/20 ( 90%) [3845/3862 (100%)]  Loss: 0.4222 (0.794)  LR: 6.729e-05  Grad: 17.3659  max=0.6766(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9476(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3403, loss_cls=0.0578, loss_bbox=0.3814, matched_ious=0.5990, d_time=0.00(0.01), f_time=1.18(1.29), b_time=1.18(1.30)  Time cost: 1:23:17/00:22 [26:31:11/2:47:39]  Acc_iter 69500       Data time: 0.00(0.01)  Forward time: 1.18(1.29)  Batch time: 1.18(1.30)
2025-09-03 00:16:02,073   INFO  Train:   18/20 ( 90%) [3861/3862 (100%)]  Loss: 0.6230 (0.793)  LR: 6.701e-05  Grad: 17.3778  max=0.6733(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9483(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3182, loss_cls=0.0532, loss_bbox=0.3613, matched_ious=0.5952, d_time=0.00(0.01), f_time=1.18(1.29), b_time=1.18(1.30)  Time cost: 1:23:37/00:01 [26:31:31/2:47:16]  Acc_iter 69516       Data time: 0.00(0.01)  Forward time: 1.18(1.29)  Batch time: 1.18(1.30)

                                               [Aepochs:  90%|█████████ | 18/20 [26:31:31<2:52:35, 5177.86s/it]epochs:  90%|█████████ | 18/20 [26:31:31<2:52:35, 5177.92s/it]epochs:  90%|█████████ | 18/20 [26:31:31<2:52:35, 5177.92s/it]epochs:  90%|█████████ | 18/20 [26:31:31<2:52:35, 5177.92s/it]epochs:  90%|█████████ | 18/20 [26:31:31<2:52:35, 5177.92s/it]epochs:  90%|█████████ | 18/20 [26:31:31<2:52:35, 5177.92s/it]epochs:  90%|█████████ | 18/20 [26:31:31<2:52:35, 5177.94s/it]epochs:  90%|█████████ | 18/20 [26:31:32<2:52:35, 5177.92s/it]
train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-03 00:16:08,205   INFO  Train:   19/20 ( 95%) [   0/3862 (  0%)]  Loss: 0.8584 (0.858)  LR: 6.700e-05  Grad: 17.3757  max=0.6736(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9485(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.4090, loss_cls=0.0691, loss_bbox=0.3804, matched_ious=0.6273, d_time=2.22(2.22), f_time=2.34(2.34), b_time=4.56(4.56)  Time cost: 00:04/4:40:31 [26:31:37/9:21:03]  Acc_iter 69517       Data time: 2.22(2.22)  Forward time: 2.34(2.34)  Batch time: 4.56(4.56)
2025-09-03 00:16:51,658   INFO  Train:   19/20 ( 95%) [  33/3862 (  1%)]  Loss: 0.7628 (0.807)  LR: 6.644e-05  Grad: 17.4033  max=0.6761(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9490(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3579, loss_cls=0.0590, loss_bbox=0.3882, matched_ious=0.5924, d_time=0.01(0.07), f_time=1.26(1.34), b_time=1.27(1.41)  Time cost: 00:47/1:29:44 [26:32:20/3:00:15]  Acc_iter 69550       Data time: 0.01(0.07)  Forward time: 1.26(1.34)  Batch time: 1.27(1.41)
2025-09-03 00:17:57,006   INFO  Train:   19/20 ( 95%) [  83/3862 (  2%)]  Loss: 0.8090 (0.796)  LR: 6.560e-05  Grad: 17.4280  max=0.6757(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9494(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3518, loss_cls=0.0594, loss_bbox=0.3776, matched_ious=0.5933, d_time=0.00(0.03), f_time=1.33(1.32), b_time=1.34(1.35)  Time cost: 01:53/1:24:50 [26:33:26/2:51:33]  Acc_iter 69600       Data time: 0.00(0.03)  Forward time: 1.33(1.32)  Batch time: 1.34(1.35)
2025-09-03 00:19:03,052   INFO  Train:   19/20 ( 95%) [ 133/3862 (  3%)]  Loss: 0.7913 (0.786)  LR: 6.476e-05  Grad: 17.4373  max=0.6679(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9545(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3403, loss_cls=0.0568, loss_bbox=0.3707, matched_ious=0.6013, d_time=0.00(0.02), f_time=1.25(1.32), b_time=1.25(1.34)  Time cost: 02:59/1:23:06 [26:34:32/2:49:11]  Acc_iter 69650       Data time: 0.00(0.02)  Forward time: 1.25(1.32)  Batch time: 1.25(1.34)
2025-09-03 00:20:08,804   INFO  Train:   19/20 ( 95%) [ 183/3862 (  5%)]  Loss: 0.8575 (0.788)  LR: 6.393e-05  Grad: 17.4687  max=0.6697(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9580(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3522, loss_cls=0.0598, loss_bbox=0.3827, matched_ious=0.5955, d_time=0.00(0.02), f_time=1.26(1.31), b_time=1.26(1.33)  Time cost: 04:04/1:21:37 [26:35:37/2:47:19]  Acc_iter 69700       Data time: 0.00(0.02)  Forward time: 1.26(1.31)  Batch time: 1.26(1.33)
2025-09-03 00:21:14,339   INFO  Train:   19/20 ( 95%) [ 233/3862 (  6%)]  Loss: 0.8875 (0.789)  LR: 6.310e-05  Grad: 17.4718  max=0.6789(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9559(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3471, loss_cls=0.0612, loss_bbox=0.3859, matched_ious=0.5872, d_time=0.01(0.02), f_time=1.46(1.31), b_time=1.46(1.33)  Time cost: 05:10/1:20:15 [26:36:43/2:45:39]  Acc_iter 69750       Data time: 0.01(0.02)  Forward time: 1.46(1.31)  Batch time: 1.46(1.33)
2025-09-03 00:22:19,462   INFO  Train:   19/20 ( 95%) [ 283/3862 (  7%)]  Loss: 1.067 (0.788)  LR: 6.228e-05  Grad: 17.5058  max=0.6822(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9596(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3405, loss_cls=0.0588, loss_bbox=0.3824, matched_ious=0.5974, d_time=0.00(0.01), f_time=1.34(1.31), b_time=1.34(1.32)  Time cost: 06:15/1:18:53 [26:37:48/2:44:01]  Acc_iter 69800       Data time: 0.00(0.01)  Forward time: 1.34(1.31)  Batch time: 1.34(1.32)
2025-09-03 00:23:24,581   INFO  Train:   19/20 ( 95%) [ 333/3862 (  9%)]  Loss: 0.6872 (0.783)  LR: 6.146e-05  Grad: 17.5437  max=0.6889(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9593(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3168, loss_cls=0.0565, loss_bbox=0.3841, matched_ious=0.5852, d_time=0.00(0.01), f_time=1.49(1.31), b_time=1.49(1.32)  Time cost: 07:20/1:17:36 [26:38:53/2:42:32]  Acc_iter 69850       Data time: 0.00(0.01)  Forward time: 1.49(1.31)  Batch time: 1.49(1.32)
2025-09-03 00:24:29,836   INFO  Train:   19/20 ( 95%) [ 383/3862 ( 10%)]  Loss: 0.4677 (0.781)  LR: 6.065e-05  Grad: 17.5609  max=0.6946(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9615(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3270, loss_cls=0.0546, loss_bbox=0.3801, matched_ious=0.5884, d_time=0.00(0.01), f_time=1.27(1.31), b_time=1.27(1.32)  Time cost: 08:25/1:16:24 [26:39:58/2:41:13]  Acc_iter 69900       Data time: 0.00(0.01)  Forward time: 1.27(1.31)  Batch time: 1.27(1.32)
2025-09-03 00:25:34,291   INFO  Train:   19/20 ( 95%) [ 433/3862 ( 11%)]  Loss: 0.6277 (0.780)  LR: 5.985e-05  Grad: 17.5691  max=0.6972(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9638(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3296, loss_cls=0.0573, loss_bbox=0.3919, matched_ious=0.5903, d_time=0.01(0.01), f_time=1.31(1.30), b_time=1.32(1.31)  Time cost: 09:30/1:15:07 [26:41:03/2:39:43]  Acc_iter 69950       Data time: 0.01(0.01)  Forward time: 1.31(1.30)  Batch time: 1.32(1.31)
2025-09-03 00:26:39,947   INFO  Train:   19/20 ( 95%) [ 483/3862 ( 13%)]  Loss: 0.8859 (0.778)  LR: 5.904e-05  Grad: 17.5717  max=0.6979(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9659(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3230, loss_cls=0.0553, loss_bbox=0.3805, matched_ious=0.5942, d_time=0.00(0.01), f_time=1.23(1.30), b_time=1.23(1.31)  Time cost: 10:36/1:14:00 [26:42:09/2:38:36]  Acc_iter 70000       Data time: 0.00(0.01)  Forward time: 1.23(1.30)  Batch time: 1.23(1.31)
2025-09-03 00:27:44,842   INFO  Train:   19/20 ( 95%) [ 533/3862 ( 14%)]  Loss: 0.6886 (0.780)  LR: 5.825e-05  Grad: 17.6073  max=0.7164(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9660(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3369, loss_cls=0.0627, loss_bbox=0.3929, matched_ious=0.5921, d_time=0.00(0.01), f_time=1.15(1.30), b_time=1.15(1.31)  Time cost: 11:40/1:12:50 [26:43:13/2:37:19]  Acc_iter 70050       Data time: 0.00(0.01)  Forward time: 1.15(1.30)  Batch time: 1.15(1.31)
2025-09-03 00:28:50,496   INFO  Train:   19/20 ( 95%) [ 583/3862 ( 15%)]  Loss: 0.7323 (0.780)  LR: 5.746e-05  Grad: 17.6185  max=0.7188(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9699(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3445, loss_cls=0.0575, loss_bbox=0.3850, matched_ious=0.6024, d_time=0.01(0.01), f_time=1.46(1.30), b_time=1.48(1.31)  Time cost: 12:46/1:11:44 [26:44:19/2:36:14]  Acc_iter 70100       Data time: 0.01(0.01)  Forward time: 1.46(1.30)  Batch time: 1.48(1.31)
2025-09-03 00:29:55,053   INFO  Train:   19/20 ( 95%) [ 633/3862 ( 16%)]  Loss: 0.5634 (0.779)  LR: 5.667e-05  Grad: 17.6577  max=0.7123(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9705(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3315, loss_cls=0.0565, loss_bbox=0.3812, matched_ious=0.5877, d_time=0.01(0.01), f_time=1.30(1.30), b_time=1.31(1.31)  Time cost: 13:51/1:10:33 [26:45:24/2:34:56]  Acc_iter 70150       Data time: 0.01(0.01)  Forward time: 1.30(1.30)  Batch time: 1.31(1.31)
2025-09-03 00:30:59,815   INFO  Train:   19/20 ( 95%) [ 683/3862 ( 18%)]  Loss: 0.8084 (0.780)  LR: 5.589e-05  Grad: 17.6456  max=0.7029(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9690(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3495, loss_cls=0.0591, loss_bbox=0.3815, matched_ious=0.5866, d_time=0.00(0.01), f_time=1.36(1.30), b_time=1.36(1.31)  Time cost: 14:55/1:09:24 [26:46:28/2:33:42]  Acc_iter 70200       Data time: 0.00(0.01)  Forward time: 1.36(1.30)  Batch time: 1.36(1.31)
2025-09-03 00:32:05,010   INFO  Train:   19/20 ( 95%) [ 733/3862 ( 19%)]  Loss: 0.6604 (0.782)  LR: 5.511e-05  Grad: 17.6572  max=0.7150(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9720(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3536, loss_cls=0.0602, loss_bbox=0.3875, matched_ious=0.5881, d_time=0.01(0.01), f_time=1.24(1.30), b_time=1.24(1.31)  Time cost: 16:01/1:08:17 [26:47:34/2:32:34]  Acc_iter 70250       Data time: 0.01(0.01)  Forward time: 1.24(1.30)  Batch time: 1.24(1.31)
2025-09-03 00:33:09,831   INFO  Train:   19/20 ( 95%) [ 783/3862 ( 20%)]  Loss: 1.093 (0.782)  LR: 5.434e-05  Grad: 17.7021  max=0.7013(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9695(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3350, loss_cls=0.0571, loss_bbox=0.3969, matched_ious=0.5980, d_time=0.00(0.01), f_time=1.19(1.30), b_time=1.20(1.31)  Time cost: 17:05/1:07:09 [26:48:38/2:31:23]  Acc_iter 70300       Data time: 0.00(0.01)  Forward time: 1.19(1.30)  Batch time: 1.20(1.31)
2025-09-03 00:34:15,099   INFO  Train:   19/20 ( 95%) [ 833/3862 ( 22%)]  Loss: 0.7038 (0.781)  LR: 5.358e-05  Grad: 17.7082  max=0.6969(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9722(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3228, loss_cls=0.0563, loss_bbox=0.3919, matched_ious=0.5952, d_time=0.00(0.01), f_time=1.69(1.30), b_time=1.69(1.31)  Time cost: 18:11/1:06:03 [26:49:44/2:30:16]  Acc_iter 70350       Data time: 0.00(0.01)  Forward time: 1.69(1.30)  Batch time: 1.69(1.31)
2025-09-03 00:35:20,052   INFO  Train:   19/20 ( 95%) [ 883/3862 ( 23%)]  Loss: 0.9371 (0.780)  LR: 5.282e-05  Grad: 17.7337  max=0.7063(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9692(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3379, loss_cls=0.0603, loss_bbox=0.3561, matched_ious=0.5993, d_time=0.00(0.01), f_time=1.25(1.30), b_time=1.25(1.31)  Time cost: 19:16/1:04:56 [26:50:49/2:29:07]  Acc_iter 70400       Data time: 0.00(0.01)  Forward time: 1.25(1.30)  Batch time: 1.25(1.31)
2025-09-03 00:36:25,259   INFO  Train:   19/20 ( 95%) [ 933/3862 ( 24%)]  Loss: 0.9722 (0.782)  LR: 5.206e-05  Grad: 17.7193  max=0.7091(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9696(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3487, loss_cls=0.0581, loss_bbox=0.4168, matched_ious=0.5915, d_time=0.00(0.01), f_time=1.25(1.30), b_time=1.26(1.31)  Time cost: 20:21/1:03:50 [26:51:54/2:28:00]  Acc_iter 70450       Data time: 0.00(0.01)  Forward time: 1.25(1.30)  Batch time: 1.26(1.31)
2025-09-03 00:37:31,299   INFO  Train:   19/20 ( 95%) [ 983/3862 ( 25%)]  Loss: 0.8013 (0.783)  LR: 5.131e-05  Grad: 17.7546  max=0.7148(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9707(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3431, loss_cls=0.0602, loss_bbox=0.3877, matched_ious=0.5906, d_time=0.01(0.01), f_time=1.28(1.30), b_time=1.29(1.31)  Time cost: 21:27/1:02:46 [26:53:00/2:26:59]  Acc_iter 70500       Data time: 0.01(0.01)  Forward time: 1.28(1.30)  Batch time: 1.29(1.31)
2025-09-03 00:38:35,918   INFO  Train:   19/20 ( 95%) [1033/3862 ( 27%)]  Loss: 0.9180 (0.783)  LR: 5.057e-05  Grad: 17.8201  max=1.0806(module.vfe.pfn_layers.0.linear.weight)  min: -0.9734(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3420, loss_cls=0.0576, loss_bbox=0.3927, matched_ious=0.5925, d_time=0.00(0.01), f_time=1.49(1.30), b_time=1.49(1.31)  Time cost: 22:32/1:01:39 [26:54:04/2:25:49]  Acc_iter 70550       Data time: 0.00(0.01)  Forward time: 1.49(1.30)  Batch time: 1.49(1.31)
2025-09-03 00:39:40,381   INFO  Train:   19/20 ( 95%) [1083/3862 ( 28%)]  Loss: 1.011 (0.784)  LR: 4.983e-05  Grad: 17.7665  max=0.6928(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9711(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3401, loss_cls=0.0591, loss_bbox=0.4078, matched_ious=0.5847, d_time=0.01(0.01), f_time=1.24(1.30), b_time=1.25(1.31)  Time cost: 23:36/1:00:31 [26:55:09/2:24:38]  Acc_iter 70600       Data time: 0.01(0.01)  Forward time: 1.24(1.30)  Batch time: 1.25(1.31)
2025-09-03 00:40:44,621   INFO  Train:   19/20 ( 95%) [1133/3862 ( 29%)]  Loss: 0.7034 (0.784)  LR: 4.909e-05  Grad: 17.8048  max=0.7048(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9718(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3440, loss_cls=0.0596, loss_bbox=0.3819, matched_ious=0.5908, d_time=0.00(0.01), f_time=1.27(1.30), b_time=1.27(1.31)  Time cost: 24:40/59:23 [26:56:13/2:23:26]  Acc_iter 70650       Data time: 0.00(0.01)  Forward time: 1.27(1.30)  Batch time: 1.27(1.31)
2025-09-03 00:41:50,415   INFO  Train:   19/20 ( 95%) [1183/3862 ( 31%)]  Loss: 0.7876 (0.784)  LR: 4.836e-05  Grad: 17.8314  max=0.7043(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9743(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3446, loss_cls=0.0581, loss_bbox=0.3812, matched_ious=0.5894, d_time=0.00(0.01), f_time=1.26(1.30), b_time=1.26(1.31)  Time cost: 25:46/58:19 [26:57:19/2:22:24]  Acc_iter 70700       Data time: 0.00(0.01)  Forward time: 1.26(1.30)  Batch time: 1.26(1.31)
2025-09-03 00:42:55,235   INFO  Train:   19/20 ( 95%) [1233/3862 ( 32%)]  Loss: 0.8564 (0.785)  LR: 4.764e-05  Grad: 17.8329  max=0.6976(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9771(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3453, loss_cls=0.0611, loss_bbox=0.3851, matched_ious=0.5895, d_time=0.00(0.01), f_time=1.41(1.30), b_time=1.42(1.31)  Time cost: 26:51/57:13 [26:58:24/2:21:16]  Acc_iter 70750       Data time: 0.00(0.01)  Forward time: 1.41(1.30)  Batch time: 1.42(1.31)
2025-09-03 00:43:59,285   INFO  Train:   19/20 ( 95%) [1283/3862 ( 33%)]  Loss: 0.8559 (0.785)  LR: 4.692e-05  Grad: 17.8447  max=0.7049(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9752(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3506, loss_cls=0.0581, loss_bbox=0.3957, matched_ious=0.5869, d_time=0.00(0.01), f_time=1.34(1.30), b_time=1.35(1.31)  Time cost: 27:55/56:05 [26:59:28/2:20:04]  Acc_iter 70800       Data time: 0.00(0.01)  Forward time: 1.34(1.30)  Batch time: 1.35(1.31)
2025-09-03 00:45:04,407   INFO  Train:   19/20 ( 95%) [1333/3862 ( 35%)]  Loss: 0.7273 (0.786)  LR: 4.620e-05  Grad: 17.8950  max=0.7031(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9801(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3397, loss_cls=0.0596, loss_bbox=0.3933, matched_ious=0.5979, d_time=0.00(0.01), f_time=1.33(1.30), b_time=1.33(1.30)  Time cost: 29:00/54:59 [27:00:33/2:18:58]  Acc_iter 70850       Data time: 0.00(0.01)  Forward time: 1.33(1.30)  Batch time: 1.33(1.30)
2025-09-03 00:46:09,984   INFO  Train:   19/20 ( 95%) [1383/3862 ( 36%)]  Loss: 0.7286 (0.785)  LR: 4.549e-05  Grad: 17.8998  max=0.7000(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9779(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3322, loss_cls=0.0576, loss_bbox=0.3787, matched_ious=0.5907, d_time=0.00(0.01), f_time=1.38(1.30), b_time=1.38(1.31)  Time cost: 30:06/53:55 [27:01:39/2:17:55]  Acc_iter 70900       Data time: 0.00(0.01)  Forward time: 1.38(1.30)  Batch time: 1.38(1.31)
2025-09-03 00:47:16,243   INFO  Train:   19/20 ( 95%) [1433/3862 ( 37%)]  Loss: 0.6094 (0.785)  LR: 4.479e-05  Grad: 17.9431  max=0.7110(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9864(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3500, loss_cls=0.0589, loss_bbox=0.3671, matched_ious=0.5952, d_time=0.00(0.01), f_time=1.28(1.30), b_time=1.29(1.31)  Time cost: 31:12/52:51 [27:02:45/2:16:54]  Acc_iter 70950       Data time: 0.00(0.01)  Forward time: 1.28(1.30)  Batch time: 1.29(1.31)
2025-09-03 00:48:20,551   INFO  Train:   19/20 ( 95%) [1483/3862 ( 38%)]  Loss: 0.6263 (0.785)  LR: 4.409e-05  Grad: 17.9666  max=0.6977(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9876(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3232, loss_cls=0.0558, loss_bbox=0.4142, matched_ious=0.6041, d_time=0.01(0.01), f_time=1.27(1.30), b_time=1.28(1.31)  Time cost: 32:16/51:44 [27:03:49/2:15:44]  Acc_iter 71000       Data time: 0.01(0.01)  Forward time: 1.27(1.30)  Batch time: 1.28(1.31)
2025-09-03 00:49:26,270   INFO  Train:   19/20 ( 95%) [1533/3862 ( 40%)]  Loss: 1.208 (0.785)  LR: 4.340e-05  Grad: 17.9828  max=0.6846(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9907(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3357, loss_cls=0.0587, loss_bbox=0.3996, matched_ious=0.5888, d_time=0.00(0.01), f_time=1.30(1.30), b_time=1.31(1.31)  Time cost: 33:22/50:40 [27:04:55/2:14:41]  Acc_iter 71050       Data time: 0.00(0.01)  Forward time: 1.30(1.30)  Batch time: 1.31(1.31)
2025-09-03 00:50:30,468   INFO  Train:   19/20 ( 95%) [1583/3862 ( 41%)]  Loss: 0.6609 (0.784)  LR: 4.271e-05  Grad: 18.0425  max=0.6901(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9924(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3146, loss_cls=0.0541, loss_bbox=0.3644, matched_ious=0.5879, d_time=0.00(0.01), f_time=1.21(1.30), b_time=1.21(1.30)  Time cost: 34:26/49:33 [27:05:59/2:13:32]  Acc_iter 71100       Data time: 0.00(0.01)  Forward time: 1.21(1.30)  Batch time: 1.21(1.30)
2025-09-03 00:51:36,363   INFO  Train:   19/20 ( 95%) [1633/3862 ( 42%)]  Loss: 0.4876 (0.784)  LR: 4.203e-05  Grad: 18.0457  max=0.6916(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9944(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3317, loss_cls=0.0589, loss_bbox=0.4121, matched_ious=0.5912, d_time=0.00(0.01), f_time=1.37(1.30), b_time=1.37(1.31)  Time cost: 35:32/48:29 [27:07:05/2:12:29]  Acc_iter 71150       Data time: 0.00(0.01)  Forward time: 1.37(1.30)  Batch time: 1.37(1.31)
2025-09-03 00:52:41,212   INFO  Train:   19/20 ( 95%) [1683/3862 ( 44%)]  Loss: 0.8978 (0.784)  LR: 4.135e-05  Grad: 18.1118  max=0.6870(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9956(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3430, loss_cls=0.0585, loss_bbox=0.3768, matched_ious=0.5857, d_time=0.00(0.01), f_time=1.20(1.30), b_time=1.21(1.30)  Time cost: 36:37/47:23 [27:08:10/2:11:22]  Acc_iter 71200       Data time: 0.00(0.01)  Forward time: 1.20(1.30)  Batch time: 1.21(1.30)
2025-09-03 00:53:46,331   INFO  Train:   19/20 ( 95%) [1733/3862 ( 45%)]  Loss: 0.6090 (0.784)  LR: 4.068e-05  Grad: 18.1068  max=0.6994(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0004(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3327, loss_cls=0.0597, loss_bbox=0.3951, matched_ious=0.5873, d_time=0.00(0.01), f_time=1.20(1.30), b_time=1.21(1.30)  Time cost: 37:42/46:17 [27:09:15/2:10:16]  Acc_iter 71250       Data time: 0.00(0.01)  Forward time: 1.20(1.30)  Batch time: 1.21(1.30)
2025-09-03 00:54:51,254   INFO  Train:   19/20 ( 95%) [1783/3862 ( 46%)]  Loss: 0.7639 (0.782)  LR: 4.001e-05  Grad: 18.1351  max=0.6769(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0015(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3195, loss_cls=0.0564, loss_bbox=0.3386, matched_ious=0.6062, d_time=0.00(0.01), f_time=1.21(1.30), b_time=1.21(1.30)  Time cost: 38:47/45:12 [27:10:20/2:09:10]  Acc_iter 71300       Data time: 0.00(0.01)  Forward time: 1.21(1.30)  Batch time: 1.21(1.30)
2025-09-03 00:55:56,226   INFO  Train:   19/20 ( 95%) [1833/3862 ( 47%)]  Loss: 0.6000 (0.782)  LR: 3.935e-05  Grad: 18.1666  max=0.6929(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0015(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3207, loss_cls=0.0562, loss_bbox=0.3844, matched_ious=0.6010, d_time=0.00(0.01), f_time=1.40(1.30), b_time=1.40(1.30)  Time cost: 39:52/44:06 [27:11:25/2:08:04]  Acc_iter 71350       Data time: 0.00(0.01)  Forward time: 1.40(1.30)  Batch time: 1.40(1.30)
2025-09-03 00:57:01,095   INFO  Train:   19/20 ( 95%) [1883/3862 ( 49%)]  Loss: 0.7639 (0.780)  LR: 3.869e-05  Grad: 18.1303  max=0.7027(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0012(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3257, loss_cls=0.0559, loss_bbox=0.3590, matched_ious=0.5919, d_time=0.01(0.01), f_time=1.20(1.30), b_time=1.21(1.30)  Time cost: 40:57/43:01 [27:12:30/2:06:58]  Acc_iter 71400       Data time: 0.01(0.01)  Forward time: 1.20(1.30)  Batch time: 1.21(1.30)
2025-09-03 00:58:06,751   INFO  Train:   19/20 ( 95%) [1933/3862 ( 50%)]  Loss: 0.8793 (0.780)  LR: 3.804e-05  Grad: 18.1578  max=0.7126(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0035(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3338, loss_cls=0.0594, loss_bbox=0.3816, matched_ious=0.5929, d_time=0.00(0.01), f_time=1.35(1.30), b_time=1.35(1.30)  Time cost: 42:02/41:56 [27:13:35/2:05:54]  Acc_iter 71450       Data time: 0.00(0.01)  Forward time: 1.35(1.30)  Batch time: 1.35(1.30)
2025-09-03 00:59:11,528   INFO  Train:   19/20 ( 95%) [1983/3862 ( 51%)]  Loss: 0.7637 (0.781)  LR: 3.740e-05  Grad: 18.1719  max=0.7115(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0046(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3370, loss_cls=0.0582, loss_bbox=0.4077, matched_ious=0.5919, d_time=0.01(0.01), f_time=1.23(1.30), b_time=1.24(1.30)  Time cost: 43:07/40:50 [27:14:40/2:04:47]  Acc_iter 71500       Data time: 0.01(0.01)  Forward time: 1.23(1.30)  Batch time: 1.24(1.30)
2025-09-03 01:00:17,485   INFO  Train:   19/20 ( 95%) [2033/3862 ( 53%)]  Loss: 0.7020 (0.782)  LR: 3.676e-05  Grad: 18.1975  max=0.6925(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0018(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3627, loss_cls=0.0620, loss_bbox=0.4017, matched_ious=0.5991, d_time=0.01(0.01), f_time=1.37(1.30), b_time=1.38(1.30)  Time cost: 44:13/39:46 [27:15:46/2:03:44]  Acc_iter 71550       Data time: 0.01(0.01)  Forward time: 1.37(1.30)  Batch time: 1.38(1.30)
2025-09-03 01:01:22,425   INFO  Train:   19/20 ( 95%) [2083/3862 ( 54%)]  Loss: 1.058 (0.782)  LR: 3.612e-05  Grad: 18.2212  max=0.6948(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0059(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3366, loss_cls=0.0576, loss_bbox=0.3865, matched_ious=0.5949, d_time=0.00(0.01), f_time=1.22(1.30), b_time=1.23(1.30)  Time cost: 45:18/38:40 [27:16:51/2:02:38]  Acc_iter 71600       Data time: 0.00(0.01)  Forward time: 1.22(1.30)  Batch time: 1.23(1.30)
2025-09-03 01:02:28,111   INFO  Train:   19/20 ( 95%) [2133/3862 ( 55%)]  Loss: 0.7850 (0.782)  LR: 3.549e-05  Grad: 18.2165  max=0.6851(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0040(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3394, loss_cls=0.0581, loss_bbox=0.3757, matched_ious=0.5989, d_time=0.01(0.01), f_time=1.29(1.30), b_time=1.30(1.30)  Time cost: 46:24/37:35 [27:17:57/2:01:34]  Acc_iter 71650       Data time: 0.01(0.01)  Forward time: 1.29(1.30)  Batch time: 1.30(1.30)
2025-09-03 01:03:33,170   INFO  Train:   19/20 ( 95%) [2183/3862 ( 57%)]  Loss: 0.8275 (0.781)  LR: 3.487e-05  Grad: 18.2537  max=0.6888(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0085(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3329, loss_cls=0.0569, loss_bbox=0.3563, matched_ious=0.5994, d_time=0.01(0.01), f_time=1.18(1.30), b_time=1.18(1.30)  Time cost: 47:29/36:30 [27:19:02/2:00:28]  Acc_iter 71700       Data time: 0.01(0.01)  Forward time: 1.18(1.30)  Batch time: 1.18(1.30)
2025-09-03 01:04:37,831   INFO  Train:   19/20 ( 95%) [2233/3862 ( 58%)]  Loss: 1.184 (0.780)  LR: 3.425e-05  Grad: 18.2854  max=0.7059(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0073(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3065, loss_cls=0.0553, loss_bbox=0.3574, matched_ious=0.6041, d_time=0.00(0.01), f_time=1.30(1.30), b_time=1.31(1.30)  Time cost: 48:33/35:24 [27:20:06/1:59:22]  Acc_iter 71750       Data time: 0.00(0.01)  Forward time: 1.30(1.30)  Batch time: 1.31(1.30)
2025-09-03 01:05:42,276   INFO  Train:   19/20 ( 95%) [2283/3862 ( 59%)]  Loss: 0.9647 (0.779)  LR: 3.364e-05  Grad: 18.3157  max=0.6945(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0120(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3398, loss_cls=0.0586, loss_bbox=0.3680, matched_ious=0.5984, d_time=0.00(0.01), f_time=1.55(1.30), b_time=1.56(1.30)  Time cost: 49:38/34:19 [27:21:11/1:58:15]  Acc_iter 71800       Data time: 0.00(0.01)  Forward time: 1.55(1.30)  Batch time: 1.56(1.30)
2025-09-03 01:06:47,168   INFO  Train:   19/20 ( 95%) [2333/3862 ( 60%)]  Loss: 0.7033 (0.778)  LR: 3.303e-05  Grad: 18.3386  max=0.7078(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0122(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3239, loss_cls=0.0544, loss_bbox=0.3570, matched_ious=0.5974, d_time=0.00(0.01), f_time=1.22(1.30), b_time=1.22(1.30)  Time cost: 50:43/33:13 [27:22:16/1:57:09]  Acc_iter 71850       Data time: 0.00(0.01)  Forward time: 1.22(1.30)  Batch time: 1.22(1.30)
2025-09-03 01:07:52,654   INFO  Train:   19/20 ( 95%) [2383/3862 ( 62%)]  Loss: 0.8660 (0.778)  LR: 3.242e-05  Grad: 18.3263  max=0.7154(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0159(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3265, loss_cls=0.0553, loss_bbox=0.3650, matched_ious=0.6015, d_time=0.01(0.01), f_time=1.15(1.30), b_time=1.16(1.30)  Time cost: 51:48/32:08 [27:23:21/1:56:04]  Acc_iter 71900       Data time: 0.01(0.01)  Forward time: 1.15(1.30)  Batch time: 1.16(1.30)
2025-09-03 01:08:57,968   INFO  Train:   19/20 ( 95%) [2433/3862 ( 63%)]  Loss: 0.5682 (0.778)  LR: 3.183e-05  Grad: 18.3599  max=0.7098(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0193(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3458, loss_cls=0.0610, loss_bbox=0.3825, matched_ious=0.5992, d_time=0.00(0.01), f_time=1.17(1.30), b_time=1.18(1.30)  Time cost: 52:54/31:03 [27:24:27/1:54:59]  Acc_iter 71950       Data time: 0.00(0.01)  Forward time: 1.17(1.30)  Batch time: 1.18(1.30)
2025-09-03 01:10:03,421   INFO  Train:   19/20 ( 95%) [2483/3862 ( 64%)]  Loss: 1.057 (0.778)  LR: 3.124e-05  Grad: 18.4110  max=0.7129(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0252(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3296, loss_cls=0.0567, loss_bbox=0.4035, matched_ious=0.5878, d_time=0.00(0.01), f_time=1.23(1.30), b_time=1.24(1.30)  Time cost: 53:59/29:58 [27:25:32/1:53:55]  Acc_iter 72000       Data time: 0.00(0.01)  Forward time: 1.23(1.30)  Batch time: 1.24(1.30)
2025-09-03 01:11:08,321   INFO  Train:   19/20 ( 95%) [2533/3862 ( 66%)]  Loss: 0.9576 (0.779)  LR: 3.065e-05  Grad: 18.4397  max=0.6975(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0308(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3598, loss_cls=0.0626, loss_bbox=0.3806, matched_ious=0.5905, d_time=0.00(0.01), f_time=1.27(1.30), b_time=1.27(1.30)  Time cost: 55:04/28:53 [27:26:37/1:52:49]  Acc_iter 72050       Data time: 0.00(0.01)  Forward time: 1.27(1.30)  Batch time: 1.27(1.30)
2025-09-03 01:12:13,741   INFO  Train:   19/20 ( 95%) [2583/3862 ( 67%)]  Loss: 0.7611 (0.778)  LR: 3.007e-05  Grad: 18.4327  max=0.7193(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0302(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3192, loss_cls=0.0530, loss_bbox=0.3644, matched_ious=0.6003, d_time=0.00(0.01), f_time=1.27(1.30), b_time=1.27(1.30)  Time cost: 56:09/27:47 [27:27:42/1:51:44]  Acc_iter 72100       Data time: 0.00(0.01)  Forward time: 1.27(1.30)  Batch time: 1.27(1.30)
2025-09-03 01:13:18,945   INFO  Train:   19/20 ( 95%) [2633/3862 ( 68%)]  Loss: 0.8040 (0.777)  LR: 2.949e-05  Grad: 18.4570  max=0.7279(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0310(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3162, loss_cls=0.0557, loss_bbox=0.3588, matched_ious=0.5962, d_time=0.00(0.01), f_time=1.30(1.30), b_time=1.30(1.30)  Time cost: 57:15/26:42 [27:28:48/1:50:39]  Acc_iter 72150       Data time: 0.00(0.01)  Forward time: 1.30(1.30)  Batch time: 1.30(1.30)
2025-09-03 01:14:23,974   INFO  Train:   19/20 ( 95%) [2683/3862 ( 69%)]  Loss: 0.6885 (0.777)  LR: 2.892e-05  Grad: 18.4607  max=0.7271(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0321(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3290, loss_cls=0.0577, loss_bbox=0.3942, matched_ious=0.5984, d_time=0.00(0.01), f_time=1.19(1.30), b_time=1.20(1.30)  Time cost: 58:20/25:37 [27:29:53/1:49:33]  Acc_iter 72200       Data time: 0.00(0.01)  Forward time: 1.19(1.30)  Batch time: 1.20(1.30)
2025-09-03 01:15:28,784   INFO  Train:   19/20 ( 95%) [2733/3862 ( 71%)]  Loss: 1.003 (0.777)  LR: 2.836e-05  Grad: 18.4993  max=0.7176(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0371(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3380, loss_cls=0.0576, loss_bbox=0.3685, matched_ious=0.6005, d_time=0.00(0.01), f_time=1.30(1.30), b_time=1.30(1.30)  Time cost: 59:24/24:32 [27:30:57/1:48:27]  Acc_iter 72250       Data time: 0.00(0.01)  Forward time: 1.30(1.30)  Batch time: 1.30(1.30)
2025-09-03 01:16:34,077   INFO  Train:   19/20 ( 95%) [2783/3862 ( 72%)]  Loss: 0.8291 (0.777)  LR: 2.780e-05  Grad: 18.4981  max=0.7087(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0415(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3314, loss_cls=0.0564, loss_bbox=0.3842, matched_ious=0.5970, d_time=0.01(0.01), f_time=1.22(1.30), b_time=1.23(1.30)  Time cost: 1:00:30/23:26 [27:32:03/1:47:22]  Acc_iter 72300       Data time: 0.01(0.01)  Forward time: 1.22(1.30)  Batch time: 1.23(1.30)
2025-09-03 01:17:39,438   INFO  Train:   19/20 ( 95%) [2833/3862 ( 73%)]  Loss: 0.7954 (0.776)  LR: 2.724e-05  Grad: 18.5264  max=0.7168(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0401(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3378, loss_cls=0.0573, loss_bbox=0.3617, matched_ious=0.5961, d_time=0.01(0.01), f_time=1.31(1.30), b_time=1.31(1.30)  Time cost: 1:01:35/22:21 [27:33:08/1:46:17]  Acc_iter 72350       Data time: 0.01(0.01)  Forward time: 1.31(1.30)  Batch time: 1.31(1.30)
2025-09-03 01:18:44,549   INFO  Train:   19/20 ( 95%) [2883/3862 ( 75%)]  Loss: 0.5304 (0.777)  LR: 2.669e-05  Grad: 18.5278  max=0.7204(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0428(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3347, loss_cls=0.0594, loss_bbox=0.3994, matched_ious=0.5917, d_time=0.00(0.01), f_time=1.23(1.30), b_time=1.23(1.30)  Time cost: 1:02:40/21:16 [27:34:13/1:45:12]  Acc_iter 72400       Data time: 0.00(0.01)  Forward time: 1.23(1.30)  Batch time: 1.23(1.30)
2025-09-03 01:19:50,014   INFO  Train:   19/20 ( 95%) [2933/3862 ( 76%)]  Loss: 0.7654 (0.776)  LR: 2.615e-05  Grad: 18.5590  max=0.7212(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0457(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3319, loss_cls=0.0579, loss_bbox=0.3686, matched_ious=0.6058, d_time=0.01(0.01), f_time=1.37(1.30), b_time=1.38(1.30)  Time cost: 1:03:46/20:11 [27:35:19/1:44:07]  Acc_iter 72450       Data time: 0.01(0.01)  Forward time: 1.37(1.30)  Batch time: 1.38(1.30)
2025-09-03 01:20:54,981   INFO  Train:   19/20 ( 95%) [2983/3862 ( 77%)]  Loss: 0.8364 (0.777)  LR: 2.561e-05  Grad: 18.5712  max=0.7253(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0477(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3743, loss_cls=0.0631, loss_bbox=0.3936, matched_ious=0.5967, d_time=0.00(0.01), f_time=1.49(1.30), b_time=1.50(1.30)  Time cost: 1:04:51/19:06 [27:36:24/1:43:02]  Acc_iter 72500       Data time: 0.00(0.01)  Forward time: 1.49(1.30)  Batch time: 1.50(1.30)
2025-09-03 01:22:00,391   INFO  Train:   19/20 ( 95%) [3033/3862 ( 79%)]  Loss: 0.6476 (0.777)  LR: 2.508e-05  Grad: 18.5930  max=0.7269(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0482(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3387, loss_cls=0.0582, loss_bbox=0.3808, matched_ious=0.5950, d_time=0.01(0.01), f_time=1.28(1.30), b_time=1.29(1.30)  Time cost: 1:05:56/18:01 [27:37:29/1:41:57]  Acc_iter 72550       Data time: 0.01(0.01)  Forward time: 1.28(1.30)  Batch time: 1.29(1.30)
2025-09-03 01:23:05,752   INFO  Train:   19/20 ( 95%) [3083/3862 ( 80%)]  Loss: 1.182 (0.778)  LR: 2.455e-05  Grad: 18.6044  max=0.7313(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0465(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3584, loss_cls=0.0628, loss_bbox=0.4213, matched_ious=0.5895, d_time=0.01(0.01), f_time=1.18(1.30), b_time=1.18(1.30)  Time cost: 1:07:01/16:55 [27:38:34/1:40:52]  Acc_iter 72600       Data time: 0.01(0.01)  Forward time: 1.18(1.30)  Batch time: 1.18(1.30)
2025-09-03 01:24:10,625   INFO  Train:   19/20 ( 95%) [3133/3862 ( 81%)]  Loss: 1.252 (0.778)  LR: 2.403e-05  Grad: 18.6239  max=0.7337(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0480(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3428, loss_cls=0.0565, loss_bbox=0.3872, matched_ious=0.5970, d_time=0.00(0.01), f_time=1.29(1.30), b_time=1.30(1.30)  Time cost: 1:08:06/15:50 [27:39:39/1:39:46]  Acc_iter 72650       Data time: 0.00(0.01)  Forward time: 1.29(1.30)  Batch time: 1.30(1.30)
2025-09-03 01:25:16,444   INFO  Train:   19/20 ( 95%) [3183/3862 ( 82%)]  Loss: 0.7170 (0.778)  LR: 2.351e-05  Grad: 18.6332  max=0.7403(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0486(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3272, loss_cls=0.0565, loss_bbox=0.3641, matched_ious=0.6028, d_time=0.00(0.01), f_time=1.39(1.30), b_time=1.40(1.30)  Time cost: 1:09:12/14:45 [27:40:45/1:38:42]  Acc_iter 72700       Data time: 0.00(0.01)  Forward time: 1.39(1.30)  Batch time: 1.40(1.30)
2025-09-03 01:26:21,035   INFO  Train:   19/20 ( 95%) [3233/3862 ( 84%)]  Loss: 0.9822 (0.778)  LR: 2.300e-05  Grad: 18.6397  max=0.7200(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0515(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3316, loss_cls=0.0585, loss_bbox=0.3961, matched_ious=0.5904, d_time=0.01(0.01), f_time=1.43(1.30), b_time=1.44(1.30)  Time cost: 1:10:17/13:40 [27:41:50/1:37:36]  Acc_iter 72750       Data time: 0.01(0.01)  Forward time: 1.43(1.30)  Batch time: 1.44(1.30)
2025-09-03 01:27:26,311   INFO  Train:   19/20 ( 95%) [3283/3862 ( 85%)]  Loss: 0.6307 (0.777)  LR: 2.250e-05  Grad: 18.6240  max=0.7506(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0482(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3038, loss_cls=0.0548, loss_bbox=0.3499, matched_ious=0.5994, d_time=0.00(0.01), f_time=1.17(1.30), b_time=1.18(1.30)  Time cost: 1:11:22/12:35 [27:42:55/1:36:31]  Acc_iter 72800       Data time: 0.00(0.01)  Forward time: 1.17(1.30)  Batch time: 1.18(1.30)
2025-09-03 01:28:32,133   INFO  Train:   19/20 ( 95%) [3333/3862 ( 86%)]  Loss: 0.8759 (0.777)  LR: 2.200e-05  Grad: 18.6567  max=0.7436(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0489(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3525, loss_cls=0.0596, loss_bbox=0.3572, matched_ious=0.6003, d_time=0.00(0.01), f_time=1.34(1.30), b_time=1.34(1.30)  Time cost: 1:12:28/11:29 [27:44:01/1:35:26]  Acc_iter 72850       Data time: 0.00(0.01)  Forward time: 1.34(1.30)  Batch time: 1.34(1.30)
2025-09-03 01:29:37,355   INFO  Train:   19/20 ( 95%) [3383/3862 ( 88%)]  Loss: 0.7565 (0.777)  LR: 2.150e-05  Grad: 18.6699  max=0.7577(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0516(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3293, loss_cls=0.0569, loss_bbox=0.3695, matched_ious=0.5922, d_time=0.03(0.01), f_time=1.28(1.30), b_time=1.30(1.30)  Time cost: 1:13:33/10:24 [27:45:06/1:34:21]  Acc_iter 72900       Data time: 0.03(0.01)  Forward time: 1.28(1.30)  Batch time: 1.30(1.30)
2025-09-03 01:30:42,284   INFO  Train:   19/20 ( 95%) [3433/3862 ( 89%)]  Loss: 0.5257 (0.776)  LR: 2.101e-05  Grad: 18.7248  max=0.7702(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0526(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3164, loss_cls=0.0552, loss_bbox=0.3848, matched_ious=0.5975, d_time=0.00(0.01), f_time=1.38(1.30), b_time=1.38(1.30)  Time cost: 1:14:38/09:19 [27:46:11/1:33:16]  Acc_iter 72950       Data time: 0.00(0.01)  Forward time: 1.38(1.30)  Batch time: 1.38(1.30)
2025-09-03 01:31:48,049   INFO  Train:   19/20 ( 95%) [3483/3862 ( 90%)]  Loss: 1.294 (0.776)  LR: 2.053e-05  Grad: 18.7524  max=0.7674(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0516(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3450, loss_cls=0.0594, loss_bbox=0.3825, matched_ious=0.5987, d_time=0.00(0.01), f_time=1.30(1.30), b_time=1.30(1.30)  Time cost: 1:15:44/08:14 [27:47:17/1:32:11]  Acc_iter 73000       Data time: 0.00(0.01)  Forward time: 1.30(1.30)  Batch time: 1.30(1.30)
2025-09-03 01:32:52,532   INFO  Train:   19/20 ( 95%) [3533/3862 ( 91%)]  Loss: 0.7714 (0.776)  LR: 2.005e-05  Grad: 18.7547  max=0.7714(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0485(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3467, loss_cls=0.0612, loss_bbox=0.3663, matched_ious=0.5879, d_time=0.01(0.01), f_time=1.22(1.30), b_time=1.23(1.30)  Time cost: 1:16:48/07:09 [27:48:21/1:31:05]  Acc_iter 73050       Data time: 0.01(0.01)  Forward time: 1.22(1.30)  Batch time: 1.23(1.30)
2025-09-03 01:33:57,930   INFO  Train:   19/20 ( 95%) [3583/3862 ( 93%)]  Loss: 1.077 (0.776)  LR: 1.958e-05  Grad: 18.7493  max=0.7758(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0502(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3263, loss_cls=0.0576, loss_bbox=0.3695, matched_ious=0.5945, d_time=0.00(0.01), f_time=1.43(1.30), b_time=1.43(1.30)  Time cost: 1:17:54/06:03 [27:49:26/1:30:00]  Acc_iter 73100       Data time: 0.00(0.01)  Forward time: 1.43(1.30)  Batch time: 1.43(1.30)
2025-09-03 01:35:02,144   INFO  Train:   19/20 ( 95%) [3633/3862 ( 94%)]  Loss: 0.4653 (0.776)  LR: 1.911e-05  Grad: 18.7741  max=0.7608(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0514(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3488, loss_cls=0.0583, loss_bbox=0.3922, matched_ious=0.5919, d_time=0.00(0.01), f_time=1.22(1.30), b_time=1.22(1.30)  Time cost: 1:18:58/04:58 [27:50:31/1:28:54]  Acc_iter 73150       Data time: 0.00(0.01)  Forward time: 1.22(1.30)  Batch time: 1.22(1.30)
2025-09-03 01:36:07,456   INFO  Train:   19/20 ( 95%) [3683/3862 ( 95%)]  Loss: 0.8268 (0.776)  LR: 1.865e-05  Grad: 18.7948  max=0.8022(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0537(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3348, loss_cls=0.0590, loss_bbox=0.3773, matched_ious=0.5892, d_time=0.01(0.01), f_time=1.27(1.30), b_time=1.28(1.30)  Time cost: 1:20:03/03:53 [27:51:36/1:27:49]  Acc_iter 73200       Data time: 0.01(0.01)  Forward time: 1.27(1.30)  Batch time: 1.28(1.30)
2025-09-03 01:37:12,991   INFO  Train:   19/20 ( 95%) [3733/3862 ( 97%)]  Loss: 0.6457 (0.776)  LR: 1.820e-05  Grad: 18.7868  max=0.7820(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0537(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3345, loss_cls=0.0578, loss_bbox=0.3701, matched_ious=0.5890, d_time=0.00(0.01), f_time=1.28(1.30), b_time=1.28(1.30)  Time cost: 1:21:09/02:48 [27:52:42/1:26:44]  Acc_iter 73250       Data time: 0.00(0.01)  Forward time: 1.28(1.30)  Batch time: 1.28(1.30)
2025-09-03 01:38:18,389   INFO  Train:   19/20 ( 95%) [3783/3862 ( 98%)]  Loss: 0.7519 (0.776)  LR: 1.775e-05  Grad: 18.8170  max=0.7742(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0555(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3323, loss_cls=0.0597, loss_bbox=0.4055, matched_ious=0.5983, d_time=0.01(0.01), f_time=1.29(1.30), b_time=1.30(1.30)  Time cost: 1:22:14/01:43 [27:53:47/1:25:39]  Acc_iter 73300       Data time: 0.01(0.01)  Forward time: 1.29(1.30)  Batch time: 1.30(1.30)
2025-09-03 01:39:23,487   INFO  Train:   19/20 ( 95%) [3833/3862 ( 99%)]  Loss: 1.055 (0.776)  LR: 1.730e-05  Grad: 18.8529  max=0.7897(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0564(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3352, loss_cls=0.0588, loss_bbox=0.3592, matched_ious=0.5954, d_time=0.00(0.01), f_time=1.25(1.30), b_time=1.25(1.30)  Time cost: 1:23:19/00:37 [27:54:52/1:24:33]  Acc_iter 73350       Data time: 0.00(0.01)  Forward time: 1.25(1.30)  Batch time: 1.25(1.30)
2025-09-03 01:39:58,780   INFO  Train:   19/20 ( 95%) [3861/3862 (100%)]  Loss: 1.044 (0.776)  LR: 1.706e-05  Grad: 18.8732  max=0.8124(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0620(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3348, loss_cls=0.0596, loss_bbox=0.3962, matched_ious=0.6002, d_time=0.00(0.01), f_time=1.25(1.30), b_time=1.25(1.30)  Time cost: 1:23:54/00:01 [27:55:27/1:23:56]  Acc_iter 73378       Data time: 0.00(0.01)  Forward time: 1.25(1.30)  Batch time: 1.25(1.30)

                                               [Aepochs:  95%|█████████▌| 19/20 [27:55:28<1:25:35, 5135.52s/it]epochs:  95%|█████████▌| 19/20 [27:55:28<1:25:35, 5135.55s/it]epochs:  95%|█████████▌| 19/20 [27:55:28<1:25:35, 5135.52s/it]epochs:  95%|█████████▌| 19/20 [27:55:28<1:25:35, 5135.52s/it]epochs:  95%|█████████▌| 19/20 [27:55:28<1:25:35, 5135.52s/it]epochs:  95%|█████████▌| 19/20 [27:55:28<1:25:35, 5135.52s/it]epochs:  95%|█████████▌| 19/20 [27:55:28<1:25:35, 5135.53s/it]epochs:  95%|█████████▌| 19/20 [27:55:29<1:25:35, 5135.59s/it]
train:   0%|          | 0/3862 [00:00<?, ?it/s][A2025-09-03 01:40:05,250   INFO  Train:   20/20 (100%) [   0/3862 (  0%)]  Loss: 0.7895 (0.789)  LR: 1.705e-05  Grad: 18.8646  max=0.8096(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0616(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3615, loss_cls=0.0734, loss_bbox=0.3546, matched_ious=0.5383, d_time=2.15(2.15), f_time=2.66(2.66), b_time=4.81(4.81)  Time cost: 00:04/4:39:44 [27:55:34/4:39:44]  Acc_iter 73379       Data time: 2.15(2.15)  Forward time: 2.66(2.66)  Batch time: 4.81(4.81)
2025-09-03 01:40:32,338   INFO  Train:   20/20 (100%) [  21/3862 (  1%)]  Loss: 1.037 (0.777)  LR: 1.686e-05  Grad: 18.8708  max=0.8024(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0608(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3421, loss_cls=0.0578, loss_bbox=0.3760, matched_ious=0.6011, d_time=0.00(0.10), f_time=1.22(1.35), b_time=1.22(1.45)  Time cost: 00:31/1:31:28 [27:56:01/1:31:28]  Acc_iter 73400       Data time: 0.00(0.10)  Forward time: 1.22(1.35)  Batch time: 1.22(1.45)
2025-09-03 01:41:38,740   INFO  Train:   20/20 (100%) [  71/3862 (  2%)]  Loss: 0.9335 (0.775)  LR: 1.643e-05  Grad: 18.8774  max=0.7917(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0622(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3319, loss_cls=0.0575, loss_bbox=0.3856, matched_ious=0.5871, d_time=0.01(0.04), f_time=1.33(1.33), b_time=1.34(1.37)  Time cost: 01:37/1:25:51 [27:57:07/1:25:51]  Acc_iter 73450       Data time: 0.01(0.04)  Forward time: 1.33(1.33)  Batch time: 1.34(1.37)
2025-09-03 01:42:43,741   INFO  Train:   20/20 (100%) [ 121/3862 (  3%)]  Loss: 1.080 (0.767)  LR: 1.600e-05  Grad: 18.9041  max=0.7849(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0635(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3338, loss_cls=0.0579, loss_bbox=0.3637, matched_ious=0.6032, d_time=0.00(0.02), f_time=1.30(1.32), b_time=1.31(1.34)  Time cost: 02:42/1:23:13 [27:58:12/1:23:13]  Acc_iter 73500       Data time: 0.00(0.02)  Forward time: 1.30(1.32)  Batch time: 1.31(1.34)
2025-09-03 01:43:48,725   INFO  Train:   20/20 (100%) [ 171/3862 (  4%)]  Loss: 0.6004 (0.769)  LR: 1.558e-05  Grad: 18.9013  max=0.8142(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0660(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3417, loss_cls=0.0601, loss_bbox=0.3729, matched_ious=0.6012, d_time=0.00(0.02), f_time=1.28(1.31), b_time=1.28(1.33)  Time cost: 03:47/1:21:28 [27:59:17/1:21:28]  Acc_iter 73550       Data time: 0.00(0.02)  Forward time: 1.28(1.31)  Batch time: 1.28(1.33)
2025-09-03 01:44:53,650   INFO  Train:   20/20 (100%) [ 221/3862 (  6%)]  Loss: 0.5238 (0.761)  LR: 1.516e-05  Grad: 18.9221  max=0.8192(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0683(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3131, loss_cls=0.0546, loss_bbox=0.3653, matched_ious=0.6053, d_time=0.01(0.02), f_time=1.26(1.30), b_time=1.27(1.32)  Time cost: 04:52/1:20:01 [28:00:22/1:20:01]  Acc_iter 73600       Data time: 0.01(0.02)  Forward time: 1.26(1.30)  Batch time: 1.27(1.32)
2025-09-03 01:45:58,842   INFO  Train:   20/20 (100%) [ 271/3862 (  7%)]  Loss: 0.4598 (0.761)  LR: 1.475e-05  Grad: 18.9435  max=0.8125(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0691(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3285, loss_cls=0.0572, loss_bbox=0.3732, matched_ious=0.6024, d_time=0.01(0.01), f_time=1.41(1.30), b_time=1.42(1.32)  Time cost: 05:57/1:18:45 [28:01:27/1:18:45]  Acc_iter 73650       Data time: 0.01(0.01)  Forward time: 1.41(1.30)  Batch time: 1.42(1.32)
2025-09-03 01:47:03,880   INFO  Train:   20/20 (100%) [ 321/3862 (  8%)]  Loss: 0.7628 (0.761)  LR: 1.435e-05  Grad: 18.9573  max=0.7901(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0704(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3418, loss_cls=0.0580, loss_bbox=0.3654, matched_ious=0.6018, d_time=0.00(0.01), f_time=1.19(1.30), b_time=1.20(1.32)  Time cost: 07:02/1:17:31 [28:02:32/1:17:31]  Acc_iter 73700       Data time: 0.00(0.01)  Forward time: 1.19(1.30)  Batch time: 1.20(1.32)
2025-09-03 01:48:08,662   INFO  Train:   20/20 (100%) [ 371/3862 ( 10%)]  Loss: 0.8185 (0.765)  LR: 1.395e-05  Grad: 18.9739  max=0.7957(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0768(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3419, loss_cls=0.0595, loss_bbox=0.3832, matched_ious=0.5923, d_time=0.02(0.01), f_time=1.67(1.30), b_time=1.69(1.31)  Time cost: 08:07/1:16:17 [28:03:37/1:16:17]  Acc_iter 73750       Data time: 0.02(0.01)  Forward time: 1.67(1.30)  Batch time: 1.69(1.31)
2025-09-03 01:49:13,278   INFO  Train:   20/20 (100%) [ 421/3862 ( 11%)]  Loss: 0.7255 (0.765)  LR: 1.355e-05  Grad: 18.9895  max=0.7893(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0799(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3306, loss_cls=0.0578, loss_bbox=0.3769, matched_ious=0.5971, d_time=0.00(0.01), f_time=1.20(1.30), b_time=1.20(1.31)  Time cost: 09:12/1:15:04 [28:04:42/1:15:04]  Acc_iter 73800       Data time: 0.00(0.01)  Forward time: 1.20(1.30)  Batch time: 1.20(1.31)
2025-09-03 01:50:18,365   INFO  Train:   20/20 (100%) [ 471/3862 ( 12%)]  Loss: 0.5873 (0.760)  LR: 1.316e-05  Grad: 19.0201  max=0.8012(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0804(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3217, loss_cls=0.0527, loss_bbox=0.3469, matched_ious=0.6040, d_time=0.00(0.01), f_time=1.21(1.30), b_time=1.21(1.31)  Time cost: 10:17/1:13:55 [28:05:47/1:13:55]  Acc_iter 73850       Data time: 0.00(0.01)  Forward time: 1.21(1.30)  Batch time: 1.21(1.31)
2025-09-03 01:51:24,171   INFO  Train:   20/20 (100%) [ 521/3862 ( 13%)]  Loss: 1.080 (0.759)  LR: 1.278e-05  Grad: 19.0276  max=0.8049(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0775(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3356, loss_cls=0.0580, loss_bbox=0.3590, matched_ious=0.5954, d_time=0.00(0.01), f_time=1.22(1.30), b_time=1.22(1.31)  Time cost: 11:23/1:12:53 [28:06:53/1:12:53]  Acc_iter 73900       Data time: 0.00(0.01)  Forward time: 1.22(1.30)  Batch time: 1.22(1.31)
2025-09-03 01:52:29,659   INFO  Train:   20/20 (100%) [ 571/3862 ( 15%)]  Loss: 0.9217 (0.763)  LR: 1.240e-05  Grad: 19.0123  max=0.8154(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0765(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3409, loss_cls=0.0587, loss_bbox=0.3984, matched_ious=0.5964, d_time=0.00(0.01), f_time=1.50(1.30), b_time=1.50(1.31)  Time cost: 12:28/1:11:47 [28:07:58/1:11:47]  Acc_iter 73950       Data time: 0.00(0.01)  Forward time: 1.50(1.30)  Batch time: 1.50(1.31)
2025-09-03 01:53:34,069   INFO  Train:   20/20 (100%) [ 621/3862 ( 16%)]  Loss: 0.6565 (0.761)  LR: 1.203e-05  Grad: 19.0444  max=0.8294(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0745(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3177, loss_cls=0.0540, loss_bbox=0.3661, matched_ious=0.5967, d_time=0.00(0.01), f_time=1.18(1.30), b_time=1.19(1.31)  Time cost: 13:33/1:10:37 [28:09:03/1:10:37]  Acc_iter 74000       Data time: 0.00(0.01)  Forward time: 1.18(1.30)  Batch time: 1.19(1.31)
2025-09-03 01:54:38,552   INFO  Train:   20/20 (100%) [ 671/3862 ( 17%)]  Loss: 1.209 (0.759)  LR: 1.166e-05  Grad: 19.0662  max=0.8258(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0794(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3249, loss_cls=0.0565, loss_bbox=0.3527, matched_ious=0.6057, d_time=0.01(0.01), f_time=1.30(1.30), b_time=1.31(1.31)  Time cost: 14:37/1:09:27 [28:10:07/1:09:27]  Acc_iter 74050       Data time: 0.01(0.01)  Forward time: 1.30(1.30)  Batch time: 1.31(1.31)
2025-09-03 01:55:43,175   INFO  Train:   20/20 (100%) [ 721/3862 ( 19%)]  Loss: 0.7940 (0.758)  LR: 1.130e-05  Grad: 19.0814  max=0.8078(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0831(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3259, loss_cls=0.0572, loss_bbox=0.3611, matched_ious=0.5955, d_time=0.01(0.01), f_time=1.21(1.30), b_time=1.22(1.31)  Time cost: 15:42/1:08:19 [28:11:12/1:08:19]  Acc_iter 74100       Data time: 0.01(0.01)  Forward time: 1.21(1.30)  Batch time: 1.22(1.31)
2025-09-03 01:56:47,914   INFO  Train:   20/20 (100%) [ 771/3862 ( 20%)]  Loss: 0.9141 (0.757)  LR: 1.095e-05  Grad: 19.1190  max=0.8230(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0837(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3233, loss_cls=0.0598, loss_bbox=0.3622, matched_ious=0.5961, d_time=0.00(0.01), f_time=1.24(1.30), b_time=1.25(1.31)  Time cost: 16:47/1:07:11 [28:12:16/1:07:11]  Acc_iter 74150       Data time: 0.00(0.01)  Forward time: 1.24(1.30)  Batch time: 1.25(1.31)
2025-09-03 01:57:52,119   INFO  Train:   20/20 (100%) [ 821/3862 ( 21%)]  Loss: 0.9591 (0.758)  LR: 1.060e-05  Grad: 19.1576  max=0.8079(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0939(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3335, loss_cls=0.0589, loss_bbox=0.3826, matched_ious=0.5989, d_time=0.00(0.01), f_time=1.21(1.29), b_time=1.22(1.30)  Time cost: 17:51/1:06:02 [28:13:21/1:06:02]  Acc_iter 74200       Data time: 0.00(0.01)  Forward time: 1.21(1.29)  Batch time: 1.22(1.30)
2025-09-03 01:58:56,105   INFO  Train:   20/20 (100%) [ 871/3862 ( 23%)]  Loss: 0.6133 (0.758)  LR: 1.025e-05  Grad: 19.1940  max=0.7998(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1028(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3473, loss_cls=0.0596, loss_bbox=0.3540, matched_ious=0.5988, d_time=0.01(0.01), f_time=1.26(1.29), b_time=1.27(1.30)  Time cost: 18:55/1:04:53 [28:14:25/1:04:53]  Acc_iter 74250       Data time: 0.01(0.01)  Forward time: 1.26(1.29)  Batch time: 1.27(1.30)
2025-09-03 02:00:00,610   INFO  Train:   20/20 (100%) [ 921/3862 ( 24%)]  Loss: 0.7851 (0.757)  LR: 9.914e-06  Grad: 19.2334  max=0.8230(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1017(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3200, loss_cls=0.0537, loss_bbox=0.3655, matched_ious=0.5952, d_time=0.00(0.01), f_time=1.24(1.29), b_time=1.24(1.30)  Time cost: 19:59/1:03:46 [28:15:29/1:03:46]  Acc_iter 74300       Data time: 0.00(0.01)  Forward time: 1.24(1.29)  Batch time: 1.24(1.30)
2025-09-03 02:01:04,649   INFO  Train:   20/20 (100%) [ 971/3862 ( 25%)]  Loss: 0.8535 (0.759)  LR: 9.581e-06  Grad: 19.2455  max=0.8311(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1017(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3375, loss_cls=0.0577, loss_bbox=0.3886, matched_ious=0.5921, d_time=0.00(0.01), f_time=1.33(1.29), b_time=1.33(1.30)  Time cost: 21:03/1:02:38 [28:16:33/1:02:38]  Acc_iter 74350       Data time: 0.00(0.01)  Forward time: 1.33(1.29)  Batch time: 1.33(1.30)
2025-09-03 02:02:09,161   INFO  Train:   20/20 (100%) [1021/3862 ( 26%)]  Loss: 0.7073 (0.758)  LR: 9.254e-06  Grad: 19.2775  max=0.8220(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1035(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3249, loss_cls=0.0551, loss_bbox=0.3585, matched_ious=0.6016, d_time=0.01(0.01), f_time=1.24(1.29), b_time=1.25(1.30)  Time cost: 22:08/1:01:32 [28:17:38/1:01:32]  Acc_iter 74400       Data time: 0.01(0.01)  Forward time: 1.24(1.29)  Batch time: 1.25(1.30)
2025-09-03 02:03:14,158   INFO  Train:   20/20 (100%) [1071/3862 ( 28%)]  Loss: 0.7677 (0.757)  LR: 8.932e-06  Grad: 19.3041  max=0.8159(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1047(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3236, loss_cls=0.0543, loss_bbox=0.3598, matched_ious=0.5995, d_time=0.00(0.01), f_time=1.32(1.29), b_time=1.33(1.30)  Time cost: 23:13/1:00:27 [28:18:43/1:00:27]  Acc_iter 74450       Data time: 0.00(0.01)  Forward time: 1.32(1.29)  Batch time: 1.33(1.30)
2025-09-03 02:04:19,072   INFO  Train:   20/20 (100%) [1121/3862 ( 29%)]  Loss: 0.6863 (0.755)  LR: 8.616e-06  Grad: 19.3331  max=0.8249(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1112(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3204, loss_cls=0.0560, loss_bbox=0.3526, matched_ious=0.5911, d_time=0.00(0.01), f_time=1.49(1.29), b_time=1.50(1.30)  Time cost: 24:18/59:22 [28:19:48/59:22]  Acc_iter 74500       Data time: 0.00(0.01)  Forward time: 1.49(1.29)  Batch time: 1.50(1.30)
2025-09-03 02:05:23,267   INFO  Train:   20/20 (100%) [1171/3862 ( 30%)]  Loss: 0.9881 (0.755)  LR: 8.306e-06  Grad: 19.3595  max=0.8141(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1102(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3328, loss_cls=0.0567, loss_bbox=0.3612, matched_ious=0.5985, d_time=0.00(0.01), f_time=1.29(1.29), b_time=1.30(1.30)  Time cost: 25:22/58:15 [28:20:52/58:15]  Acc_iter 74550       Data time: 0.00(0.01)  Forward time: 1.29(1.29)  Batch time: 1.30(1.30)
2025-09-03 02:06:27,187   INFO  Train:   20/20 (100%) [1221/3862 ( 32%)]  Loss: 0.6207 (0.754)  LR: 8.001e-06  Grad: 19.3487  max=0.8026(module.vfe.pfn_layers.0.linear.weight)  min: -1.1107(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3199, loss_cls=0.0556, loss_bbox=0.3528, matched_ious=0.5985, d_time=0.00(0.01), f_time=1.21(1.29), b_time=1.21(1.30)  Time cost: 26:26/57:08 [28:21:56/57:08]  Acc_iter 74600       Data time: 0.00(0.01)  Forward time: 1.21(1.29)  Batch time: 1.21(1.30)
2025-09-03 02:07:31,585   INFO  Train:   20/20 (100%) [1271/3862 ( 33%)]  Loss: 0.5551 (0.755)  LR: 7.702e-06  Grad: 19.3607  max=0.8303(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1163(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3381, loss_cls=0.0619, loss_bbox=0.3880, matched_ious=0.5973, d_time=0.01(0.01), f_time=1.24(1.29), b_time=1.25(1.30)  Time cost: 27:30/56:02 [28:23:00/56:02]  Acc_iter 74650       Data time: 0.01(0.01)  Forward time: 1.24(1.29)  Batch time: 1.25(1.30)
2025-09-03 02:08:36,083   INFO  Train:   20/20 (100%) [1321/3862 ( 34%)]  Loss: 0.7536 (0.755)  LR: 7.409e-06  Grad: 19.4205  max=0.8057(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1226(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3249, loss_cls=0.0568, loss_bbox=0.3537, matched_ious=0.5951, d_time=0.01(0.01), f_time=1.31(1.29), b_time=1.32(1.30)  Time cost: 28:35/54:56 [28:24:05/54:56]  Acc_iter 74700       Data time: 0.01(0.01)  Forward time: 1.31(1.29)  Batch time: 1.32(1.30)
2025-09-03 02:09:40,624   INFO  Train:   20/20 (100%) [1371/3862 ( 35%)]  Loss: 0.5489 (0.756)  LR: 7.122e-06  Grad: 19.3864  max=0.7913(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1257(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3485, loss_cls=0.0623, loss_bbox=0.3916, matched_ious=0.5928, d_time=0.00(0.01), f_time=1.26(1.29), b_time=1.27(1.30)  Time cost: 29:39/53:51 [28:25:09/53:51]  Acc_iter 74750       Data time: 0.00(0.01)  Forward time: 1.26(1.29)  Batch time: 1.27(1.30)
2025-09-03 02:10:45,087   INFO  Train:   20/20 (100%) [1421/3862 ( 37%)]  Loss: 0.8820 (0.758)  LR: 6.840e-06  Grad: 19.3765  max=0.7994(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1277(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3378, loss_cls=0.0588, loss_bbox=0.3939, matched_ious=0.5943, d_time=0.01(0.01), f_time=1.27(1.29), b_time=1.28(1.30)  Time cost: 30:44/52:45 [28:26:14/52:45]  Acc_iter 74800       Data time: 0.01(0.01)  Forward time: 1.27(1.29)  Batch time: 1.28(1.30)
2025-09-03 02:11:48,843   INFO  Train:   20/20 (100%) [1471/3862 ( 38%)]  Loss: 0.6708 (0.758)  LR: 6.563e-06  Grad: 19.4015  max=0.7845(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1261(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3407, loss_cls=0.0595, loss_bbox=0.3783, matched_ious=0.5919, d_time=0.01(0.01), f_time=1.24(1.29), b_time=1.25(1.30)  Time cost: 31:47/51:39 [28:27:17/51:39]  Acc_iter 74850       Data time: 0.01(0.01)  Forward time: 1.24(1.29)  Batch time: 1.25(1.30)
2025-09-03 02:12:53,964   INFO  Train:   20/20 (100%) [1521/3862 ( 39%)]  Loss: 0.7721 (0.758)  LR: 6.293e-06  Grad: 19.4954  max=1.0891(module.vfe.pfn_layers.0.linear.weight)  min: -1.1308(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3169, loss_cls=0.0574, loss_bbox=0.3631, matched_ious=0.5952, d_time=0.01(0.01), f_time=1.40(1.29), b_time=1.41(1.30)  Time cost: 32:53/50:34 [28:28:23/50:34]  Acc_iter 74900       Data time: 0.01(0.01)  Forward time: 1.40(1.29)  Batch time: 1.41(1.30)
2025-09-03 02:13:58,604   INFO  Train:   20/20 (100%) [1571/3862 ( 41%)]  Loss: 0.8350 (0.756)  LR: 6.028e-06  Grad: 19.4442  max=0.8110(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1315(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3068, loss_cls=0.0556, loss_bbox=0.3558, matched_ious=0.6071, d_time=0.00(0.01), f_time=1.25(1.29), b_time=1.26(1.30)  Time cost: 33:57/49:29 [28:29:27/49:29]  Acc_iter 74950       Data time: 0.00(0.01)  Forward time: 1.25(1.29)  Batch time: 1.26(1.30)
2025-09-03 02:15:03,707   INFO  Train:   20/20 (100%) [1621/3862 ( 42%)]  Loss: 0.7530 (0.757)  LR: 5.768e-06  Grad: 19.4816  max=0.8055(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1342(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3473, loss_cls=0.0592, loss_bbox=0.3669, matched_ious=0.5986, d_time=0.01(0.01), f_time=1.30(1.29), b_time=1.31(1.30)  Time cost: 35:02/48:25 [28:30:32/48:25]  Acc_iter 75000       Data time: 0.01(0.01)  Forward time: 1.30(1.29)  Batch time: 1.31(1.30)
2025-09-03 02:16:08,345   INFO  Train:   20/20 (100%) [1671/3862 ( 43%)]  Loss: 0.7670 (0.759)  LR: 5.515e-06  Grad: 19.5135  max=0.8086(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1370(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3515, loss_cls=0.0618, loss_bbox=0.4142, matched_ious=0.5899, d_time=0.00(0.01), f_time=1.23(1.29), b_time=1.23(1.30)  Time cost: 36:07/47:20 [28:31:37/47:20]  Acc_iter 75050       Data time: 0.00(0.01)  Forward time: 1.23(1.29)  Batch time: 1.23(1.30)
2025-09-03 02:17:13,576   INFO  Train:   20/20 (100%) [1721/3862 ( 45%)]  Loss: 0.4943 (0.759)  LR: 5.267e-06  Grad: 19.5330  max=0.8150(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1402(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3536, loss_cls=0.0610, loss_bbox=0.3592, matched_ious=0.5946, d_time=0.00(0.01), f_time=1.21(1.29), b_time=1.22(1.30)  Time cost: 37:12/46:15 [28:32:42/46:15]  Acc_iter 75100       Data time: 0.00(0.01)  Forward time: 1.21(1.29)  Batch time: 1.22(1.30)
2025-09-03 02:18:17,799   INFO  Train:   20/20 (100%) [1771/3862 ( 46%)]  Loss: 0.7755 (0.760)  LR: 5.025e-06  Grad: 19.5292  max=0.8073(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1396(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3330, loss_cls=0.0582, loss_bbox=0.3728, matched_ious=0.5979, d_time=0.01(0.01), f_time=1.30(1.29), b_time=1.31(1.30)  Time cost: 38:16/45:10 [28:33:46/45:10]  Acc_iter 75150       Data time: 0.01(0.01)  Forward time: 1.30(1.29)  Batch time: 1.31(1.30)
2025-09-03 02:19:22,554   INFO  Train:   20/20 (100%) [1821/3862 ( 47%)]  Loss: 0.9044 (0.759)  LR: 4.788e-06  Grad: 19.5755  max=0.8091(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1402(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3161, loss_cls=0.0556, loss_bbox=0.3814, matched_ious=0.5994, d_time=0.00(0.01), f_time=1.25(1.29), b_time=1.25(1.30)  Time cost: 39:21/44:05 [28:34:51/44:05]  Acc_iter 75200       Data time: 0.00(0.01)  Forward time: 1.25(1.29)  Batch time: 1.25(1.30)
2025-09-03 02:20:27,793   INFO  Train:   20/20 (100%) [1871/3862 ( 48%)]  Loss: 0.5439 (0.758)  LR: 4.557e-06  Grad: 19.5583  max=0.7905(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1395(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3144, loss_cls=0.0550, loss_bbox=0.3452, matched_ious=0.5961, d_time=0.01(0.01), f_time=1.51(1.29), b_time=1.52(1.30)  Time cost: 40:26/43:01 [28:35:56/43:01]  Acc_iter 75250       Data time: 0.01(0.01)  Forward time: 1.51(1.29)  Batch time: 1.52(1.30)
2025-09-03 02:21:32,531   INFO  Train:   20/20 (100%) [1921/3862 ( 50%)]  Loss: 0.7170 (0.757)  LR: 4.332e-06  Grad: 19.5590  max=0.7993(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1355(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3017, loss_cls=0.0533, loss_bbox=0.3719, matched_ious=0.5961, d_time=0.00(0.01), f_time=1.36(1.29), b_time=1.36(1.30)  Time cost: 41:31/41:56 [28:37:01/41:56]  Acc_iter 75300       Data time: 0.00(0.01)  Forward time: 1.36(1.29)  Batch time: 1.36(1.30)
2025-09-03 02:22:37,348   INFO  Train:   20/20 (100%) [1971/3862 ( 51%)]  Loss: 0.8731 (0.758)  LR: 4.112e-06  Grad: 19.5589  max=0.7980(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1363(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3384, loss_cls=0.0577, loss_bbox=0.4038, matched_ious=0.5935, d_time=0.00(0.01), f_time=1.34(1.29), b_time=1.34(1.30)  Time cost: 42:36/40:51 [28:38:06/40:51]  Acc_iter 75350       Data time: 0.00(0.01)  Forward time: 1.34(1.29)  Batch time: 1.34(1.30)
2025-09-03 02:23:41,799   INFO  Train:   20/20 (100%) [2021/3862 ( 52%)]  Loss: 0.7022 (0.759)  LR: 3.899e-06  Grad: 19.5753  max=0.7921(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1376(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3302, loss_cls=0.0594, loss_bbox=0.3880, matched_ious=0.5817, d_time=0.00(0.01), f_time=1.46(1.29), b_time=1.46(1.30)  Time cost: 43:40/39:46 [28:39:10/39:46]  Acc_iter 75400       Data time: 0.00(0.01)  Forward time: 1.46(1.29)  Batch time: 1.46(1.30)
2025-09-03 02:24:46,684   INFO  Train:   20/20 (100%) [2071/3862 ( 54%)]  Loss: 0.5984 (0.760)  LR: 3.690e-06  Grad: 19.5978  max=0.8208(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1408(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3484, loss_cls=0.0579, loss_bbox=0.3901, matched_ious=0.5856, d_time=0.00(0.01), f_time=1.30(1.29), b_time=1.30(1.30)  Time cost: 44:45/38:41 [28:40:15/38:41]  Acc_iter 75450       Data time: 0.00(0.01)  Forward time: 1.30(1.29)  Batch time: 1.30(1.30)
2025-09-03 02:25:51,586   INFO  Train:   20/20 (100%) [2121/3862 ( 55%)]  Loss: 0.8158 (0.761)  LR: 3.488e-06  Grad: 19.6362  max=0.8204(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1443(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3531, loss_cls=0.0603, loss_bbox=0.3796, matched_ious=0.6019, d_time=0.00(0.01), f_time=1.28(1.29), b_time=1.28(1.30)  Time cost: 45:50/37:36 [28:41:20/37:36]  Acc_iter 75500       Data time: 0.00(0.01)  Forward time: 1.28(1.29)  Batch time: 1.28(1.30)
2025-09-03 02:26:55,836   INFO  Train:   20/20 (100%) [2171/3862 ( 56%)]  Loss: 0.8020 (0.761)  LR: 3.291e-06  Grad: 19.6484  max=0.8103(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1466(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3346, loss_cls=0.0549, loss_bbox=0.3736, matched_ious=0.5981, d_time=0.01(0.01), f_time=1.23(1.29), b_time=1.23(1.30)  Time cost: 46:54/36:31 [28:42:24/36:31]  Acc_iter 75550       Data time: 0.01(0.01)  Forward time: 1.23(1.29)  Batch time: 1.23(1.30)
2025-09-03 02:28:01,105   INFO  Train:   20/20 (100%) [2221/3862 ( 58%)]  Loss: 0.9030 (0.760)  LR: 3.100e-06  Grad: 19.6333  max=0.8140(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1485(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3265, loss_cls=0.0553, loss_bbox=0.3653, matched_ious=0.6076, d_time=0.00(0.01), f_time=1.33(1.29), b_time=1.33(1.30)  Time cost: 48:00/35:27 [28:43:30/35:27]  Acc_iter 75600       Data time: 0.00(0.01)  Forward time: 1.33(1.29)  Batch time: 1.33(1.30)
2025-09-03 02:29:05,327   INFO  Train:   20/20 (100%) [2271/3862 ( 59%)]  Loss: 0.5964 (0.760)  LR: 2.915e-06  Grad: 19.6326  max=0.8110(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1483(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3177, loss_cls=0.0548, loss_bbox=0.3505, matched_ious=0.6044, d_time=0.00(0.01), f_time=1.21(1.29), b_time=1.22(1.30)  Time cost: 49:04/34:21 [28:44:34/34:21]  Acc_iter 75650       Data time: 0.00(0.01)  Forward time: 1.21(1.29)  Batch time: 1.22(1.30)
2025-09-03 02:30:10,013   INFO  Train:   20/20 (100%) [2321/3862 ( 60%)]  Loss: 0.6855 (0.760)  LR: 2.736e-06  Grad: 19.6720  max=0.8254(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1497(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3421, loss_cls=0.0605, loss_bbox=0.3658, matched_ious=0.6002, d_time=0.01(0.01), f_time=1.22(1.29), b_time=1.24(1.30)  Time cost: 50:09/33:17 [28:45:39/33:17]  Acc_iter 75700       Data time: 0.01(0.01)  Forward time: 1.22(1.29)  Batch time: 1.24(1.30)
2025-09-03 02:31:15,154   INFO  Train:   20/20 (100%) [2371/3862 ( 61%)]  Loss: 0.8851 (0.760)  LR: 2.562e-06  Grad: 19.7127  max=0.8297(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1538(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3412, loss_cls=0.0572, loss_bbox=0.3781, matched_ious=0.5950, d_time=0.00(0.01), f_time=1.27(1.29), b_time=1.27(1.30)  Time cost: 51:14/32:12 [28:46:44/32:12]  Acc_iter 75750       Data time: 0.00(0.01)  Forward time: 1.27(1.29)  Batch time: 1.27(1.30)
2025-09-03 02:32:19,738   INFO  Train:   20/20 (100%) [2421/3862 ( 63%)]  Loss: 0.8509 (0.761)  LR: 2.394e-06  Grad: 19.7248  max=0.8094(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1550(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3402, loss_cls=0.0592, loss_bbox=0.3944, matched_ious=0.5894, d_time=0.03(0.01), f_time=1.27(1.29), b_time=1.30(1.30)  Time cost: 52:18/31:07 [28:47:48/31:07]  Acc_iter 75800       Data time: 0.03(0.01)  Forward time: 1.27(1.29)  Batch time: 1.30(1.30)
2025-09-03 02:33:24,701   INFO  Train:   20/20 (100%) [2471/3862 ( 64%)]  Loss: 0.5549 (0.760)  LR: 2.231e-06  Grad: 19.7533  max=0.8089(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1604(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3110, loss_cls=0.0537, loss_bbox=0.3614, matched_ious=0.5975, d_time=0.01(0.01), f_time=1.34(1.29), b_time=1.35(1.30)  Time cost: 53:23/30:02 [28:48:53/30:02]  Acc_iter 75850       Data time: 0.01(0.01)  Forward time: 1.34(1.29)  Batch time: 1.35(1.30)
2025-09-03 02:34:29,395   INFO  Train:   20/20 (100%) [2521/3862 ( 65%)]  Loss: 0.9846 (0.760)  LR: 2.074e-06  Grad: 19.7832  max=0.7985(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1644(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3215, loss_cls=0.0552, loss_bbox=0.3803, matched_ious=0.5953, d_time=0.00(0.01), f_time=1.20(1.29), b_time=1.21(1.30)  Time cost: 54:28/28:57 [28:49:58/28:57]  Acc_iter 75900       Data time: 0.00(0.01)  Forward time: 1.20(1.29)  Batch time: 1.21(1.30)
2025-09-03 02:35:34,675   INFO  Train:   20/20 (100%) [2571/3862 ( 67%)]  Loss: 0.6673 (0.760)  LR: 1.923e-06  Grad: 19.7950  max=0.7778(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1640(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3325, loss_cls=0.0598, loss_bbox=0.3782, matched_ious=0.5927, d_time=0.00(0.01), f_time=1.28(1.29), b_time=1.28(1.30)  Time cost: 55:33/27:53 [28:51:03/27:53]  Acc_iter 75950       Data time: 0.00(0.01)  Forward time: 1.28(1.29)  Batch time: 1.28(1.30)
2025-09-03 02:36:39,832   INFO  Train:   20/20 (100%) [2621/3862 ( 68%)]  Loss: 1.002 (0.760)  LR: 1.778e-06  Grad: 19.8169  max=0.7882(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1659(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3310, loss_cls=0.0563, loss_bbox=0.3816, matched_ious=0.5913, d_time=0.00(0.01), f_time=1.25(1.29), b_time=1.25(1.30)  Time cost: 56:38/26:48 [28:52:08/26:48]  Acc_iter 76000       Data time: 0.00(0.01)  Forward time: 1.25(1.29)  Batch time: 1.25(1.30)
2025-09-03 02:37:44,346   INFO  Train:   20/20 (100%) [2671/3862 ( 69%)]  Loss: 0.9064 (0.760)  LR: 1.639e-06  Grad: 19.8791  max=0.8034(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1708(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3220, loss_cls=0.0536, loss_bbox=0.3732, matched_ious=0.5903, d_time=0.01(0.01), f_time=1.20(1.29), b_time=1.21(1.30)  Time cost: 57:43/25:43 [28:53:13/25:43]  Acc_iter 76050       Data time: 0.01(0.01)  Forward time: 1.20(1.29)  Batch time: 1.21(1.30)
2025-09-03 02:38:49,516   INFO  Train:   20/20 (100%) [2721/3862 ( 70%)]  Loss: 1.223 (0.760)  LR: 1.505e-06  Grad: 19.8651  max=0.7962(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1762(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3349, loss_cls=0.0591, loss_bbox=0.3715, matched_ious=0.5990, d_time=0.01(0.01), f_time=1.32(1.29), b_time=1.33(1.30)  Time cost: 58:48/24:39 [28:54:18/24:39]  Acc_iter 76100       Data time: 0.01(0.01)  Forward time: 1.32(1.29)  Batch time: 1.33(1.30)
2025-09-03 02:39:54,424   INFO  Train:   20/20 (100%) [2771/3862 ( 72%)]  Loss: 0.6113 (0.761)  LR: 1.377e-06  Grad: 19.8813  max=0.8102(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1783(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3556, loss_cls=0.0597, loss_bbox=0.4015, matched_ious=0.5893, d_time=0.00(0.01), f_time=1.25(1.29), b_time=1.25(1.30)  Time cost: 59:53/23:34 [28:55:23/23:34]  Acc_iter 76150       Data time: 0.00(0.01)  Forward time: 1.25(1.29)  Batch time: 1.25(1.30)
2025-09-03 02:40:59,936   INFO  Train:   20/20 (100%) [2821/3862 ( 73%)]  Loss: 0.4731 (0.761)  LR: 1.254e-06  Grad: 19.8953  max=0.8458(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1777(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3256, loss_cls=0.0565, loss_bbox=0.3678, matched_ious=0.5994, d_time=0.00(0.01), f_time=1.32(1.29), b_time=1.32(1.30)  Time cost: 1:00:59/22:29 [28:56:29/22:29]  Acc_iter 76200       Data time: 0.00(0.01)  Forward time: 1.32(1.29)  Batch time: 1.32(1.30)
2025-09-03 02:42:04,277   INFO  Train:   20/20 (100%) [2871/3862 ( 74%)]  Loss: 0.7591 (0.761)  LR: 1.138e-06  Grad: 19.9258  max=0.8317(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1772(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3290, loss_cls=0.0547, loss_bbox=0.3982, matched_ious=0.6011, d_time=0.01(0.01), f_time=1.17(1.29), b_time=1.18(1.30)  Time cost: 1:02:03/21:24 [28:57:33/21:24]  Acc_iter 76250       Data time: 0.01(0.01)  Forward time: 1.17(1.29)  Batch time: 1.18(1.30)
2025-09-03 02:43:09,477   INFO  Train:   20/20 (100%) [2921/3862 ( 76%)]  Loss: 0.6429 (0.762)  LR: 1.027e-06  Grad: 19.9318  max=0.8124(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1747(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3456, loss_cls=0.0609, loss_bbox=0.3871, matched_ious=0.5980, d_time=0.00(0.01), f_time=1.43(1.29), b_time=1.44(1.30)  Time cost: 1:03:08/20:20 [28:58:38/20:20]  Acc_iter 76300       Data time: 0.00(0.01)  Forward time: 1.43(1.29)  Batch time: 1.44(1.30)
2025-09-03 02:44:14,089   INFO  Train:   20/20 (100%) [2971/3862 ( 77%)]  Loss: 0.6855 (0.763)  LR: 9.217e-07  Grad: 19.9808  max=0.8423(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1773(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3341, loss_cls=0.0559, loss_bbox=0.4150, matched_ious=0.5939, d_time=0.00(0.01), f_time=1.19(1.29), b_time=1.19(1.30)  Time cost: 1:04:13/19:15 [28:59:43/19:15]  Acc_iter 76350       Data time: 0.00(0.01)  Forward time: 1.19(1.29)  Batch time: 1.19(1.30)
2025-09-03 02:45:19,022   INFO  Train:   20/20 (100%) [3021/3862 ( 78%)]  Loss: 0.7187 (0.763)  LR: 8.223e-07  Grad: 19.9984  max=0.8500(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1824(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3304, loss_cls=0.0565, loss_bbox=0.3797, matched_ious=0.5953, d_time=0.01(0.01), f_time=1.25(1.29), b_time=1.26(1.30)  Time cost: 1:05:18/18:10 [29:00:48/18:10]  Acc_iter 76400       Data time: 0.01(0.01)  Forward time: 1.25(1.29)  Batch time: 1.26(1.30)
2025-09-03 02:46:23,877   INFO  Train:   20/20 (100%) [3071/3862 ( 80%)]  Loss: 0.9995 (0.763)  LR: 7.286e-07  Grad: 19.9993  max=0.8678(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1831(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3293, loss_cls=0.0586, loss_bbox=0.3698, matched_ious=0.5908, d_time=0.00(0.01), f_time=1.31(1.29), b_time=1.31(1.30)  Time cost: 1:06:22/17:05 [29:01:52/17:05]  Acc_iter 76450       Data time: 0.00(0.01)  Forward time: 1.31(1.29)  Batch time: 1.31(1.30)
2025-09-03 02:47:29,568   INFO  Train:   20/20 (100%) [3121/3862 ( 81%)]  Loss: 0.7678 (0.763)  LR: 6.407e-07  Grad: 20.0626  max=0.8599(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1827(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3595, loss_cls=0.0638, loss_bbox=0.3873, matched_ious=0.5996, d_time=0.01(0.01), f_time=1.27(1.29), b_time=1.28(1.30)  Time cost: 1:07:28/16:00 [29:02:58/16:00]  Acc_iter 76500       Data time: 0.01(0.01)  Forward time: 1.27(1.29)  Batch time: 1.28(1.30)
2025-09-03 02:48:33,157   INFO  Train:   20/20 (100%) [3171/3862 ( 82%)]  Loss: 1.134 (0.763)  LR: 5.584e-07  Grad: 20.0609  max=0.8488(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1854(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3123, loss_cls=0.0556, loss_bbox=0.3602, matched_ious=0.6007, d_time=0.00(0.01), f_time=1.29(1.29), b_time=1.29(1.30)  Time cost: 1:08:32/14:55 [29:04:02/14:55]  Acc_iter 76550       Data time: 0.00(0.01)  Forward time: 1.29(1.29)  Batch time: 1.29(1.30)
2025-09-03 02:49:37,268   INFO  Train:   20/20 (100%) [3221/3862 ( 83%)]  Loss: 0.7072 (0.762)  LR: 4.820e-07  Grad: 20.0682  max=0.8500(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1839(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3112, loss_cls=0.0530, loss_bbox=0.3653, matched_ious=0.5926, d_time=0.00(0.01), f_time=1.46(1.29), b_time=1.47(1.30)  Time cost: 1:09:36/13:50 [29:05:06/13:50]  Acc_iter 76600       Data time: 0.00(0.01)  Forward time: 1.46(1.29)  Batch time: 1.47(1.30)
2025-09-03 02:50:42,052   INFO  Train:   20/20 (100%) [3271/3862 ( 85%)]  Loss: 0.6838 (0.763)  LR: 4.112e-07  Grad: 20.0983  max=0.8809(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1874(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3267, loss_cls=0.0564, loss_bbox=0.3950, matched_ious=0.5978, d_time=0.00(0.01), f_time=1.25(1.29), b_time=1.25(1.30)  Time cost: 1:10:41/12:46 [29:06:11/12:46]  Acc_iter 76650       Data time: 0.00(0.01)  Forward time: 1.25(1.29)  Batch time: 1.25(1.30)
2025-09-03 02:51:46,614   INFO  Train:   20/20 (100%) [3321/3862 ( 86%)]  Loss: 0.6030 (0.762)  LR: 3.462e-07  Grad: 20.1684  max=0.8994(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1926(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3264, loss_cls=0.0572, loss_bbox=0.3609, matched_ious=0.5982, d_time=0.00(0.01), f_time=1.24(1.29), b_time=1.24(1.30)  Time cost: 1:11:45/11:41 [29:07:15/11:41]  Acc_iter 76700       Data time: 0.00(0.01)  Forward time: 1.24(1.29)  Batch time: 1.24(1.30)
2025-09-03 02:52:51,567   INFO  Train:   20/20 (100%) [3371/3862 ( 87%)]  Loss: 0.5563 (0.763)  LR: 2.869e-07  Grad: 20.1500  max=0.9065(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1948(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3266, loss_cls=0.0543, loss_bbox=0.4208, matched_ious=0.6049, d_time=0.01(0.01), f_time=1.26(1.29), b_time=1.27(1.30)  Time cost: 1:12:50/10:36 [29:08:20/10:36]  Acc_iter 76750       Data time: 0.01(0.01)  Forward time: 1.26(1.29)  Batch time: 1.27(1.30)
2025-09-03 02:53:56,204   INFO  Train:   20/20 (100%) [3421/3862 ( 89%)]  Loss: 0.7986 (0.763)  LR: 2.334e-07  Grad: 20.1812  max=0.8959(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1965(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3409, loss_cls=0.0594, loss_bbox=0.3889, matched_ious=0.5921, d_time=0.00(0.01), f_time=1.32(1.29), b_time=1.32(1.30)  Time cost: 1:13:55/09:31 [29:09:25/09:31]  Acc_iter 76800       Data time: 0.00(0.01)  Forward time: 1.32(1.29)  Batch time: 1.32(1.30)
2025-09-03 02:55:01,671   INFO  Train:   20/20 (100%) [3471/3862 ( 90%)]  Loss: 0.8877 (0.763)  LR: 1.856e-07  Grad: 20.2172  max=0.9007(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2010(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3256, loss_cls=0.0604, loss_bbox=0.3611, matched_ious=0.5908, d_time=0.02(0.01), f_time=1.39(1.29), b_time=1.41(1.30)  Time cost: 1:15:00/08:26 [29:10:30/08:26]  Acc_iter 76850       Data time: 0.02(0.01)  Forward time: 1.39(1.29)  Batch time: 1.41(1.30)
2025-09-03 02:56:06,284   INFO  Train:   20/20 (100%) [3521/3862 ( 91%)]  Loss: 1.116 (0.763)  LR: 1.436e-07  Grad: 20.2316  max=0.8718(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2050(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3187, loss_cls=0.0563, loss_bbox=0.3760, matched_ious=0.5969, d_time=0.01(0.01), f_time=1.57(1.29), b_time=1.58(1.30)  Time cost: 1:16:05/07:22 [29:11:35/07:22]  Acc_iter 76900       Data time: 0.01(0.01)  Forward time: 1.57(1.29)  Batch time: 1.58(1.30)
2025-09-03 02:57:10,456   INFO  Train:   20/20 (100%) [3571/3862 ( 92%)]  Loss: 0.6539 (0.763)  LR: 1.073e-07  Grad: 20.2721  max=0.8769(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2041(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3215, loss_cls=0.0567, loss_bbox=0.3620, matched_ious=0.5904, d_time=0.01(0.01), f_time=1.28(1.29), b_time=1.29(1.30)  Time cost: 1:17:09/06:17 [29:12:39/06:17]  Acc_iter 76950       Data time: 0.01(0.01)  Forward time: 1.28(1.29)  Batch time: 1.29(1.30)
2025-09-03 02:58:15,561   INFO  Train:   20/20 (100%) [3621/3862 ( 94%)]  Loss: 0.5504 (0.763)  LR: 7.672e-08  Grad: 20.2827  max=0.8907(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2036(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3334, loss_cls=0.0590, loss_bbox=0.3793, matched_ious=0.5867, d_time=0.00(0.01), f_time=1.34(1.29), b_time=1.34(1.30)  Time cost: 1:18:14/05:12 [29:13:44/05:12]  Acc_iter 77000       Data time: 0.00(0.01)  Forward time: 1.34(1.29)  Batch time: 1.34(1.30)
2025-09-03 02:59:19,766   INFO  Train:   20/20 (100%) [3671/3862 ( 95%)]  Loss: 0.6362 (0.762)  LR: 5.191e-08  Grad: 20.2880  max=0.9107(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2043(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3209, loss_cls=0.0553, loss_bbox=0.3597, matched_ious=0.6043, d_time=0.00(0.01), f_time=1.47(1.29), b_time=1.47(1.30)  Time cost: 1:19:18/04:07 [29:14:48/04:07]  Acc_iter 77050       Data time: 0.00(0.01)  Forward time: 1.47(1.29)  Batch time: 1.47(1.30)
2025-09-03 03:00:24,276   INFO  Train:   20/20 (100%) [3721/3862 ( 96%)]  Loss: 0.8170 (0.762)  LR: 3.284e-08  Grad: 20.2997  max=0.8988(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2054(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3199, loss_cls=0.0569, loss_bbox=0.3773, matched_ious=0.5947, d_time=0.00(0.01), f_time=1.35(1.29), b_time=1.36(1.30)  Time cost: 1:20:23/03:02 [29:15:53/03:02]  Acc_iter 77100       Data time: 0.00(0.01)  Forward time: 1.35(1.29)  Batch time: 1.36(1.30)
2025-09-03 03:01:28,951   INFO  Train:   20/20 (100%) [3771/3862 ( 98%)]  Loss: 0.6515 (0.762)  LR: 1.951e-08  Grad: 20.3080  max=0.9023(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2050(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3270, loss_cls=0.0551, loss_bbox=0.3532, matched_ious=0.6005, d_time=0.02(0.01), f_time=1.41(1.29), b_time=1.43(1.30)  Time cost: 1:21:28/01:57 [29:16:58/01:57]  Acc_iter 77150       Data time: 0.02(0.01)  Forward time: 1.41(1.29)  Batch time: 1.43(1.30)
2025-09-03 03:02:33,638   INFO  Train:   20/20 (100%) [3821/3862 ( 99%)]  Loss: 0.9316 (0.762)  LR: 1.193e-08  Grad: 20.4418  max=0.8819(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.5358(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3375, loss_cls=0.0589, loss_bbox=0.3860, matched_ious=0.5884, d_time=0.00(0.01), f_time=1.23(1.29), b_time=1.23(1.30)  Time cost: 1:22:32/00:53 [29:18:02/00:53]  Acc_iter 77200       Data time: 0.00(0.01)  Forward time: 1.23(1.29)  Batch time: 1.23(1.30)
2025-09-03 03:03:25,100   INFO  Train:   20/20 (100%) [3861/3862 (100%)]  Loss: 0.6743 (0.762)  LR: 1.000e-08  Grad: 20.3764  max=0.8659(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.2137(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.3277, loss_cls=0.0560, loss_bbox=0.3938, matched_ious=0.5915, d_time=0.00(0.01), f_time=1.26(1.29), b_time=1.26(1.30)  Time cost: 1:23:24/00:01 [29:18:54/00:01]  Acc_iter 77240       Data time: 0.00(0.01)  Forward time: 1.26(1.29)  Batch time: 1.26(1.30)
train:   0%|          | 0/3862 [1:23:24<?, ?it/s]
epochs: 100%|██████████| 20/20 [29:18:54<00:00, 5096.66s/it]                                                              epochs: 100%|██████████| 20/20 [29:18:54<00:00, 5096.67s/it]                                                              epochs: 100%|██████████| 20/20 [29:18:54<00:00, 5096.75s/it]                                                              epochs: 100%|██████████| 20/20 [29:18:54<00:00, 5096.74s/it]  epochs: 100%|██████████| 20/20 [29:18:54<00:00, 5096.74s/it]                                                                                                                          epochs: 100%|██████████| 20/20 [29:18:54<00:00, 5096.74s/it]                                                              epochs: 100%|██████████| 20/20 [29:18:54<00:00, 5096.74s/it]                                                              epochs: 100%|██████████| 20/20 [29:18:55<00:00, 5096.72s/it]  epochs: 100%|██████████| 20/20 [29:18:55<00:00, 5276.76s/it]
2025-09-03 03:03:26,225   INFO  **********************End training sparse_models/sparse_former_light(default)**********************



2025-09-03 03:03:26,225   INFO  **********************Start evaluation sparse_models/sparse_former_light(default)**********************
2025-09-03 03:03:26,226   INFO  Loading NuScenes dataset
2025-09-03 03:03:26,614   INFO  Total samples for NuScenes dataset: 6019
2025-09-03 03:03:26,622   INFO  ==> Loading parameters from checkpoint /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/sparseformer/all/voxelnext_en_3layer_1lc_zhx_de_1layer_v3_32p_.0diff_.1lr_.1wd/ckpt/checkpoint_epoch_20.pth to CPU
2025-09-03 03:03:26,769   INFO  ==> Checkpoint trained from version: pcdet+0.6.0+8694e7f
2025-09-03 03:03:26,813   INFO  ==> Done (loaded 679/679)
2025-09-03 03:03:26,823   INFO  *************** EPOCH 20 EVALUATION *****************
eval:   0%|          | 0/189 [00:00<?, ?it/s]/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:66: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], boxes_b.shape[0]))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:66: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], boxes_b.shape[0]))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:66: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], boxes_b.shape[0]))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:66: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], boxes_b.shape[0]))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:66: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], boxes_b.shape[0]))).zero_()  # (N, M)
eval:   0%|          | 0/189 [00:03<?, ?it/s, recall_0.3=(0, 110) / 141]eval:   1%|          | 1/189 [00:03<11:24,  3.64s/it, recall_0.3=(0, 110) / 141]/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:66: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], boxes_b.shape[0]))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:66: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], boxes_b.shape[0]))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:66: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], boxes_b.shape[0]))).zero_()  # (N, M)
eval:   1%|          | 1/189 [00:04<11:24,  3.64s/it, recall_0.3=(0, 139) / 178]eval:   1%|          | 2/189 [00:04<06:18,  2.02s/it, recall_0.3=(0, 139) / 178]eval:   1%|          | 2/189 [00:05<06:18,  2.02s/it, recall_0.3=(0, 144) / 187]eval:   2%|▏         | 3/189 [00:05<04:09,  1.34s/it, recall_0.3=(0, 144) / 187]eval:   2%|▏         | 3/189 [00:05<04:09,  1.34s/it, recall_0.3=(0, 172) / 220]eval:   2%|▏         | 4/189 [00:05<03:03,  1.01it/s, recall_0.3=(0, 172) / 220]eval:   2%|▏         | 4/189 [00:06<03:03,  1.01it/s, recall_0.3=(0, 180) / 233]eval:   3%|▎         | 5/189 [00:06<02:34,  1.19it/s, recall_0.3=(0, 180) / 233]eval:   3%|▎         | 5/189 [00:06<02:34,  1.19it/s, recall_0.3=(0, 203) / 267]eval:   3%|▎         | 6/189 [00:06<02:06,  1.45it/s, recall_0.3=(0, 203) / 267]eval:   3%|▎         | 6/189 [00:06<02:06,  1.45it/s, recall_0.3=(0, 263) / 334]eval:   4%|▎         | 7/189 [00:06<01:47,  1.70it/s, recall_0.3=(0, 263) / 334]eval:   4%|▎         | 7/189 [00:07<01:47,  1.70it/s, recall_0.3=(0, 358) / 457]eval:   4%|▍         | 8/189 [00:07<01:36,  1.87it/s, recall_0.3=(0, 358) / 457]eval:   4%|▍         | 8/189 [00:07<01:36,  1.87it/s, recall_0.3=(0, 561) / 731]eval:   5%|▍         | 9/189 [00:07<01:27,  2.05it/s, recall_0.3=(0, 561) / 731]eval:   5%|▍         | 9/189 [00:08<01:27,  2.05it/s, recall_0.3=(0, 757) / 966]eval:   5%|▌         | 10/189 [00:08<01:21,  2.19it/s, recall_0.3=(0, 757) / 966]eval:   5%|▌         | 10/189 [00:08<01:21,  2.19it/s, recall_0.3=(0, 819) / 1051]eval:   6%|▌         | 11/189 [00:08<01:21,  2.19it/s, recall_0.3=(0, 819) / 1051]eval:   6%|▌         | 11/189 [00:08<01:21,  2.19it/s, recall_0.3=(0, 965) / 1242]eval:   6%|▋         | 12/189 [00:08<01:17,  2.29it/s, recall_0.3=(0, 965) / 1242]eval:   6%|▋         | 12/189 [00:09<01:17,  2.29it/s, recall_0.3=(0, 1001) / 1296]eval:   7%|▋         | 13/189 [00:09<01:15,  2.32it/s, recall_0.3=(0, 1001) / 1296]eval:   7%|▋         | 13/189 [00:09<01:15,  2.32it/s, recall_0.3=(0, 1038) / 1339]eval:   7%|▋         | 14/189 [00:09<01:12,  2.41it/s, recall_0.3=(0, 1038) / 1339]eval:   7%|▋         | 14/189 [00:10<01:12,  2.41it/s, recall_0.3=(0, 1076) / 1402]eval:   8%|▊         | 15/189 [00:10<01:13,  2.37it/s, recall_0.3=(0, 1076) / 1402]eval:   8%|▊         | 15/189 [00:10<01:13,  2.37it/s, recall_0.3=(0, 1131) / 1475]eval:   8%|▊         | 16/189 [00:10<01:08,  2.52it/s, recall_0.3=(0, 1131) / 1475]eval:   8%|▊         | 16/189 [00:10<01:08,  2.52it/s, recall_0.3=(0, 1228) / 1613]eval:   9%|▉         | 17/189 [00:10<01:09,  2.49it/s, recall_0.3=(0, 1228) / 1613]eval:   9%|▉         | 17/189 [00:11<01:09,  2.49it/s, recall_0.3=(0, 1289) / 1696]eval:  10%|▉         | 18/189 [00:11<01:10,  2.43it/s, recall_0.3=(0, 1289) / 1696]eval:  10%|▉         | 18/189 [00:11<01:10,  2.43it/s, recall_0.3=(0, 1343) / 1759]eval:  10%|█         | 19/189 [00:11<01:09,  2.44it/s, recall_0.3=(0, 1343) / 1759]eval:  10%|█         | 19/189 [00:12<01:09,  2.44it/s, recall_0.3=(0, 1420) / 1849]eval:  11%|█         | 20/189 [00:12<01:06,  2.54it/s, recall_0.3=(0, 1420) / 1849]eval:  11%|█         | 20/189 [00:12<01:06,  2.54it/s, recall_0.3=(0, 1554) / 1997]eval:  11%|█         | 21/189 [00:12<01:06,  2.53it/s, recall_0.3=(0, 1554) / 1997]eval:  11%|█         | 21/189 [00:12<01:06,  2.53it/s, recall_0.3=(0, 1710) / 2194]eval:  12%|█▏        | 22/189 [00:12<01:03,  2.62it/s, recall_0.3=(0, 1710) / 2194]eval:  12%|█▏        | 22/189 [00:13<01:03,  2.62it/s, recall_0.3=(0, 1797) / 2303]eval:  12%|█▏        | 23/189 [00:13<01:05,  2.52it/s, recall_0.3=(0, 1797) / 2303]eval:  12%|█▏        | 23/189 [00:13<01:05,  2.52it/s, recall_0.3=(0, 1862) / 2375]eval:  13%|█▎        | 24/189 [00:13<01:08,  2.41it/s, recall_0.3=(0, 1862) / 2375]eval:  13%|█▎        | 24/189 [00:14<01:08,  2.41it/s, recall_0.3=(0, 2024) / 2563]eval:  13%|█▎        | 25/189 [00:14<01:05,  2.50it/s, recall_0.3=(0, 2024) / 2563]eval:  13%|█▎        | 25/189 [00:14<01:05,  2.50it/s, recall_0.3=(0, 2174) / 2736]eval:  14%|█▍        | 26/189 [00:14<01:00,  2.69it/s, recall_0.3=(0, 2174) / 2736]eval:  14%|█▍        | 26/189 [00:14<01:00,  2.69it/s, recall_0.3=(0, 2355) / 2957]eval:  14%|█▍        | 27/189 [00:14<00:59,  2.71it/s, recall_0.3=(0, 2355) / 2957]eval:  14%|█▍        | 27/189 [00:15<00:59,  2.71it/s, recall_0.3=(0, 2501) / 3122]eval:  15%|█▍        | 28/189 [00:15<01:01,  2.61it/s, recall_0.3=(0, 2501) / 3122]eval:  15%|█▍        | 28/189 [00:15<01:01,  2.61it/s, recall_0.3=(0, 2614) / 3259]eval:  15%|█▌        | 29/189 [00:15<00:59,  2.70it/s, recall_0.3=(0, 2614) / 3259]eval:  15%|█▌        | 29/189 [00:15<00:59,  2.70it/s, recall_0.3=(0, 2775) / 3448]eval:  16%|█▌        | 30/189 [00:15<00:58,  2.70it/s, recall_0.3=(0, 2775) / 3448]eval:  16%|█▌        | 30/189 [00:16<00:58,  2.70it/s, recall_0.3=(0, 2903) / 3606]eval:  16%|█▋        | 31/189 [00:16<00:58,  2.70it/s, recall_0.3=(0, 2903) / 3606]eval:  16%|█▋        | 31/189 [00:16<00:58,  2.70it/s, recall_0.3=(0, 3118) / 3863]eval:  17%|█▋        | 32/189 [00:16<00:59,  2.63it/s, recall_0.3=(0, 3118) / 3863]eval:  17%|█▋        | 32/189 [00:17<00:59,  2.63it/s, recall_0.3=(0, 3347) / 4163]eval:  17%|█▋        | 33/189 [00:17<00:57,  2.70it/s, recall_0.3=(0, 3347) / 4163]eval:  17%|█▋        | 33/189 [00:17<00:57,  2.70it/s, recall_0.3=(0, 3521) / 4365]eval:  18%|█▊        | 34/189 [00:17<00:57,  2.71it/s, recall_0.3=(0, 3521) / 4365]eval:  18%|█▊        | 34/189 [00:17<00:57,  2.71it/s, recall_0.3=(0, 3636) / 4502]eval:  19%|█▊        | 35/189 [00:17<00:58,  2.65it/s, recall_0.3=(0, 3636) / 4502]eval:  19%|█▊        | 35/189 [00:18<00:58,  2.65it/s, recall_0.3=(0, 3803) / 4706]eval:  19%|█▉        | 36/189 [00:18<00:56,  2.73it/s, recall_0.3=(0, 3803) / 4706]eval:  19%|█▉        | 36/189 [00:18<00:56,  2.73it/s, recall_0.3=(0, 3933) / 4876]eval:  20%|█▉        | 37/189 [00:18<00:56,  2.69it/s, recall_0.3=(0, 3933) / 4876]eval:  20%|█▉        | 37/189 [00:18<00:56,  2.69it/s, recall_0.3=(0, 4041) / 5010]eval:  20%|██        | 38/189 [00:18<00:59,  2.56it/s, recall_0.3=(0, 4041) / 5010]eval:  20%|██        | 38/189 [00:19<00:59,  2.56it/s, recall_0.3=(0, 4103) / 5084]eval:  21%|██        | 39/189 [00:19<01:17,  1.94it/s, recall_0.3=(0, 4103) / 5084]eval:  21%|██        | 39/189 [00:20<01:17,  1.94it/s, recall_0.3=(0, 4139) / 5127]eval:  21%|██        | 40/189 [00:20<01:11,  2.07it/s, recall_0.3=(0, 4139) / 5127]eval:  21%|██        | 40/189 [00:20<01:11,  2.07it/s, recall_0.3=(0, 4201) / 5196]eval:  22%|██▏       | 41/189 [00:20<01:07,  2.19it/s, recall_0.3=(0, 4201) / 5196]eval:  22%|██▏       | 41/189 [00:20<01:07,  2.19it/s, recall_0.3=(0, 4272) / 5300]eval:  22%|██▏       | 42/189 [00:20<01:05,  2.25it/s, recall_0.3=(0, 4272) / 5300]eval:  22%|██▏       | 42/189 [00:21<01:05,  2.25it/s, recall_0.3=(0, 4359) / 5400]eval:  23%|██▎       | 43/189 [00:21<01:04,  2.25it/s, recall_0.3=(0, 4359) / 5400]eval:  23%|██▎       | 43/189 [00:21<01:04,  2.25it/s, recall_0.3=(0, 4427) / 5482]eval:  23%|██▎       | 44/189 [00:21<01:02,  2.33it/s, recall_0.3=(0, 4427) / 5482]eval:  23%|██▎       | 44/189 [00:22<01:02,  2.33it/s, recall_0.3=(0, 4454) / 5517]eval:  24%|██▍       | 45/189 [00:22<01:00,  2.39it/s, recall_0.3=(0, 4454) / 5517]eval:  24%|██▍       | 45/189 [00:22<01:00,  2.39it/s, recall_0.3=(0, 4488) / 5559]eval:  24%|██▍       | 46/189 [00:22<01:01,  2.34it/s, recall_0.3=(0, 4488) / 5559]eval:  24%|██▍       | 46/189 [00:23<01:01,  2.34it/s, recall_0.3=(0, 4544) / 5623]eval:  25%|██▍       | 47/189 [00:23<01:00,  2.34it/s, recall_0.3=(0, 4544) / 5623]eval:  25%|██▍       | 47/189 [00:23<01:00,  2.34it/s, recall_0.3=(0, 4724) / 5830]eval:  25%|██▌       | 48/189 [00:23<00:59,  2.35it/s, recall_0.3=(0, 4724) / 5830]eval:  25%|██▌       | 48/189 [00:23<00:59,  2.35it/s, recall_0.3=(0, 4868) / 6035]eval:  26%|██▌       | 49/189 [00:23<00:58,  2.41it/s, recall_0.3=(0, 4868) / 6035]eval:  26%|██▌       | 49/189 [00:24<00:58,  2.41it/s, recall_0.3=(0, 5021) / 6269]eval:  26%|██▋       | 50/189 [00:24<00:55,  2.48it/s, recall_0.3=(0, 5021) / 6269]eval:  26%|██▋       | 50/189 [00:24<00:55,  2.48it/s, recall_0.3=(0, 5212) / 6535]eval:  27%|██▋       | 51/189 [00:24<00:57,  2.40it/s, recall_0.3=(0, 5212) / 6535]eval:  27%|██▋       | 51/189 [00:25<00:57,  2.40it/s, recall_0.3=(0, 5337) / 6691]eval:  28%|██▊       | 52/189 [00:25<00:56,  2.41it/s, recall_0.3=(0, 5337) / 6691]eval:  28%|██▊       | 52/189 [00:25<00:56,  2.41it/s, recall_0.3=(0, 5428) / 6794]eval:  28%|██▊       | 53/189 [00:25<00:55,  2.46it/s, recall_0.3=(0, 5428) / 6794]eval:  28%|██▊       | 53/189 [00:25<00:55,  2.46it/s, recall_0.3=(0, 5513) / 6892]eval:  29%|██▊       | 54/189 [00:25<00:55,  2.45it/s, recall_0.3=(0, 5513) / 6892]eval:  29%|██▊       | 54/189 [00:26<00:55,  2.45it/s, recall_0.3=(0, 5592) / 7002]eval:  29%|██▉       | 55/189 [00:26<00:56,  2.37it/s, recall_0.3=(0, 5592) / 7002]eval:  29%|██▉       | 55/189 [00:26<00:56,  2.37it/s, recall_0.3=(0, 5670) / 7092]eval:  30%|██▉       | 56/189 [00:26<00:55,  2.42it/s, recall_0.3=(0, 5670) / 7092]eval:  30%|██▉       | 56/189 [00:27<00:55,  2.42it/s, recall_0.3=(0, 5789) / 7252]eval:  30%|███       | 57/189 [00:27<00:55,  2.36it/s, recall_0.3=(0, 5789) / 7252]eval:  30%|███       | 57/189 [00:27<00:55,  2.36it/s, recall_0.3=(0, 5976) / 7448]eval:  31%|███       | 58/189 [00:27<00:53,  2.43it/s, recall_0.3=(0, 5976) / 7448]eval:  31%|███       | 58/189 [00:27<00:53,  2.43it/s, recall_0.3=(0, 6143) / 7660]eval:  31%|███       | 59/189 [00:27<00:52,  2.48it/s, recall_0.3=(0, 6143) / 7660]eval:  31%|███       | 59/189 [00:28<00:52,  2.48it/s, recall_0.3=(0, 6341) / 7929]eval:  32%|███▏      | 60/189 [00:28<00:49,  2.58it/s, recall_0.3=(0, 6341) / 7929]eval:  32%|███▏      | 60/189 [00:28<00:49,  2.58it/s, recall_0.3=(0, 6504) / 8133]eval:  32%|███▏      | 61/189 [00:28<00:51,  2.50it/s, recall_0.3=(0, 6504) / 8133]eval:  32%|███▏      | 61/189 [00:29<00:51,  2.50it/s, recall_0.3=(0, 6528) / 8157]eval:  33%|███▎      | 62/189 [00:29<00:53,  2.39it/s, recall_0.3=(0, 6528) / 8157]eval:  33%|███▎      | 62/189 [00:29<00:53,  2.39it/s, recall_0.3=(0, 6690) / 8350]eval:  33%|███▎      | 63/189 [00:29<00:52,  2.40it/s, recall_0.3=(0, 6690) / 8350]eval:  33%|███▎      | 63/189 [00:30<00:52,  2.40it/s, recall_0.3=(0, 6786) / 8470]eval:  34%|███▍      | 64/189 [00:30<00:52,  2.40it/s, recall_0.3=(0, 6786) / 8470]eval:  34%|███▍      | 64/189 [00:30<00:52,  2.40it/s, recall_0.3=(0, 6895) / 8599]eval:  34%|███▍      | 65/189 [00:30<00:51,  2.40it/s, recall_0.3=(0, 6895) / 8599]eval:  34%|███▍      | 65/189 [00:30<00:51,  2.40it/s, recall_0.3=(0, 6971) / 8696]eval:  35%|███▍      | 66/189 [00:30<00:49,  2.47it/s, recall_0.3=(0, 6971) / 8696]eval:  35%|███▍      | 66/189 [00:31<00:49,  2.47it/s, recall_0.3=(0, 7095) / 8854]eval:  35%|███▌      | 67/189 [00:31<00:49,  2.49it/s, recall_0.3=(0, 7095) / 8854]eval:  35%|███▌      | 67/189 [00:31<00:49,  2.49it/s, recall_0.3=(0, 7215) / 9004]eval:  36%|███▌      | 68/189 [00:31<00:47,  2.56it/s, recall_0.3=(0, 7215) / 9004]eval:  36%|███▌      | 68/189 [00:32<00:47,  2.56it/s, recall_0.3=(0, 7307) / 9141]eval:  37%|███▋      | 69/189 [00:32<00:48,  2.50it/s, recall_0.3=(0, 7307) / 9141]eval:  37%|███▋      | 69/189 [00:32<00:48,  2.50it/s, recall_0.3=(0, 7383) / 9234]eval:  37%|███▋      | 70/189 [00:32<00:48,  2.47it/s, recall_0.3=(0, 7383) / 9234]eval:  37%|███▋      | 70/189 [00:32<00:48,  2.47it/s, recall_0.3=(0, 7471) / 9326]eval:  38%|███▊      | 71/189 [00:32<00:49,  2.37it/s, recall_0.3=(0, 7471) / 9326]eval:  38%|███▊      | 71/189 [00:33<00:49,  2.37it/s, recall_0.3=(0, 7550) / 9448]eval:  38%|███▊      | 72/189 [00:33<00:45,  2.55it/s, recall_0.3=(0, 7550) / 9448]eval:  38%|███▊      | 72/189 [00:33<00:45,  2.55it/s, recall_0.3=(0, 7615) / 9572]eval:  39%|███▊      | 73/189 [00:33<00:43,  2.69it/s, recall_0.3=(0, 7615) / 9572]eval:  39%|███▊      | 73/189 [00:33<00:43,  2.69it/s, recall_0.3=(0, 7693) / 9705]eval:  39%|███▉      | 74/189 [00:33<00:42,  2.71it/s, recall_0.3=(0, 7693) / 9705]eval:  39%|███▉      | 74/189 [00:34<00:42,  2.71it/s, recall_0.3=(0, 7817) / 9868]eval:  40%|███▉      | 75/189 [00:34<00:42,  2.71it/s, recall_0.3=(0, 7817) / 9868]eval:  40%|███▉      | 75/189 [00:34<00:42,  2.71it/s, recall_0.3=(0, 7854) / 9915]eval:  40%|████      | 76/189 [00:34<00:41,  2.72it/s, recall_0.3=(0, 7854) / 9915]eval:  40%|████      | 76/189 [00:34<00:41,  2.72it/s, recall_0.3=(0, 7902) / 9973]eval:  41%|████      | 77/189 [00:34<00:40,  2.79it/s, recall_0.3=(0, 7902) / 9973]eval:  41%|████      | 77/189 [00:35<00:40,  2.79it/s, recall_0.3=(0, 7965) / 10051]eval:  41%|████▏     | 78/189 [00:35<00:40,  2.75it/s, recall_0.3=(0, 7965) / 10051]eval:  41%|████▏     | 78/189 [00:35<00:40,  2.75it/s, recall_0.3=(0, 8052) / 10155]eval:  42%|████▏     | 79/189 [00:35<00:39,  2.79it/s, recall_0.3=(0, 8052) / 10155]eval:  42%|████▏     | 79/189 [00:36<00:39,  2.79it/s, recall_0.3=(0, 8136) / 10254]eval:  42%|████▏     | 80/189 [00:36<00:40,  2.71it/s, recall_0.3=(0, 8136) / 10254]eval:  42%|████▏     | 80/189 [00:36<00:40,  2.71it/s, recall_0.3=(0, 8197) / 10340]eval:  43%|████▎     | 81/189 [00:36<00:38,  2.80it/s, recall_0.3=(0, 8197) / 10340]eval:  43%|████▎     | 81/189 [00:36<00:38,  2.80it/s, recall_0.3=(0, 8290) / 10456]eval:  43%|████▎     | 82/189 [00:36<00:37,  2.88it/s, recall_0.3=(0, 8290) / 10456]eval:  43%|████▎     | 82/189 [00:37<00:37,  2.88it/s, recall_0.3=(0, 8466) / 10664]eval:  44%|████▍     | 83/189 [00:37<00:40,  2.64it/s, recall_0.3=(0, 8466) / 10664]eval:  44%|████▍     | 83/189 [00:37<00:40,  2.64it/s, recall_0.3=(0, 8590) / 10833]eval:  44%|████▍     | 84/189 [00:37<00:40,  2.61it/s, recall_0.3=(0, 8590) / 10833]eval:  44%|████▍     | 84/189 [00:37<00:40,  2.61it/s, recall_0.3=(0, 8695) / 10957]eval:  45%|████▍     | 85/189 [00:37<00:38,  2.72it/s, recall_0.3=(0, 8695) / 10957]eval:  45%|████▍     | 85/189 [00:38<00:38,  2.72it/s, recall_0.3=(0, 8804) / 11089]eval:  46%|████▌     | 86/189 [00:38<00:37,  2.73it/s, recall_0.3=(0, 8804) / 11089]eval:  46%|████▌     | 86/189 [00:38<00:37,  2.73it/s, recall_0.3=(0, 8914) / 11238]eval:  46%|████▌     | 87/189 [00:38<00:38,  2.68it/s, recall_0.3=(0, 8914) / 11238]eval:  46%|████▌     | 87/189 [00:39<00:38,  2.68it/s, recall_0.3=(0, 8990) / 11331]eval:  47%|████▋     | 88/189 [00:39<00:37,  2.68it/s, recall_0.3=(0, 8990) / 11331]eval:  47%|████▋     | 88/189 [00:39<00:37,  2.68it/s, recall_0.3=(0, 9102) / 11465]eval:  47%|████▋     | 89/189 [00:39<00:36,  2.74it/s, recall_0.3=(0, 9102) / 11465]eval:  47%|████▋     | 89/189 [00:39<00:36,  2.74it/s, recall_0.3=(0, 9190) / 11579]eval:  48%|████▊     | 90/189 [00:39<00:35,  2.81it/s, recall_0.3=(0, 9190) / 11579]eval:  48%|████▊     | 90/189 [00:40<00:35,  2.81it/s, recall_0.3=(0, 9235) / 11633]eval:  48%|████▊     | 91/189 [00:40<00:34,  2.86it/s, recall_0.3=(0, 9235) / 11633]eval:  48%|████▊     | 91/189 [00:40<00:34,  2.86it/s, recall_0.3=(0, 9330) / 11769]eval:  49%|████▊     | 92/189 [00:40<00:34,  2.81it/s, recall_0.3=(0, 9330) / 11769]eval:  49%|████▊     | 92/189 [00:40<00:34,  2.81it/s, recall_0.3=(0, 9396) / 11858]eval:  49%|████▉     | 93/189 [00:40<00:34,  2.75it/s, recall_0.3=(0, 9396) / 11858]eval:  49%|████▉     | 93/189 [00:41<00:34,  2.75it/s, recall_0.3=(0, 9432) / 11901]eval:  50%|████▉     | 94/189 [00:41<00:35,  2.71it/s, recall_0.3=(0, 9432) / 11901]eval:  50%|████▉     | 94/189 [00:41<00:35,  2.71it/s, recall_0.3=(0, 9460) / 11941]eval:  50%|█████     | 95/189 [00:41<00:34,  2.74it/s, recall_0.3=(0, 9460) / 11941]eval:  50%|█████     | 95/189 [00:41<00:34,  2.74it/s, recall_0.3=(0, 9514) / 12013]eval:  51%|█████     | 96/189 [00:41<00:34,  2.68it/s, recall_0.3=(0, 9514) / 12013]eval:  51%|█████     | 96/189 [00:42<00:34,  2.68it/s, recall_0.3=(0, 9659) / 12218]eval:  51%|█████▏    | 97/189 [00:42<00:33,  2.73it/s, recall_0.3=(0, 9659) / 12218]eval:  51%|█████▏    | 97/189 [00:42<00:33,  2.73it/s, recall_0.3=(0, 9803) / 12386]eval:  52%|█████▏    | 98/189 [00:42<00:32,  2.81it/s, recall_0.3=(0, 9803) / 12386]eval:  52%|█████▏    | 98/189 [00:42<00:32,  2.81it/s, recall_0.3=(0, 9917) / 12527]eval:  52%|█████▏    | 99/189 [00:42<00:31,  2.85it/s, recall_0.3=(0, 9917) / 12527]eval:  52%|█████▏    | 99/189 [00:43<00:31,  2.85it/s, recall_0.3=(0, 9975) / 12600]eval:  53%|█████▎    | 100/189 [00:43<00:31,  2.79it/s, recall_0.3=(0, 9975) / 12600]eval:  53%|█████▎    | 100/189 [00:43<00:31,  2.79it/s, recall_0.3=(0, 10064) / 12707]eval:  53%|█████▎    | 101/189 [00:43<00:32,  2.68it/s, recall_0.3=(0, 10064) / 12707]eval:  53%|█████▎    | 101/189 [00:44<00:32,  2.68it/s, recall_0.3=(0, 10149) / 12814]eval:  54%|█████▍    | 102/189 [00:44<00:34,  2.55it/s, recall_0.3=(0, 10149) / 12814]eval:  54%|█████▍    | 102/189 [00:44<00:34,  2.55it/s, recall_0.3=(0, 10187) / 12860]eval:  54%|█████▍    | 103/189 [00:44<00:33,  2.57it/s, recall_0.3=(0, 10187) / 12860]eval:  54%|█████▍    | 103/189 [00:44<00:33,  2.57it/s, recall_0.3=(0, 10240) / 12935]eval:  55%|█████▌    | 104/189 [00:44<00:31,  2.72it/s, recall_0.3=(0, 10240) / 12935]eval:  55%|█████▌    | 104/189 [00:45<00:31,  2.72it/s, recall_0.3=(0, 10303) / 13023]eval:  56%|█████▌    | 105/189 [00:45<00:30,  2.72it/s, recall_0.3=(0, 10303) / 13023]eval:  56%|█████▌    | 105/189 [00:45<00:30,  2.72it/s, recall_0.3=(0, 10372) / 13113]eval:  56%|█████▌    | 106/189 [00:45<00:29,  2.84it/s, recall_0.3=(0, 10372) / 13113]eval:  56%|█████▌    | 106/189 [00:45<00:29,  2.84it/s, recall_0.3=(0, 10431) / 13190]eval:  57%|█████▋    | 107/189 [00:45<00:28,  2.91it/s, recall_0.3=(0, 10431) / 13190]eval:  57%|█████▋    | 107/189 [00:46<00:28,  2.91it/s, recall_0.3=(0, 10475) / 13257]eval:  57%|█████▋    | 108/189 [00:46<00:28,  2.89it/s, recall_0.3=(0, 10475) / 13257]eval:  57%|█████▋    | 108/189 [00:46<00:28,  2.89it/s, recall_0.3=(0, 10534) / 13328]eval:  58%|█████▊    | 109/189 [00:46<00:27,  2.89it/s, recall_0.3=(0, 10534) / 13328]eval:  58%|█████▊    | 109/189 [00:46<00:27,  2.89it/s, recall_0.3=(0, 10618) / 13418]eval:  58%|█████▊    | 110/189 [00:46<00:26,  3.02it/s, recall_0.3=(0, 10618) / 13418]eval:  58%|█████▊    | 110/189 [00:47<00:26,  3.02it/s, recall_0.3=(0, 10722) / 13526]eval:  59%|█████▊    | 111/189 [00:47<00:25,  3.05it/s, recall_0.3=(0, 10722) / 13526]eval:  59%|█████▊    | 111/189 [00:47<00:25,  3.05it/s, recall_0.3=(0, 10810) / 13622]eval:  59%|█████▉    | 112/189 [00:47<00:25,  3.00it/s, recall_0.3=(0, 10810) / 13622]eval:  59%|█████▉    | 112/189 [00:47<00:25,  3.00it/s, recall_0.3=(0, 10865) / 13691]eval:  60%|█████▉    | 113/189 [00:47<00:26,  2.87it/s, recall_0.3=(0, 10865) / 13691]eval:  60%|█████▉    | 113/189 [00:48<00:26,  2.87it/s, recall_0.3=(0, 10954) / 13809]eval:  60%|██████    | 114/189 [00:48<00:27,  2.74it/s, recall_0.3=(0, 10954) / 13809]eval:  60%|██████    | 114/189 [00:48<00:27,  2.74it/s, recall_0.3=(0, 11018) / 13908]eval:  61%|██████    | 115/189 [00:48<00:26,  2.81it/s, recall_0.3=(0, 11018) / 13908]eval:  61%|██████    | 115/189 [00:49<00:26,  2.81it/s, recall_0.3=(0, 11129) / 14030]eval:  61%|██████▏   | 116/189 [00:49<00:26,  2.78it/s, recall_0.3=(0, 11129) / 14030]eval:  61%|██████▏   | 116/189 [00:49<00:26,  2.78it/s, recall_0.3=(0, 11236) / 14161]eval:  62%|██████▏   | 117/189 [00:49<00:28,  2.52it/s, recall_0.3=(0, 11236) / 14161]eval:  62%|██████▏   | 117/189 [00:49<00:28,  2.52it/s, recall_0.3=(0, 11302) / 14256]eval:  62%|██████▏   | 118/189 [00:49<00:29,  2.41it/s, recall_0.3=(0, 11302) / 14256]eval:  62%|██████▏   | 118/189 [00:50<00:29,  2.41it/s, recall_0.3=(0, 11328) / 14289]eval:  63%|██████▎   | 119/189 [00:50<00:30,  2.33it/s, recall_0.3=(0, 11328) / 14289]eval:  63%|██████▎   | 119/189 [00:50<00:30,  2.33it/s, recall_0.3=(0, 11349) / 14319]eval:  63%|██████▎   | 120/189 [00:50<00:28,  2.42it/s, recall_0.3=(0, 11349) / 14319]eval:  63%|██████▎   | 120/189 [00:51<00:28,  2.42it/s, recall_0.3=(0, 11396) / 14380]eval:  64%|██████▍   | 121/189 [00:51<00:28,  2.41it/s, recall_0.3=(0, 11396) / 14380]eval:  64%|██████▍   | 121/189 [00:51<00:28,  2.41it/s, recall_0.3=(0, 11435) / 14428]eval:  65%|██████▍   | 122/189 [00:51<00:28,  2.32it/s, recall_0.3=(0, 11435) / 14428]eval:  65%|██████▍   | 122/189 [00:52<00:28,  2.32it/s, recall_0.3=(0, 11466) / 14474]eval:  65%|██████▌   | 123/189 [00:52<00:29,  2.21it/s, recall_0.3=(0, 11466) / 14474]eval:  65%|██████▌   | 123/189 [00:52<00:29,  2.21it/s, recall_0.3=(0, 11491) / 14511]eval:  66%|██████▌   | 124/189 [00:52<00:29,  2.22it/s, recall_0.3=(0, 11491) / 14511]eval:  66%|██████▌   | 124/189 [00:53<00:29,  2.22it/s, recall_0.3=(0, 11524) / 14555]eval:  66%|██████▌   | 125/189 [00:53<00:28,  2.24it/s, recall_0.3=(0, 11524) / 14555]eval:  66%|██████▌   | 125/189 [00:53<00:28,  2.24it/s, recall_0.3=(0, 11637) / 14684]eval:  67%|██████▋   | 126/189 [00:53<00:25,  2.43it/s, recall_0.3=(0, 11637) / 14684]eval:  67%|██████▋   | 126/189 [00:53<00:25,  2.43it/s, recall_0.3=(0, 11736) / 14817]eval:  67%|██████▋   | 127/189 [00:53<00:24,  2.48it/s, recall_0.3=(0, 11736) / 14817]eval:  67%|██████▋   | 127/189 [00:54<00:24,  2.48it/s, recall_0.3=(0, 11781) / 14877]eval:  68%|██████▊   | 128/189 [00:54<00:23,  2.55it/s, recall_0.3=(0, 11781) / 14877]eval:  68%|██████▊   | 128/189 [00:54<00:23,  2.55it/s, recall_0.3=(0, 11883) / 15004]eval:  68%|██████▊   | 129/189 [00:54<00:23,  2.56it/s, recall_0.3=(0, 11883) / 15004]eval:  68%|██████▊   | 129/189 [00:54<00:23,  2.56it/s, recall_0.3=(0, 11930) / 15069]eval:  69%|██████▉   | 130/189 [00:54<00:23,  2.51it/s, recall_0.3=(0, 11930) / 15069]eval:  69%|██████▉   | 130/189 [00:55<00:23,  2.51it/s, recall_0.3=(0, 11967) / 15114]eval:  69%|██████▉   | 131/189 [00:55<00:22,  2.57it/s, recall_0.3=(0, 11967) / 15114]eval:  69%|██████▉   | 131/189 [00:55<00:22,  2.57it/s, recall_0.3=(0, 12056) / 15224]eval:  70%|██████▉   | 132/189 [00:55<00:24,  2.36it/s, recall_0.3=(0, 12056) / 15224]eval:  70%|██████▉   | 132/189 [00:56<00:24,  2.36it/s, recall_0.3=(0, 12122) / 15304]eval:  70%|███████   | 133/189 [00:56<00:22,  2.46it/s, recall_0.3=(0, 12122) / 15304]eval:  70%|███████   | 133/189 [00:56<00:22,  2.46it/s, recall_0.3=(0, 12230) / 15435]eval:  71%|███████   | 134/189 [00:56<00:22,  2.48it/s, recall_0.3=(0, 12230) / 15435]eval:  71%|███████   | 134/189 [00:56<00:22,  2.48it/s, recall_0.3=(0, 12307) / 15528]eval:  71%|███████▏  | 135/189 [00:56<00:21,  2.52it/s, recall_0.3=(0, 12307) / 15528]eval:  71%|███████▏  | 135/189 [00:57<00:21,  2.52it/s, recall_0.3=(0, 12383) / 15618]eval:  72%|███████▏  | 136/189 [00:57<00:20,  2.55it/s, recall_0.3=(0, 12383) / 15618]eval:  72%|███████▏  | 136/189 [00:57<00:20,  2.55it/s, recall_0.3=(0, 12497) / 15760]eval:  72%|███████▏  | 137/189 [00:57<00:20,  2.49it/s, recall_0.3=(0, 12497) / 15760]eval:  72%|███████▏  | 137/189 [00:58<00:20,  2.49it/s, recall_0.3=(0, 12717) / 16017]eval:  73%|███████▎  | 138/189 [00:58<00:20,  2.43it/s, recall_0.3=(0, 12717) / 16017]eval:  73%|███████▎  | 138/189 [00:58<00:20,  2.43it/s, recall_0.3=(0, 12765) / 16073]eval:  74%|███████▎  | 139/189 [00:58<00:20,  2.46it/s, recall_0.3=(0, 12765) / 16073]eval:  74%|███████▎  | 139/189 [00:58<00:20,  2.46it/s, recall_0.3=(0, 12849) / 16175]eval:  74%|███████▍  | 140/189 [00:58<00:19,  2.54it/s, recall_0.3=(0, 12849) / 16175]eval:  74%|███████▍  | 140/189 [00:59<00:19,  2.54it/s, recall_0.3=(0, 13016) / 16360]eval:  75%|███████▍  | 141/189 [00:59<00:18,  2.55it/s, recall_0.3=(0, 13016) / 16360]eval:  75%|███████▍  | 141/189 [00:59<00:18,  2.55it/s, recall_0.3=(0, 13115) / 16470]eval:  75%|███████▌  | 142/189 [00:59<00:18,  2.48it/s, recall_0.3=(0, 13115) / 16470]eval:  75%|███████▌  | 142/189 [01:00<00:18,  2.48it/s, recall_0.3=(0, 13164) / 16523]eval:  76%|███████▌  | 143/189 [01:00<00:18,  2.49it/s, recall_0.3=(0, 13164) / 16523]eval:  76%|███████▌  | 143/189 [01:00<00:18,  2.49it/s, recall_0.3=(0, 13169) / 16529]eval:  76%|███████▌  | 144/189 [01:00<00:18,  2.45it/s, recall_0.3=(0, 13169) / 16529]eval:  76%|███████▌  | 144/189 [01:01<00:18,  2.45it/s, recall_0.3=(0, 13187) / 16553]eval:  77%|███████▋  | 145/189 [01:01<00:17,  2.45it/s, recall_0.3=(0, 13187) / 16553]eval:  77%|███████▋  | 145/189 [01:01<00:17,  2.45it/s, recall_0.3=(0, 13215) / 16584]eval:  77%|███████▋  | 146/189 [01:01<00:17,  2.48it/s, recall_0.3=(0, 13215) / 16584]eval:  77%|███████▋  | 146/189 [01:01<00:17,  2.48it/s, recall_0.3=(0, 13243) / 16615]eval:  78%|███████▊  | 147/189 [01:01<00:17,  2.38it/s, recall_0.3=(0, 13243) / 16615]eval:  78%|███████▊  | 147/189 [01:02<00:17,  2.38it/s, recall_0.3=(0, 13268) / 16641]eval:  78%|███████▊  | 148/189 [01:02<00:17,  2.41it/s, recall_0.3=(0, 13268) / 16641]eval:  78%|███████▊  | 148/189 [01:02<00:17,  2.41it/s, recall_0.3=(0, 13286) / 16671]eval:  79%|███████▉  | 149/189 [01:02<00:17,  2.29it/s, recall_0.3=(0, 13286) / 16671]eval:  79%|███████▉  | 149/189 [01:03<00:17,  2.29it/s, recall_0.3=(0, 13317) / 16712]eval:  79%|███████▉  | 150/189 [01:03<00:17,  2.26it/s, recall_0.3=(0, 13317) / 16712]eval:  79%|███████▉  | 150/189 [01:03<00:17,  2.26it/s, recall_0.3=(0, 13353) / 16756]eval:  80%|███████▉  | 151/189 [01:03<00:16,  2.32it/s, recall_0.3=(0, 13353) / 16756]eval:  80%|███████▉  | 151/189 [01:04<00:16,  2.32it/s, recall_0.3=(0, 13382) / 16791]eval:  80%|████████  | 152/189 [01:04<00:15,  2.38it/s, recall_0.3=(0, 13382) / 16791]eval:  80%|████████  | 152/189 [01:04<00:15,  2.38it/s, recall_0.3=(0, 13421) / 16838]eval:  81%|████████  | 153/189 [01:04<00:14,  2.44it/s, recall_0.3=(0, 13421) / 16838]eval:  81%|████████  | 153/189 [01:04<00:14,  2.44it/s, recall_0.3=(0, 13472) / 16900]eval:  81%|████████▏ | 154/189 [01:04<00:14,  2.45it/s, recall_0.3=(0, 13472) / 16900]eval:  81%|████████▏ | 154/189 [01:05<00:14,  2.45it/s, recall_0.3=(0, 13569) / 17014]eval:  82%|████████▏ | 155/189 [01:05<00:13,  2.45it/s, recall_0.3=(0, 13569) / 17014]eval:  82%|████████▏ | 155/189 [01:05<00:13,  2.45it/s, recall_0.3=(0, 13608) / 17068]eval:  83%|████████▎ | 156/189 [01:05<00:13,  2.42it/s, recall_0.3=(0, 13608) / 17068]eval:  83%|████████▎ | 156/189 [01:06<00:13,  2.42it/s, recall_0.3=(0, 13613) / 17080]eval:  83%|████████▎ | 157/189 [01:06<00:12,  2.47it/s, recall_0.3=(0, 13613) / 17080]eval:  83%|████████▎ | 157/189 [01:06<00:12,  2.47it/s, recall_0.3=(0, 13671) / 17139]eval:  84%|████████▎ | 158/189 [01:06<00:12,  2.47it/s, recall_0.3=(0, 13671) / 17139]eval:  84%|████████▎ | 158/189 [01:06<00:12,  2.47it/s, recall_0.3=(0, 13762) / 17242]eval:  84%|████████▍ | 159/189 [01:06<00:12,  2.49it/s, recall_0.3=(0, 13762) / 17242]eval:  84%|████████▍ | 159/189 [01:07<00:12,  2.49it/s, recall_0.3=(0, 13793) / 17284]eval:  85%|████████▍ | 160/189 [01:07<00:11,  2.45it/s, recall_0.3=(0, 13793) / 17284]eval:  85%|████████▍ | 160/189 [01:07<00:11,  2.45it/s, recall_0.3=(0, 13836) / 17363]eval:  85%|████████▌ | 161/189 [01:07<00:11,  2.34it/s, recall_0.3=(0, 13836) / 17363]eval:  85%|████████▌ | 161/189 [01:08<00:11,  2.34it/s, recall_0.3=(0, 13948) / 17508]eval:  86%|████████▌ | 162/189 [01:08<00:11,  2.34it/s, recall_0.3=(0, 13948) / 17508]eval:  86%|████████▌ | 162/189 [01:08<00:11,  2.34it/s, recall_0.3=(0, 14078) / 17682]eval:  86%|████████▌ | 163/189 [01:08<00:10,  2.46it/s, recall_0.3=(0, 14078) / 17682]eval:  86%|████████▌ | 163/189 [01:08<00:10,  2.46it/s, recall_0.3=(0, 14134) / 17747]eval:  87%|████████▋ | 164/189 [01:08<00:09,  2.57it/s, recall_0.3=(0, 14134) / 17747]eval:  87%|████████▋ | 164/189 [01:09<00:09,  2.57it/s, recall_0.3=(0, 14197) / 17841]eval:  87%|████████▋ | 165/189 [01:09<00:09,  2.64it/s, recall_0.3=(0, 14197) / 17841]eval:  87%|████████▋ | 165/189 [01:09<00:09,  2.64it/s, recall_0.3=(0, 14278) / 17958]eval:  88%|████████▊ | 166/189 [01:09<00:08,  2.68it/s, recall_0.3=(0, 14278) / 17958]eval:  88%|████████▊ | 166/189 [01:09<00:08,  2.68it/s, recall_0.3=(0, 14422) / 18127]eval:  88%|████████▊ | 167/189 [01:09<00:08,  2.69it/s, recall_0.3=(0, 14422) / 18127]eval:  88%|████████▊ | 167/189 [01:10<00:08,  2.69it/s, recall_0.3=(0, 14526) / 18247]eval:  89%|████████▉ | 168/189 [01:10<00:07,  2.72it/s, recall_0.3=(0, 14526) / 18247]eval:  89%|████████▉ | 168/189 [01:10<00:07,  2.72it/s, recall_0.3=(0, 14587) / 18322]eval:  89%|████████▉ | 169/189 [01:10<00:07,  2.75it/s, recall_0.3=(0, 14587) / 18322]eval:  89%|████████▉ | 169/189 [01:11<00:07,  2.75it/s, recall_0.3=(0, 14644) / 18382]eval:  90%|████████▉ | 170/189 [01:11<00:06,  2.75it/s, recall_0.3=(0, 14644) / 18382]eval:  90%|████████▉ | 170/189 [01:11<00:06,  2.75it/s, recall_0.3=(0, 14657) / 18395]eval:  90%|█████████ | 171/189 [01:11<00:06,  2.81it/s, recall_0.3=(0, 14657) / 18395]eval:  90%|█████████ | 171/189 [01:11<00:06,  2.81it/s, recall_0.3=(0, 14658) / 18396]eval:  91%|█████████ | 172/189 [01:11<00:06,  2.80it/s, recall_0.3=(0, 14658) / 18396]eval:  91%|█████████ | 172/189 [01:12<00:06,  2.80it/s, recall_0.3=(0, 14663) / 18402]eval:  92%|█████████▏| 173/189 [01:12<00:05,  2.86it/s, recall_0.3=(0, 14663) / 18402]eval:  92%|█████████▏| 173/189 [01:12<00:05,  2.86it/s, recall_0.3=(0, 14672) / 18419]eval:  92%|█████████▏| 174/189 [01:12<00:05,  2.84it/s, recall_0.3=(0, 14672) / 18419]eval:  92%|█████████▏| 174/189 [01:12<00:05,  2.84it/s, recall_0.3=(0, 14697) / 18448]eval:  93%|█████████▎| 175/189 [01:12<00:04,  2.80it/s, recall_0.3=(0, 14697) / 18448]eval:  93%|█████████▎| 175/189 [01:13<00:04,  2.80it/s, recall_0.3=(0, 14702) / 18454]eval:  93%|█████████▎| 176/189 [01:13<00:04,  2.76it/s, recall_0.3=(0, 14702) / 18454]eval:  93%|█████████▎| 176/189 [01:13<00:04,  2.76it/s, recall_0.3=(0, 14716) / 18472]eval:  94%|█████████▎| 177/189 [01:13<00:04,  2.74it/s, recall_0.3=(0, 14716) / 18472]eval:  94%|█████████▎| 177/189 [01:13<00:04,  2.74it/s, recall_0.3=(0, 14724) / 18480]eval:  94%|█████████▍| 178/189 [01:13<00:04,  2.70it/s, recall_0.3=(0, 14724) / 18480]eval:  94%|█████████▍| 178/189 [01:14<00:04,  2.70it/s, recall_0.3=(0, 14750) / 18509]eval:  95%|█████████▍| 179/189 [01:14<00:03,  2.66it/s, recall_0.3=(0, 14750) / 18509]eval:  95%|█████████▍| 179/189 [01:14<00:03,  2.66it/s, recall_0.3=(0, 14770) / 18533]eval:  95%|█████████▌| 180/189 [01:14<00:03,  2.64it/s, recall_0.3=(0, 14770) / 18533]eval:  95%|█████████▌| 180/189 [01:15<00:03,  2.64it/s, recall_0.3=(0, 14791) / 18559]eval:  96%|█████████▌| 181/189 [01:15<00:02,  2.77it/s, recall_0.3=(0, 14791) / 18559]eval:  96%|█████████▌| 181/189 [01:15<00:02,  2.77it/s, recall_0.3=(0, 14820) / 18589]eval:  96%|█████████▋| 182/189 [01:15<00:02,  2.75it/s, recall_0.3=(0, 14820) / 18589]eval:  96%|█████████▋| 182/189 [01:15<00:02,  2.75it/s, recall_0.3=(0, 14867) / 18641]eval:  97%|█████████▋| 183/189 [01:15<00:02,  2.63it/s, recall_0.3=(0, 14867) / 18641]eval:  97%|█████████▋| 183/189 [01:16<00:02,  2.63it/s, recall_0.3=(0, 14904) / 18681]eval:  97%|█████████▋| 184/189 [01:16<00:01,  2.55it/s, recall_0.3=(0, 14904) / 18681]eval:  97%|█████████▋| 184/189 [01:16<00:01,  2.55it/s, recall_0.3=(0, 14926) / 18706]eval:  98%|█████████▊| 185/189 [01:16<00:01,  2.49it/s, recall_0.3=(0, 14926) / 18706]eval:  98%|█████████▊| 185/189 [01:17<00:01,  2.49it/s, recall_0.3=(0, 14940) / 18725]eval:  98%|█████████▊| 186/189 [01:17<00:01,  2.52it/s, recall_0.3=(0, 14940) / 18725]eval:  98%|█████████▊| 186/189 [01:17<00:01,  2.52it/s, recall_0.3=(0, 14959) / 18749]eval:  99%|█████████▉| 187/189 [01:17<00:00,  2.58it/s, recall_0.3=(0, 14959) / 18749]eval:  99%|█████████▉| 187/189 [01:17<00:00,  2.58it/s, recall_0.3=(0, 15016) / 18809]eval:  99%|█████████▉| 188/189 [01:17<00:00,  2.74it/s, recall_0.3=(0, 15016) / 18809]eval:  99%|█████████▉| 188/189 [01:17<00:00,  2.74it/s, recall_0.3=(0, 15032) / 18825]eval: 100%|██████████| 189/189 [01:17<00:00,  3.24it/s, recall_0.3=(0, 15032) / 18825]eval: 100%|██████████| 189/189 [01:18<00:00,  2.42it/s, recall_0.3=(0, 15032) / 18825]
2025-09-03 03:04:47,469   INFO  *************** Performance of EPOCH 20 *****************
2025-09-03 03:04:47,469   INFO  Generate label finished(sec_per_example: 0.0134 second).
2025-09-03 03:04:47,469   INFO  recall_roi_0.3: 0.000000
2025-09-03 03:04:47,469   INFO  recall_rcnn_0.3: 0.803192
2025-09-03 03:04:47,469   INFO  recall_roi_0.5: 0.000000
2025-09-03 03:04:47,469   INFO  recall_rcnn_0.5: 0.610093
2025-09-03 03:04:47,469   INFO  recall_roi_0.7: 0.000000
2025-09-03 03:04:47,469   INFO  recall_rcnn_0.7: 0.295629
2025-09-03 03:04:47,472   INFO  Average predicted number of objects(6019 samples): 178.659
======
Loading NuScenes tables for version v1.0-trainval...
32 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 31.9 seconds.
======
Reverse indexing ...
Done reverse indexing in 7.3 seconds.
======
2025-09-03 03:07:36,135   INFO  The predictions of NuScenes have been saved to /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/sparseformer/all/voxelnext_en_3layer_1lc_zhx_de_1layer_v3_32p_.0diff_.1lr_.1wd/eval/eval_with_train/epoch_20/val/final_result/data/results_nusc.json
Initializing nuScenes detection evaluation
Loaded results from /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/sparseformer/all/voxelnext_en_3layer_1lc_zhx_de_1layer_v3_32p_.0diff_.1lr_.1wd/eval/eval_with_train/epoch_20/val/final_result/data/results_nusc.json. Found detections for 6019 samples.
Loading annotations for val split from nuScenes version: v1.0-trainval
  0%|          | 0/6019 [00:00<?, ?it/s]  4%|▎         | 216/6019 [00:00<00:02, 2151.96it/s]  7%|▋         | 432/6019 [00:00<00:05, 1028.19it/s]  9%|▉         | 565/6019 [00:00<00:05, 1065.60it/s] 11%|█▏        | 690/6019 [00:00<00:05, 1034.25it/s] 13%|█▎        | 804/6019 [00:00<00:05, 989.42it/s]  15%|█▌        | 909/6019 [00:00<00:05, 871.51it/s] 17%|█▋        | 1001/6019 [00:01<00:06, 819.84it/s] 18%|█▊        | 1086/6019 [00:01<00:07, 665.42it/s] 19%|█▉        | 1166/6019 [00:01<00:06, 694.60it/s] 21%|██▏       | 1288/6019 [00:01<00:05, 819.75it/s] 24%|██▎       | 1417/6019 [00:01<00:04, 939.54it/s] 25%|██▌       | 1534/6019 [00:01<00:04, 996.67it/s] 27%|██▋       | 1640/6019 [00:01<00:05, 784.80it/s] 29%|██▉       | 1737/6019 [00:01<00:05, 826.77it/s] 30%|███       | 1832/6019 [00:02<00:04, 853.82it/s] 32%|███▏      | 1924/6019 [00:02<00:05, 724.54it/s] 33%|███▎      | 2008/6019 [00:02<00:05, 750.88it/s] 35%|███▍      | 2089/6019 [00:02<00:05, 746.03it/s] 36%|███▌      | 2168/6019 [00:02<00:05, 728.36it/s] 37%|███▋      | 2247/6019 [00:02<00:05, 743.96it/s] 39%|███▊      | 2326/6019 [00:02<00:04, 753.96it/s] 40%|███▉      | 2403/6019 [00:02<00:04, 732.37it/s] 42%|████▏     | 2539/6019 [00:02<00:03, 906.81it/s] 44%|████▍     | 2638/6019 [00:03<00:03, 926.68it/s] 45%|████▌     | 2733/6019 [00:03<00:03, 841.76it/s] 47%|████▋     | 2823/6019 [00:03<00:03, 855.78it/s] 49%|████▊     | 2925/6019 [00:03<00:03, 898.52it/s] 51%|█████     | 3063/6019 [00:03<00:02, 1035.11it/s] 53%|█████▎    | 3169/6019 [00:03<00:03, 905.95it/s]  55%|█████▍    | 3284/6019 [00:03<00:02, 969.95it/s] 56%|█████▋    | 3386/6019 [00:03<00:02, 982.71it/s] 58%|█████▊    | 3513/6019 [00:03<00:02, 1061.03it/s] 60%|██████    | 3622/6019 [00:04<00:02, 1058.93it/s] 62%|██████▏   | 3730/6019 [00:04<00:02, 1018.27it/s] 65%|██████▌   | 3919/6019 [00:04<00:01, 1263.74it/s] 67%|██████▋   | 4048/6019 [00:04<00:01, 1261.74it/s] 69%|██████▉   | 4176/6019 [00:04<00:01, 1223.42it/s] 71%|███████▏  | 4300/6019 [00:04<00:01, 1150.86it/s] 73%|███████▎  | 4417/6019 [00:04<00:01, 970.18it/s]  75%|███████▌  | 4520/6019 [00:04<00:01, 926.22it/s] 80%|███████▉  | 4811/6019 [00:05<00:00, 1423.04it/s] 83%|████████▎ | 4974/6019 [00:05<00:00, 1476.60it/s] 85%|████████▌ | 5144/6019 [00:05<00:00, 1535.90it/s] 88%|████████▊ | 5305/6019 [00:05<00:00, 1270.71it/s] 90%|█████████ | 5445/6019 [00:05<00:00, 1190.53it/s] 98%|█████████▊| 5905/6019 [00:05<00:00, 2020.67it/s]100%|██████████| 6019/6019 [00:05<00:00, 1059.81it/s]
Loaded ground truth annotations for 6019 samples.
Filtering predictions
=> Original number of boxes: 1075350
=> After distance based filtering: 638762
=> After LIDAR points based filtering: 638762
=> After bike rack filtering: 638594
Filtering ground truth annotations
=> Original number of boxes: 187528
=> After distance based filtering: 134565
=> After LIDAR points based filtering: 121871
=> After bike rack filtering: 121861
Accumulating metric data...
Calculating metrics...
Saving metrics to: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/sparseformer/all/voxelnext_en_3layer_1lc_zhx_de_1layer_v3_32p_.0diff_.1lr_.1wd/eval/eval_with_train/epoch_20/val/final_result/data
mAP: 0.6417
mATE: 0.2855
mASE: 0.2525
mAOE: 0.3060
mAVE: 0.2469
mAAE: 0.1896
NDS: 0.6928
Eval time: 73.0s

Per-class results:
Object Class	AP	ATE	ASE	AOE	AVE	AAE
car	0.862	0.170	0.150	0.084	0.251	0.187
truck	0.586	0.333	0.189	0.103	0.225	0.224
bus	0.740	0.318	0.178	0.048	0.434	0.219
trailer	0.414	0.521	0.210	0.547	0.203	0.174
construction_vehicle	0.240	0.748	0.429	1.049	0.130	0.321
pedestrian	0.871	0.128	0.282	0.348	0.210	0.098
motorcycle	0.707	0.180	0.227	0.190	0.349	0.276
bicycle	0.543	0.155	0.258	0.340	0.173	0.017
traffic_cone	0.794	0.101	0.315	nan	nan	nan
barrier	0.661	0.201	0.287	0.044	nan	nan
2025-09-03 03:09:38,528   INFO  ----------------Nuscene detection_cvpr_2019 results-----------------
***car error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.17, 0.15, 0.08, 0.25, 0.19 | 77.64, 86.93, 89.56, 90.74 | mean AP: 0.8621487421671369
***truck error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.33, 0.19, 0.10, 0.23, 0.22 | 39.24, 57.70, 66.91, 70.35 | mean AP: 0.5855025974277108
***construction_vehicle error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.75, 0.43, 1.05, 0.13, 0.32 | 2.46, 15.27, 34.12, 44.22 | mean AP: 0.2401489692703008
***bus error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.32, 0.18, 0.05, 0.43, 0.22 | 49.64, 74.42, 84.32, 87.64 | mean AP: 0.740031636385648
***trailer error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.52, 0.21, 0.55, 0.20, 0.17 | 13.62, 35.95, 52.76, 63.43 | mean AP: 0.414401172293745
***barrier error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.20, 0.29, 0.04, nan, nan | 57.11, 66.01, 69.80, 71.38 | mean AP: 0.660745558748004
***motorcycle error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.18, 0.23, 0.19, 0.35, 0.28 | 61.13, 72.56, 74.19, 74.87 | mean AP: 0.7068738419280238
***bicycle error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.16, 0.26, 0.34, 0.17, 0.02 | 52.05, 54.63, 54.92, 55.46 | mean AP: 0.5426741241464559
***pedestrian error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.13, 0.28, 0.35, 0.21, 0.10 | 85.32, 86.57, 87.66, 88.68 | mean AP: 0.8705987605953571
***traffic_cone error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.10, 0.32, nan, nan, nan | 77.55, 78.41, 79.75, 81.71 | mean AP: 0.7935131217376393
--------------average performance-------------
trans_err:	 0.2855
scale_err:	 0.2525
orient_err:	 0.3060
vel_err:	 0.2469
attr_err:	 0.1896
mAP:	 0.6417
NDS:	 0.6928

2025-09-03 03:09:38,530   INFO  Result is saved to /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/sparseformer/all/voxelnext_en_3layer_1lc_zhx_de_1layer_v3_32p_.0diff_.1lr_.1wd/eval/eval_with_train/epoch_20/val
2025-09-03 03:09:38,530   INFO  ****************Evaluation done.*****************
2025-09-03 03:09:38,563   INFO  Epoch 20 has been evaluated
Wait 30 seconds for next check (progress: 0.0 / 0 minutes): /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/sparseformer/all/voxelnext_en_3layer_1lc_zhx_de_1layer_v3_32p_.0diff_.1lr_.1wd/ckpt 2025-09-03 03:10:08,568   INFO  **********************End evaluation sparse_models/sparse_former_light(default)**********************
