/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
2025-09-03 22:55:42,764   INFO  **********************Start logging**********************
2025-09-03 22:55:42,765   INFO  CUDA_VISIBLE_DEVICES=ALL
2025-09-03 22:55:42,765   INFO  Training in distributed mode : total_batch_size: 16
2025-09-03 22:55:42,765   INFO  cfg_file         cfgs/sparse_models/sparse_former_base.yaml
2025-09-03 22:55:42,765   INFO  batch_size       2
2025-09-03 22:55:42,765   INFO  epochs           36
2025-09-03 22:55:42,765   INFO  workers          12
2025-09-03 22:55:42,765   INFO  extra_tag        default
2025-09-03 22:55:42,765   INFO  ckpt             None
2025-09-03 22:55:42,765   INFO  pretrained_model None
2025-09-03 22:55:42,765   INFO  launcher         pytorch
2025-09-03 22:55:42,765   INFO  tcp_port         18888
2025-09-03 22:55:42,765   INFO  sync_bn          True
2025-09-03 22:55:42,765   INFO  fix_random_seed  False
2025-09-03 22:55:42,765   INFO  ckpt_save_interval 1
2025-09-03 22:55:42,766   INFO  local_rank       0
2025-09-03 22:55:42,766   INFO  max_ckpt_save_num 30
2025-09-03 22:55:42,766   INFO  merge_all_iters_to_one_epoch False
2025-09-03 22:55:42,766   INFO  set_cfgs         None
2025-09-03 22:55:42,766   INFO  max_waiting_mins 0
2025-09-03 22:55:42,766   INFO  start_epoch      0
2025-09-03 22:55:42,766   INFO  num_epochs_to_eval 0
2025-09-03 22:55:42,766   INFO  save_to_file     False
2025-09-03 22:55:42,766   INFO  use_tqdm_to_record False
2025-09-03 22:55:42,766   INFO  logger_iter_interval 50
2025-09-03 22:55:42,766   INFO  ckpt_save_time_interval 300
2025-09-03 22:55:42,766   INFO  wo_gpu_stat      True
2025-09-03 22:55:42,766   INFO  use_amp          False
2025-09-03 22:55:42,766   INFO  eval_map         False
2025-09-03 22:55:42,767   INFO  dataset          nuscenes
2025-09-03 22:55:42,767   INFO  root_dir         /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2
2025-09-03 22:55:42,767   INFO  output_dir       /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_.0diff_dense_hm_.1lr_.1wd
2025-09-03 22:55:42,767   INFO  cfg.ROOT_DIR: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2
2025-09-03 22:55:42,767   INFO  cfg.LOCAL_RANK: 0
2025-09-03 22:55:42,767   INFO  cfg.CLASS_NAMES: ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']
2025-09-03 22:55:42,767   INFO  ----------- DATA_CONFIG -----------
2025-09-03 22:55:42,767   INFO  cfg.DATA_CONFIG.DATASET: NuScenesDataset
2025-09-03 22:55:42,767   INFO  cfg.DATA_CONFIG.DATA_PATH: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/data/nuscenes
2025-09-03 22:55:42,767   INFO  cfg.DATA_CONFIG.VERSION: v1.0-trainval
2025-09-03 22:55:42,767   INFO  cfg.DATA_CONFIG.MAX_SWEEPS: 10
2025-09-03 22:55:42,767   INFO  cfg.DATA_CONFIG.PRED_VELOCITY: True
2025-09-03 22:55:42,767   INFO  cfg.DATA_CONFIG.SET_NAN_VELOCITY_TO_ZEROS: True
2025-09-03 22:55:42,767   INFO  cfg.DATA_CONFIG.FILTER_MIN_POINTS_IN_GT: 1
2025-09-03 22:55:42,767   INFO  ----------- DATA_SPLIT -----------
2025-09-03 22:55:42,768   INFO  cfg.DATA_CONFIG.DATA_SPLIT.train: train
2025-09-03 22:55:42,768   INFO  cfg.DATA_CONFIG.DATA_SPLIT.test: val
2025-09-03 22:55:42,768   INFO  ----------- INFO_PATH -----------
2025-09-03 22:55:42,768   INFO  cfg.DATA_CONFIG.INFO_PATH.train: ['nuscenes_infos_10sweeps_train.pkl']
2025-09-03 22:55:42,768   INFO  cfg.DATA_CONFIG.INFO_PATH.test: ['nuscenes_infos_10sweeps_val.pkl']
2025-09-03 22:55:42,768   INFO  cfg.DATA_CONFIG.POINT_CLOUD_RANGE: [-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]
2025-09-03 22:55:42,768   INFO  cfg.DATA_CONFIG.BALANCED_RESAMPLING: False
2025-09-03 22:55:42,768   INFO  ----------- DATA_AUGMENTOR -----------
2025-09-03 22:55:42,768   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.DISABLE_AUG_LIST: ['placeholder']
2025-09-03 22:55:42,768   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.AUG_CONFIG_LIST: [{'NAME': 'gt_sampling', 'DB_INFO_PATH': ['nuscenes_dbinfos_10sweeps_withvelo.pkl'], 'USE_SHARED_MEMORY': True, 'DB_DATA_PATH': ['nuscenes_10sweeps_withvelo_lidar.npy'], 'PREPARE': {'filter_by_min_points': ['car:5', 'truck:5', 'construction_vehicle:5', 'bus:5', 'trailer:5', 'barrier:5', 'motorcycle:5', 'bicycle:5', 'pedestrian:5', 'traffic_cone:5']}, 'SAMPLE_GROUPS': ['car:2', 'truck:3', 'construction_vehicle:7', 'bus:4', 'trailer:6', 'barrier:2', 'motorcycle:6', 'bicycle:6', 'pedestrian:2', 'traffic_cone:2'], 'NUM_POINT_FEATURES': 5, 'DATABASE_WITH_FAKELIDAR': False, 'REMOVE_EXTRA_WIDTH': [0.0, 0.0, 0.0], 'LIMIT_WHOLE_SCENE': True}, {'NAME': 'random_world_flip', 'ALONG_AXIS_LIST': ['x', 'y']}, {'NAME': 'random_world_rotation', 'WORLD_ROT_ANGLE': [-0.78539816, 0.78539816]}, {'NAME': 'random_world_scaling', 'WORLD_SCALE_RANGE': [0.9, 1.1]}, {'NAME': 'random_world_translation', 'NOISE_TRANSLATE_STD': [0.5, 0.5, 0.5]}]
2025-09-03 22:55:42,769   INFO  ----------- POINT_FEATURE_ENCODING -----------
2025-09-03 22:55:42,769   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.encoding_type: absolute_coordinates_encoding
2025-09-03 22:55:42,769   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.used_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-09-03 22:55:42,769   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.src_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-09-03 22:55:42,769   INFO  cfg.DATA_CONFIG.DATA_PROCESSOR: [{'NAME': 'mask_points_and_boxes_outside_range', 'REMOVE_OUTSIDE_BOXES': True}, {'NAME': 'shuffle_points', 'SHUFFLE_ENABLED': {'train': True, 'test': True}}, {'NAME': 'transform_points_to_voxels_placeholder', 'VOXEL_SIZE': [0.3, 0.3, 0.2]}]
2025-09-03 22:55:42,769   INFO  cfg.DATA_CONFIG._BASE_CONFIG_: cfgs/dataset_configs/nuscenes_dataset.yaml
2025-09-03 22:55:42,769   INFO  ----------- SAMPLED_INTERVAL -----------
2025-09-03 22:55:42,769   INFO  cfg.DATA_CONFIG.SAMPLED_INTERVAL.train: 1
2025-09-03 22:55:42,769   INFO  cfg.DATA_CONFIG.SAMPLED_INTERVAL.test: 1
2025-09-03 22:55:42,769   INFO  ----------- MODEL -----------
2025-09-03 22:55:42,770   INFO  cfg.MODEL.NAME: TransFusion
2025-09-03 22:55:42,770   INFO  ----------- VFE -----------
2025-09-03 22:55:42,770   INFO  cfg.MODEL.VFE.NAME: DynamicVoxelVFE
2025-09-03 22:55:42,770   INFO  cfg.MODEL.VFE.WITH_DISTANCE: False
2025-09-03 22:55:42,770   INFO  cfg.MODEL.VFE.USE_ABSLOTE_XYZ: True
2025-09-03 22:55:42,770   INFO  cfg.MODEL.VFE.USE_NORM: True
2025-09-03 22:55:42,770   INFO  cfg.MODEL.VFE.NUM_FILTERS: [64, 64]
2025-09-03 22:55:42,770   INFO  ----------- BACKBONE_3D -----------
2025-09-03 22:55:42,770   INFO  cfg.MODEL.BACKBONE_3D.NAME: FSHNet_nusc
2025-09-03 22:55:42,770   INFO  cfg.MODEL.BACKBONE_3D.FEATURE_DIM: 128
2025-09-03 22:55:42,770   INFO  ----------- SPENCODER -----------
2025-09-03 22:55:42,770   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.DIM: 3
2025-09-03 22:55:42,770   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.NUM_LAYERS: 3
2025-09-03 22:55:42,771   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.ORDERS: [['z', 'z-trans'], ['hilbert', 'hilbert-trans'], ['x', 'y']]
2025-09-03 22:55:42,771   INFO  ----------- SMSA -----------
2025-09-03 22:55:42,771   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.EMBED_DIM: 128
2025-09-03 22:55:42,771   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DEPTH: 9
2025-09-03 22:55:42,771   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.NUM_LEVELS: 3
2025-09-03 22:55:42,771   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.WINDOW_SHAPE: [9, 9, 5]
2025-09-03 22:55:42,771   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DROP_PATH: 0.2
2025-09-03 22:55:42,771   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.SPATIAL_ENHANCE: True
2025-09-03 22:55:42,771   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.FUSED_ADD_NORM: True
2025-09-03 22:55:42,771   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.RMS_NORM: True
2025-09-03 22:55:42,772   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.NORM_EPSILON: 1e-05
2025-09-03 22:55:42,772   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.RESIDUAL_IN_FP32: True
2025-09-03 22:55:42,772   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DIFF_COEF: 0.0
2025-09-03 22:55:42,772   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DIFF_KERNEL: [7, 5, 3]
2025-09-03 22:55:42,772   INFO  cfg.MODEL.BACKBONE_3D.AFD_FEATURE_DIM: 128
2025-09-03 22:55:42,772   INFO  cfg.MODEL.BACKBONE_3D.AFD_NUM_LAYERS: 4
2025-09-03 22:55:42,772   INFO  cfg.MODEL.BACKBONE_3D.AFD_NUM_SBB: [2, 1, 1]
2025-09-03 22:55:42,772   INFO  cfg.MODEL.BACKBONE_3D.AFD_DOWN_STRIDE: [1, 2, 2]
2025-09-03 22:55:42,772   INFO  cfg.MODEL.BACKBONE_3D.AFD_DOWN_KERNEL_SIZE: [3, 3, 3]
2025-09-03 22:55:42,772   INFO  cfg.MODEL.BACKBONE_3D.AFD: True
2025-09-03 22:55:42,772   INFO  cfg.MODEL.BACKBONE_3D.FG_THRESHOLD: 0.2
2025-09-03 22:55:42,772   INFO  cfg.MODEL.BACKBONE_3D.FEATMAP_STRIDE: 2
2025-09-03 22:55:42,773   INFO  cfg.MODEL.BACKBONE_3D.DETACH_FEATURE: True
2025-09-03 22:55:42,773   INFO  cfg.MODEL.BACKBONE_3D.GREOUP_POOLING_KERNEL_SIZE: [9, 15, 5, 5]
2025-09-03 22:55:42,773   INFO  cfg.MODEL.BACKBONE_3D.GROUP_CLASS_NAMES: [['car', 'truck', 'construction_vehicle'], ['bus', 'trailer'], ['barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']]
2025-09-03 22:55:42,773   INFO  ----------- DENSE_HEAD -----------
2025-09-03 22:55:42,773   INFO  cfg.MODEL.DENSE_HEAD.CLASS_AGNOSTIC: False
2025-09-03 22:55:42,773   INFO  cfg.MODEL.DENSE_HEAD.NAME: TransFusionHead
2025-09-03 22:55:42,773   INFO  cfg.MODEL.DENSE_HEAD.QUERY_RADIUS: 20
2025-09-03 22:55:42,773   INFO  cfg.MODEL.DENSE_HEAD.QUERY_LOCAL: True
2025-09-03 22:55:42,773   INFO  cfg.MODEL.DENSE_HEAD.USE_BIAS_BEFORE_NORM: True
2025-09-03 22:55:42,773   INFO  cfg.MODEL.DENSE_HEAD.INPUT_FEATURES: 128
2025-09-03 22:55:42,773   INFO  cfg.MODEL.DENSE_HEAD.NUM_PROPOSALS: 600
2025-09-03 22:55:42,773   INFO  cfg.MODEL.DENSE_HEAD.HIDDEN_CHANNEL: 128
2025-09-03 22:55:42,773   INFO  cfg.MODEL.DENSE_HEAD.NUM_CLASSES: 10
2025-09-03 22:55:42,773   INFO  cfg.MODEL.DENSE_HEAD.NUM_HEADS: 8
2025-09-03 22:55:42,773   INFO  cfg.MODEL.DENSE_HEAD.NMS_KERNEL_SIZE: 3
2025-09-03 22:55:42,773   INFO  cfg.MODEL.DENSE_HEAD.FFN_CHANNEL: 256
2025-09-03 22:55:42,773   INFO  cfg.MODEL.DENSE_HEAD.DROPOUT: 0.1
2025-09-03 22:55:42,774   INFO  cfg.MODEL.DENSE_HEAD.BN_MOMENTUM: 0.1
2025-09-03 22:55:42,774   INFO  cfg.MODEL.DENSE_HEAD.ACTIVATION: relu
2025-09-03 22:55:42,774   INFO  cfg.MODEL.DENSE_HEAD.NUM_HM_CONV: 2
2025-09-03 22:55:42,774   INFO  ----------- SEPARATE_HEAD_CFG -----------
2025-09-03 22:55:42,774   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_ORDER: ['center', 'height', 'dim', 'rot', 'vel', 'iou']
2025-09-03 22:55:42,774   INFO  ----------- HEAD_DICT -----------
2025-09-03 22:55:42,774   INFO  ----------- center -----------
2025-09-03 22:55:42,774   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.out_channels: 2
2025-09-03 22:55:42,774   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.num_conv: 2
2025-09-03 22:55:42,774   INFO  ----------- height -----------
2025-09-03 22:55:42,774   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.height.out_channels: 1
2025-09-03 22:55:42,774   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.height.num_conv: 2
2025-09-03 22:55:42,774   INFO  ----------- dim -----------
2025-09-03 22:55:42,775   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.out_channels: 3
2025-09-03 22:55:42,775   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.num_conv: 2
2025-09-03 22:55:42,775   INFO  ----------- rot -----------
2025-09-03 22:55:42,775   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.out_channels: 2
2025-09-03 22:55:42,775   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.num_conv: 2
2025-09-03 22:55:42,775   INFO  ----------- vel -----------
2025-09-03 22:55:42,775   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.out_channels: 2
2025-09-03 22:55:42,775   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.num_conv: 2
2025-09-03 22:55:42,775   INFO  ----------- iou -----------
2025-09-03 22:55:42,775   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.iou.out_channels: 1
2025-09-03 22:55:42,775   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.iou.num_conv: 2
2025-09-03 22:55:42,775   INFO  ----------- TARGET_ASSIGNER_CONFIG -----------
2025-09-03 22:55:42,775   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.FEATURE_MAP_STRIDE: 2
2025-09-03 22:55:42,775   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.DATASET: nuScenes
2025-09-03 22:55:42,775   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.GAUSSIAN_OVERLAP: 0.1
2025-09-03 22:55:42,775   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.MIN_RADIUS: 2
2025-09-03 22:55:42,776   INFO  ----------- HUNGARIAN_ASSIGNER -----------
2025-09-03 22:55:42,776   INFO  ----------- cls_cost -----------
2025-09-03 22:55:42,776   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.gamma: 2.0
2025-09-03 22:55:42,776   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.alpha: 0.25
2025-09-03 22:55:42,776   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.weight: 0.15
2025-09-03 22:55:42,776   INFO  ----------- reg_cost -----------
2025-09-03 22:55:42,776   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.reg_cost.weight: 0.25
2025-09-03 22:55:42,776   INFO  ----------- iou_cost -----------
2025-09-03 22:55:42,776   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.iou_cost.weight: 0.25
2025-09-03 22:55:42,776   INFO  ----------- LOSS_CONFIG -----------
2025-09-03 22:55:42,776   INFO  ----------- LOSS_WEIGHTS -----------
2025-09-03 22:55:42,776   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.cls_weight: 1.0
2025-09-03 22:55:42,776   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.bbox_weight: 0.25
2025-09-03 22:55:42,776   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.hm_weight: 1.0
2025-09-03 22:55:42,776   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.iou_weight: 0.5
2025-09-03 22:55:42,776   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.iou_reg_weight: 0.5
2025-09-03 22:55:42,777   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.code_weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2]
2025-09-03 22:55:42,777   INFO  ----------- LOSS_CLS -----------
2025-09-03 22:55:42,777   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.use_sigmoid: True
2025-09-03 22:55:42,777   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.gamma: 2.0
2025-09-03 22:55:42,777   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.alpha: 0.25
2025-09-03 22:55:42,777   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_IOU: True
2025-09-03 22:55:42,777   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_IOU_REG: True
2025-09-03 22:55:42,777   INFO  ----------- POST_PROCESSING -----------
2025-09-03 22:55:42,777   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.SCORE_THRESH: 0.0
2025-09-03 22:55:42,777   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.POST_CENTER_RANGE: [-61.2, -61.2, -10.0, 61.2, 61.2, 10.0]
2025-09-03 22:55:42,777   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.USE_IOU_TO_RECTIFY_SCORE: True
2025-09-03 22:55:42,777   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.IOU_RECTIFIER: [0.5]
2025-09-03 22:55:42,777   INFO  ----------- NMS_CONFIG -----------
2025-09-03 22:55:42,777   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_TYPE: nms_gpu
2025-09-03 22:55:42,777   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_THRESH: 0.2
2025-09-03 22:55:42,777   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_PRE_MAXSIZE: 1000
2025-09-03 22:55:42,778   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_POST_MAXSIZE: 100
2025-09-03 22:55:42,778   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.SCORE_THRES: 0.0
2025-09-03 22:55:42,778   INFO  ----------- POST_PROCESSING -----------
2025-09-03 22:55:42,778   INFO  cfg.MODEL.POST_PROCESSING.RECALL_THRESH_LIST: [0.3, 0.5, 0.7]
2025-09-03 22:55:42,778   INFO  cfg.MODEL.POST_PROCESSING.SCORE_THRESH: 0.1
2025-09-03 22:55:42,778   INFO  cfg.MODEL.POST_PROCESSING.OUTPUT_RAW_SCORE: False
2025-09-03 22:55:42,778   INFO  cfg.MODEL.POST_PROCESSING.EVAL_METRIC: kitti
2025-09-03 22:55:42,778   INFO  ----------- NMS_CONFIG -----------
2025-09-03 22:55:42,778   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.MULTI_CLASSES_NMS: True
2025-09-03 22:55:42,778   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_TYPE: nms_gpu
2025-09-03 22:55:42,778   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_THRESH: 0.2
2025-09-03 22:55:42,778   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_PRE_MAXSIZE: 1000
2025-09-03 22:55:42,778   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_POST_MAXSIZE: 83
2025-09-03 22:55:42,778   INFO  ----------- OPTIMIZATION -----------
2025-09-03 22:55:42,779   INFO  cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU: 2
2025-09-03 22:55:42,779   INFO  cfg.OPTIMIZATION.NUM_EPOCHS: 36
2025-09-03 22:55:42,779   INFO  cfg.OPTIMIZATION.OPTIMIZER: adam_onecycle
2025-09-03 22:55:42,779   INFO  cfg.OPTIMIZATION.LR: 0.003
2025-09-03 22:55:42,779   INFO  cfg.OPTIMIZATION.WEIGHT_DECAY: 0.03
2025-09-03 22:55:42,779   INFO  cfg.OPTIMIZATION.MOMENTUM: 0.9
2025-09-03 22:55:42,779   INFO  cfg.OPTIMIZATION.MOMS: [0.95, 0.85]
2025-09-03 22:55:42,779   INFO  cfg.OPTIMIZATION.PCT_START: 0.4
2025-09-03 22:55:42,779   INFO  cfg.OPTIMIZATION.DIV_FACTOR: 10
2025-09-03 22:55:42,779   INFO  cfg.OPTIMIZATION.DECAY_STEP_LIST: [35, 45]
2025-09-03 22:55:42,779   INFO  cfg.OPTIMIZATION.LR_DECAY: 0.1
2025-09-03 22:55:42,779   INFO  cfg.OPTIMIZATION.LR_CLIP: 1e-07
2025-09-03 22:55:42,779   INFO  cfg.OPTIMIZATION.LR_WARMUP: False
2025-09-03 22:55:42,779   INFO  cfg.OPTIMIZATION.WARMUP_EPOCH: 1
2025-09-03 22:55:42,779   INFO  cfg.OPTIMIZATION.GRAD_NORM_CLIP: 10
2025-09-03 22:55:42,779   INFO  cfg.OPTIMIZATION.LOSS_SCALE_FP16: 32
2025-09-03 22:55:42,779   INFO  ----------- HOOK -----------
2025-09-03 22:55:42,780   INFO  ----------- DisableAugmentationHook -----------
2025-09-03 22:55:42,780   INFO  cfg.HOOK.DisableAugmentationHook.DISABLE_AUG_LIST: ['gt_sampling']
2025-09-03 22:55:42,780   INFO  cfg.HOOK.DisableAugmentationHook.NUM_LAST_EPOCHS: 5
2025-09-03 22:55:42,780   INFO  cfg.TAG: sparse_former_base
2025-09-03 22:55:42,780   INFO  cfg.EXP_GROUP_PATH: sparse_models
2025-09-03 22:55:42,780   INFO  cfg.OUTPUT_DIR: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_.0diff_dense_hm_.1lr_.1wd
2025-09-03 22:55:42,793   INFO  ----------- Create dataloader & network & optimizer -----------
2025-09-03 22:55:47,838   INFO  Database filter by min points car: 339949 => 294532
2025-09-03 22:55:47,847   INFO  Database filter by min points truck: 65262 => 60344
2025-09-03 22:55:47,848   INFO  Database filter by min points construction_vehicle: 11050 => 10589
2025-09-03 22:55:47,850   INFO  Database filter by min points bus: 12286 => 11619
2025-09-03 22:55:47,852   INFO  Database filter by min points trailer: 19202 => 17934
2025-09-03 22:55:47,864   INFO  Database filter by min points barrier: 107507 => 101993
2025-09-03 22:55:47,865   INFO  Database filter by min points motorcycle: 8846 => 8055
2025-09-03 22:55:47,866   INFO  Database filter by min points bicycle: 8185 => 7531
2025-09-03 22:55:47,884   INFO  Database filter by min points pedestrian: 161928 => 148520
2025-09-03 22:55:47,891   INFO  Database filter by min points traffic_cone: 62964 => 55504
2025-09-03 22:55:47,891   INFO  Loading GT database to shared memory
eflops83:23:23 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:23:23 [0] NCCL INFO Bootstrap : Using bond0:10.16.10.229<0>
eflops83:23:23 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops83:23:23 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops83:23:23 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.18.5+cuda11.8
eflops83:27:27 [4] NCCL INFO cudaDriverVersion 12050
eflops83:27:27 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:31:31 [7] NCCL INFO cudaDriverVersion 12050
eflops83:30:30 [6] NCCL INFO cudaDriverVersion 12050
eflops83:31:31 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:30:30 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:25:25 [2] NCCL INFO cudaDriverVersion 12050
eflops83:25:25 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:28:28 [5] NCCL INFO cudaDriverVersion 12050
eflops83:28:28 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:26:26 [3] NCCL INFO cudaDriverVersion 12050
eflops83:26:26 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:24:24 [1] NCCL INFO cudaDriverVersion 12050
eflops83:24:24 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:27:27 [4] NCCL INFO Bootstrap : Using bond0:10.16.10.229<0>
eflops83:25:25 [2] NCCL INFO Bootstrap : Using bond0:10.16.10.229<0>
eflops83:31:31 [7] NCCL INFO Bootstrap : Using bond0:10.16.10.229<0>
eflops83:30:30 [6] NCCL INFO Bootstrap : Using bond0:10.16.10.229<0>
eflops83:25:25 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops83:25:25 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops83:27:27 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops83:27:27 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops83:30:30 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops83:30:30 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops83:31:31 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops83:31:31 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops83:28:28 [5] NCCL INFO Bootstrap : Using bond0:10.16.10.229<0>
eflops83:26:26 [3] NCCL INFO Bootstrap : Using bond0:10.16.10.229<0>
eflops83:24:24 [1] NCCL INFO Bootstrap : Using bond0:10.16.10.229<0>
eflops83:26:26 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops83:26:26 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops83:24:24 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops83:24:24 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops83:28:28 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops83:28:28 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops83:23:99 [0] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops83:23:99 [0] NCCL INFO P2P plugin IBext
eflops83:23:99 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:25:100 [2] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops83:25:100 [2] NCCL INFO P2P plugin IBext
eflops83:25:100 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:26:106 [3] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops83:26:106 [3] NCCL INFO P2P plugin IBext
eflops83:26:106 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:27:101 [4] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops83:27:101 [4] NCCL INFO P2P plugin IBext
eflops83:27:101 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:31:102 [7] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops83:31:102 [7] NCCL INFO P2P plugin IBext
eflops83:31:102 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:24:104 [1] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops83:24:104 [1] NCCL INFO P2P plugin IBext
eflops83:24:104 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:30:103 [6] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops83:30:103 [6] NCCL INFO P2P plugin IBext
eflops83:30:103 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:28:105 [5] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops83:28:105 [5] NCCL INFO P2P plugin IBext
eflops83:28:105 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops83:25:100 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops83:31:102 [7] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops83:25:100 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops83:25:100 [2] NCCL INFO NET/IB : No device found.

eflops83:31:102 [7] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops83:31:102 [7] NCCL INFO NET/IB : No device found.

eflops83:23:99 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops83:28:105 [5] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops83:26:106 [3] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops83:30:103 [6] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops83:24:104 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops83:28:105 [5] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0

eflops83:26:106 [3] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0

eflops83:24:104 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0

eflops83:23:99 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0

eflops83:30:103 [6] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops83:24:104 [1] NCCL INFO NET/IB : No device found.
eflops83:23:99 [0] NCCL INFO NET/IB : No device found.
eflops83:28:105 [5] NCCL INFO NET/IB : No device found.

eflops83:27:101 [4] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed
eflops83:26:106 [3] NCCL INFO NET/IB : No device found.
eflops83:30:103 [6] NCCL INFO NET/IB : No device found.

eflops83:27:101 [4] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops83:27:101 [4] NCCL INFO NET/IB : No device found.
eflops83:24:104 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops83:28:105 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops83:26:106 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops83:30:103 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops83:31:102 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops83:24:104 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:25:100 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops83:28:105 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:27:101 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops83:26:106 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:30:103 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:27:101 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:31:102 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:23:99 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops83:25:100 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:23:99 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops83:26:106 [3] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.229<0>
eflops83:26:106 [3] NCCL INFO Using network Socket
eflops83:30:103 [6] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.229<0>
eflops83:30:103 [6] NCCL INFO Using network Socket
eflops83:31:102 [7] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.229<0>
eflops83:31:102 [7] NCCL INFO Using network Socket
eflops83:27:101 [4] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.229<0>
eflops83:27:101 [4] NCCL INFO Using network Socket
eflops83:24:104 [1] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.229<0>
eflops83:24:104 [1] NCCL INFO Using network Socket
eflops83:28:105 [5] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.229<0>
eflops83:23:99 [0] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.229<0>
eflops83:28:105 [5] NCCL INFO Using network Socket
eflops83:25:100 [2] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.229<0>
eflops83:23:99 [0] NCCL INFO Using network Socket
eflops83:25:100 [2] NCCL INFO Using network Socket
eflops83:24:104 [1] NCCL INFO comm 0x11fcca60 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 26000 commId 0x9c77a8e2ff83d914 - Init START
eflops83:25:100 [2] NCCL INFO comm 0x10d8fd70 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 29000 commId 0x9c77a8e2ff83d914 - Init START
eflops83:26:106 [3] NCCL INFO comm 0x107340e0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 2d000 commId 0x9c77a8e2ff83d914 - Init START
eflops83:23:99 [0] NCCL INFO comm 0x1115b9e0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 25000 commId 0x9c77a8e2ff83d914 - Init START
eflops83:31:102 [7] NCCL INFO comm 0x10e6a7e0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId ad000 commId 0x9c77a8e2ff83d914 - Init START
eflops83:27:101 [4] NCCL INFO comm 0x108e6290 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId a5000 commId 0x9c77a8e2ff83d914 - Init START
eflops83:28:105 [5] NCCL INFO comm 0x10f18810 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId a6000 commId 0x9c77a8e2ff83d914 - Init START
eflops83:30:103 [6] NCCL INFO comm 0x11a44da0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a9000 commId 0x9c77a8e2ff83d914 - Init START
eflops83:31:102 [7] NCCL INFO Setting affinity for GPU 7 to ff0000,00000000,00ff0000,00000000
eflops83:30:103 [6] NCCL INFO Setting affinity for GPU 6 to ff0000,00000000,00ff0000,00000000
eflops83:25:100 [2] NCCL INFO Setting affinity for GPU 2 to ff0000,00000000,00ff0000
eflops83:23:99 [0] NCCL INFO Setting affinity for GPU 0 to ff0000,00000000,00ff0000
eflops83:24:104 [1] NCCL INFO Setting affinity for GPU 1 to ff0000,00000000,00ff0000
eflops83:27:101 [4] NCCL INFO Setting affinity for GPU 4 to ff0000,00000000,00ff0000,00000000
eflops83:26:106 [3] NCCL INFO Setting affinity for GPU 3 to ff0000,00000000,00ff0000
eflops83:28:105 [5] NCCL INFO Setting affinity for GPU 5 to ff0000,00000000,00ff0000,00000000
eflops83:25:100 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1
eflops83:25:100 [2] NCCL INFO P2P Chunksize set to 131072
eflops83:23:99 [0] NCCL INFO Channel 00/04 :    0   1   2   3   4   5   6   7
eflops83:30:103 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5
eflops83:24:104 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0
eflops83:31:102 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6
eflops83:23:99 [0] NCCL INFO Channel 01/04 :    0   1   2   3   4   5   6   7
eflops83:30:103 [6] NCCL INFO P2P Chunksize set to 131072
eflops83:31:102 [7] NCCL INFO P2P Chunksize set to 131072
eflops83:23:99 [0] NCCL INFO Channel 02/04 :    0   1   2   3   4   5   6   7
eflops83:24:104 [1] NCCL INFO P2P Chunksize set to 131072
eflops83:23:99 [0] NCCL INFO Channel 03/04 :    0   1   2   3   4   5   6   7
eflops83:26:106 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2
eflops83:26:106 [3] NCCL INFO P2P Chunksize set to 131072
eflops83:23:99 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1
eflops83:28:105 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4
eflops83:23:99 [0] NCCL INFO P2P Chunksize set to 131072
eflops83:28:105 [5] NCCL INFO P2P Chunksize set to 131072
eflops83:27:101 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3
eflops83:27:101 [4] NCCL INFO P2P Chunksize set to 131072
eflops83:23:99 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
eflops83:24:104 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
eflops83:28:105 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC
eflops83:31:102 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/IPC
eflops83:30:103 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC
eflops83:23:99 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
eflops83:27:101 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
eflops83:24:104 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
eflops83:31:102 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/IPC
eflops83:28:105 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC
eflops83:30:103 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC
eflops83:23:99 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC
eflops83:27:101 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
eflops83:24:104 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/IPC
eflops83:28:105 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/IPC
eflops83:31:102 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/IPC
eflops83:30:103 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/IPC
eflops83:23:99 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC
eflops83:24:104 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC
eflops83:28:105 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/IPC
eflops83:31:102 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/IPC
eflops83:30:103 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/IPC
eflops83:25:100 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
eflops83:26:106 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC
eflops83:25:100 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
eflops83:26:106 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC
eflops83:25:100 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC
eflops83:27:101 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/IPC
eflops83:26:106 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/IPC
eflops83:25:100 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/IPC
eflops83:27:101 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/IPC
eflops83:26:106 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/IPC
eflops83:23:99 [0] NCCL INFO Connected all rings
eflops83:31:102 [7] NCCL INFO Connected all rings
eflops83:31:102 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC
eflops83:31:102 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC
eflops83:30:103 [6] NCCL INFO Connected all rings
eflops83:31:102 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/IPC
eflops83:31:102 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/IPC
eflops83:24:104 [1] NCCL INFO Connected all rings
eflops83:28:105 [5] NCCL INFO Connected all rings
eflops83:26:106 [3] NCCL INFO Connected all rings
eflops83:27:101 [4] NCCL INFO Connected all rings
eflops83:25:100 [2] NCCL INFO Connected all rings
eflops83:30:103 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC
eflops83:30:103 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC
eflops83:30:103 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/IPC
eflops83:30:103 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/IPC
eflops83:24:104 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
eflops83:28:105 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
eflops83:26:106 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
eflops83:27:101 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC
eflops83:25:100 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
eflops83:24:104 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
eflops83:28:105 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
eflops83:26:106 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
eflops83:27:101 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC
eflops83:25:100 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
eflops83:26:106 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC
eflops83:25:100 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC
eflops83:24:104 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC
eflops83:28:105 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/IPC
eflops83:27:101 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/IPC
eflops83:25:100 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC
eflops83:26:106 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/IPC
eflops83:24:104 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC
eflops83:31:102 [7] NCCL INFO Connected all trees
eflops83:31:102 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops83:31:102 [7] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
eflops83:28:105 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/IPC
eflops83:27:101 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/IPC
eflops83:23:99 [0] NCCL INFO Connected all trees
eflops83:23:99 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops83:23:99 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
eflops83:26:106 [3] NCCL INFO Connected all trees
eflops83:26:106 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops83:26:106 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
eflops83:28:105 [5] NCCL INFO Connected all trees
eflops83:28:105 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops83:28:105 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
eflops83:25:100 [2] NCCL INFO Connected all trees
eflops83:25:100 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops83:25:100 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
eflops83:30:103 [6] NCCL INFO Connected all trees
eflops83:30:103 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops83:30:103 [6] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
eflops83:24:104 [1] NCCL INFO Connected all trees
eflops83:24:104 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops83:24:104 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
eflops83:27:101 [4] NCCL INFO Connected all trees
eflops83:27:101 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops83:27:101 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
eflops83:31:102 [7] NCCL INFO comm 0x10e6a7e0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId ad000 commId 0x9c77a8e2ff83d914 - Init COMPLETE
eflops83:28:105 [5] NCCL INFO comm 0x10f18810 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId a6000 commId 0x9c77a8e2ff83d914 - Init COMPLETE
eflops83:24:104 [1] NCCL INFO comm 0x11fcca60 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 26000 commId 0x9c77a8e2ff83d914 - Init COMPLETE
eflops83:26:106 [3] NCCL INFO comm 0x107340e0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 2d000 commId 0x9c77a8e2ff83d914 - Init COMPLETE
eflops83:23:99 [0] NCCL INFO comm 0x1115b9e0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 25000 commId 0x9c77a8e2ff83d914 - Init COMPLETE
eflops83:30:103 [6] NCCL INFO comm 0x11a44da0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a9000 commId 0x9c77a8e2ff83d914 - Init COMPLETE
eflops83:27:101 [4] NCCL INFO comm 0x108e6290 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId a5000 commId 0x9c77a8e2ff83d914 - Init COMPLETE
eflops83:25:100 [2] NCCL INFO comm 0x10d8fd70 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 29000 commId 0x9c77a8e2ff83d914 - Init COMPLETE
2025-09-03 22:55:57,988   INFO  GT database has been saved to shared memory
2025-09-03 22:55:58,169   INFO  Loading NuScenes dataset
2025-09-03 22:55:59,944   INFO  Total samples for NuScenes dataset: 28130
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2025-09-03 22:56:00,269   INFO  ==> Loading parameters from checkpoint /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_.0diff_dense_hm_.1lr_.1wd/ckpt/latest_model.pth to CPU
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
==> Checkpoint trained from version: pcdet+0.6.0+8694e7f
==> Checkpoint trained from version: pcdet+0.6.0+8694e7f
2025-09-03 22:56:00,455   INFO  ==> Loading optimizer parameters from checkpoint /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_.0diff_dense_hm_.1lr_.1wd/ckpt/latest_model.pth to CPU
==> Checkpoint trained from version: pcdet+0.6.0+8694e7f
==> Checkpoint trained from version: pcdet+0.6.0+8694e7f
2025-09-03 22:56:00,507   INFO  ==> Done
==> Checkpoint trained from version: pcdet+0.6.0+8694e7f
==> Checkpoint trained from version: pcdet+0.6.0+8694e7f
==> Checkpoint trained from version: pcdet+0.6.0+8694e7f
==> Checkpoint trained from version: pcdet+0.6.0+8694e7f
2025-09-03 22:56:00,757   INFO  ----------- Model TransFusion created, param count: 24443458 -----------
epochs:   0%|          | 0/15 [00:00<?, ?it/s]epochs:   0%|          | 0/15 [00:00<?, ?it/s]epochs:   0%|          | 0/15 [00:00<?, ?it/s]epochs:   0%|          | 0/15 [00:00<?, ?it/s]epochs:   0%|          | 0/15 [00:00<?, ?it/s]epochs:   0%|          | 0/15 [00:00<?, ?it/s]2025-09-03 22:56:00,757   INFO  DistributedDataParallel(
  (module): TransFusion(
    (vfe): DynamicVoxelVFE(
      (pfn_layers): ModuleList(
        (0): PFNLayerV2(
          (linear): Linear(in_features=11, out_features=32, bias=False)
          (norm): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
        )
        (1): PFNLayerV2(
          (linear): Linear(in_features=64, out_features=64, bias=False)
          (norm): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
        )
      )
    )
    (backbone_3d): FSHNet_nusc(
      (cpe): SparseSequential(
        (0): SparseSequential(
          (0): SubMConv3d(64, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (2): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
      )
      (down1): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (down2): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (down3): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (stage): SPEncoder(
        (blocks): ModuleList(
          (0): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): ZOrderSerialization()
            )
          )
          (1): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): HilbertSerialization()
            )
          )
          (2): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): FlattenWindowsSerialization()
            )
          )
        )
        (embeddings): ModuleList(
          (0-2): 3 x MSSubConvEmbeddingLearned(
            (stem): ModuleList(
              (0-2): 3 x SparseBasicBlock(
                (conv1): SubMConv3d(3, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
              )
            )
          )
        )
      )
      (app): AttnPillarPool(
        (query_func): SparseMaxPool3d(kernel_size=[10, 1, 1], stride=[10, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], algo=ConvAlgo.MaskImplicitGemm)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (pos_embedding): Embedding(10, 128)
      )
      (cls_conv): SparseSequential(
        (0): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): SubMConv2d(128, 3, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
      )
      (afd_layers): ModuleList(
        (0-3): 4 x SEDLayer(
          (encoder): ModuleList(
            (0): SparseSequential(
              (0): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
              (1): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
            )
            (1-2): 2 x SparseSequential(
              (0): SparseSequential(
                (0): SparseConv2d(128, 128, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
            )
          )
          (decoder): ModuleList(
            (0-1): 2 x SparseSequential(
              (0): SparseInverseConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
              (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (decoder_norm): ModuleList(
            (0-1): 2 x SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (shared_conv): SparseSequential(
        (0): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (map_to_bev_module): None
    (pfe): None
    (backbone_2d): None
    (dense_head): TransFusionHead(
      (loss_cls): SigmoidFocalClassificationLoss()
      (loss_bbox): L1Loss()
      (loss_heatmap): GaussianFocalLoss()
      (shared_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (heatmap_head): Sequential(
        (0): BasicBlock2D(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): Conv2d(128, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (class_encoding): Conv1d(10, 128, kernel_size=(1,), stride=(1,))
      (decoder): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_posembed): PositionEmbeddingLearned(
          (position_embedding_head): Sequential(
            (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
        )
        (cross_posembed): PositionEmbeddingLearned(
          (position_embedding_head): Sequential(
            (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
        )
      )
      (prediction_head): SeparateHead_Transfusion(
        (center): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (height): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
        (dim): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 3, kernel_size=(1,), stride=(1,))
        )
        (rot): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (vel): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (iou): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
        (heatmap): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 10, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (point_head): None
    (roi_head): None
  )
)
epochs:   0%|          | 0/15 [00:00<?, ?it/s]2025-09-03 22:56:00,763   INFO  **********************Start training sparse_models/sparse_former_base(default)**********************
epochs:   0%|          | 0/15 [00:00<?, ?it/s]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
2025-09-03 22:56:13,837   INFO  Train:   22/36 ( 61%) [1191/1759 ( 68%)]  Loss: 4.081 (4.08)  LR: 2.235e-03  Grad: 1.8950  max=1.2935(module.vfe.pfn_layers.0.linear.weight)  min: -0.3383(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8467, loss_cls=0.1201, loss_bbox=0.9277, matched_ious=0.5251, loss_iou=0.0818, loss_iou_reg=0.2282, d_time=1.48(1.48), f_time=9.88(9.88), b_time=11.36(11.36)  Time cost: 00:11/1:50:22 [00:12/81:36:03]  Acc_iter 38131       Data time: 1.48(1.48)  Forward time: 9.88(9.88)  Batch time: 11.36(11.36)
2025-09-03 22:56:42,232   INFO  Train:   22/36 ( 61%) [1210/1759 ( 69%)]  Loss: 3.419 (3.36)  LR: 2.233e-03  Grad: 6.6610  max=0.6442(module.vfe.pfn_layers.0.linear.weight)  min: -5.3410(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6218, loss_cls=0.1161, loss_bbox=0.6850, matched_ious=0.5286, loss_iou=0.0937, loss_iou_reg=0.2242, d_time=0.01(0.10), f_time=1.23(1.89), b_time=1.24(1.99)  Time cost: 00:40/18:19 [00:41/14:00:24]  Acc_iter 38150       Data time: 0.01(0.10)  Forward time: 1.23(1.89)  Batch time: 1.24(1.99)
2025-09-03 22:57:45,121   INFO  Train:   22/36 ( 61%) [1260/1759 ( 72%)]  Loss: 2.571 (3.34)  LR: 2.228e-03  Grad: 2.3578  max=1.2298(module.vfe.pfn_layers.0.linear.weight)  min: -0.5455(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6254, loss_cls=0.1097, loss_bbox=0.6938, matched_ious=0.5341, loss_iou=0.0926, loss_iou_reg=0.2195, d_time=0.01(0.03), f_time=1.29(1.43), b_time=1.30(1.47)  Time cost: 01:42/12:13 [01:44/10:15:50]  Acc_iter 38200       Data time: 0.01(0.03)  Forward time: 1.29(1.43)  Batch time: 1.30(1.47)
2025-09-03 22:58:48,301   INFO  Train:   22/36 ( 61%) [1310/1759 ( 74%)]  Loss: 3.053 (3.28)  LR: 2.222e-03  Grad: 2.2740  max=1.0982(module.vfe.pfn_layers.0.linear.weight)  min: -0.3954(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5929, loss_cls=0.1091, loss_bbox=0.6385, matched_ious=0.5396, loss_iou=0.0895, loss_iou_reg=0.2177, d_time=0.01(0.02), f_time=1.22(1.36), b_time=1.23(1.38)  Time cost: 02:46/10:21 [02:47/9:38:34]  Acc_iter 38250       Data time: 0.01(0.02)  Forward time: 1.22(1.36)  Batch time: 1.23(1.38)
2025-09-03 22:59:50,749   INFO  Train:   22/36 ( 61%) [1360/1759 ( 77%)]  Loss: 3.484 (3.30)  LR: 2.217e-03  Grad: 2.2302  max=0.4559(module.vfe.pfn_layers.0.linear.weight)  min: -0.6406(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6072, loss_cls=0.1114, loss_bbox=0.7011, matched_ious=0.5361, loss_iou=0.0916, loss_iou_reg=0.2185, d_time=0.01(0.02), f_time=1.22(1.32), b_time=1.23(1.34)  Time cost: 03:48/08:56 [03:49/9:20:48]  Acc_iter 38300       Data time: 0.01(0.02)  Forward time: 1.22(1.32)  Batch time: 1.23(1.34)
2025-09-03 23:00:55,037   INFO  Train:   22/36 ( 61%) [1410/1759 ( 80%)]  Loss: 3.083 (3.28)  LR: 2.212e-03  Grad: 2.8259  max=0.4464(module.vfe.pfn_layers.0.linear.weight)  min: -1.5677(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5901, loss_cls=0.1036, loss_bbox=0.6606, matched_ious=0.5416, loss_iou=0.0900, loss_iou_reg=0.2151, d_time=0.01(0.02), f_time=1.23(1.31), b_time=1.24(1.33)  Time cost: 04:52/07:44 [04:54/9:14:06]  Acc_iter 38350       Data time: 0.01(0.02)  Forward time: 1.23(1.31)  Batch time: 1.24(1.33)
2025-09-03 23:02:00,444   INFO  Train:   22/36 ( 61%) [1460/1759 ( 83%)]  Loss: 3.036 (3.29)  LR: 2.206e-03  Grad: 2.3191  max=0.4008(module.vfe.pfn_layers.0.linear.weight)  min: -0.2998(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6166, loss_cls=0.1100, loss_bbox=0.7232, matched_ious=0.5314, loss_iou=0.0914, loss_iou_reg=0.2207, d_time=0.01(0.02), f_time=1.26(1.31), b_time=1.27(1.33)  Time cost: 05:58/06:36 [05:59/9:11:13]  Acc_iter 38400       Data time: 0.01(0.02)  Forward time: 1.26(1.31)  Batch time: 1.27(1.33)
2025-09-03 23:03:04,338   INFO  Train:   22/36 ( 61%) [1510/1759 ( 86%)]  Loss: 2.886 (3.28)  LR: 2.201e-03  Grad: 2.5239  max=0.5071(module.vfe.pfn_layers.0.linear.weight)  min: -0.3895(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5967, loss_cls=0.1085, loss_bbox=0.6330, matched_ious=0.5465, loss_iou=0.0888, loss_iou_reg=0.2156, d_time=0.01(0.02), f_time=1.23(1.30), b_time=1.24(1.32)  Time cost: 07:02/05:28 [07:03/9:06:56]  Acc_iter 38450       Data time: 0.01(0.02)  Forward time: 1.23(1.30)  Batch time: 1.24(1.32)
2025-09-03 23:04:07,918   INFO  Train:   22/36 ( 61%) [1560/1759 ( 89%)]  Loss: 3.129 (3.29)  LR: 2.195e-03  Grad: 3.3718  max=1.8108(module.vfe.pfn_layers.0.linear.weight)  min: -0.4093(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5901, loss_cls=0.1052, loss_bbox=0.7005, matched_ious=0.5364, loss_iou=0.0903, loss_iou_reg=0.2183, d_time=0.01(0.01), f_time=1.26(1.30), b_time=1.27(1.31)  Time cost: 08:05/04:21 [08:07/9:03:11]  Acc_iter 38500       Data time: 0.01(0.01)  Forward time: 1.26(1.30)  Batch time: 1.27(1.31)
2025-09-03 23:05:10,154   INFO  Train:   22/36 ( 61%) [1610/1759 ( 92%)]  Loss: 2.913 (3.28)  LR: 2.190e-03  Grad: 2.7501  max=0.5301(module.vfe.pfn_layers.0.linear.weight)  min: -0.2058(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5805, loss_cls=0.1055, loss_bbox=0.6429, matched_ious=0.5416, loss_iou=0.0893, loss_iou_reg=0.2172, d_time=0.01(0.01), f_time=1.22(1.29), b_time=1.23(1.30)  Time cost: 09:07/03:14 [09:09/8:58:44]  Acc_iter 38550       Data time: 0.01(0.01)  Forward time: 1.22(1.29)  Batch time: 1.23(1.30)
2025-09-03 23:06:13,145   INFO  Train:   22/36 ( 61%) [1660/1759 ( 94%)]  Loss: 3.099 (3.27)  LR: 2.184e-03  Grad: 2.3712  max=0.1420(module.vfe.pfn_layers.0.linear.weight)  min: -0.1544(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5919, loss_cls=0.1068, loss_bbox=0.6654, matched_ious=0.5448, loss_iou=0.0886, loss_iou_reg=0.2132, d_time=0.01(0.01), f_time=1.26(1.29), b_time=1.27(1.30)  Time cost: 10:10/02:08 [10:12/8:55:41]  Acc_iter 38600       Data time: 0.01(0.01)  Forward time: 1.26(1.29)  Batch time: 1.27(1.30)
2025-09-03 23:07:15,242   INFO  Train:   22/36 ( 61%) [1710/1759 ( 97%)]  Loss: 2.620 (3.27)  LR: 2.179e-03  Grad: 2.8798  max=0.4757(module.vfe.pfn_layers.0.linear.weight)  min: -0.7614(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5966, loss_cls=0.1115, loss_bbox=0.6734, matched_ious=0.5293, loss_iou=0.0912, loss_iou_reg=0.2230, d_time=0.01(0.01), f_time=1.17(1.28), b_time=1.17(1.29)  Time cost: 11:13/01:03 [11:14/8:52:18]  Acc_iter 38650       Data time: 0.01(0.01)  Forward time: 1.17(1.28)  Batch time: 1.17(1.29)
2025-09-03 23:08:15,460   INFO  Train:   22/36 ( 61%) [1758/1759 (100%)]  Loss: 3.791 (3.27)  LR: 2.173e-03  Grad: 2.9114  max=0.5287(module.vfe.pfn_layers.0.linear.weight)  min: -0.4443(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5998, loss_cls=0.1077, loss_bbox=0.6568, matched_ious=0.5416, loss_iou=0.0896, loss_iou_reg=0.2163, d_time=0.01(0.01), f_time=1.26(1.28), b_time=1.27(1.29)  Time cost: 12:13/00:01 [12:14/8:49:53]  Acc_iter 38698       Data time: 0.01(0.01)  Forward time: 1.26(1.28)  Batch time: 1.27(1.29)

                                               [Aepochs:   7%|▋         | 1/15 [12:14<2:51:26, 734.74s/it]epochs:   7%|▋         | 1/15 [12:14<2:51:26, 734.76s/it]epochs:   7%|▋         | 1/15 [12:14<2:51:26, 734.78s/it]epochs:   7%|▋         | 1/15 [12:14<2:51:27, 734.80s/it]epochs:   7%|▋         | 1/15 [12:14<2:51:27, 734.81s/it]epochs:   7%|▋         | 1/15 [12:14<2:51:27, 734.83s/it]epochs:   7%|▋         | 1/15 [12:14<2:51:27, 734.84s/it]epochs:   7%|▋         | 1/15 [12:15<2:51:31, 735.09s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 23:08:19,408   INFO  Train:   23/36 ( 64%) [   0/1759 (  0%)]  Loss: 2.798 (2.80)  LR: 2.173e-03  Grad: 2.8255  max=0.4205(module.vfe.pfn_layers.0.linear.weight)  min: -0.5044(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4872, loss_cls=0.0849, loss_bbox=0.5978, matched_ious=0.5393, loss_iou=0.0764, loss_iou_reg=0.2145, d_time=0.85(0.85), f_time=2.24(2.24), b_time=3.09(3.09)  Time cost: 00:03/1:28:01 [12:18/20:32:24]  Acc_iter 38699       Data time: 0.85(0.85)  Forward time: 2.24(2.24)  Batch time: 3.09(3.09)
2025-09-03 23:08:20,695   INFO  Train:   23/36 ( 64%) [   1/1759 (  0%)]  Loss: 3.523 (3.16)  LR: 2.173e-03  Grad: 3.1125  max=0.6153(module.vfe.pfn_layers.0.linear.weight)  min: -0.7687(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6606, loss_cls=0.1268, loss_bbox=0.4615, matched_ious=0.5303, loss_iou=0.0863, loss_iou_reg=0.2326, d_time=0.02(0.43), f_time=1.28(1.76), b_time=1.29(2.19)  Time cost: 00:04/1:02:54 [12:19/14:41:10]  Acc_iter 38700       Data time: 0.02(0.43)  Forward time: 1.28(1.76)  Batch time: 1.29(2.19)
2025-09-03 23:09:23,442   INFO  Train:   23/36 ( 64%) [  51/1759 (  3%)]  Loss: 2.856 (3.22)  LR: 2.168e-03  Grad: 3.1779  max=0.5938(module.vfe.pfn_layers.0.linear.weight)  min: -0.6615(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6009, loss_cls=0.1097, loss_bbox=0.6710, matched_ious=0.5386, loss_iou=0.0912, loss_iou_reg=0.2170, d_time=0.01(0.02), f_time=1.33(1.27), b_time=1.33(1.29)  Time cost: 01:07/36:41 [13:22/8:48:02]  Acc_iter 38750       Data time: 0.01(0.02)  Forward time: 1.33(1.27)  Batch time: 1.33(1.29)
2025-09-03 23:10:25,553   INFO  Train:   23/36 ( 64%) [ 101/1759 (  6%)]  Loss: 3.332 (3.28)  LR: 2.162e-03  Grad: 2.0614  max=0.4169(module.vfe.pfn_layers.0.linear.weight)  min: -0.5264(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6164, loss_cls=0.1115, loss_bbox=0.7213, matched_ious=0.5241, loss_iou=0.0891, loss_iou_reg=0.2234, d_time=0.01(0.01), f_time=1.30(1.25), b_time=1.31(1.27)  Time cost: 02:09/34:59 [14:24/8:37:33]  Acc_iter 38800       Data time: 0.01(0.01)  Forward time: 1.30(1.25)  Batch time: 1.31(1.27)
2025-09-03 23:11:29,090   INFO  Train:   23/36 ( 64%) [ 151/1759 (  9%)]  Loss: 2.975 (3.28)  LR: 2.156e-03  Grad: 2.2243  max=0.1553(module.vfe.pfn_layers.0.linear.weight)  min: -0.5801(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6012, loss_cls=0.1090, loss_bbox=0.6651, matched_ious=0.5405, loss_iou=0.0896, loss_iou_reg=0.2179, d_time=0.01(0.02), f_time=1.27(1.25), b_time=1.27(1.27)  Time cost: 03:12/33:58 [15:28/8:37:06]  Acc_iter 38850       Data time: 0.01(0.02)  Forward time: 1.27(1.25)  Batch time: 1.27(1.27)
2025-09-03 23:12:34,970   INFO  Train:   23/36 ( 64%) [ 201/1759 ( 11%)]  Loss: 2.681 (3.27)  LR: 2.151e-03  Grad: 2.2352  max=0.2213(module.vfe.pfn_layers.0.linear.weight)  min: -0.2437(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5925, loss_cls=0.1086, loss_bbox=0.6636, matched_ious=0.5388, loss_iou=0.0913, loss_iou_reg=0.2160, d_time=0.01(0.02), f_time=2.13(1.26), b_time=2.13(1.28)  Time cost: 04:18/33:14 [16:34/8:41:04]  Acc_iter 38900       Data time: 0.01(0.02)  Forward time: 2.13(1.26)  Batch time: 2.13(1.28)
2025-09-03 23:13:37,868   INFO  Train:   23/36 ( 64%) [ 251/1759 ( 14%)]  Loss: 2.724 (3.27)  LR: 2.145e-03  Grad: 2.6686  max=0.7424(module.vfe.pfn_layers.0.linear.weight)  min: -0.5161(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5906, loss_cls=0.1057, loss_bbox=0.6622, matched_ious=0.5429, loss_iou=0.0915, loss_iou_reg=0.2143, d_time=0.01(0.02), f_time=1.31(1.26), b_time=1.32(1.28)  Time cost: 05:21/32:03 [17:37/8:38:14]  Acc_iter 38950       Data time: 0.01(0.02)  Forward time: 1.31(1.26)  Batch time: 1.32(1.28)
2025-09-03 23:14:41,230   INFO  Train:   23/36 ( 64%) [ 301/1759 ( 17%)]  Loss: 2.846 (3.25)  LR: 2.140e-03  Grad: 2.6990  max=0.4972(module.vfe.pfn_layers.0.linear.weight)  min: -0.4134(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5700, loss_cls=0.1076, loss_bbox=0.6213, matched_ious=0.5456, loss_iou=0.0893, loss_iou_reg=0.2162, d_time=0.01(0.02), f_time=1.28(1.26), b_time=1.29(1.27)  Time cost: 06:24/30:57 [18:40/8:36:36]  Acc_iter 39000       Data time: 0.01(0.02)  Forward time: 1.28(1.26)  Batch time: 1.29(1.27)
2025-09-03 23:15:43,851   INFO  Train:   23/36 ( 64%) [ 351/1759 ( 20%)]  Loss: 2.783 (3.25)  LR: 2.134e-03  Grad: 2.8175  max=0.2246(module.vfe.pfn_layers.0.linear.weight)  min: -0.3843(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6077, loss_cls=0.1096, loss_bbox=0.6958, matched_ious=0.5387, loss_iou=0.0904, loss_iou_reg=0.2175, d_time=0.01(0.02), f_time=1.26(1.26), b_time=1.26(1.27)  Time cost: 07:27/29:49 [19:43/8:34:17]  Acc_iter 39050       Data time: 0.01(0.02)  Forward time: 1.26(1.26)  Batch time: 1.26(1.27)
2025-09-03 23:16:47,025   INFO  Train:   23/36 ( 64%) [ 401/1759 ( 23%)]  Loss: 3.338 (3.25)  LR: 2.128e-03  Grad: 3.2959  max=0.6763(module.vfe.pfn_layers.0.linear.weight)  min: -0.3205(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5971, loss_cls=0.1092, loss_bbox=0.6849, matched_ious=0.5345, loss_iou=0.0919, loss_iou_reg=0.2206, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 08:30/28:44 [20:46/8:32:50]  Acc_iter 39100       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-03 23:17:50,000   INFO  Train:   23/36 ( 64%) [ 451/1759 ( 26%)]  Loss: 3.639 (3.25)  LR: 2.123e-03  Grad: 3.2795  max=0.6559(module.vfe.pfn_layers.0.linear.weight)  min: -0.3630(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5935, loss_cls=0.1073, loss_bbox=0.6354, matched_ious=0.5442, loss_iou=0.0900, loss_iou_reg=0.2171, d_time=0.01(0.01), f_time=1.18(1.25), b_time=1.18(1.27)  Time cost: 09:33/27:39 [21:49/8:31:18]  Acc_iter 39150       Data time: 0.01(0.01)  Forward time: 1.18(1.25)  Batch time: 1.18(1.27)
2025-09-03 23:18:53,428   INFO  Train:   23/36 ( 64%) [ 501/1759 ( 28%)]  Loss: 2.419 (3.25)  LR: 2.117e-03  Grad: 3.4226  max=0.3260(module.vfe.pfn_layers.0.linear.weight)  min: -0.7114(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6007, loss_cls=0.1131, loss_bbox=0.6678, matched_ious=0.5441, loss_iou=0.0898, loss_iou_reg=0.2143, d_time=0.01(0.01), f_time=1.27(1.26), b_time=1.28(1.27)  Time cost: 10:37/26:36 [22:52/8:30:14]  Acc_iter 39200       Data time: 0.01(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.27)
2025-09-03 23:19:56,232   INFO  Train:   23/36 ( 64%) [ 551/1759 ( 31%)]  Loss: 3.003 (3.24)  LR: 2.111e-03  Grad: 3.5506  max=0.3446(module.vfe.pfn_layers.0.linear.weight)  min: -0.8209(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5799, loss_cls=0.1045, loss_bbox=0.6473, matched_ious=0.5432, loss_iou=0.0895, loss_iou_reg=0.2168, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 11:39/25:31 [23:55/8:28:42]  Acc_iter 39250       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-03 23:20:59,163   INFO  Train:   23/36 ( 64%) [ 601/1759 ( 34%)]  Loss: 2.661 (3.25)  LR: 2.106e-03  Grad: 3.6687  max=0.2302(module.vfe.pfn_layers.0.linear.weight)  min: -0.5113(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6203, loss_cls=0.1112, loss_bbox=0.6915, matched_ious=0.5406, loss_iou=0.0913, loss_iou_reg=0.2149, d_time=0.01(0.01), f_time=1.16(1.25), b_time=1.17(1.27)  Time cost: 12:42/24:27 [24:58/8:27:20]  Acc_iter 39300       Data time: 0.01(0.01)  Forward time: 1.16(1.25)  Batch time: 1.17(1.27)
2025-09-03 23:22:02,146   INFO  Train:   23/36 ( 64%) [ 651/1759 ( 37%)]  Loss: 3.094 (3.25)  LR: 2.100e-03  Grad: 3.8597  max=0.3887(module.vfe.pfn_layers.0.linear.weight)  min: -0.3980(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6074, loss_cls=0.1076, loss_bbox=0.6950, matched_ious=0.5395, loss_iou=0.0912, loss_iou_reg=0.2173, d_time=0.01(0.01), f_time=1.27(1.25), b_time=1.27(1.27)  Time cost: 13:45/23:23 [26:01/8:26:03]  Acc_iter 39350       Data time: 0.01(0.01)  Forward time: 1.27(1.25)  Batch time: 1.27(1.27)
2025-09-03 23:23:07,480   INFO  Train:   23/36 ( 64%) [ 701/1759 ( 40%)]  Loss: 2.847 (3.24)  LR: 2.094e-03  Grad: 3.9942  max=0.5114(module.vfe.pfn_layers.0.linear.weight)  min: -0.4254(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5621, loss_cls=0.1034, loss_bbox=0.6052, matched_ious=0.5419, loss_iou=0.0901, loss_iou_reg=0.2191, d_time=0.01(0.01), f_time=1.18(1.26), b_time=1.19(1.27)  Time cost: 14:51/22:22 [27:06/8:26:09]  Acc_iter 39400       Data time: 0.01(0.01)  Forward time: 1.18(1.26)  Batch time: 1.19(1.27)
2025-09-03 23:24:10,544   INFO  Train:   23/36 ( 64%) [ 751/1759 ( 43%)]  Loss: 2.691 (3.24)  LR: 2.089e-03  Grad: 4.1416  max=0.2823(module.vfe.pfn_layers.0.linear.weight)  min: -0.4690(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5940, loss_cls=0.1074, loss_bbox=0.6413, matched_ious=0.5390, loss_iou=0.0900, loss_iou_reg=0.2189, d_time=0.01(0.01), f_time=1.27(1.26), b_time=1.28(1.27)  Time cost: 15:54/21:18 [28:09/8:24:52]  Acc_iter 39450       Data time: 0.01(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.27)
2025-09-03 23:25:13,294   INFO  Train:   23/36 ( 64%) [ 801/1759 ( 46%)]  Loss: 3.206 (3.23)  LR: 2.083e-03  Grad: 4.5463  max=0.4580(module.vfe.pfn_layers.0.linear.weight)  min: -0.7409(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5747, loss_cls=0.1070, loss_bbox=0.6251, matched_ious=0.5472, loss_iou=0.0885, loss_iou_reg=0.2137, d_time=0.01(0.01), f_time=1.28(1.26), b_time=1.28(1.27)  Time cost: 16:56/20:14 [29:12/8:23:28]  Acc_iter 39500       Data time: 0.01(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.27)
2025-09-03 23:26:16,437   INFO  Train:   23/36 ( 64%) [ 851/1759 ( 48%)]  Loss: 2.380 (3.23)  LR: 2.077e-03  Grad: 4.5996  max=0.7557(module.vfe.pfn_layers.0.linear.weight)  min: -0.8512(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5743, loss_cls=0.1052, loss_bbox=0.6427, matched_ious=0.5447, loss_iou=0.0889, loss_iou_reg=0.2151, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.26(1.27)  Time cost: 18:00/19:11 [30:15/8:22:18]  Acc_iter 39550       Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.27)
2025-09-03 23:27:19,424   INFO  Train:   23/36 ( 64%) [ 901/1759 ( 51%)]  Loss: 3.828 (3.23)  LR: 2.072e-03  Grad: 4.3304  max=0.2582(module.vfe.pfn_layers.0.linear.weight)  min: -0.7074(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5881, loss_cls=0.1058, loss_bbox=0.6781, matched_ious=0.5391, loss_iou=0.0907, loss_iou_reg=0.2176, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.22(1.27)  Time cost: 19:03/18:07 [31:18/8:21:04]  Acc_iter 39600       Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.27)
2025-09-03 23:28:22,495   INFO  Train:   23/36 ( 64%) [ 951/1759 ( 54%)]  Loss: 3.215 (3.23)  LR: 2.066e-03  Grad: 4.4493  max=0.3320(module.vfe.pfn_layers.0.linear.weight)  min: -0.6639(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5949, loss_cls=0.1079, loss_bbox=0.6592, matched_ious=0.5480, loss_iou=0.0900, loss_iou_reg=0.2127, d_time=0.01(0.01), f_time=1.17(1.26), b_time=1.18(1.27)  Time cost: 20:06/17:03 [32:21/8:19:54]  Acc_iter 39650       Data time: 0.01(0.01)  Forward time: 1.17(1.26)  Batch time: 1.18(1.27)
2025-09-03 23:29:26,024   INFO  Train:   23/36 ( 64%) [1001/1759 ( 57%)]  Loss: 3.037 (3.23)  LR: 2.060e-03  Grad: 4.5863  max=0.3719(module.vfe.pfn_layers.0.linear.weight)  min: -0.3307(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6020, loss_cls=0.1089, loss_bbox=0.6853, matched_ious=0.5291, loss_iou=0.0923, loss_iou_reg=0.2221, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.26(1.27)  Time cost: 21:09/16:00 [33:25/8:18:55]  Acc_iter 39700       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.27)
2025-09-03 23:30:28,338   INFO  Train:   23/36 ( 64%) [1051/1759 ( 60%)]  Loss: 3.575 (3.24)  LR: 2.054e-03  Grad: 4.7109  max=0.1761(module.vfe.pfn_layers.0.linear.weight)  min: -0.2916(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6134, loss_cls=0.1069, loss_bbox=0.7098, matched_ious=0.5344, loss_iou=0.0901, loss_iou_reg=0.2184, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.23(1.27)  Time cost: 22:11/14:56 [34:27/8:17:28]  Acc_iter 39750       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.27)
2025-09-03 23:31:31,197   INFO  Train:   23/36 ( 64%) [1101/1759 ( 63%)]  Loss: 3.226 (3.23)  LR: 2.049e-03  Grad: 3.6273  max=0.5382(module.vfe.pfn_layers.0.linear.weight)  min: -0.4877(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5741, loss_cls=0.1058, loss_bbox=0.6420, matched_ious=0.5446, loss_iou=0.0897, loss_iou_reg=0.2158, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.25(1.27)  Time cost: 23:14/13:52 [35:30/8:16:15]  Acc_iter 39800       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.27)
2025-09-03 23:32:33,961   INFO  Train:   23/36 ( 64%) [1151/1759 ( 65%)]  Loss: 2.719 (3.23)  LR: 2.043e-03  Grad: 3.7623  max=0.4127(module.vfe.pfn_layers.0.linear.weight)  min: -0.2741(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5735, loss_cls=0.1037, loss_bbox=0.6381, matched_ious=0.5430, loss_iou=0.0890, loss_iou_reg=0.2167, d_time=0.01(0.01), f_time=1.15(1.26), b_time=1.16(1.27)  Time cost: 24:17/12:49 [36:33/8:15:01]  Acc_iter 39850       Data time: 0.01(0.01)  Forward time: 1.15(1.26)  Batch time: 1.16(1.27)
2025-09-03 23:33:40,220   INFO  Train:   23/36 ( 64%) [1201/1759 ( 68%)]  Loss: 4.361 (3.23)  LR: 2.037e-03  Grad: 3.9853  max=0.4209(module.vfe.pfn_layers.0.linear.weight)  min: -0.3469(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5837, loss_cls=0.1078, loss_bbox=0.6569, matched_ious=0.5423, loss_iou=0.0880, loss_iou_reg=0.2158, d_time=0.01(0.01), f_time=1.23(1.26), b_time=1.24(1.27)  Time cost: 25:23/11:47 [37:39/8:14:56]  Acc_iter 39900       Data time: 0.01(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.27)
2025-09-03 23:34:42,770   INFO  Train:   23/36 ( 64%) [1251/1759 ( 71%)]  Loss: 2.929 (3.22)  LR: 2.031e-03  Grad: 4.3436  max=1.1926(module.vfe.pfn_layers.0.linear.weight)  min: -0.2234(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5536, loss_cls=0.0979, loss_bbox=0.6165, matched_ious=0.5502, loss_iou=0.0905, loss_iou_reg=0.2148, d_time=0.01(0.01), f_time=1.32(1.26), b_time=1.33(1.27)  Time cost: 26:26/10:43 [38:41/8:13:37]  Acc_iter 39950       Data time: 0.01(0.01)  Forward time: 1.32(1.26)  Batch time: 1.33(1.27)
2025-09-03 23:35:45,745   INFO  Train:   23/36 ( 64%) [1301/1759 ( 74%)]  Loss: 2.990 (3.22)  LR: 2.025e-03  Grad: 9.3631  max=0.5555(module.vfe.pfn_layers.0.linear.weight)  min: -5.4848(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5622, loss_cls=0.1015, loss_bbox=0.6546, matched_ious=0.5427, loss_iou=0.0902, loss_iou_reg=0.2167, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.21(1.27)  Time cost: 27:29/09:40 [39:44/8:12:27]  Acc_iter 40000       Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.27)
2025-09-03 23:36:48,325   INFO  Train:   23/36 ( 64%) [1351/1759 ( 77%)]  Loss: 3.189 (3.22)  LR: 2.020e-03  Grad: 3.2528  max=0.5722(module.vfe.pfn_layers.0.linear.weight)  min: -0.5216(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6009, loss_cls=0.1086, loss_bbox=0.7016, matched_ious=0.5385, loss_iou=0.0912, loss_iou_reg=0.2148, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 28:31/08:36 [40:47/8:11:11]  Acc_iter 40050       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-03 23:37:52,028   INFO  Train:   23/36 ( 64%) [1401/1759 ( 80%)]  Loss: 3.294 (3.23)  LR: 2.014e-03  Grad: 2.8103  max=0.3509(module.vfe.pfn_layers.0.linear.weight)  min: -0.3269(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5999, loss_cls=0.1056, loss_bbox=0.6979, matched_ious=0.5402, loss_iou=0.0939, loss_iou_reg=0.2167, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.22(1.27)  Time cost: 29:35/07:33 [41:51/8:10:14]  Acc_iter 40100       Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.27)
2025-09-03 23:38:55,152   INFO  Train:   23/36 ( 64%) [1451/1759 ( 82%)]  Loss: 3.698 (3.23)  LR: 2.008e-03  Grad: 3.1333  max=0.4839(module.vfe.pfn_layers.0.linear.weight)  min: -0.3661(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5812, loss_cls=0.1067, loss_bbox=0.6638, matched_ious=0.5471, loss_iou=0.0898, loss_iou_reg=0.2149, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.26(1.27)  Time cost: 30:38/06:30 [42:54/8:09:07]  Acc_iter 40150       Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.27)
2025-09-03 23:39:58,491   INFO  Train:   23/36 ( 64%) [1501/1759 ( 85%)]  Loss: 2.897 (3.23)  LR: 2.002e-03  Grad: 7.3347  max=5.9589(module.vfe.pfn_layers.0.linear.weight)  min: -0.4295(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6021, loss_cls=0.1060, loss_bbox=0.6925, matched_ious=0.5351, loss_iou=0.0912, loss_iou_reg=0.2172, d_time=0.01(0.01), f_time=1.31(1.26), b_time=1.32(1.27)  Time cost: 31:42/05:26 [43:57/8:08:04]  Acc_iter 40200       Data time: 0.01(0.01)  Forward time: 1.31(1.26)  Batch time: 1.32(1.27)
2025-09-03 23:41:02,329   INFO  Train:   23/36 ( 64%) [1551/1759 ( 88%)]  Loss: 3.967 (3.22)  LR: 1.996e-03  Grad: 3.3614  max=0.3920(module.vfe.pfn_layers.0.linear.weight)  min: -0.3950(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5815, loss_cls=0.1083, loss_bbox=0.6088, matched_ious=0.5469, loss_iou=0.0895, loss_iou_reg=0.2145, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 32:45/04:23 [45:01/8:07:09]  Acc_iter 40250       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-03 23:42:04,925   INFO  Train:   23/36 ( 64%) [1601/1759 ( 91%)]  Loss: 3.274 (3.23)  LR: 1.990e-03  Grad: 3.7244  max=0.5734(module.vfe.pfn_layers.0.linear.weight)  min: -0.7278(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5838, loss_cls=0.1067, loss_bbox=0.7024, matched_ious=0.5375, loss_iou=0.0915, loss_iou_reg=0.2188, d_time=0.01(0.01), f_time=1.20(1.26), b_time=1.21(1.27)  Time cost: 33:48/03:20 [46:04/8:05:55]  Acc_iter 40300       Data time: 0.01(0.01)  Forward time: 1.20(1.26)  Batch time: 1.21(1.27)
2025-09-03 23:43:07,334   INFO  Train:   23/36 ( 64%) [1651/1759 ( 94%)]  Loss: 2.849 (3.23)  LR: 1.984e-03  Grad: 4.2277  max=1.8145(module.vfe.pfn_layers.0.linear.weight)  min: -0.4901(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5838, loss_cls=0.1042, loss_bbox=0.6772, matched_ious=0.5430, loss_iou=0.0902, loss_iou_reg=0.2129, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 34:50/02:16 [47:06/8:04:39]  Acc_iter 40350       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 23:44:12,178   INFO  Train:   23/36 ( 64%) [1701/1759 ( 97%)]  Loss: 3.692 (3.23)  LR: 1.979e-03  Grad: 3.9496  max=0.7692(module.vfe.pfn_layers.0.linear.weight)  min: -0.6101(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5728, loss_cls=0.1036, loss_bbox=0.6446, matched_ious=0.5419, loss_iou=0.0885, loss_iou_reg=0.2156, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.25(1.27)  Time cost: 35:55/01:13 [48:11/8:03:57]  Acc_iter 40400       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.27)
2025-09-03 23:45:14,300   INFO  Train:   23/36 ( 64%) [1751/1759 (100%)]  Loss: 2.748 (3.22)  LR: 1.973e-03  Grad: 4.3866  max=1.3224(module.vfe.pfn_layers.0.linear.weight)  min: -0.8334(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5658, loss_cls=0.1024, loss_bbox=0.6203, matched_ious=0.5498, loss_iou=0.0886, loss_iou_reg=0.2153, d_time=0.00(0.01), f_time=1.17(1.26), b_time=1.17(1.27)  Time cost: 36:57/00:10 [49:13/8:02:38]  Acc_iter 40450       Data time: 0.00(0.01)  Forward time: 1.17(1.26)  Batch time: 1.17(1.27)
2025-09-03 23:45:23,519   INFO  Train:   23/36 ( 64%) [1758/1759 (100%)]  Loss: 3.815 (3.22)  LR: 1.972e-03  Grad: 4.4536  max=1.0465(module.vfe.pfn_layers.0.linear.weight)  min: -0.9452(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6454, loss_cls=0.1161, loss_bbox=0.7163, matched_ious=0.5379, loss_iou=0.0887, loss_iou_reg=0.2169, d_time=0.00(0.01), f_time=1.55(1.26), b_time=1.55(1.27)  Time cost: 37:07/00:01 [49:22/8:02:33]  Acc_iter 40457       Data time: 0.00(0.01)  Forward time: 1.55(1.26)  Batch time: 1.55(1.27)

                                               [Aepochs:  13%|█▎        | 2/15 [49:22<5:49:31, 1613.20s/it]epochs:  13%|█▎        | 2/15 [49:22<5:49:31, 1613.22s/it]epochs:  13%|█▎        | 2/15 [49:22<5:49:31, 1613.23s/it]epochs:  13%|█▎        | 2/15 [49:22<5:49:31, 1613.21s/it]epochs:  13%|█▎        | 2/15 [49:22<5:49:31, 1613.22s/it]epochs:  13%|█▎        | 2/15 [49:22<5:49:32, 1613.25s/it]epochs:  13%|█▎        | 2/15 [49:22<5:49:32, 1613.24s/it]epochs:  13%|█▎        | 2/15 [49:23<5:49:33, 1613.33s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 23:45:27,592   INFO  Train:   24/36 ( 67%) [   0/1759 (  0%)]  Loss: 3.421 (3.42)  LR: 1.972e-03  Grad: 4.0990  max=0.2917(module.vfe.pfn_layers.0.linear.weight)  min: -0.4778(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5674, loss_cls=0.1148, loss_bbox=0.5502, matched_ious=0.5321, loss_iou=0.0730, loss_iou_reg=0.2171, d_time=1.08(1.08), f_time=2.06(2.06), b_time=3.14(3.14)  Time cost: 00:03/1:29:40 [49:26/19:25:44]  Acc_iter 40458       Data time: 1.08(1.08)  Forward time: 2.06(2.06)  Batch time: 3.14(3.14)
2025-09-03 23:46:20,330   INFO  Train:   24/36 ( 67%) [  42/1759 (  2%)]  Loss: 3.065 (3.16)  LR: 1.967e-03  Grad: 4.4703  max=1.3233(module.vfe.pfn_layers.0.linear.weight)  min: -0.4629(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5543, loss_cls=0.0985, loss_bbox=0.6377, matched_ious=0.5446, loss_iou=0.0930, loss_iou_reg=0.2185, d_time=0.00(0.03), f_time=1.16(1.27), b_time=1.16(1.30)  Time cost: 00:55/37:08 [50:19/8:13:39]  Acc_iter 40500       Data time: 0.00(0.03)  Forward time: 1.16(1.27)  Batch time: 1.16(1.30)
2025-09-03 23:47:23,355   INFO  Train:   24/36 ( 67%) [  92/1759 (  5%)]  Loss: 3.015 (3.14)  LR: 1.961e-03  Grad: 4.4010  max=0.3464(module.vfe.pfn_layers.0.linear.weight)  min: -0.1901(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5705, loss_cls=0.1033, loss_bbox=0.6258, matched_ious=0.5479, loss_iou=0.0879, loss_iou_reg=0.2140, d_time=0.01(0.02), f_time=1.30(1.26), b_time=1.31(1.28)  Time cost: 01:58/35:29 [51:22/8:04:59]  Acc_iter 40550       Data time: 0.01(0.02)  Forward time: 1.30(1.26)  Batch time: 1.31(1.28)
2025-09-03 23:48:25,493   INFO  Train:   24/36 ( 67%) [ 142/1759 (  8%)]  Loss: 2.631 (3.15)  LR: 1.955e-03  Grad: 4.6044  max=0.4242(module.vfe.pfn_layers.0.linear.weight)  min: -0.3967(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5799, loss_cls=0.1043, loss_bbox=0.6465, matched_ious=0.5493, loss_iou=0.0894, loss_iou_reg=0.2125, d_time=0.01(0.01), f_time=1.30(1.25), b_time=1.30(1.27)  Time cost: 03:00/34:06 [52:24/7:59:17]  Acc_iter 40600       Data time: 0.01(0.01)  Forward time: 1.30(1.25)  Batch time: 1.30(1.27)
2025-09-03 23:49:28,528   INFO  Train:   24/36 ( 67%) [ 192/1759 ( 11%)]  Loss: 3.471 (3.16)  LR: 1.949e-03  Grad: 4.9128  max=1.0674(module.vfe.pfn_layers.0.linear.weight)  min: -0.9464(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5809, loss_cls=0.1072, loss_bbox=0.6512, matched_ious=0.5444, loss_iou=0.0886, loss_iou_reg=0.2140, d_time=0.01(0.01), f_time=1.26(1.25), b_time=1.26(1.26)  Time cost: 04:04/33:01 [53:27/7:57:46]  Acc_iter 40650       Data time: 0.01(0.01)  Forward time: 1.26(1.25)  Batch time: 1.26(1.26)
2025-09-03 23:50:32,415   INFO  Train:   24/36 ( 67%) [ 242/1759 ( 14%)]  Loss: 3.010 (3.18)  LR: 1.943e-03  Grad: 5.0517  max=1.0164(module.vfe.pfn_layers.0.linear.weight)  min: -0.6544(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6023, loss_cls=0.1080, loss_bbox=0.6889, matched_ious=0.5438, loss_iou=0.0898, loss_iou_reg=0.2135, d_time=0.01(0.01), f_time=1.30(1.26), b_time=1.31(1.27)  Time cost: 05:07/32:02 [54:31/7:57:46]  Acc_iter 40700       Data time: 0.01(0.01)  Forward time: 1.30(1.26)  Batch time: 1.31(1.27)
2025-09-03 23:51:35,313   INFO  Train:   24/36 ( 67%) [ 292/1759 ( 17%)]  Loss: 2.903 (3.18)  LR: 1.937e-03  Grad: 5.1473  max=0.9549(module.vfe.pfn_layers.0.linear.weight)  min: -0.2793(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5747, loss_cls=0.1040, loss_bbox=0.6407, matched_ious=0.5494, loss_iou=0.0881, loss_iou_reg=0.2128, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 06:10/30:56 [55:34/7:56:08]  Acc_iter 40750       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-03 23:52:38,112   INFO  Train:   24/36 ( 67%) [ 342/1759 ( 19%)]  Loss: 3.476 (3.20)  LR: 1.931e-03  Grad: 5.2587  max=0.5024(module.vfe.pfn_layers.0.linear.weight)  min: -0.4135(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5846, loss_cls=0.1038, loss_bbox=0.6974, matched_ious=0.5402, loss_iou=0.0904, loss_iou_reg=0.2171, d_time=0.01(0.01), f_time=1.30(1.26), b_time=1.30(1.26)  Time cost: 07:13/29:51 [56:37/7:54:33]  Acc_iter 40800       Data time: 0.01(0.01)  Forward time: 1.30(1.26)  Batch time: 1.30(1.26)
2025-09-03 23:53:40,519   INFO  Train:   24/36 ( 67%) [ 392/1759 ( 22%)]  Loss: 2.926 (3.19)  LR: 1.925e-03  Grad: 5.4687  max=0.6925(module.vfe.pfn_layers.0.linear.weight)  min: -0.4768(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5613, loss_cls=0.1014, loss_bbox=0.6498, matched_ious=0.5432, loss_iou=0.0914, loss_iou_reg=0.2175, d_time=0.01(0.01), f_time=1.20(1.25), b_time=1.20(1.26)  Time cost: 08:15/28:45 [57:39/7:52:44]  Acc_iter 40850       Data time: 0.01(0.01)  Forward time: 1.20(1.25)  Batch time: 1.20(1.26)
2025-09-03 23:54:45,616   INFO  Train:   24/36 ( 67%) [ 442/1759 ( 25%)]  Loss: 3.433 (3.18)  LR: 1.919e-03  Grad: 6.0052  max=0.2873(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -2.2153(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5642, loss_cls=0.1006, loss_bbox=0.6301, matched_ious=0.5486, loss_iou=0.0901, loss_iou_reg=0.2147, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 09:21/27:48 [58:44/7:53:22]  Acc_iter 40900       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 23:55:49,216   INFO  Train:   24/36 ( 67%) [ 492/1759 ( 28%)]  Loss: 3.011 (3.18)  LR: 1.913e-03  Grad: 5.7647  max=0.4437(module.vfe.pfn_layers.0.linear.weight)  min: -0.7214(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5691, loss_cls=0.1018, loss_bbox=0.6259, matched_ious=0.5496, loss_iou=0.0893, loss_iou_reg=0.2145, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.23(1.27)  Time cost: 10:24/26:45 [59:48/7:52:31]  Acc_iter 40950       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.27)
2025-09-03 23:56:52,057   INFO  Train:   24/36 ( 67%) [ 542/1759 ( 31%)]  Loss: 3.607 (3.18)  LR: 1.908e-03  Grad: 5.9916  max=0.5512(module.vfe.pfn_layers.0.linear.weight)  min: -0.6428(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5947, loss_cls=0.1073, loss_bbox=0.6503, matched_ious=0.5500, loss_iou=0.0884, loss_iou_reg=0.2116, d_time=0.01(0.01), f_time=1.28(1.26), b_time=1.29(1.27)  Time cost: 11:27/25:40 [1:00:51/7:51:07]  Acc_iter 41000       Data time: 0.01(0.01)  Forward time: 1.28(1.26)  Batch time: 1.29(1.27)
2025-09-03 23:57:55,445   INFO  Train:   24/36 ( 67%) [ 592/1759 ( 34%)]  Loss: 3.071 (3.18)  LR: 1.902e-03  Grad: 6.2710  max=0.9640(module.vfe.pfn_layers.0.linear.weight)  min: -0.8603(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5696, loss_cls=0.1049, loss_bbox=0.6364, matched_ious=0.5382, loss_iou=0.0910, loss_iou_reg=0.2170, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.22(1.27)  Time cost: 12:30/24:37 [1:01:54/7:50:06]  Acc_iter 41050       Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.27)
2025-09-03 23:58:58,146   INFO  Train:   24/36 ( 67%) [ 642/1759 ( 36%)]  Loss: 2.868 (3.18)  LR: 1.896e-03  Grad: 4.0701  max=0.5280(module.vfe.pfn_layers.0.linear.weight)  min: -0.4517(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5864, loss_cls=0.1049, loss_bbox=0.6611, matched_ious=0.5404, loss_iou=0.0893, loss_iou_reg=0.2182, d_time=0.01(0.01), f_time=1.28(1.26), b_time=1.29(1.27)  Time cost: 13:33/23:33 [1:02:57/7:48:42]  Acc_iter 41100       Data time: 0.01(0.01)  Forward time: 1.28(1.26)  Batch time: 1.29(1.27)
2025-09-04 00:00:01,612   INFO  Train:   24/36 ( 67%) [ 692/1759 ( 39%)]  Loss: 3.389 (3.19)  LR: 1.890e-03  Grad: 4.1539  max=0.2323(module.vfe.pfn_layers.0.linear.weight)  min: -0.3122(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5791, loss_cls=0.1023, loss_bbox=0.6582, matched_ious=0.5444, loss_iou=0.0884, loss_iou_reg=0.2166, d_time=0.01(0.01), f_time=1.27(1.26), b_time=1.28(1.27)  Time cost: 14:37/22:30 [1:04:00/7:47:45]  Acc_iter 41150       Data time: 0.01(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.27)
2025-09-04 00:01:04,148   INFO  Train:   24/36 ( 67%) [ 742/1759 ( 42%)]  Loss: 4.382 (3.19)  LR: 1.884e-03  Grad: 4.4680  max=0.7664(module.vfe.pfn_layers.0.linear.weight)  min: -0.4648(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5720, loss_cls=0.1009, loss_bbox=0.6609, matched_ious=0.5381, loss_iou=0.0918, loss_iou_reg=0.2186, d_time=0.01(0.01), f_time=1.31(1.26), b_time=1.31(1.26)  Time cost: 15:39/21:26 [1:05:03/7:46:19]  Acc_iter 41200       Data time: 0.01(0.01)  Forward time: 1.31(1.26)  Batch time: 1.31(1.26)
2025-09-04 00:02:06,056   INFO  Train:   24/36 ( 67%) [ 792/1759 ( 45%)]  Loss: 3.093 (3.19)  LR: 1.878e-03  Grad: 4.5118  max=0.4245(module.vfe.pfn_layers.0.linear.weight)  min: -0.1761(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5905, loss_cls=0.1072, loss_bbox=0.6586, matched_ious=0.5393, loss_iou=0.0908, loss_iou_reg=0.2188, d_time=0.01(0.01), f_time=1.17(1.25), b_time=1.18(1.26)  Time cost: 16:41/20:21 [1:06:05/7:44:39]  Acc_iter 41250       Data time: 0.01(0.01)  Forward time: 1.17(1.25)  Batch time: 1.18(1.26)
2025-09-04 00:03:08,412   INFO  Train:   24/36 ( 67%) [ 842/1759 ( 48%)]  Loss: 3.368 (3.19)  LR: 1.872e-03  Grad: 4.8755  max=0.5772(module.vfe.pfn_layers.0.linear.weight)  min: -1.0151(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5779, loss_cls=0.1042, loss_bbox=0.6287, matched_ious=0.5456, loss_iou=0.0909, loss_iou_reg=0.2155, d_time=0.01(0.01), f_time=1.27(1.25), b_time=1.27(1.26)  Time cost: 17:43/19:17 [1:07:07/7:43:16]  Acc_iter 41300       Data time: 0.01(0.01)  Forward time: 1.27(1.25)  Batch time: 1.27(1.26)
2025-09-04 00:04:11,334   INFO  Train:   24/36 ( 67%) [ 892/1759 ( 51%)]  Loss: 3.523 (3.19)  LR: 1.866e-03  Grad: 4.7996  max=0.5579(module.vfe.pfn_layers.0.linear.weight)  min: -0.5464(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5782, loss_cls=0.1043, loss_bbox=0.6797, matched_ious=0.5396, loss_iou=0.0912, loss_iou_reg=0.2171, d_time=0.01(0.01), f_time=1.33(1.25), b_time=1.34(1.26)  Time cost: 18:46/18:14 [1:08:10/7:42:08]  Acc_iter 41350       Data time: 0.01(0.01)  Forward time: 1.33(1.25)  Batch time: 1.34(1.26)
2025-09-04 00:05:15,220   INFO  Train:   24/36 ( 67%) [ 942/1759 ( 54%)]  Loss: 2.947 (3.19)  LR: 1.860e-03  Grad: 5.0947  max=0.8195(module.vfe.pfn_layers.0.linear.weight)  min: -0.8710(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5687, loss_cls=0.1026, loss_bbox=0.6482, matched_ious=0.5442, loss_iou=0.0915, loss_iou_reg=0.2152, d_time=0.01(0.01), f_time=1.20(1.25), b_time=1.20(1.26)  Time cost: 19:50/17:11 [1:09:14/7:41:23]  Acc_iter 41400       Data time: 0.01(0.01)  Forward time: 1.20(1.25)  Batch time: 1.20(1.26)
2025-09-04 00:06:18,212   INFO  Train:   24/36 ( 67%) [ 992/1759 ( 56%)]  Loss: 3.316 (3.19)  LR: 1.854e-03  Grad: 3.0077  max=0.3985(module.vfe.pfn_layers.0.linear.weight)  min: -1.0750(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5733, loss_cls=0.1013, loss_bbox=0.6538, matched_ious=0.5439, loss_iou=0.0884, loss_iou_reg=0.2160, d_time=0.01(0.01), f_time=1.26(1.25), b_time=1.27(1.26)  Time cost: 20:53/16:08 [1:10:17/7:40:17]  Acc_iter 41450       Data time: 0.01(0.01)  Forward time: 1.26(1.25)  Batch time: 1.27(1.26)
2025-09-04 00:07:21,322   INFO  Train:   24/36 ( 67%) [1042/1759 ( 59%)]  Loss: 3.804 (3.19)  LR: 1.848e-03  Grad: 3.1164  max=0.6731(module.vfe.pfn_layers.0.linear.weight)  min: -0.2657(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5652, loss_cls=0.1015, loss_bbox=0.6096, matched_ious=0.5542, loss_iou=0.0859, loss_iou_reg=0.2096, d_time=0.01(0.01), f_time=1.32(1.25), b_time=1.33(1.26)  Time cost: 21:56/15:05 [1:11:20/7:39:14]  Acc_iter 41500       Data time: 0.01(0.01)  Forward time: 1.32(1.25)  Batch time: 1.33(1.26)
2025-09-04 00:08:23,597   INFO  Train:   24/36 ( 67%) [1092/1759 ( 62%)]  Loss: 2.700 (3.19)  LR: 1.841e-03  Grad: 3.3325  max=0.5655(module.vfe.pfn_layers.0.linear.weight)  min: -0.7337(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5687, loss_cls=0.0999, loss_bbox=0.6400, matched_ious=0.5503, loss_iou=0.0902, loss_iou_reg=0.2122, d_time=0.01(0.01), f_time=1.21(1.25), b_time=1.21(1.26)  Time cost: 22:59/14:01 [1:12:22/7:37:54]  Acc_iter 41550       Data time: 0.01(0.01)  Forward time: 1.21(1.25)  Batch time: 1.21(1.26)
2025-09-04 00:09:26,388   INFO  Train:   24/36 ( 67%) [1142/1759 ( 65%)]  Loss: 2.525 (3.18)  LR: 1.835e-03  Grad: 3.3162  max=0.2785(module.vfe.pfn_layers.0.linear.weight)  min: -0.4861(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5706, loss_cls=0.1049, loss_bbox=0.6450, matched_ious=0.5453, loss_iou=0.0883, loss_iou_reg=0.2160, d_time=0.01(0.01), f_time=1.24(1.25), b_time=1.25(1.26)  Time cost: 24:01/12:58 [1:13:25/7:36:45]  Acc_iter 41600       Data time: 0.01(0.01)  Forward time: 1.24(1.25)  Batch time: 1.25(1.26)
2025-09-04 00:10:29,630   INFO  Train:   24/36 ( 67%) [1192/1759 ( 68%)]  Loss: 3.113 (3.18)  LR: 1.829e-03  Grad: 3.5035  max=0.5019(module.vfe.pfn_layers.0.linear.weight)  min: -0.4185(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5602, loss_cls=0.1005, loss_bbox=0.6109, matched_ious=0.5494, loss_iou=0.0879, loss_iou_reg=0.2128, d_time=0.01(0.01), f_time=1.27(1.25), b_time=1.28(1.26)  Time cost: 25:05/11:55 [1:14:28/7:35:45]  Acc_iter 41650       Data time: 0.01(0.01)  Forward time: 1.27(1.25)  Batch time: 1.28(1.26)
2025-09-04 00:11:32,393   INFO  Train:   24/36 ( 67%) [1242/1759 ( 71%)]  Loss: 2.585 (3.18)  LR: 1.823e-03  Grad: 3.7161  max=0.3571(module.vfe.pfn_layers.0.linear.weight)  min: -0.3308(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5694, loss_cls=0.0992, loss_bbox=0.6125, matched_ious=0.5506, loss_iou=0.0868, loss_iou_reg=0.2113, d_time=0.01(0.01), f_time=1.26(1.25), b_time=1.27(1.26)  Time cost: 26:07/10:52 [1:15:31/7:34:36]  Acc_iter 41700       Data time: 0.01(0.01)  Forward time: 1.26(1.25)  Batch time: 1.27(1.26)
2025-09-04 00:12:34,701   INFO  Train:   24/36 ( 67%) [1292/1759 ( 73%)]  Loss: 3.122 (3.17)  LR: 1.817e-03  Grad: 3.7485  max=0.1803(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.3129(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5732, loss_cls=0.1034, loss_bbox=0.6379, matched_ious=0.5474, loss_iou=0.0889, loss_iou_reg=0.2141, d_time=0.01(0.01), f_time=1.26(1.25), b_time=1.26(1.26)  Time cost: 27:10/09:48 [1:16:33/7:33:21]  Acc_iter 41750       Data time: 0.01(0.01)  Forward time: 1.26(1.25)  Batch time: 1.26(1.26)
2025-09-04 00:13:37,470   INFO  Train:   24/36 ( 67%) [1342/1759 ( 76%)]  Loss: 3.801 (3.17)  LR: 1.811e-03  Grad: 4.2265  max=1.1340(module.vfe.pfn_layers.0.linear.weight)  min: -0.8589(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5424, loss_cls=0.1001, loss_bbox=0.6458, matched_ious=0.5479, loss_iou=0.0870, loss_iou_reg=0.2148, d_time=0.01(0.01), f_time=1.26(1.25), b_time=1.27(1.26)  Time cost: 28:12/08:45 [1:17:36/7:32:13]  Acc_iter 41800       Data time: 0.01(0.01)  Forward time: 1.26(1.25)  Batch time: 1.27(1.26)
2025-09-04 00:14:38,985   INFO  Train:   24/36 ( 67%) [1392/1759 ( 79%)]  Loss: 2.933 (3.17)  LR: 1.805e-03  Grad: 4.1609  max=0.3871(module.vfe.pfn_layers.0.linear.weight)  min: -0.4566(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5408, loss_cls=0.0985, loss_bbox=0.6188, matched_ious=0.5423, loss_iou=0.0910, loss_iou_reg=0.2190, d_time=0.01(0.01), f_time=1.20(1.25), b_time=1.21(1.26)  Time cost: 29:14/07:42 [1:18:38/7:30:47]  Acc_iter 41850       Data time: 0.01(0.01)  Forward time: 1.20(1.25)  Batch time: 1.21(1.26)
2025-09-04 00:15:44,141   INFO  Train:   24/36 ( 67%) [1442/1759 ( 82%)]  Loss: 3.619 (3.17)  LR: 1.799e-03  Grad: 4.6110  max=0.6047(module.vfe.pfn_layers.0.linear.weight)  min: -0.6627(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5831, loss_cls=0.1039, loss_bbox=0.6572, matched_ious=0.5442, loss_iou=0.0889, loss_iou_reg=0.2140, d_time=0.01(0.01), f_time=1.22(1.25), b_time=1.22(1.26)  Time cost: 30:19/06:39 [1:19:43/7:30:16]  Acc_iter 41900       Data time: 0.01(0.01)  Forward time: 1.22(1.25)  Batch time: 1.22(1.26)
2025-09-04 00:16:47,222   INFO  Train:   24/36 ( 67%) [1492/1759 ( 85%)]  Loss: 3.178 (3.17)  LR: 1.793e-03  Grad: 4.6601  max=0.4614(module.vfe.pfn_layers.0.linear.weight)  min: -0.8183(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5836, loss_cls=0.1035, loss_bbox=0.6857, matched_ious=0.5417, loss_iou=0.0887, loss_iou_reg=0.2178, d_time=0.01(0.01), f_time=1.28(1.25), b_time=1.29(1.26)  Time cost: 31:22/05:36 [1:20:46/7:29:14]  Acc_iter 41950       Data time: 0.01(0.01)  Forward time: 1.28(1.25)  Batch time: 1.29(1.26)
2025-09-04 00:17:50,135   INFO  Train:   24/36 ( 67%) [1542/1759 ( 88%)]  Loss: 3.848 (3.17)  LR: 1.787e-03  Grad: 4.7395  max=0.7569(module.vfe.pfn_layers.0.linear.weight)  min: -0.2474(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5779, loss_cls=0.1062, loss_bbox=0.6693, matched_ious=0.5408, loss_iou=0.0914, loss_iou_reg=0.2169, d_time=0.01(0.01), f_time=1.35(1.25), b_time=1.35(1.26)  Time cost: 32:25/04:33 [1:21:49/7:28:09]  Acc_iter 42000       Data time: 0.01(0.01)  Forward time: 1.35(1.25)  Batch time: 1.35(1.26)
2025-09-04 00:18:52,867   INFO  Train:   24/36 ( 67%) [1592/1759 ( 91%)]  Loss: 3.400 (3.17)  LR: 1.781e-03  Grad: 4.9129  max=0.5354(module.vfe.pfn_layers.0.linear.weight)  min: -0.7675(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5858, loss_cls=0.1047, loss_bbox=0.6612, matched_ious=0.5454, loss_iou=0.0904, loss_iou_reg=0.2138, d_time=0.01(0.01), f_time=1.16(1.25), b_time=1.17(1.26)  Time cost: 33:28/03:30 [1:22:52/7:27:01]  Acc_iter 42050       Data time: 0.01(0.01)  Forward time: 1.16(1.25)  Batch time: 1.17(1.26)
2025-09-04 00:19:56,624   INFO  Train:   24/36 ( 67%) [1642/1759 ( 93%)]  Loss: 2.718 (3.17)  LR: 1.775e-03  Grad: 5.1669  max=0.7769(module.vfe.pfn_layers.0.linear.weight)  min: -0.6999(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5762, loss_cls=0.1060, loss_bbox=0.6422, matched_ious=0.5504, loss_iou=0.0891, loss_iou_reg=0.2124, d_time=0.01(0.01), f_time=1.31(1.25), b_time=1.32(1.26)  Time cost: 34:32/02:27 [1:23:55/7:26:08]  Acc_iter 42100       Data time: 0.01(0.01)  Forward time: 1.31(1.25)  Batch time: 1.32(1.26)
2025-09-04 00:20:59,854   INFO  Train:   24/36 ( 67%) [1692/1759 ( 96%)]  Loss: 3.847 (3.17)  LR: 1.769e-03  Grad: 5.1596  max=0.4908(module.vfe.pfn_layers.0.linear.weight)  min: -0.4428(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5443, loss_cls=0.0973, loss_bbox=0.5989, matched_ious=0.5538, loss_iou=0.0881, loss_iou_reg=0.2106, d_time=0.01(0.01), f_time=1.32(1.25), b_time=1.33(1.26)  Time cost: 35:35/01:24 [1:24:59/7:25:07]  Acc_iter 42150       Data time: 0.01(0.01)  Forward time: 1.32(1.25)  Batch time: 1.33(1.26)
2025-09-04 00:22:02,508   INFO  Train:   24/36 ( 67%) [1742/1759 ( 99%)]  Loss: 2.760 (3.17)  LR: 1.763e-03  Grad: 5.2711  max=0.2942(module.vfe.pfn_layers.0.linear.weight)  min: -0.1744(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.5507, loss_cls=0.0988, loss_bbox=0.6219, matched_ious=0.5570, loss_iou=0.0875, loss_iou_reg=0.2096, d_time=0.00(0.01), f_time=1.20(1.25), b_time=1.21(1.26)  Time cost: 36:37/00:21 [1:26:01/7:23:59]  Acc_iter 42200       Data time: 0.00(0.01)  Forward time: 1.20(1.25)  Batch time: 1.21(1.26)
2025-09-04 00:22:23,092   INFO  Train:   24/36 ( 67%) [1758/1759 (100%)]  Loss: 2.654 (3.16)  LR: 1.761e-03  Grad: 3.8058  max=0.1909(module.vfe.pfn_layers.0.linear.weight)  min: -0.4620(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5470, loss_cls=0.0996, loss_bbox=0.5995, matched_ious=0.5575, loss_iou=0.0888, loss_iou_reg=0.2077, d_time=0.01(0.01), f_time=2.21(1.25), b_time=2.21(1.26)  Time cost: 36:58/00:01 [1:26:22/7:23:44]  Acc_iter 42216       Data time: 0.01(0.01)  Forward time: 2.21(1.25)  Batch time: 2.21(1.26)

                                               [Aepochs:  20%|██        | 3/15 [1:26:22<6:18:00, 1890.06s/it]epochs:  20%|██        | 3/15 [1:26:22<6:18:00, 1890.06s/it]epochs:  20%|██        | 3/15 [1:26:22<6:18:00, 1890.08s/it]epochs:  20%|██        | 3/15 [1:26:22<6:18:01, 1890.09s/it]epochs:  20%|██        | 3/15 [1:26:22<6:18:01, 1890.10s/it]epochs:  20%|██        | 3/15 [1:26:22<6:18:01, 1890.10s/it]epochs:  20%|██        | 3/15 [1:26:22<6:18:01, 1890.10s/it]epochs:  20%|██        | 3/15 [1:26:22<6:18:01, 1890.15s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 00:22:26,990   INFO  Train:   25/36 ( 69%) [   0/1759 (  0%)]  Loss: 3.636 (3.64)  LR: 1.760e-03  Grad: 5.7729  max=2.9641(module.vfe.pfn_layers.0.linear.weight)  min: -2.6464(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8099, loss_cls=0.1575, loss_bbox=0.9487, matched_ious=0.5386, loss_iou=0.0914, loss_iou_reg=0.2166, d_time=0.88(0.88), f_time=2.09(2.09), b_time=2.97(2.97)  Time cost: 00:02/1:24:55 [1:26:26/16:59:08]  Acc_iter 42217       Data time: 0.88(0.88)  Forward time: 2.09(2.09)  Batch time: 2.97(2.97)
2025-09-04 00:23:09,633   INFO  Train:   25/36 ( 69%) [  33/1759 (  2%)]  Loss: 2.660 (3.14)  LR: 1.756e-03  Grad: 4.0385  max=0.8966(module.vfe.pfn_layers.0.linear.weight)  min: -0.4235(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5626, loss_cls=0.1001, loss_bbox=0.6374, matched_ious=0.5540, loss_iou=0.0885, loss_iou_reg=0.2102, d_time=0.01(0.03), f_time=1.37(1.31), b_time=1.38(1.34)  Time cost: 00:45/38:31 [1:27:08/7:50:29]  Acc_iter 42250       Data time: 0.01(0.03)  Forward time: 1.37(1.31)  Batch time: 1.38(1.34)
2025-09-04 00:24:12,975   INFO  Train:   25/36 ( 69%) [  83/1759 (  5%)]  Loss: 3.503 (3.19)  LR: 1.750e-03  Grad: 4.0762  max=0.4448(module.vfe.pfn_layers.0.linear.weight)  min: -0.4606(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5850, loss_cls=0.1034, loss_bbox=0.6345, matched_ious=0.5422, loss_iou=0.0912, loss_iou_reg=0.2174, d_time=0.01(0.02), f_time=1.31(1.28), b_time=1.31(1.30)  Time cost: 01:48/36:12 [1:28:12/7:34:13]  Acc_iter 42300       Data time: 0.01(0.02)  Forward time: 1.31(1.28)  Batch time: 1.31(1.30)
2025-09-04 00:25:15,257   INFO  Train:   25/36 ( 69%) [ 133/1759 (  8%)]  Loss: 3.589 (3.20)  LR: 1.744e-03  Grad: 4.3419  max=0.7208(module.vfe.pfn_layers.0.linear.weight)  min: -0.3215(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5833, loss_cls=0.1051, loss_bbox=0.6496, matched_ious=0.5415, loss_iou=0.0885, loss_iou_reg=0.2181, d_time=0.01(0.01), f_time=1.24(1.27), b_time=1.24(1.28)  Time cost: 02:51/34:37 [1:29:14/7:26:33]  Acc_iter 42350       Data time: 0.01(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.28)
2025-09-04 00:26:20,409   INFO  Train:   25/36 ( 69%) [ 183/1759 ( 10%)]  Loss: 3.701 (3.18)  LR: 1.738e-03  Grad: 4.5155  max=0.2485(module.backbone_3d.cls_conv.3.bias)  min: -0.7270(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5515, loss_cls=0.1004, loss_bbox=0.6411, matched_ious=0.5460, loss_iou=0.0905, loss_iou_reg=0.2146, d_time=0.01(0.01), f_time=1.28(1.27), b_time=1.28(1.28)  Time cost: 03:56/33:44 [1:30:19/7:27:54]  Acc_iter 42400       Data time: 0.01(0.01)  Forward time: 1.28(1.27)  Batch time: 1.28(1.28)
2025-09-04 00:27:23,259   INFO  Train:   25/36 ( 69%) [ 233/1759 ( 13%)]  Loss: 2.959 (3.17)  LR: 1.732e-03  Grad: 7.1368  max=4.2063(module.vfe.pfn_layers.0.linear.weight)  min: -5.0017(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5581, loss_cls=0.1041, loss_bbox=0.6127, matched_ious=0.5483, loss_iou=0.0892, loss_iou_reg=0.2147, d_time=0.01(0.01), f_time=1.21(1.27), b_time=1.21(1.28)  Time cost: 04:59/32:30 [1:31:22/7:24:48]  Acc_iter 42450       Data time: 0.01(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.28)
2025-09-04 00:28:26,337   INFO  Train:   25/36 ( 69%) [ 283/1759 ( 16%)]  Loss: 3.367 (3.17)  LR: 1.726e-03  Grad: 3.1054  max=0.4223(module.vfe.pfn_layers.0.linear.weight)  min: -0.3148(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5605, loss_cls=0.1015, loss_bbox=0.6157, matched_ious=0.5517, loss_iou=0.0894, loss_iou_reg=0.2115, d_time=0.01(0.01), f_time=1.27(1.27), b_time=1.27(1.28)  Time cost: 06:02/31:22 [1:32:25/7:22:42]  Acc_iter 42500       Data time: 0.01(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.28)
2025-09-04 00:29:30,069   INFO  Train:   25/36 ( 69%) [ 333/1759 ( 19%)]  Loss: 3.074 (3.17)  LR: 1.720e-03  Grad: 3.3662  max=0.6751(module.vfe.pfn_layers.0.linear.weight)  min: -0.4418(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5655, loss_cls=0.1051, loss_bbox=0.6526, matched_ious=0.5471, loss_iou=0.0912, loss_iou_reg=0.2170, d_time=0.01(0.01), f_time=1.26(1.27), b_time=1.26(1.28)  Time cost: 07:05/30:18 [1:33:29/7:21:36]  Acc_iter 42550       Data time: 0.01(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.28)
2025-09-04 00:30:32,125   INFO  Train:   25/36 ( 69%) [ 383/1759 ( 22%)]  Loss: 3.099 (3.17)  LR: 1.714e-03  Grad: 3.6006  max=0.9114(module.vfe.pfn_layers.0.linear.weight)  min: -0.5455(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5723, loss_cls=0.1012, loss_bbox=0.6564, matched_ious=0.5513, loss_iou=0.0868, loss_iou_reg=0.2120, d_time=0.01(0.01), f_time=1.27(1.26), b_time=1.28(1.27)  Time cost: 08:08/29:08 [1:34:31/7:18:59]  Acc_iter 42600       Data time: 0.01(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.27)
2025-09-04 00:31:35,404   INFO  Train:   25/36 ( 69%) [ 433/1759 ( 25%)]  Loss: 3.326 (3.17)  LR: 1.707e-03  Grad: 3.5008  max=0.2449(module.vfe.pfn_layers.0.linear.weight)  min: -0.3038(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5629, loss_cls=0.1010, loss_bbox=0.6722, matched_ious=0.5542, loss_iou=0.0862, loss_iou_reg=0.2079, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.23(1.27)  Time cost: 09:11/28:04 [1:35:34/7:17:43]  Acc_iter 42650       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.27)
2025-09-04 00:32:39,624   INFO  Train:   25/36 ( 69%) [ 483/1759 ( 27%)]  Loss: 3.122 (3.16)  LR: 1.701e-03  Grad: 3.7528  max=0.5916(module.vfe.pfn_layers.0.linear.weight)  min: -0.3574(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5565, loss_cls=0.0993, loss_bbox=0.6367, matched_ious=0.5499, loss_iou=0.0896, loss_iou_reg=0.2114, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.23(1.27)  Time cost: 10:15/27:02 [1:36:38/7:17:10]  Acc_iter 42700       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.27)
2025-09-04 00:33:42,933   INFO  Train:   25/36 ( 69%) [ 533/1759 ( 30%)]  Loss: 3.167 (3.15)  LR: 1.695e-03  Grad: 3.9212  max=0.4099(module.vfe.pfn_layers.0.linear.weight)  min: -0.3720(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5412, loss_cls=0.0968, loss_bbox=0.5855, matched_ious=0.5473, loss_iou=0.0886, loss_iou_reg=0.2121, d_time=0.01(0.01), f_time=1.20(1.26), b_time=1.21(1.27)  Time cost: 11:18/25:58 [1:37:42/7:15:55]  Acc_iter 42750       Data time: 0.01(0.01)  Forward time: 1.20(1.26)  Batch time: 1.21(1.27)
2025-09-04 00:34:45,667   INFO  Train:   25/36 ( 69%) [ 583/1759 ( 33%)]  Loss: 2.872 (3.14)  LR: 1.689e-03  Grad: 4.0457  max=0.3109(module.vfe.pfn_layers.0.linear.weight)  min: -0.3554(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5407, loss_cls=0.0979, loss_bbox=0.6240, matched_ious=0.5509, loss_iou=0.0907, loss_iou_reg=0.2144, d_time=0.01(0.01), f_time=1.30(1.26), b_time=1.30(1.27)  Time cost: 12:21/24:53 [1:38:44/7:14:23]  Acc_iter 42800       Data time: 0.01(0.01)  Forward time: 1.30(1.26)  Batch time: 1.30(1.27)
2025-09-04 00:35:48,859   INFO  Train:   25/36 ( 69%) [ 633/1759 ( 36%)]  Loss: 2.647 (3.13)  LR: 1.683e-03  Grad: 4.3505  max=0.5605(module.vfe.pfn_layers.0.linear.weight)  min: -1.0433(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5478, loss_cls=0.0993, loss_bbox=0.5965, matched_ious=0.5544, loss_iou=0.0876, loss_iou_reg=0.2110, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.25(1.27)  Time cost: 13:24/23:49 [1:39:48/7:13:09]  Acc_iter 42850       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.27)
2025-09-04 00:36:53,351   INFO  Train:   25/36 ( 69%) [ 683/1759 ( 39%)]  Loss: 3.228 (3.12)  LR: 1.677e-03  Grad: 4.5177  max=1.0204(module.vfe.pfn_layers.0.linear.weight)  min: -0.7689(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5393, loss_cls=0.0991, loss_bbox=0.6103, matched_ious=0.5490, loss_iou=0.0902, loss_iou_reg=0.2148, d_time=0.01(0.01), f_time=1.18(1.26), b_time=1.18(1.27)  Time cost: 14:29/22:47 [1:40:52/7:12:37]  Acc_iter 42900       Data time: 0.01(0.01)  Forward time: 1.18(1.26)  Batch time: 1.18(1.27)
2025-09-04 00:37:56,484   INFO  Train:   25/36 ( 69%) [ 733/1759 ( 42%)]  Loss: 2.989 (3.13)  LR: 1.670e-03  Grad: 3.4404  max=1.7704(module.vfe.pfn_layers.0.linear.weight)  min: -0.5887(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5910, loss_cls=0.1037, loss_bbox=0.6844, matched_ious=0.5551, loss_iou=0.0879, loss_iou_reg=0.2089, d_time=0.01(0.01), f_time=1.28(1.26), b_time=1.28(1.27)  Time cost: 15:32/21:43 [1:41:55/7:11:22]  Acc_iter 42950       Data time: 0.01(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.27)
2025-09-04 00:38:59,884   INFO  Train:   25/36 ( 69%) [ 783/1759 ( 45%)]  Loss: 3.259 (3.13)  LR: 1.664e-03  Grad: 3.0012  max=0.3529(module.vfe.pfn_layers.0.linear.weight)  min: -0.4704(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5730, loss_cls=0.1040, loss_bbox=0.6212, matched_ious=0.5497, loss_iou=0.0881, loss_iou_reg=0.2152, d_time=0.01(0.01), f_time=1.14(1.26), b_time=1.15(1.27)  Time cost: 16:35/20:39 [1:42:59/7:10:15]  Acc_iter 43000       Data time: 0.01(0.01)  Forward time: 1.14(1.26)  Batch time: 1.15(1.27)
2025-09-04 00:40:02,657   INFO  Train:   25/36 ( 69%) [ 833/1759 ( 47%)]  Loss: 3.257 (3.13)  LR: 1.658e-03  Grad: 3.2990  max=0.6807(module.vfe.pfn_layers.0.linear.weight)  min: -0.6911(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5633, loss_cls=0.1005, loss_bbox=0.5974, matched_ious=0.5594, loss_iou=0.0898, loss_iou_reg=0.2101, d_time=0.01(0.01), f_time=1.20(1.26), b_time=1.20(1.27)  Time cost: 17:38/19:35 [1:44:01/7:08:54]  Acc_iter 43050       Data time: 0.01(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.27)
2025-09-04 00:41:05,157   INFO  Train:   25/36 ( 69%) [ 883/1759 ( 50%)]  Loss: 2.669 (3.13)  LR: 1.652e-03  Grad: 3.3101  max=0.4430(module.vfe.pfn_layers.0.linear.weight)  min: -0.4243(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5615, loss_cls=0.1021, loss_bbox=0.6577, matched_ious=0.5489, loss_iou=0.0902, loss_iou_reg=0.2152, d_time=0.01(0.01), f_time=1.18(1.26), b_time=1.18(1.27)  Time cost: 18:41/18:30 [1:45:04/7:07:28]  Acc_iter 43100       Data time: 0.01(0.01)  Forward time: 1.18(1.26)  Batch time: 1.18(1.27)
2025-09-04 00:42:07,240   INFO  Train:   25/36 ( 69%) [ 933/1759 ( 53%)]  Loss: 2.843 (3.13)  LR: 1.646e-03  Grad: 3.5676  max=0.5055(module.vfe.pfn_layers.0.linear.weight)  min: -0.3932(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5561, loss_cls=0.1003, loss_bbox=0.6472, matched_ious=0.5546, loss_iou=0.0892, loss_iou_reg=0.2100, d_time=0.01(0.01), f_time=1.32(1.26), b_time=1.32(1.27)  Time cost: 19:43/17:26 [1:46:06/7:05:56]  Acc_iter 43150       Data time: 0.01(0.01)  Forward time: 1.32(1.26)  Batch time: 1.32(1.27)
2025-09-04 00:43:09,545   INFO  Train:   25/36 ( 69%) [ 983/1759 ( 56%)]  Loss: 2.657 (3.13)  LR: 1.640e-03  Grad: 3.5935  max=0.2631(module.vfe.pfn_layers.0.linear.weight)  min: -0.3651(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5616, loss_cls=0.1020, loss_bbox=0.6501, matched_ious=0.5498, loss_iou=0.0871, loss_iou_reg=0.2121, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.23(1.27)  Time cost: 20:45/16:22 [1:47:08/7:04:32]  Acc_iter 43200       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.27)
2025-09-04 00:44:12,238   INFO  Train:   25/36 ( 69%) [1033/1759 ( 59%)]  Loss: 2.838 (3.13)  LR: 1.633e-03  Grad: 3.8528  max=0.6966(module.vfe.pfn_layers.0.linear.weight)  min: -0.3420(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5700, loss_cls=0.1021, loss_bbox=0.6510, matched_ious=0.5469, loss_iou=0.0908, loss_iou_reg=0.2140, d_time=0.01(0.01), f_time=1.27(1.26), b_time=1.27(1.27)  Time cost: 21:48/15:18 [1:48:11/7:03:17]  Acc_iter 43250       Data time: 0.01(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.27)
2025-09-04 00:45:14,957   INFO  Train:   25/36 ( 69%) [1083/1759 ( 62%)]  Loss: 2.724 (3.13)  LR: 1.627e-03  Grad: 3.9763  max=0.2332(module.vfe.pfn_layers.0.linear.weight)  min: -0.5239(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5409, loss_cls=0.0988, loss_bbox=0.6235, matched_ious=0.5537, loss_iou=0.0879, loss_iou_reg=0.2128, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.24(1.26)  Time cost: 22:50/14:14 [1:49:14/7:02:04]  Acc_iter 43300       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.26)
2025-09-04 00:46:17,486   INFO  Train:   25/36 ( 69%) [1133/1759 ( 64%)]  Loss: 3.024 (3.12)  LR: 1.621e-03  Grad: 4.3309  max=0.3160(module.vfe.pfn_layers.0.linear.weight)  min: -1.0212(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5428, loss_cls=0.0980, loss_bbox=0.5910, matched_ious=0.5536, loss_iou=0.0887, loss_iou_reg=0.2140, d_time=0.01(0.01), f_time=1.31(1.26), b_time=1.31(1.26)  Time cost: 23:53/13:11 [1:50:16/7:00:48]  Acc_iter 43350       Data time: 0.01(0.01)  Forward time: 1.31(1.26)  Batch time: 1.31(1.26)
2025-09-04 00:47:21,844   INFO  Train:   25/36 ( 69%) [1183/1759 ( 67%)]  Loss: 2.351 (3.12)  LR: 1.615e-03  Grad: 4.3915  max=0.7081(module.vfe.pfn_layers.0.linear.weight)  min: -0.6813(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5435, loss_cls=0.0998, loss_bbox=0.6131, matched_ious=0.5534, loss_iou=0.0890, loss_iou_reg=0.2128, d_time=0.01(0.01), f_time=1.28(1.26), b_time=1.29(1.27)  Time cost: 24:57/12:08 [1:51:21/7:00:05]  Acc_iter 43400       Data time: 0.01(0.01)  Forward time: 1.28(1.26)  Batch time: 1.29(1.27)
2025-09-04 00:48:25,037   INFO  Train:   25/36 ( 69%) [1233/1759 ( 70%)]  Loss: 3.161 (3.12)  LR: 1.609e-03  Grad: 4.4722  max=0.5572(module.vfe.pfn_layers.0.linear.weight)  min: -0.4587(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5532, loss_cls=0.1013, loss_bbox=0.6190, matched_ious=0.5545, loss_iou=0.0870, loss_iou_reg=0.2127, d_time=0.01(0.01), f_time=1.27(1.26), b_time=1.27(1.27)  Time cost: 26:00/11:05 [1:52:24/6:59:00]  Acc_iter 43450       Data time: 0.01(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.27)
2025-09-04 00:49:27,886   INFO  Train:   25/36 ( 69%) [1283/1759 ( 73%)]  Loss: 3.097 (3.12)  LR: 1.603e-03  Grad: 4.6323  max=0.5402(module.vfe.pfn_layers.0.linear.weight)  min: -0.4387(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5573, loss_cls=0.1035, loss_bbox=0.6219, matched_ious=0.5524, loss_iou=0.0906, loss_iou_reg=0.2150, d_time=0.01(0.01), f_time=1.31(1.26), b_time=1.31(1.26)  Time cost: 27:03/10:01 [1:53:27/6:57:51]  Acc_iter 43500       Data time: 0.01(0.01)  Forward time: 1.31(1.26)  Batch time: 1.31(1.26)
2025-09-04 00:50:30,107   INFO  Train:   25/36 ( 69%) [1333/1759 ( 76%)]  Loss: 2.657 (3.12)  LR: 1.596e-03  Grad: 4.6671  max=0.3078(module.vfe.pfn_layers.0.linear.weight)  min: -0.2248(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5753, loss_cls=0.1021, loss_bbox=0.6421, matched_ious=0.5455, loss_iou=0.0895, loss_iou_reg=0.2133, d_time=0.01(0.01), f_time=1.29(1.26), b_time=1.29(1.26)  Time cost: 28:06/08:58 [1:54:29/6:56:33]  Acc_iter 43550       Data time: 0.01(0.01)  Forward time: 1.29(1.26)  Batch time: 1.29(1.26)
2025-09-04 00:51:32,912   INFO  Train:   25/36 ( 69%) [1383/1759 ( 79%)]  Loss: 3.247 (3.12)  LR: 1.590e-03  Grad: 4.8460  max=0.5596(module.vfe.pfn_layers.0.linear.weight)  min: -0.4577(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5498, loss_cls=0.0986, loss_bbox=0.6162, matched_ious=0.5524, loss_iou=0.0882, loss_iou_reg=0.2113, d_time=0.01(0.01), f_time=1.29(1.26), b_time=1.29(1.26)  Time cost: 29:08/07:55 [1:55:32/6:55:24]  Acc_iter 43600       Data time: 0.01(0.01)  Forward time: 1.29(1.26)  Batch time: 1.29(1.26)
2025-09-04 00:52:35,931   INFO  Train:   25/36 ( 69%) [1433/1759 ( 81%)]  Loss: 3.145 (3.12)  LR: 1.584e-03  Grad: 1.9801  max=0.3169(module.vfe.pfn_layers.0.linear.weight)  min: -0.6104(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5471, loss_cls=0.0978, loss_bbox=0.6181, matched_ious=0.5541, loss_iou=0.0887, loss_iou_reg=0.2124, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.26(1.26)  Time cost: 30:11/06:51 [1:56:35/6:54:19]  Acc_iter 43650       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.26)
2025-09-04 00:53:38,309   INFO  Train:   25/36 ( 69%) [1483/1759 ( 84%)]  Loss: 2.798 (3.12)  LR: 1.578e-03  Grad: 2.6830  max=1.1925(module.vfe.pfn_layers.0.linear.weight)  min: -0.8244(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5655, loss_cls=0.0988, loss_bbox=0.6505, matched_ious=0.5507, loss_iou=0.0897, loss_iou_reg=0.2134, d_time=0.01(0.01), f_time=1.36(1.26), b_time=1.36(1.26)  Time cost: 31:14/05:48 [1:57:37/6:53:05]  Acc_iter 43700       Data time: 0.01(0.01)  Forward time: 1.36(1.26)  Batch time: 1.36(1.26)
2025-09-04 00:54:40,550   INFO  Train:   25/36 ( 69%) [1533/1759 ( 87%)]  Loss: 3.038 (3.11)  LR: 1.572e-03  Grad: 2.4424  max=0.5567(module.vfe.pfn_layers.0.linear.weight)  min: -0.4001(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5462, loss_cls=0.0958, loss_bbox=0.6050, matched_ious=0.5564, loss_iou=0.0898, loss_iou_reg=0.2100, d_time=0.01(0.01), f_time=1.27(1.26), b_time=1.28(1.26)  Time cost: 32:16/04:45 [1:58:39/6:51:50]  Acc_iter 43750       Data time: 0.01(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.26)
2025-09-04 00:55:43,107   INFO  Train:   25/36 ( 69%) [1583/1759 ( 90%)]  Loss: 2.826 (3.11)  LR: 1.565e-03  Grad: 2.5729  max=0.4109(module.vfe.pfn_layers.0.linear.weight)  min: -0.3882(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5683, loss_cls=0.1044, loss_bbox=0.6289, matched_ious=0.5512, loss_iou=0.0890, loss_iou_reg=0.2130, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.26(1.26)  Time cost: 33:19/03:42 [1:59:42/6:50:40]  Acc_iter 43800       Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.26)
2025-09-04 00:56:46,516   INFO  Train:   25/36 ( 69%) [1633/1759 ( 93%)]  Loss: 3.124 (3.12)  LR: 1.559e-03  Grad: 2.6181  max=0.3568(module.vfe.pfn_layers.0.linear.weight)  min: -0.4320(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5776, loss_cls=0.1019, loss_bbox=0.6598, matched_ious=0.5544, loss_iou=0.0892, loss_iou_reg=0.2103, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.27(1.26)  Time cost: 34:22/02:39 [2:00:45/6:49:41]  Acc_iter 43850       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.26)
2025-09-04 00:57:51,974   INFO  Train:   25/36 ( 69%) [1683/1759 ( 96%)]  Loss: 3.183 (3.11)  LR: 1.553e-03  Grad: 2.7785  max=0.1678(module.vfe.pfn_layers.0.linear.weight)  min: -0.3558(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5616, loss_cls=0.1015, loss_bbox=0.5803, matched_ious=0.5557, loss_iou=0.0899, loss_iou_reg=0.2112, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.26(1.26)  Time cost: 35:27/01:36 [2:01:51/6:49:05]  Acc_iter 43900       Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.26)
2025-09-04 00:58:54,198   INFO  Train:   25/36 ( 69%) [1733/1759 ( 99%)]  Loss: 3.301 (3.12)  LR: 1.547e-03  Grad: 2.9847  max=0.3439(module.vfe.pfn_layers.0.linear.weight)  min: -0.3414(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5841, loss_cls=0.1050, loss_bbox=0.6705, matched_ious=0.5552, loss_iou=0.0863, loss_iou_reg=0.2073, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.22(1.26)  Time cost: 36:30/00:32 [2:02:53/6:47:51]  Acc_iter 43950       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.26)
2025-09-04 00:59:24,383   INFO  Train:   25/36 ( 69%) [1758/1759 (100%)]  Loss: 3.421 (3.11)  LR: 1.544e-03  Grad: 3.5416  max=1.1010(module.vfe.pfn_layers.0.linear.weight)  min: -0.4628(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5223, loss_cls=0.0998, loss_bbox=0.5720, matched_ious=0.5500, loss_iou=0.0905, loss_iou_reg=0.2171, d_time=0.01(0.01), f_time=0.65(1.26), b_time=0.65(1.26)  Time cost: 37:00/00:01 [2:03:23/6:47:04]  Acc_iter 43975       Data time: 0.01(0.01)  Forward time: 0.65(1.26)  Batch time: 0.65(1.26)

                                               [Aepochs:  27%|██▋       | 4/15 [2:03:23<6:10:28, 2020.82s/it]epochs:  27%|██▋       | 4/15 [2:03:23<6:10:29, 2020.83s/it]epochs:  27%|██▋       | 4/15 [2:03:23<6:10:29, 2020.84s/it]epochs:  27%|██▋       | 4/15 [2:03:23<6:10:29, 2020.84s/it]epochs:  27%|██▋       | 4/15 [2:03:23<6:10:29, 2020.83s/it]epochs:  27%|██▋       | 4/15 [2:03:23<6:10:29, 2020.83s/it]epochs:  27%|██▋       | 4/15 [2:03:23<6:10:29, 2020.85s/it]epochs:  27%|██▋       | 4/15 [2:03:24<6:10:29, 2020.88s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 00:59:28,521   INFO  Train:   26/36 ( 72%) [   0/1759 (  0%)]  Loss: 3.045 (3.04)  LR: 1.544e-03  Grad: 4.2519  max=2.1808(module.vfe.pfn_layers.0.linear.weight)  min: -0.9221(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5685, loss_cls=0.0984, loss_bbox=0.5696, matched_ious=0.5205, loss_iou=0.0853, loss_iou_reg=0.2290, d_time=1.11(1.11), f_time=2.10(2.10), b_time=3.22(3.22)  Time cost: 00:03/1:32:02 [2:03:27/16:52:31]  Acc_iter 43976       Data time: 1.11(1.11)  Forward time: 2.10(2.10)  Batch time: 3.22(3.22)
2025-09-04 00:59:58,502   INFO  Train:   26/36 ( 72%) [  24/1759 (  1%)]  Loss: 2.734 (3.04)  LR: 1.541e-03  Grad: 3.1877  max=0.2404(module.vfe.pfn_layers.0.linear.weight)  min: -0.6794(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5269, loss_cls=0.0968, loss_bbox=0.6032, matched_ious=0.5507, loss_iou=0.0889, loss_iou_reg=0.2117, d_time=0.01(0.05), f_time=1.22(1.28), b_time=1.23(1.33)  Time cost: 00:33/38:18 [2:03:57/7:06:45]  Acc_iter 44000       Data time: 0.01(0.05)  Forward time: 1.22(1.28)  Batch time: 1.23(1.33)
2025-09-04 01:01:01,543   INFO  Train:   26/36 ( 72%) [  74/1759 (  4%)]  Loss: 2.618 (3.04)  LR: 1.534e-03  Grad: 2.7599  max=0.4098(module.vfe.pfn_layers.0.linear.weight)  min: -0.2874(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5333, loss_cls=0.0946, loss_bbox=0.5890, matched_ious=0.5515, loss_iou=0.0905, loss_iou_reg=0.2141, d_time=0.01(0.02), f_time=1.17(1.26), b_time=1.17(1.28)  Time cost: 01:36/36:00 [2:05:00/6:51:54]  Acc_iter 44050       Data time: 0.01(0.02)  Forward time: 1.17(1.26)  Batch time: 1.17(1.28)
2025-09-04 01:02:04,295   INFO  Train:   26/36 ( 72%) [ 124/1759 (  7%)]  Loss: 3.115 (3.08)  LR: 1.528e-03  Grad: 2.9889  max=0.3446(module.vfe.pfn_layers.0.linear.weight)  min: -0.2336(module.vfe.pfn_layers.0.norm.weight)  NaN: False  loss_hm=0.5736, loss_cls=0.1036, loss_bbox=0.6162, matched_ious=0.5520, loss_iou=0.0890, loss_iou_reg=0.2132, d_time=0.01(0.01), f_time=1.35(1.26), b_time=1.35(1.27)  Time cost: 02:38/34:38 [2:06:03/6:47:21]  Acc_iter 44100       Data time: 0.01(0.01)  Forward time: 1.35(1.26)  Batch time: 1.35(1.27)
2025-09-04 01:03:07,402   INFO  Train:   26/36 ( 72%) [ 174/1759 ( 10%)]  Loss: 3.841 (3.06)  LR: 1.522e-03  Grad: 3.2784  max=0.6589(module.vfe.pfn_layers.0.linear.weight)  min: -0.8384(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5281, loss_cls=0.0966, loss_bbox=0.5737, matched_ious=0.5550, loss_iou=0.0877, loss_iou_reg=0.2115, d_time=0.00(0.01), f_time=1.33(1.26), b_time=1.33(1.27)  Time cost: 03:42/33:30 [2:07:06/6:45:27]  Acc_iter 44150       Data time: 0.00(0.01)  Forward time: 1.33(1.26)  Batch time: 1.33(1.27)
2025-09-04 01:04:09,602   INFO  Train:   26/36 ( 72%) [ 224/1759 ( 13%)]  Loss: 3.231 (3.06)  LR: 1.516e-03  Grad: 3.3449  max=0.2816(module.vfe.pfn_layers.0.linear.weight)  min: -0.6804(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5343, loss_cls=0.0956, loss_bbox=0.6075, matched_ious=0.5518, loss_iou=0.0879, loss_iou_reg=0.2120, d_time=0.01(0.01), f_time=1.23(1.25), b_time=1.23(1.26)  Time cost: 04:44/32:19 [2:08:08/6:42:38]  Acc_iter 44200       Data time: 0.01(0.01)  Forward time: 1.23(1.25)  Batch time: 1.23(1.26)
2025-09-04 01:05:13,056   INFO  Train:   26/36 ( 72%) [ 274/1759 ( 16%)]  Loss: 2.774 (3.07)  LR: 1.510e-03  Grad: 3.8604  max=1.1618(module.vfe.pfn_layers.0.linear.weight)  min: -0.2083(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5634, loss_cls=0.0994, loss_bbox=0.6419, matched_ious=0.5547, loss_iou=0.0866, loss_iou_reg=0.2105, d_time=0.01(0.01), f_time=1.29(1.25), b_time=1.30(1.26)  Time cost: 05:47/31:17 [2:09:12/6:41:56]  Acc_iter 44250       Data time: 0.01(0.01)  Forward time: 1.29(1.25)  Batch time: 1.30(1.26)
2025-09-04 01:06:15,687   INFO  Train:   26/36 ( 72%) [ 324/1759 ( 18%)]  Loss: 3.203 (3.06)  LR: 1.503e-03  Grad: 3.7160  max=0.2870(module.vfe.pfn_layers.0.linear.weight)  min: -0.9891(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5265, loss_cls=0.0927, loss_bbox=0.5948, matched_ious=0.5556, loss_iou=0.0875, loss_iou_reg=0.2113, d_time=0.01(0.01), f_time=1.35(1.25), b_time=1.36(1.26)  Time cost: 06:50/30:11 [2:10:14/6:40:18]  Acc_iter 44300       Data time: 0.01(0.01)  Forward time: 1.35(1.25)  Batch time: 1.36(1.26)
2025-09-04 01:07:18,375   INFO  Train:   26/36 ( 72%) [ 374/1759 ( 21%)]  Loss: 2.997 (3.06)  LR: 1.497e-03  Grad: 3.7157  max=0.4526(module.vfe.pfn_layers.0.linear.weight)  min: -0.5008(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5235, loss_cls=0.0977, loss_bbox=0.5982, matched_ious=0.5557, loss_iou=0.0875, loss_iou_reg=0.2121, d_time=0.01(0.01), f_time=1.36(1.25), b_time=1.37(1.26)  Time cost: 07:52/29:06 [2:11:17/6:38:53]  Acc_iter 44350       Data time: 0.01(0.01)  Forward time: 1.36(1.25)  Batch time: 1.37(1.26)
2025-09-04 01:08:24,017   INFO  Train:   26/36 ( 72%) [ 424/1759 ( 24%)]  Loss: 3.240 (3.06)  LR: 1.491e-03  Grad: 3.8219  max=0.5084(module.vfe.pfn_layers.0.linear.weight)  min: -0.4173(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5516, loss_cls=0.1001, loss_bbox=0.6114, matched_ious=0.5568, loss_iou=0.0876, loss_iou_reg=0.2118, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.26(1.27)  Time cost: 08:58/28:11 [2:12:23/6:39:45]  Acc_iter 44400       Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.27)
2025-09-04 01:09:27,277   INFO  Train:   26/36 ( 72%) [ 474/1759 ( 27%)]  Loss: 2.819 (3.05)  LR: 1.485e-03  Grad: 4.1957  max=0.8244(module.vfe.pfn_layers.0.linear.weight)  min: -0.9898(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5308, loss_cls=0.0956, loss_bbox=0.5898, matched_ious=0.5533, loss_iou=0.0890, loss_iou_reg=0.2148, d_time=0.04(0.01), f_time=1.49(1.26), b_time=1.53(1.27)  Time cost: 10:01/27:08 [2:13:26/6:38:37]  Acc_iter 44450       Data time: 0.04(0.01)  Forward time: 1.49(1.26)  Batch time: 1.53(1.27)
2025-09-04 01:10:30,721   INFO  Train:   26/36 ( 72%) [ 524/1759 ( 30%)]  Loss: 3.025 (3.05)  LR: 1.479e-03  Grad: 4.2944  max=0.4094(module.vfe.pfn_layers.0.linear.weight)  min: -0.8396(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5295, loss_cls=0.0942, loss_bbox=0.6611, matched_ious=0.5527, loss_iou=0.0872, loss_iou_reg=0.2126, d_time=0.01(0.01), f_time=1.35(1.26), b_time=1.36(1.27)  Time cost: 11:05/26:05 [2:14:29/6:37:37]  Acc_iter 44500       Data time: 0.01(0.01)  Forward time: 1.35(1.26)  Batch time: 1.36(1.27)
2025-09-04 01:11:34,057   INFO  Train:   26/36 ( 72%) [ 574/1759 ( 33%)]  Loss: 2.830 (3.05)  LR: 1.472e-03  Grad: 4.2832  max=0.5539(module.vfe.pfn_layers.0.linear.weight)  min: -0.3788(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5525, loss_cls=0.0993, loss_bbox=0.6001, matched_ious=0.5502, loss_iou=0.0918, loss_iou_reg=0.2131, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 12:08/25:01 [2:15:33/6:36:32]  Acc_iter 44550       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-04 01:12:36,445   INFO  Train:   26/36 ( 72%) [ 624/1759 ( 35%)]  Loss: 3.258 (3.05)  LR: 1.466e-03  Grad: 4.4078  max=0.4719(module.vfe.pfn_layers.0.linear.weight)  min: -0.7791(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5402, loss_cls=0.0984, loss_bbox=0.6187, matched_ious=0.5562, loss_iou=0.0877, loss_iou_reg=0.2110, d_time=0.01(0.01), f_time=1.19(1.26), b_time=1.20(1.27)  Time cost: 13:11/23:56 [2:16:35/6:35:00]  Acc_iter 44600       Data time: 0.01(0.01)  Forward time: 1.19(1.26)  Batch time: 1.20(1.27)
2025-09-04 01:13:39,170   INFO  Train:   26/36 ( 72%) [ 674/1759 ( 38%)]  Loss: 3.181 (3.05)  LR: 1.460e-03  Grad: 2.9762  max=0.5097(module.vfe.pfn_layers.0.linear.weight)  min: -0.4907(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5574, loss_cls=0.0989, loss_bbox=0.6307, matched_ious=0.5594, loss_iou=0.0889, loss_iou_reg=0.2077, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.25(1.26)  Time cost: 14:13/22:52 [2:17:38/6:33:41]  Acc_iter 44650       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.26)
2025-09-04 01:14:41,516   INFO  Train:   26/36 ( 72%) [ 724/1759 ( 41%)]  Loss: 2.752 (3.05)  LR: 1.454e-03  Grad: 2.8616  max=0.9069(module.vfe.pfn_layers.0.linear.weight)  min: -0.5702(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5339, loss_cls=0.0976, loss_bbox=0.6032, matched_ious=0.5486, loss_iou=0.0882, loss_iou_reg=0.2140, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.26(1.26)  Time cost: 15:16/21:47 [2:18:40/6:32:15]  Acc_iter 44700       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.26)
2025-09-04 01:15:43,781   INFO  Train:   26/36 ( 72%) [ 774/1759 ( 44%)]  Loss: 2.920 (3.05)  LR: 1.448e-03  Grad: 2.8878  max=0.8662(module.vfe.pfn_layers.0.linear.weight)  min: -0.4479(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5358, loss_cls=0.0966, loss_bbox=0.5904, matched_ious=0.5524, loss_iou=0.0906, loss_iou_reg=0.2123, d_time=0.01(0.01), f_time=1.13(1.25), b_time=1.13(1.26)  Time cost: 16:18/20:43 [2:19:42/6:30:50]  Acc_iter 44750       Data time: 0.01(0.01)  Forward time: 1.13(1.25)  Batch time: 1.13(1.26)
2025-09-04 01:16:46,191   INFO  Train:   26/36 ( 72%) [ 824/1759 ( 47%)]  Loss: 2.555 (3.05)  LR: 1.441e-03  Grad: 3.1641  max=1.1173(module.vfe.pfn_layers.0.linear.weight)  min: -0.4195(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5456, loss_cls=0.0957, loss_bbox=0.5993, matched_ious=0.5600, loss_iou=0.0873, loss_iou_reg=0.2087, d_time=0.01(0.01), f_time=1.20(1.25), b_time=1.21(1.26)  Time cost: 17:20/19:39 [2:20:45/6:29:30]  Acc_iter 44800       Data time: 0.01(0.01)  Forward time: 1.20(1.25)  Batch time: 1.21(1.26)
2025-09-04 01:17:50,530   INFO  Train:   26/36 ( 72%) [ 874/1759 ( 50%)]  Loss: 3.027 (3.05)  LR: 1.435e-03  Grad: 3.0576  max=0.2129(module.vfe.pfn_layers.0.linear.weight)  min: -0.4205(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5370, loss_cls=0.0955, loss_bbox=0.6114, matched_ious=0.5590, loss_iou=0.0871, loss_iou_reg=0.2124, d_time=0.01(0.01), f_time=1.29(1.26), b_time=1.30(1.26)  Time cost: 18:25/18:37 [2:21:49/6:28:54]  Acc_iter 44850       Data time: 0.01(0.01)  Forward time: 1.29(1.26)  Batch time: 1.30(1.26)
2025-09-04 01:18:56,749   INFO  Train:   26/36 ( 72%) [ 924/1759 ( 53%)]  Loss: 3.604 (3.05)  LR: 1.429e-03  Grad: 3.2701  max=0.7248(module.vfe.pfn_layers.0.linear.weight)  min: -0.1956(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.5501, loss_cls=0.0971, loss_bbox=0.6086, matched_ious=0.5579, loss_iou=0.0890, loss_iou_reg=0.2099, d_time=0.01(0.01), f_time=1.23(1.26), b_time=1.24(1.27)  Time cost: 19:31/17:37 [2:22:55/6:28:52]  Acc_iter 44900       Data time: 0.01(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.27)
2025-09-04 01:19:59,803   INFO  Train:   26/36 ( 72%) [ 974/1759 ( 55%)]  Loss: 2.479 (3.05)  LR: 1.423e-03  Grad: 3.4020  max=0.7327(module.vfe.pfn_layers.0.linear.weight)  min: -0.2536(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5435, loss_cls=0.0974, loss_bbox=0.5785, matched_ious=0.5580, loss_iou=0.0886, loss_iou_reg=0.2110, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.26(1.27)  Time cost: 20:34/16:33 [2:23:58/6:27:44]  Acc_iter 44950       Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.27)
2025-09-04 01:21:03,018   INFO  Train:   26/36 ( 72%) [1024/1759 ( 58%)]  Loss: 2.851 (3.05)  LR: 1.417e-03  Grad: 3.6054  max=0.5107(module.vfe.pfn_layers.0.linear.weight)  min: -0.8052(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5431, loss_cls=0.0981, loss_bbox=0.6150, matched_ious=0.5471, loss_iou=0.0935, loss_iou_reg=0.2151, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.26(1.27)  Time cost: 21:37/15:30 [2:25:02/6:26:39]  Acc_iter 45000       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.27)
2025-09-04 01:22:05,658   INFO  Train:   26/36 ( 72%) [1074/1759 ( 61%)]  Loss: 2.540 (3.05)  LR: 1.410e-03  Grad: 3.8350  max=0.9989(module.vfe.pfn_layers.0.linear.weight)  min: -0.4070(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5283, loss_cls=0.0957, loss_bbox=0.5895, matched_ious=0.5569, loss_iou=0.0868, loss_iou_reg=0.2107, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.25(1.27)  Time cost: 22:40/14:26 [2:26:04/6:25:24]  Acc_iter 45050       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.27)
2025-09-04 01:23:08,608   INFO  Train:   26/36 ( 72%) [1124/1759 ( 64%)]  Loss: 3.070 (3.05)  LR: 1.404e-03  Grad: 3.7760  max=0.2909(module.vfe.pfn_layers.0.linear.weight)  min: -0.5227(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5405, loss_cls=0.0959, loss_bbox=0.6109, matched_ious=0.5490, loss_iou=0.0899, loss_iou_reg=0.2132, d_time=0.01(0.01), f_time=1.34(1.26), b_time=1.34(1.27)  Time cost: 23:43/13:23 [2:27:07/6:24:16]  Acc_iter 45100       Data time: 0.01(0.01)  Forward time: 1.34(1.26)  Batch time: 1.34(1.27)
2025-09-04 01:24:10,831   INFO  Train:   26/36 ( 72%) [1174/1759 ( 67%)]  Loss: 3.284 (3.05)  LR: 1.398e-03  Grad: 4.0767  max=1.4730(module.vfe.pfn_layers.0.linear.weight)  min: -2.0225(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5357, loss_cls=0.0959, loss_bbox=0.5734, matched_ious=0.5589, loss_iou=0.0847, loss_iou_reg=0.2095, d_time=0.01(0.01), f_time=1.33(1.26), b_time=1.33(1.26)  Time cost: 24:45/12:19 [2:28:09/6:22:57]  Acc_iter 45150       Data time: 0.01(0.01)  Forward time: 1.33(1.26)  Batch time: 1.33(1.26)
2025-09-04 01:25:14,174   INFO  Train:   26/36 ( 72%) [1224/1759 ( 70%)]  Loss: 2.742 (3.05)  LR: 1.392e-03  Grad: 3.1608  max=0.1624(module.vfe.pfn_layers.0.linear.weight)  min: -0.4089(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5449, loss_cls=0.0968, loss_bbox=0.5958, matched_ious=0.5527, loss_iou=0.0870, loss_iou_reg=0.2129, d_time=0.01(0.01), f_time=1.29(1.26), b_time=1.29(1.26)  Time cost: 25:48/11:16 [2:29:13/6:21:55]  Acc_iter 45200       Data time: 0.01(0.01)  Forward time: 1.29(1.26)  Batch time: 1.29(1.26)
2025-09-04 01:26:16,358   INFO  Train:   26/36 ( 72%) [1274/1759 ( 72%)]  Loss: 2.167 (3.05)  LR: 1.386e-03  Grad: 3.3578  max=0.4721(module.vfe.pfn_layers.0.linear.weight)  min: -0.4585(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5342, loss_cls=0.0976, loss_bbox=0.6522, matched_ious=0.5511, loss_iou=0.0881, loss_iou_reg=0.2133, d_time=0.01(0.01), f_time=1.30(1.26), b_time=1.30(1.26)  Time cost: 26:50/10:12 [2:30:15/6:20:38]  Acc_iter 45250       Data time: 0.01(0.01)  Forward time: 1.30(1.26)  Batch time: 1.30(1.26)
2025-09-04 01:27:20,244   INFO  Train:   26/36 ( 72%) [1324/1759 ( 75%)]  Loss: 3.173 (3.05)  LR: 1.380e-03  Grad: 3.4726  max=0.4887(module.vfe.pfn_layers.0.linear.weight)  min: -0.1268(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.5339, loss_cls=0.0972, loss_bbox=0.6055, matched_ious=0.5564, loss_iou=0.0877, loss_iou_reg=0.2117, d_time=0.01(0.01), f_time=1.31(1.26), b_time=1.32(1.26)  Time cost: 27:54/09:09 [2:31:19/6:19:44]  Acc_iter 45300       Data time: 0.01(0.01)  Forward time: 1.31(1.26)  Batch time: 1.32(1.26)
2025-09-04 01:28:23,808   INFO  Train:   26/36 ( 72%) [1374/1759 ( 78%)]  Loss: 3.021 (3.05)  LR: 1.373e-03  Grad: 3.6151  max=0.5514(module.vfe.pfn_layers.0.linear.weight)  min: -0.2652(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5198, loss_cls=0.0958, loss_bbox=0.6123, matched_ious=0.5585, loss_iou=0.0880, loss_iou_reg=0.2094, d_time=0.01(0.01), f_time=1.33(1.26), b_time=1.33(1.26)  Time cost: 28:58/08:06 [2:32:22/6:18:46]  Acc_iter 45350       Data time: 0.01(0.01)  Forward time: 1.33(1.26)  Batch time: 1.33(1.26)
2025-09-04 01:29:29,450   INFO  Train:   26/36 ( 72%) [1424/1759 ( 81%)]  Loss: 2.204 (3.05)  LR: 1.367e-03  Grad: 3.2978  max=0.5625(module.vfe.pfn_layers.0.linear.weight)  min: -0.3981(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5348, loss_cls=0.0960, loss_bbox=0.5975, matched_ious=0.5566, loss_iou=0.0893, loss_iou_reg=0.2120, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 30:04/07:04 [2:33:28/6:18:13]  Acc_iter 45400       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-04 01:30:32,574   INFO  Train:   26/36 ( 72%) [1474/1759 ( 84%)]  Loss: 2.899 (3.04)  LR: 1.361e-03  Grad: 2.8970  max=0.5298(module.vfe.pfn_layers.0.linear.weight)  min: -0.3297(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5011, loss_cls=0.0897, loss_bbox=0.5710, matched_ious=0.5698, loss_iou=0.0864, loss_iou_reg=0.2052, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.26(1.27)  Time cost: 31:07/06:00 [2:34:31/6:17:07]  Acc_iter 45450       Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.27)
2025-09-04 01:31:34,730   INFO  Train:   26/36 ( 72%) [1524/1759 ( 87%)]  Loss: 3.142 (3.04)  LR: 1.355e-03  Grad: 3.3408  max=0.4416(module.vfe.pfn_layers.0.linear.weight)  min: -0.7259(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5251, loss_cls=0.0937, loss_bbox=0.5981, matched_ious=0.5650, loss_iou=0.0857, loss_iou_reg=0.2075, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 32:09/04:57 [2:35:33/6:15:51]  Acc_iter 45500       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-04 01:32:37,519   INFO  Train:   26/36 ( 72%) [1574/1759 ( 89%)]  Loss: 3.151 (3.04)  LR: 1.349e-03  Grad: 3.2308  max=0.4356(module.vfe.pfn_layers.0.linear.weight)  min: -0.2666(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5395, loss_cls=0.0956, loss_bbox=0.6040, matched_ious=0.5607, loss_iou=0.0881, loss_iou_reg=0.2093, d_time=0.01(0.01), f_time=1.17(1.26), b_time=1.18(1.26)  Time cost: 33:12/03:53 [2:36:36/6:14:42]  Acc_iter 45550       Data time: 0.01(0.01)  Forward time: 1.17(1.26)  Batch time: 1.18(1.26)
2025-09-04 01:33:40,132   INFO  Train:   26/36 ( 72%) [1624/1759 ( 92%)]  Loss: 3.063 (3.04)  LR: 1.342e-03  Grad: 3.5278  max=0.4729(module.vfe.pfn_layers.0.linear.weight)  min: -0.9415(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5404, loss_cls=0.0970, loss_bbox=0.5729, matched_ious=0.5631, loss_iou=0.0870, loss_iou_reg=0.2074, d_time=0.01(0.01), f_time=1.28(1.26), b_time=1.28(1.26)  Time cost: 34:14/02:50 [2:37:39/6:13:32]  Acc_iter 45600       Data time: 0.01(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.26)
2025-09-04 01:34:42,728   INFO  Train:   26/36 ( 72%) [1674/1759 ( 95%)]  Loss: 3.350 (3.04)  LR: 1.336e-03  Grad: 3.6202  max=0.5426(module.vfe.pfn_layers.0.linear.weight)  min: -0.5810(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5430, loss_cls=0.0956, loss_bbox=0.6230, matched_ious=0.5567, loss_iou=0.0882, loss_iou_reg=0.2116, d_time=0.01(0.01), f_time=1.32(1.26), b_time=1.32(1.26)  Time cost: 35:17/01:47 [2:38:41/6:12:22]  Acc_iter 45650       Data time: 0.01(0.01)  Forward time: 1.32(1.26)  Batch time: 1.32(1.26)
2025-09-04 01:35:45,685   INFO  Train:   26/36 ( 72%) [1724/1759 ( 98%)]  Loss: 3.785 (3.04)  LR: 1.330e-03  Grad: 3.5518  max=0.2083(module.dense_head.prediction_head.height.1.bias)  min: -0.5035(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5191, loss_cls=0.0926, loss_bbox=0.5950, matched_ious=0.5593, loss_iou=0.0879, loss_iou_reg=0.2097, d_time=0.01(0.01), f_time=1.27(1.26), b_time=1.28(1.26)  Time cost: 36:20/00:44 [2:39:44/6:11:17]  Acc_iter 45700       Data time: 0.01(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.26)
2025-09-04 01:36:26,966   INFO  Train:   26/36 ( 72%) [1758/1759 (100%)]  Loss: 2.595 (3.04)  LR: 1.326e-03  Grad: 4.2208  max=1.1203(module.vfe.pfn_layers.0.linear.weight)  min: -0.6121(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5211, loss_cls=0.0956, loss_bbox=0.5918, matched_ious=0.5636, loss_iou=0.0834, loss_iou_reg=0.2090, d_time=0.00(0.01), f_time=0.78(1.26), b_time=0.79(1.26)  Time cost: 37:01/00:01 [2:40:26/6:10:17]  Acc_iter 45734       Data time: 0.00(0.01)  Forward time: 0.78(1.26)  Batch time: 0.79(1.26)

                                               [Aepochs:  33%|███▎      | 5/15 [2:40:26<5:48:55, 2093.58s/it]epochs:  33%|███▎      | 5/15 [2:40:26<5:48:55, 2093.59s/it]epochs:  33%|███▎      | 5/15 [2:40:26<5:48:55, 2093.58s/it]epochs:  33%|███▎      | 5/15 [2:40:26<5:48:55, 2093.59s/it]epochs:  33%|███▎      | 5/15 [2:40:26<5:48:56, 2093.60s/it]epochs:  33%|███▎      | 5/15 [2:40:26<5:48:56, 2093.61s/it]epochs:  33%|███▎      | 5/15 [2:40:26<5:48:56, 2093.61s/it]epochs:  33%|███▎      | 5/15 [2:40:26<5:48:56, 2093.62s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 01:36:31,025   INFO  Train:   27/36 ( 75%) [   0/1759 (  0%)]  Loss: 2.954 (2.95)  LR: 1.326e-03  Grad: 3.6856  max=0.6074(module.vfe.pfn_layers.0.linear.weight)  min: -0.3343(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4810, loss_cls=0.0823, loss_bbox=0.4502, matched_ious=0.5750, loss_iou=0.0931, loss_iou_reg=0.2074, d_time=0.76(0.76), f_time=2.34(2.34), b_time=3.10(3.10)  Time cost: 00:03/1:29:20 [2:40:30/14:53:21]  Acc_iter 45735       Data time: 0.76(0.76)  Forward time: 2.34(2.34)  Batch time: 3.10(3.10)
2025-09-04 01:36:50,748   INFO  Train:   27/36 ( 75%) [  15/1759 (  1%)]  Loss: 3.035 (2.99)  LR: 1.324e-03  Grad: 3.8044  max=0.3647(module.vfe.pfn_layers.0.linear.weight)  min: -0.6775(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5568, loss_cls=0.0947, loss_bbox=0.5831, matched_ious=0.5613, loss_iou=0.0858, loss_iou_reg=0.2105, d_time=0.01(0.09), f_time=1.28(1.34), b_time=1.28(1.43)  Time cost: 00:22/41:21 [2:40:49/6:56:50]  Acc_iter 45750       Data time: 0.01(0.09)  Forward time: 1.28(1.34)  Batch time: 1.28(1.43)
2025-09-04 01:37:54,849   INFO  Train:   27/36 ( 75%) [  65/1759 (  4%)]  Loss: 3.194 (2.99)  LR: 1.318e-03  Grad: 3.8088  max=0.4193(module.vfe.pfn_layers.0.linear.weight)  min: -0.3058(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5183, loss_cls=0.0968, loss_bbox=0.5618, matched_ious=0.5583, loss_iou=0.0879, loss_iou_reg=0.2104, d_time=0.01(0.03), f_time=1.34(1.29), b_time=1.34(1.32)  Time cost: 01:26/37:09 [2:41:54/6:24:27]  Acc_iter 45800       Data time: 0.01(0.03)  Forward time: 1.34(1.29)  Batch time: 1.34(1.32)
2025-09-04 01:38:57,923   INFO  Train:   27/36 ( 75%) [ 115/1759 (  7%)]  Loss: 3.209 (2.98)  LR: 1.312e-03  Grad: 3.9918  max=0.2402(module.dense_head.prediction_head.height.1.bias)  min: -0.3408(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5305, loss_cls=0.0946, loss_bbox=0.5777, matched_ious=0.5676, loss_iou=0.0869, loss_iou_reg=0.2048, d_time=0.01(0.02), f_time=1.27(1.28), b_time=1.27(1.29)  Time cost: 02:29/35:25 [2:42:57/6:16:29]  Acc_iter 45850       Data time: 0.01(0.02)  Forward time: 1.27(1.28)  Batch time: 1.27(1.29)
2025-09-04 01:40:03,384   INFO  Train:   27/36 ( 75%) [ 165/1759 (  9%)]  Loss: 3.435 (2.98)  LR: 1.306e-03  Grad: 4.1026  max=0.2485(module.vfe.pfn_layers.0.linear.weight)  min: -0.4011(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5310, loss_cls=0.0961, loss_bbox=0.5663, matched_ious=0.5616, loss_iou=0.0873, loss_iou_reg=0.2104, d_time=0.01(0.02), f_time=1.29(1.28), b_time=1.29(1.30)  Time cost: 03:35/34:28 [2:44:02/6:16:51]  Acc_iter 45900       Data time: 0.01(0.02)  Forward time: 1.29(1.28)  Batch time: 1.29(1.30)
2025-09-04 01:41:05,924   INFO  Train:   27/36 ( 75%) [ 215/1759 ( 12%)]  Loss: 2.819 (2.96)  LR: 1.299e-03  Grad: 4.3741  max=0.4117(module.vfe.pfn_layers.0.linear.weight)  min: -0.6204(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4996, loss_cls=0.0914, loss_bbox=0.5530, matched_ious=0.5570, loss_iou=0.0891, loss_iou_reg=0.2126, d_time=0.01(0.02), f_time=1.26(1.27), b_time=1.27(1.29)  Time cost: 04:37/33:06 [2:45:05/6:12:38]  Acc_iter 45950       Data time: 0.01(0.02)  Forward time: 1.26(1.27)  Batch time: 1.27(1.29)
2025-09-04 01:42:09,139   INFO  Train:   27/36 ( 75%) [ 265/1759 ( 15%)]  Loss: 2.574 (2.96)  LR: 1.293e-03  Grad: 4.5124  max=0.4877(module.vfe.pfn_layers.0.linear.weight)  min: -0.7370(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5162, loss_cls=0.0959, loss_bbox=0.5487, matched_ious=0.5646, loss_iou=0.0883, loss_iou_reg=0.2092, d_time=0.00(0.01), f_time=1.20(1.27), b_time=1.20(1.28)  Time cost: 05:41/31:56 [2:46:08/6:10:20]  Acc_iter 46000       Data time: 0.00(0.01)  Forward time: 1.20(1.27)  Batch time: 1.20(1.28)
2025-09-04 01:43:11,963   INFO  Train:   27/36 ( 75%) [ 315/1759 ( 18%)]  Loss: 3.936 (2.94)  LR: 1.287e-03  Grad: 4.5381  max=0.4995(module.vfe.pfn_layers.0.linear.weight)  min: -0.6908(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4921, loss_cls=0.0884, loss_bbox=0.5407, matched_ious=0.5710, loss_iou=0.0859, loss_iou_reg=0.2064, d_time=0.01(0.01), f_time=1.14(1.27), b_time=1.15(1.28)  Time cost: 06:43/30:46 [2:47:11/6:08:04]  Acc_iter 46050       Data time: 0.01(0.01)  Forward time: 1.14(1.27)  Batch time: 1.15(1.28)
2025-09-04 01:44:14,218   INFO  Train:   27/36 ( 75%) [ 365/1759 ( 21%)]  Loss: 2.907 (2.95)  LR: 1.281e-03  Grad: 4.7558  max=1.0078(module.vfe.pfn_layers.0.linear.weight)  min: -0.4144(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5471, loss_cls=0.0949, loss_bbox=0.6299, matched_ious=0.5620, loss_iou=0.0884, loss_iou_reg=0.2091, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.23(1.27)  Time cost: 07:46/29:35 [2:48:13/6:05:42]  Acc_iter 46100       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.27)
2025-09-04 01:45:17,584   INFO  Train:   27/36 ( 75%) [ 415/1759 ( 24%)]  Loss: 2.664 (2.96)  LR: 1.275e-03  Grad: 4.7182  max=0.3516(module.vfe.pfn_layers.0.linear.weight)  min: -0.3311(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5300, loss_cls=0.0965, loss_bbox=0.5803, matched_ious=0.5607, loss_iou=0.0878, loss_iou_reg=0.2096, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 08:49/28:31 [2:49:16/6:04:25]  Acc_iter 46150       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-04 01:46:20,324   INFO  Train:   27/36 ( 75%) [ 465/1759 ( 26%)]  Loss: 3.039 (2.95)  LR: 1.269e-03  Grad: 4.9687  max=0.6804(module.vfe.pfn_layers.0.linear.weight)  min: -0.5786(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5077, loss_cls=0.0912, loss_bbox=0.5564, matched_ious=0.5679, loss_iou=0.0864, loss_iou_reg=0.2060, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.26(1.27)  Time cost: 09:52/27:24 [2:50:19/6:02:48]  Acc_iter 46200       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.27)
2025-09-04 01:47:24,452   INFO  Train:   27/36 ( 75%) [ 515/1759 ( 29%)]  Loss: 3.170 (2.96)  LR: 1.263e-03  Grad: 4.5550  max=0.6106(module.vfe.pfn_layers.0.linear.weight)  min: -0.5301(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5141, loss_cls=0.0908, loss_bbox=0.5875, matched_ious=0.5655, loss_iou=0.0870, loss_iou_reg=0.2061, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.21(1.27)  Time cost: 10:56/26:22 [2:51:23/6:02:03]  Acc_iter 46250       Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.27)
2025-09-04 01:48:27,623   INFO  Train:   27/36 ( 75%) [ 565/1759 ( 32%)]  Loss: 3.364 (2.95)  LR: 1.256e-03  Grad: 4.9409  max=0.5757(module.vfe.pfn_layers.0.linear.weight)  min: -1.3015(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5094, loss_cls=0.0946, loss_bbox=0.5569, matched_ious=0.5641, loss_iou=0.0879, loss_iou_reg=0.2079, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 11:59/25:18 [2:52:26/6:00:46]  Acc_iter 46300       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-04 01:49:29,873   INFO  Train:   27/36 ( 75%) [ 615/1759 ( 35%)]  Loss: 2.701 (2.96)  LR: 1.250e-03  Grad: 4.8827  max=0.5679(module.vfe.pfn_layers.0.linear.weight)  min: -0.4321(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5449, loss_cls=0.0978, loss_bbox=0.6038, matched_ious=0.5689, loss_iou=0.0862, loss_iou_reg=0.2052, d_time=0.01(0.01), f_time=1.15(1.26), b_time=1.16(1.27)  Time cost: 13:01/24:12 [2:53:29/5:59:06]  Acc_iter 46350       Data time: 0.01(0.01)  Forward time: 1.15(1.26)  Batch time: 1.16(1.27)
2025-09-04 01:50:34,452   INFO  Train:   27/36 ( 75%) [ 665/1759 ( 38%)]  Loss: 2.638 (2.96)  LR: 1.244e-03  Grad: 2.4495  max=0.4629(module.vfe.pfn_layers.0.linear.weight)  min: -0.7136(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5127, loss_cls=0.0915, loss_bbox=0.5827, matched_ious=0.5630, loss_iou=0.0856, loss_iou_reg=0.2052, d_time=0.01(0.01), f_time=1.28(1.26), b_time=1.28(1.27)  Time cost: 14:06/23:10 [2:54:33/5:58:31]  Acc_iter 46400       Data time: 0.01(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.27)
2025-09-04 01:51:36,807   INFO  Train:   27/36 ( 75%) [ 715/1759 ( 41%)]  Loss: 3.434 (2.96)  LR: 1.238e-03  Grad: 2.7422  max=0.5881(module.vfe.pfn_layers.0.linear.weight)  min: -0.5419(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5403, loss_cls=0.0948, loss_bbox=0.5838, matched_ious=0.5676, loss_iou=0.0852, loss_iou_reg=0.2049, d_time=0.01(0.01), f_time=1.23(1.26), b_time=1.24(1.27)  Time cost: 15:08/22:05 [2:55:35/5:56:59]  Acc_iter 46450       Data time: 0.01(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.27)
2025-09-04 01:52:38,941   INFO  Train:   27/36 ( 75%) [ 765/1759 ( 43%)]  Loss: 2.954 (2.97)  LR: 1.232e-03  Grad: 2.0657  max=0.3638(module.vfe.pfn_layers.0.linear.weight)  min: -0.4468(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5446, loss_cls=0.0979, loss_bbox=0.6310, matched_ious=0.5594, loss_iou=0.0874, loss_iou_reg=0.2081, d_time=0.01(0.01), f_time=1.33(1.26), b_time=1.33(1.27)  Time cost: 16:10/20:59 [2:56:38/5:55:26]  Acc_iter 46500       Data time: 0.01(0.01)  Forward time: 1.33(1.26)  Batch time: 1.33(1.27)
2025-09-04 01:53:41,146   INFO  Train:   27/36 ( 75%) [ 815/1759 ( 46%)]  Loss: 2.527 (2.97)  LR: 1.226e-03  Grad: 2.2593  max=0.4110(module.vfe.pfn_layers.0.linear.weight)  min: -0.2928(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5195, loss_cls=0.0964, loss_bbox=0.5603, matched_ious=0.5625, loss_iou=0.0873, loss_iou_reg=0.2097, d_time=0.01(0.01), f_time=1.12(1.26), b_time=1.12(1.27)  Time cost: 17:13/19:55 [2:57:40/5:53:59]  Acc_iter 46550       Data time: 0.01(0.01)  Forward time: 1.12(1.26)  Batch time: 1.12(1.27)
2025-09-04 01:54:44,077   INFO  Train:   27/36 ( 75%) [ 865/1759 ( 49%)]  Loss: 2.588 (2.97)  LR: 1.220e-03  Grad: 2.3485  max=0.3810(module.vfe.pfn_layers.0.linear.weight)  min: -0.3411(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5283, loss_cls=0.0980, loss_bbox=0.5905, matched_ious=0.5565, loss_iou=0.0864, loss_iou_reg=0.2128, d_time=0.01(0.01), f_time=1.18(1.26), b_time=1.18(1.27)  Time cost: 18:16/18:51 [2:58:43/5:52:48]  Acc_iter 46600       Data time: 0.01(0.01)  Forward time: 1.18(1.26)  Batch time: 1.18(1.27)
2025-09-04 01:55:46,140   INFO  Train:   27/36 ( 75%) [ 915/1759 ( 52%)]  Loss: 4.089 (2.97)  LR: 1.214e-03  Grad: 2.8111  max=0.7057(module.vfe.pfn_layers.0.linear.weight)  min: -0.6099(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5252, loss_cls=0.0935, loss_bbox=0.5869, matched_ious=0.5623, loss_iou=0.0855, loss_iou_reg=0.2086, d_time=0.01(0.01), f_time=1.27(1.26), b_time=1.27(1.26)  Time cost: 19:18/17:47 [2:59:45/5:51:23]  Acc_iter 46650       Data time: 0.01(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.26)
2025-09-04 01:56:50,073   INFO  Train:   27/36 ( 75%) [ 965/1759 ( 55%)]  Loss: 3.073 (2.98)  LR: 1.208e-03  Grad: 2.7517  max=0.7434(module.vfe.pfn_layers.0.linear.weight)  min: -0.3193(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5383, loss_cls=0.0958, loss_bbox=0.6107, matched_ious=0.5574, loss_iou=0.0895, loss_iou_reg=0.2124, d_time=0.01(0.01), f_time=1.20(1.26), b_time=1.20(1.27)  Time cost: 20:22/16:44 [3:00:49/5:50:32]  Acc_iter 46700       Data time: 0.01(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.27)
2025-09-04 01:57:52,954   INFO  Train:   27/36 ( 75%) [1015/1759 ( 58%)]  Loss: 2.568 (2.98)  LR: 1.202e-03  Grad: 2.8672  max=0.2177(module.vfe.pfn_layers.0.linear.weight)  min: -0.6932(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5240, loss_cls=0.0914, loss_bbox=0.5988, matched_ious=0.5590, loss_iou=0.0875, loss_iou_reg=0.2092, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.22(1.26)  Time cost: 21:24/15:40 [3:01:52/5:49:23]  Acc_iter 46750       Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.26)
2025-09-04 01:58:55,495   INFO  Train:   27/36 ( 75%) [1065/1759 ( 61%)]  Loss: 2.943 (2.98)  LR: 1.195e-03  Grad: 3.0478  max=0.6109(module.vfe.pfn_layers.0.linear.weight)  min: -0.4490(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5583, loss_cls=0.0979, loss_bbox=0.6483, matched_ious=0.5596, loss_iou=0.0906, loss_iou_reg=0.2076, d_time=0.01(0.01), f_time=1.35(1.26), b_time=1.35(1.26)  Time cost: 22:27/14:37 [3:02:54/5:48:08]  Acc_iter 46800       Data time: 0.01(0.01)  Forward time: 1.35(1.26)  Batch time: 1.35(1.26)
2025-09-04 01:59:58,140   INFO  Train:   27/36 ( 75%) [1115/1759 ( 63%)]  Loss: 3.773 (2.98)  LR: 1.189e-03  Grad: 3.1729  max=0.2697(module.vfe.pfn_layers.0.linear.weight)  min: -0.4782(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5093, loss_cls=0.0890, loss_bbox=0.5729, matched_ious=0.5600, loss_iou=0.0902, loss_iou_reg=0.2116, d_time=0.01(0.01), f_time=1.23(1.26), b_time=1.23(1.26)  Time cost: 23:30/13:33 [3:03:57/5:46:57]  Acc_iter 46850       Data time: 0.01(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.26)
2025-09-04 02:01:02,541   INFO  Train:   27/36 ( 75%) [1165/1759 ( 66%)]  Loss: 2.909 (2.98)  LR: 1.183e-03  Grad: 3.2154  max=0.6064(module.vfe.pfn_layers.0.linear.weight)  min: -0.2237(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5125, loss_cls=0.0940, loss_bbox=0.5483, matched_ious=0.5624, loss_iou=0.0878, loss_iou_reg=0.2107, d_time=0.01(0.01), f_time=1.23(1.26), b_time=1.23(1.26)  Time cost: 24:34/12:31 [3:05:01/5:46:11]  Acc_iter 46900       Data time: 0.01(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.26)
2025-09-04 02:02:05,309   INFO  Train:   27/36 ( 75%) [1215/1759 ( 69%)]  Loss: 2.325 (2.98)  LR: 1.177e-03  Grad: 3.2770  max=0.4515(module.vfe.pfn_layers.0.linear.weight)  min: -0.1334(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5189, loss_cls=0.0948, loss_bbox=0.5892, matched_ious=0.5532, loss_iou=0.0896, loss_iou_reg=0.2113, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.26(1.26)  Time cost: 25:37/11:27 [3:06:04/5:45:02]  Acc_iter 46950       Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.26)
2025-09-04 02:03:07,685   INFO  Train:   27/36 ( 75%) [1265/1759 ( 72%)]  Loss: 2.825 (2.97)  LR: 1.171e-03  Grad: 3.5188  max=0.4980(module.vfe.pfn_layers.0.linear.weight)  min: -0.4360(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5025, loss_cls=0.0886, loss_bbox=0.5321, matched_ious=0.5680, loss_iou=0.0879, loss_iou_reg=0.2063, d_time=0.01(0.01), f_time=1.32(1.26), b_time=1.33(1.26)  Time cost: 26:39/10:24 [3:07:06/5:43:48]  Acc_iter 47000       Data time: 0.01(0.01)  Forward time: 1.32(1.26)  Batch time: 1.33(1.26)
2025-09-04 02:04:10,454   INFO  Train:   27/36 ( 75%) [1315/1759 ( 75%)]  Loss: 3.567 (2.98)  LR: 1.165e-03  Grad: 3.5227  max=0.2755(module.vfe.pfn_layers.0.linear.weight)  min: -0.4830(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5428, loss_cls=0.0980, loss_bbox=0.6220, matched_ious=0.5631, loss_iou=0.0882, loss_iou_reg=0.2075, d_time=0.01(0.01), f_time=1.35(1.26), b_time=1.35(1.26)  Time cost: 27:42/09:20 [3:08:09/5:42:39]  Acc_iter 47050       Data time: 0.01(0.01)  Forward time: 1.35(1.26)  Batch time: 1.35(1.26)
2025-09-04 02:05:13,594   INFO  Train:   27/36 ( 75%) [1365/1759 ( 78%)]  Loss: 3.063 (2.98)  LR: 1.159e-03  Grad: 3.7257  max=0.6535(module.vfe.pfn_layers.0.linear.weight)  min: -0.1391(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5169, loss_cls=0.0907, loss_bbox=0.5795, matched_ious=0.5619, loss_iou=0.0867, loss_iou_reg=0.2080, d_time=0.01(0.01), f_time=1.32(1.26), b_time=1.32(1.26)  Time cost: 28:45/08:17 [3:09:12/5:41:36]  Acc_iter 47100       Data time: 0.01(0.01)  Forward time: 1.32(1.26)  Batch time: 1.32(1.26)
2025-09-04 02:06:16,996   INFO  Train:   27/36 ( 75%) [1415/1759 ( 80%)]  Loss: 2.848 (2.97)  LR: 1.153e-03  Grad: 3.7970  max=0.3213(module.vfe.pfn_layers.0.linear.weight)  min: -0.2981(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5117, loss_cls=0.0941, loss_bbox=0.5715, matched_ious=0.5616, loss_iou=0.0871, loss_iou_reg=0.2094, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.25(1.26)  Time cost: 29:49/07:14 [3:10:16/5:40:36]  Acc_iter 47150       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.26)
2025-09-04 02:07:19,498   INFO  Train:   27/36 ( 75%) [1465/1759 ( 83%)]  Loss: 3.310 (2.97)  LR: 1.147e-03  Grad: 4.1267  max=0.4561(module.vfe.pfn_layers.0.linear.weight)  min: -0.6923(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5118, loss_cls=0.0909, loss_bbox=0.5776, matched_ious=0.5625, loss_iou=0.0853, loss_iou_reg=0.2087, d_time=0.01(0.01), f_time=1.27(1.26), b_time=1.28(1.26)  Time cost: 30:51/06:11 [3:11:18/5:39:25]  Acc_iter 47200       Data time: 0.01(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.26)
2025-09-04 02:08:21,178   INFO  Train:   27/36 ( 75%) [1515/1759 ( 86%)]  Loss: 2.412 (2.97)  LR: 1.141e-03  Grad: 2.2486  max=0.6697(module.vfe.pfn_layers.0.linear.weight)  min: -0.3164(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5062, loss_cls=0.0909, loss_bbox=0.5610, matched_ious=0.5696, loss_iou=0.0871, loss_iou_reg=0.2056, d_time=0.01(0.01), f_time=1.21(1.25), b_time=1.22(1.26)  Time cost: 31:53/05:07 [3:12:20/5:38:06]  Acc_iter 47250       Data time: 0.01(0.01)  Forward time: 1.21(1.25)  Batch time: 1.22(1.26)
2025-09-04 02:09:23,743   INFO  Train:   27/36 ( 75%) [1565/1759 ( 89%)]  Loss: 3.280 (2.97)  LR: 1.135e-03  Grad: 2.5508  max=0.9097(module.vfe.pfn_layers.0.linear.weight)  min: -1.2220(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5274, loss_cls=0.0938, loss_bbox=0.5980, matched_ious=0.5627, loss_iou=0.0870, loss_iou_reg=0.2071, d_time=0.01(0.01), f_time=1.28(1.25), b_time=1.28(1.26)  Time cost: 32:55/04:04 [3:13:22/5:36:58]  Acc_iter 47300       Data time: 0.01(0.01)  Forward time: 1.28(1.25)  Batch time: 1.28(1.26)
2025-09-04 02:10:27,480   INFO  Train:   27/36 ( 75%) [1615/1759 ( 92%)]  Loss: 2.866 (2.97)  LR: 1.129e-03  Grad: 2.0935  max=0.4071(module.vfe.pfn_layers.0.linear.weight)  min: -0.3065(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4966, loss_cls=0.0874, loss_bbox=0.5709, matched_ious=0.5650, loss_iou=0.0877, loss_iou_reg=0.2038, d_time=0.01(0.01), f_time=1.23(1.25), b_time=1.23(1.26)  Time cost: 33:59/03:01 [3:14:26/5:36:01]  Acc_iter 47350       Data time: 0.01(0.01)  Forward time: 1.23(1.25)  Batch time: 1.23(1.26)
2025-09-04 02:11:31,394   INFO  Train:   27/36 ( 75%) [1665/1759 ( 95%)]  Loss: 2.874 (2.97)  LR: 1.123e-03  Grad: 2.3474  max=0.6837(module.vfe.pfn_layers.0.linear.weight)  min: -0.3152(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4845, loss_cls=0.0856, loss_bbox=0.5882, matched_ious=0.5728, loss_iou=0.0875, loss_iou_reg=0.2034, d_time=0.01(0.01), f_time=1.38(1.26), b_time=1.39(1.26)  Time cost: 35:03/01:58 [3:15:30/5:35:06]  Acc_iter 47400       Data time: 0.01(0.01)  Forward time: 1.38(1.26)  Batch time: 1.39(1.26)
2025-09-04 02:12:33,782   INFO  Train:   27/36 ( 75%) [1715/1759 ( 97%)]  Loss: 2.504 (2.97)  LR: 1.117e-03  Grad: 2.5025  max=0.5736(module.vfe.pfn_layers.0.linear.weight)  min: -0.4782(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4993, loss_cls=0.0895, loss_bbox=0.5449, matched_ious=0.5622, loss_iou=0.0893, loss_iou_reg=0.2096, d_time=0.01(0.01), f_time=1.19(1.25), b_time=1.19(1.26)  Time cost: 36:05/00:55 [3:16:32/5:33:56]  Acc_iter 47450       Data time: 0.01(0.01)  Forward time: 1.19(1.25)  Batch time: 1.19(1.26)
2025-09-04 02:13:27,241   INFO  Train:   27/36 ( 75%) [1758/1759 (100%)]  Loss: 3.651 (2.97)  LR: 1.112e-03  Grad: 10.0000  max=5.0929(module.vfe.pfn_layers.0.linear.weight)  min: -7.3090(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5041, loss_cls=0.0912, loss_bbox=0.5868, matched_ious=0.5642, loss_iou=0.0876, loss_iou_reg=0.2087, d_time=0.01(0.01), f_time=1.09(1.25), b_time=1.10(1.26)  Time cost: 36:59/00:01 [3:17:26/5:32:54]  Acc_iter 47493       Data time: 0.01(0.01)  Forward time: 1.09(1.25)  Batch time: 1.10(1.26)

                                               [Aepochs:  40%|████      | 6/15 [3:17:26<5:20:29, 2136.65s/it]epochs:  40%|████      | 6/15 [3:17:26<5:20:29, 2136.66s/it]epochs:  40%|████      | 6/15 [3:17:26<5:20:29, 2136.66s/it]epochs:  40%|████      | 6/15 [3:17:26<5:20:29, 2136.67s/it]epochs:  40%|████      | 6/15 [3:17:26<5:20:29, 2136.66s/it]epochs:  40%|████      | 6/15 [3:17:26<5:20:30, 2136.67s/it]epochs:  40%|████      | 6/15 [3:17:26<5:20:30, 2136.68s/it]epochs:  40%|████      | 6/15 [3:17:26<5:20:30, 2136.68s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 02:13:31,365   INFO  Train:   28/36 ( 78%) [   0/1759 (  0%)]  Loss: 3.441 (3.44)  LR: 1.112e-03  Grad: 2.1658  max=0.5273(module.vfe.pfn_layers.0.linear.weight)  min: -1.1898(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6813, loss_cls=0.1148, loss_bbox=0.5505, matched_ious=0.5567, loss_iou=0.0908, loss_iou_reg=0.2116, d_time=0.93(0.93), f_time=2.24(2.24), b_time=3.16(3.16)  Time cost: 00:03/1:30:42 [3:17:30/13:36:26]  Acc_iter 47494       Data time: 0.93(0.93)  Forward time: 2.24(2.24)  Batch time: 3.16(3.16)
2025-09-04 02:13:38,976   INFO  Train:   28/36 ( 78%) [   6/1759 (  0%)]  Loss: 3.164 (3.16)  LR: 1.111e-03  Grad: 1.7492  max=0.7026(module.vfe.pfn_layers.0.linear.weight)  min: -0.6368(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5635, loss_cls=0.0958, loss_bbox=0.6126, matched_ious=0.5544, loss_iou=0.0845, loss_iou_reg=0.2139, d_time=0.01(0.14), f_time=1.23(1.40), b_time=1.23(1.54)  Time cost: 00:10/44:42 [3:17:38/6:43:34]  Acc_iter 47500       Data time: 0.01(0.14)  Forward time: 1.23(1.40)  Batch time: 1.23(1.54)
2025-09-04 02:14:42,298   INFO  Train:   28/36 ( 78%) [  56/1759 (  3%)]  Loss: 3.263 (2.97)  LR: 1.105e-03  Grad: 2.0629  max=0.5917(module.vfe.pfn_layers.0.linear.weight)  min: -0.2397(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5250, loss_cls=0.0930, loss_bbox=0.5604, matched_ious=0.5708, loss_iou=0.0870, loss_iou_reg=0.2041, d_time=0.01(0.02), f_time=1.26(1.28), b_time=1.26(1.30)  Time cost: 01:14/36:51 [3:18:41/5:41:28]  Acc_iter 47550       Data time: 0.01(0.02)  Forward time: 1.26(1.28)  Batch time: 1.26(1.30)
2025-09-04 02:15:46,349   INFO  Train:   28/36 ( 78%) [ 106/1759 (  6%)]  Loss: 2.746 (2.94)  LR: 1.099e-03  Grad: 2.0739  max=0.2546(module.dense_head.prediction_head.height.1.bias)  min: -0.3542(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5149, loss_cls=0.0880, loss_bbox=0.5714, matched_ious=0.5706, loss_iou=0.0860, loss_iou_reg=0.2056, d_time=0.01(0.01), f_time=1.21(1.28), b_time=1.22(1.29)  Time cost: 02:18/35:33 [3:19:45/5:38:12]  Acc_iter 47600       Data time: 0.01(0.01)  Forward time: 1.21(1.28)  Batch time: 1.22(1.29)
2025-09-04 02:16:49,332   INFO  Train:   28/36 ( 78%) [ 156/1759 (  9%)]  Loss: 2.565 (2.95)  LR: 1.093e-03  Grad: 2.3720  max=0.6687(module.vfe.pfn_layers.0.linear.weight)  min: -0.6812(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5222, loss_cls=0.0924, loss_bbox=0.6002, matched_ious=0.5619, loss_iou=0.0867, loss_iou_reg=0.2092, d_time=0.01(0.01), f_time=1.18(1.27), b_time=1.18(1.28)  Time cost: 03:21/34:12 [3:20:48/5:34:34]  Acc_iter 47650       Data time: 0.01(0.01)  Forward time: 1.18(1.27)  Batch time: 1.18(1.28)
2025-09-04 02:17:51,794   INFO  Train:   28/36 ( 78%) [ 206/1759 ( 12%)]  Loss: 3.304 (2.95)  LR: 1.087e-03  Grad: 2.2993  max=0.4796(module.vfe.pfn_layers.0.linear.weight)  min: -0.2942(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5101, loss_cls=0.0885, loss_bbox=0.6011, matched_ious=0.5676, loss_iou=0.0847, loss_iou_reg=0.2047, d_time=0.01(0.01), f_time=1.19(1.26), b_time=1.19(1.27)  Time cost: 04:23/32:57 [3:21:50/5:31:31]  Acc_iter 47700       Data time: 0.01(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.27)
2025-09-04 02:18:54,303   INFO  Train:   28/36 ( 78%) [ 256/1759 ( 15%)]  Loss: 2.797 (2.94)  LR: 1.081e-03  Grad: 10.0000  max=7.7955(module.vfe.pfn_layers.0.linear.weight)  min: -6.0282(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5185, loss_cls=0.0924, loss_bbox=0.5562, matched_ious=0.5717, loss_iou=0.0888, loss_iou_reg=0.2039, d_time=0.01(0.01), f_time=1.31(1.26), b_time=1.32(1.27)  Time cost: 05:26/31:46 [3:22:53/5:29:18]  Acc_iter 47750       Data time: 0.01(0.01)  Forward time: 1.31(1.26)  Batch time: 1.32(1.27)
2025-09-04 02:19:57,373   INFO  Train:   28/36 ( 78%) [ 306/1759 ( 17%)]  Loss: 2.715 (2.94)  LR: 1.075e-03  Grad: 2.2494  max=0.8421(module.vfe.pfn_layers.0.linear.weight)  min: -0.3811(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5038, loss_cls=0.0899, loss_bbox=0.5600, matched_ious=0.5726, loss_iou=0.0871, loss_iou_reg=0.2041, d_time=0.01(0.01), f_time=1.36(1.26), b_time=1.36(1.27)  Time cost: 06:29/30:41 [3:23:56/5:27:57]  Acc_iter 47800       Data time: 0.01(0.01)  Forward time: 1.36(1.26)  Batch time: 1.36(1.27)
2025-09-04 02:20:59,719   INFO  Train:   28/36 ( 78%) [ 356/1759 ( 20%)]  Loss: 3.080 (2.94)  LR: 1.069e-03  Grad: 2.3716  max=0.3406(module.vfe.pfn_layers.0.linear.weight)  min: -0.4929(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5129, loss_cls=0.0929, loss_bbox=0.5872, matched_ious=0.5667, loss_iou=0.0882, loss_iou_reg=0.2053, d_time=0.01(0.01), f_time=1.39(1.26), b_time=1.39(1.26)  Time cost: 07:31/29:34 [3:24:58/5:26:09]  Acc_iter 47850       Data time: 0.01(0.01)  Forward time: 1.39(1.26)  Batch time: 1.39(1.26)
2025-09-04 02:22:05,179   INFO  Train:   28/36 ( 78%) [ 406/1759 ( 23%)]  Loss: 2.615 (2.94)  LR: 1.063e-03  Grad: 2.5071  max=0.3985(module.vfe.pfn_layers.0.linear.weight)  min: -0.6211(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5172, loss_cls=0.0908, loss_bbox=0.5875, matched_ious=0.5666, loss_iou=0.0860, loss_iou_reg=0.2083, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.22(1.27)  Time cost: 08:36/28:38 [3:26:04/5:26:30]  Acc_iter 47900       Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.27)
2025-09-04 02:23:07,915   INFO  Train:   28/36 ( 78%) [ 456/1759 ( 26%)]  Loss: 2.903 (2.94)  LR: 1.057e-03  Grad: 3.1497  max=1.2361(module.vfe.pfn_layers.0.linear.weight)  min: -0.6823(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5235, loss_cls=0.0925, loss_bbox=0.5703, matched_ious=0.5662, loss_iou=0.0863, loss_iou_reg=0.2073, d_time=0.01(0.01), f_time=1.31(1.26), b_time=1.32(1.27)  Time cost: 09:39/27:32 [3:27:07/5:25:01]  Acc_iter 47950       Data time: 0.01(0.01)  Forward time: 1.31(1.26)  Batch time: 1.32(1.27)
2025-09-04 02:24:10,716   INFO  Train:   28/36 ( 78%) [ 506/1759 ( 29%)]  Loss: 2.823 (2.94)  LR: 1.051e-03  Grad: 2.8683  max=0.6049(module.vfe.pfn_layers.0.linear.weight)  min: -0.3804(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5150, loss_cls=0.0909, loss_bbox=0.5996, matched_ious=0.5709, loss_iou=0.0878, loss_iou_reg=0.2063, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.22(1.27)  Time cost: 10:42/26:27 [3:28:09/5:23:39]  Acc_iter 48000       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.27)
2025-09-04 02:25:13,960   INFO  Train:   28/36 ( 78%) [ 556/1759 ( 32%)]  Loss: 2.852 (2.93)  LR: 1.046e-03  Grad: 2.8001  max=0.3266(module.vfe.pfn_layers.0.linear.weight)  min: -0.5226(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4971, loss_cls=0.0908, loss_bbox=0.5325, matched_ious=0.5700, loss_iou=0.0856, loss_iou_reg=0.2051, d_time=0.01(0.01), f_time=1.23(1.26), b_time=1.24(1.27)  Time cost: 11:45/25:24 [3:29:13/5:22:32]  Acc_iter 48050       Data time: 0.01(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.27)
2025-09-04 02:26:15,839   INFO  Train:   28/36 ( 78%) [ 606/1759 ( 34%)]  Loss: 2.961 (2.93)  LR: 1.040e-03  Grad: 3.0133  max=0.3752(module.vfe.pfn_layers.0.linear.weight)  min: -0.2697(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4957, loss_cls=0.0857, loss_bbox=0.5510, matched_ious=0.5693, loss_iou=0.0867, loss_iou_reg=0.2057, d_time=0.01(0.01), f_time=1.23(1.26), b_time=1.24(1.26)  Time cost: 12:47/24:18 [3:30:14/5:20:52]  Acc_iter 48100       Data time: 0.01(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.26)
2025-09-04 02:27:18,635   INFO  Train:   28/36 ( 78%) [ 656/1759 ( 37%)]  Loss: 3.479 (2.93)  LR: 1.034e-03  Grad: 3.2095  max=1.0863(module.vfe.pfn_layers.0.linear.weight)  min: -0.6078(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5064, loss_cls=0.0904, loss_bbox=0.6059, matched_ious=0.5656, loss_iou=0.0868, loss_iou_reg=0.2075, d_time=0.01(0.01), f_time=1.28(1.26), b_time=1.28(1.26)  Time cost: 13:50/23:14 [3:31:17/5:19:39]  Acc_iter 48150       Data time: 0.01(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.26)
2025-09-04 02:28:20,975   INFO  Train:   28/36 ( 78%) [ 706/1759 ( 40%)]  Loss: 2.989 (2.93)  LR: 1.028e-03  Grad: 3.1547  max=0.5894(module.vfe.pfn_layers.0.linear.weight)  min: -0.4913(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4903, loss_cls=0.0867, loss_bbox=0.5395, matched_ious=0.5695, loss_iou=0.0866, loss_iou_reg=0.2058, d_time=0.01(0.01), f_time=1.29(1.25), b_time=1.30(1.26)  Time cost: 14:52/22:09 [3:32:20/5:18:17]  Acc_iter 48200       Data time: 0.01(0.01)  Forward time: 1.29(1.25)  Batch time: 1.30(1.26)
2025-09-04 02:29:23,531   INFO  Train:   28/36 ( 78%) [ 756/1759 ( 43%)]  Loss: 3.064 (2.93)  LR: 1.022e-03  Grad: 3.4234  max=0.7315(module.vfe.pfn_layers.0.linear.weight)  min: -0.6510(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5026, loss_cls=0.0896, loss_bbox=0.5692, matched_ious=0.5675, loss_iou=0.0857, loss_iou_reg=0.2078, d_time=0.01(0.01), f_time=1.17(1.25), b_time=1.18(1.26)  Time cost: 15:55/21:05 [3:33:22/5:17:03]  Acc_iter 48250       Data time: 0.01(0.01)  Forward time: 1.17(1.25)  Batch time: 1.18(1.26)
2025-09-04 02:30:25,529   INFO  Train:   28/36 ( 78%) [ 806/1759 ( 46%)]  Loss: 2.628 (2.94)  LR: 1.016e-03  Grad: 3.5240  max=0.6215(module.vfe.pfn_layers.0.linear.weight)  min: -0.8236(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5328, loss_cls=0.0949, loss_bbox=0.6108, matched_ious=0.5570, loss_iou=0.0882, loss_iou_reg=0.2109, d_time=0.01(0.01), f_time=1.30(1.25), b_time=1.30(1.26)  Time cost: 16:57/20:01 [3:34:24/5:15:39]  Acc_iter 48300       Data time: 0.01(0.01)  Forward time: 1.30(1.25)  Batch time: 1.30(1.26)
2025-09-04 02:31:28,910   INFO  Train:   28/36 ( 78%) [ 856/1759 ( 49%)]  Loss: 3.035 (2.93)  LR: 1.010e-03  Grad: 4.0440  max=0.6292(module.vfe.pfn_layers.0.linear.weight)  min: -1.5866(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4995, loss_cls=0.0907, loss_bbox=0.5360, matched_ious=0.5702, loss_iou=0.0861, loss_iou_reg=0.2047, d_time=0.01(0.01), f_time=2.06(1.25), b_time=2.06(1.26)  Time cost: 18:00/18:58 [3:35:28/5:14:42]  Acc_iter 48350       Data time: 0.01(0.01)  Forward time: 2.06(1.25)  Batch time: 2.06(1.26)
2025-09-04 02:32:33,760   INFO  Train:   28/36 ( 78%) [ 906/1759 ( 52%)]  Loss: 3.051 (2.93)  LR: 1.004e-03  Grad: 3.5824  max=0.3285(module.vfe.pfn_layers.0.linear.weight)  min: -0.7023(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5159, loss_cls=0.0903, loss_bbox=0.5863, matched_ious=0.5670, loss_iou=0.0857, loss_iou_reg=0.2050, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.24(1.26)  Time cost: 19:05/17:57 [3:36:32/5:14:09]  Acc_iter 48400       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.26)
2025-09-04 02:33:36,853   INFO  Train:   28/36 ( 78%) [ 956/1759 ( 54%)]  Loss: 2.841 (2.93)  LR: 9.986e-04  Grad: 2.3993  max=0.2600(module.vfe.pfn_layers.0.linear.weight)  min: -0.5317(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5039, loss_cls=0.0882, loss_bbox=0.5615, matched_ious=0.5736, loss_iou=0.0857, loss_iou_reg=0.2026, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.22(1.26)  Time cost: 20:08/16:54 [3:37:36/5:13:05]  Acc_iter 48450       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.26)
2025-09-04 02:34:39,477   INFO  Train:   28/36 ( 78%) [1006/1759 ( 57%)]  Loss: 2.652 (2.93)  LR: 9.927e-04  Grad: 2.7208  max=0.6780(module.vfe.pfn_layers.0.linear.weight)  min: -0.3858(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4873, loss_cls=0.0887, loss_bbox=0.5433, matched_ious=0.5644, loss_iou=0.0894, loss_iou_reg=0.2083, d_time=0.01(0.01), f_time=1.25(1.25), b_time=1.26(1.26)  Time cost: 21:11/15:50 [3:38:38/5:11:54]  Acc_iter 48500       Data time: 0.01(0.01)  Forward time: 1.25(1.25)  Batch time: 1.26(1.26)
2025-09-04 02:35:42,523   INFO  Train:   28/36 ( 78%) [1056/1759 ( 60%)]  Loss: 2.797 (2.93)  LR: 9.869e-04  Grad: 2.7296  max=0.2958(module.vfe.pfn_layers.0.linear.weight)  min: -0.6830(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5050, loss_cls=0.0908, loss_bbox=0.5758, matched_ious=0.5685, loss_iou=0.0854, loss_iou_reg=0.2074, d_time=0.01(0.01), f_time=1.23(1.25), b_time=1.24(1.26)  Time cost: 22:14/14:47 [3:39:41/5:10:50]  Acc_iter 48550       Data time: 0.01(0.01)  Forward time: 1.23(1.25)  Batch time: 1.24(1.26)
2025-09-04 02:36:45,029   INFO  Train:   28/36 ( 78%) [1106/1759 ( 63%)]  Loss: 2.853 (2.93)  LR: 9.811e-04  Grad: 2.8060  max=0.2384(module.vfe.pfn_layers.0.linear.weight)  min: -0.3175(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5105, loss_cls=0.0892, loss_bbox=0.5580, matched_ious=0.5764, loss_iou=0.0844, loss_iou_reg=0.2011, d_time=0.01(0.01), f_time=1.23(1.25), b_time=1.23(1.26)  Time cost: 23:16/13:43 [3:40:44/5:09:39]  Acc_iter 48600       Data time: 0.01(0.01)  Forward time: 1.23(1.25)  Batch time: 1.23(1.26)
2025-09-04 02:37:47,491   INFO  Train:   28/36 ( 78%) [1156/1759 ( 66%)]  Loss: 3.218 (2.93)  LR: 9.753e-04  Grad: 2.7093  max=0.5299(module.vfe.pfn_layers.0.linear.weight)  min: -0.6526(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5115, loss_cls=0.0915, loss_bbox=0.5611, matched_ious=0.5663, loss_iou=0.0885, loss_iou_reg=0.2052, d_time=0.01(0.01), f_time=1.16(1.25), b_time=1.17(1.26)  Time cost: 24:19/12:40 [3:41:46/5:08:28]  Acc_iter 48650       Data time: 0.01(0.01)  Forward time: 1.16(1.25)  Batch time: 1.17(1.26)
2025-09-04 02:38:50,466   INFO  Train:   28/36 ( 78%) [1206/1759 ( 69%)]  Loss: 3.284 (2.93)  LR: 9.695e-04  Grad: 2.7336  max=0.5476(module.vfe.pfn_layers.0.linear.weight)  min: -0.5072(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5093, loss_cls=0.0916, loss_bbox=0.5903, matched_ious=0.5675, loss_iou=0.0880, loss_iou_reg=0.2034, d_time=0.01(0.01), f_time=1.27(1.25), b_time=1.28(1.26)  Time cost: 25:22/11:37 [3:42:49/5:07:24]  Acc_iter 48700       Data time: 0.01(0.01)  Forward time: 1.27(1.25)  Batch time: 1.28(1.26)
2025-09-04 02:39:53,504   INFO  Train:   28/36 ( 78%) [1256/1759 ( 71%)]  Loss: 3.204 (2.93)  LR: 9.637e-04  Grad: 3.0848  max=1.0843(module.vfe.pfn_layers.0.linear.weight)  min: -0.6203(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4971, loss_cls=0.0885, loss_bbox=0.5426, matched_ious=0.5688, loss_iou=0.0890, loss_iou_reg=0.2084, d_time=0.01(0.01), f_time=1.23(1.25), b_time=1.23(1.26)  Time cost: 26:25/10:34 [3:43:52/5:06:20]  Acc_iter 48750       Data time: 0.01(0.01)  Forward time: 1.23(1.25)  Batch time: 1.23(1.26)
2025-09-04 02:40:56,806   INFO  Train:   28/36 ( 78%) [1306/1759 ( 74%)]  Loss: 2.362 (2.93)  LR: 9.579e-04  Grad: 2.7929  max=0.1986(module.vfe.pfn_layers.0.linear.weight)  min: -0.2309(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4939, loss_cls=0.0890, loss_bbox=0.5662, matched_ious=0.5704, loss_iou=0.0861, loss_iou_reg=0.2046, d_time=0.01(0.01), f_time=1.23(1.25), b_time=1.24(1.26)  Time cost: 27:28/09:31 [3:44:55/5:05:20]  Acc_iter 48800       Data time: 0.01(0.01)  Forward time: 1.23(1.25)  Batch time: 1.24(1.26)
2025-09-04 02:42:00,502   INFO  Train:   28/36 ( 78%) [1356/1759 ( 77%)]  Loss: 3.322 (2.93)  LR: 9.521e-04  Grad: 3.2558  max=0.6544(module.vfe.pfn_layers.0.linear.weight)  min: -0.7060(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5108, loss_cls=0.0930, loss_bbox=0.5792, matched_ious=0.5751, loss_iou=0.0857, loss_iou_reg=0.2016, d_time=0.01(0.01), f_time=1.20(1.25), b_time=1.21(1.26)  Time cost: 28:32/08:28 [3:45:59/5:04:24]  Acc_iter 48850       Data time: 0.01(0.01)  Forward time: 1.20(1.25)  Batch time: 1.21(1.26)
2025-09-04 02:43:04,213   INFO  Train:   28/36 ( 78%) [1406/1759 ( 80%)]  Loss: 3.014 (2.93)  LR: 9.463e-04  Grad: 3.2173  max=0.5027(module.vfe.pfn_layers.0.linear.weight)  min: -0.2427(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5281, loss_cls=0.0900, loss_bbox=0.6212, matched_ious=0.5658, loss_iou=0.0876, loss_iou_reg=0.2054, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.26(1.26)  Time cost: 29:35/07:25 [3:47:03/5:03:27]  Acc_iter 48900       Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.26)
2025-09-04 02:44:07,310   INFO  Train:   28/36 ( 78%) [1456/1759 ( 83%)]  Loss: 2.914 (2.93)  LR: 9.406e-04  Grad: 3.5059  max=0.5203(module.vfe.pfn_layers.0.linear.weight)  min: -1.0473(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4953, loss_cls=0.0890, loss_bbox=0.5409, matched_ious=0.5744, loss_iou=0.0862, loss_iou_reg=0.2046, d_time=0.01(0.01), f_time=1.29(1.26), b_time=1.29(1.26)  Time cost: 30:39/06:22 [3:48:06/5:02:24]  Acc_iter 48950       Data time: 0.01(0.01)  Forward time: 1.29(1.26)  Batch time: 1.29(1.26)
2025-09-04 02:45:10,253   INFO  Train:   28/36 ( 78%) [1506/1759 ( 86%)]  Loss: 2.328 (2.93)  LR: 9.348e-04  Grad: 3.3752  max=0.1674(module.vfe.pfn_layers.0.linear.weight)  min: -0.3947(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4998, loss_cls=0.0873, loss_bbox=0.5809, matched_ious=0.5707, loss_iou=0.0865, loss_iou_reg=0.2054, d_time=0.01(0.01), f_time=1.23(1.25), b_time=1.23(1.26)  Time cost: 31:41/05:19 [3:49:09/5:01:19]  Acc_iter 49000       Data time: 0.01(0.01)  Forward time: 1.23(1.25)  Batch time: 1.23(1.26)
2025-09-04 02:46:12,873   INFO  Train:   28/36 ( 78%) [1556/1759 ( 88%)]  Loss: 2.749 (2.93)  LR: 9.291e-04  Grad: 2.0376  max=0.2724(module.vfe.pfn_layers.0.linear.weight)  min: -0.6364(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5032, loss_cls=0.0908, loss_bbox=0.5622, matched_ious=0.5720, loss_iou=0.0860, loss_iou_reg=0.2065, d_time=0.00(0.01), f_time=1.22(1.25), b_time=1.22(1.26)  Time cost: 32:44/04:16 [3:50:12/5:00:12]  Acc_iter 49050       Data time: 0.00(0.01)  Forward time: 1.22(1.25)  Batch time: 1.22(1.26)
2025-09-04 02:47:15,300   INFO  Train:   28/36 ( 78%) [1606/1759 ( 91%)]  Loss: 3.465 (2.92)  LR: 9.233e-04  Grad: 2.6118  max=0.8125(module.vfe.pfn_layers.0.linear.weight)  min: -0.6124(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4960, loss_cls=0.0879, loss_bbox=0.5121, matched_ious=0.5741, loss_iou=0.0833, loss_iou_reg=0.2040, d_time=0.01(0.01), f_time=1.23(1.25), b_time=1.24(1.26)  Time cost: 33:47/03:12 [3:51:14/4:59:03]  Acc_iter 49100       Data time: 0.01(0.01)  Forward time: 1.23(1.25)  Batch time: 1.24(1.26)
2025-09-04 02:48:18,088   INFO  Train:   28/36 ( 78%) [1656/1759 ( 94%)]  Loss: 2.767 (2.92)  LR: 9.176e-04  Grad: 2.5168  max=0.4589(module.vfe.pfn_layers.0.linear.weight)  min: -0.7570(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4878, loss_cls=0.0842, loss_bbox=0.5510, matched_ious=0.5706, loss_iou=0.0845, loss_iou_reg=0.2053, d_time=0.01(0.01), f_time=1.20(1.25), b_time=1.21(1.26)  Time cost: 34:49/02:09 [3:52:17/4:57:57]  Acc_iter 49150       Data time: 0.01(0.01)  Forward time: 1.20(1.25)  Batch time: 1.21(1.26)
2025-09-04 02:49:21,069   INFO  Train:   28/36 ( 78%) [1706/1759 ( 97%)]  Loss: 3.552 (2.92)  LR: 9.119e-04  Grad: 2.6343  max=0.7524(module.vfe.pfn_layers.0.linear.weight)  min: -0.3055(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4863, loss_cls=0.0874, loss_bbox=0.5411, matched_ious=0.5761, loss_iou=0.0849, loss_iou_reg=0.2023, d_time=0.01(0.01), f_time=1.26(1.25), b_time=1.26(1.26)  Time cost: 35:52/01:06 [3:53:20/4:56:53]  Acc_iter 49200       Data time: 0.01(0.01)  Forward time: 1.26(1.25)  Batch time: 1.26(1.26)
2025-09-04 02:50:22,469   INFO  Train:   28/36 ( 78%) [1756/1759 (100%)]  Loss: 2.274 (2.92)  LR: 9.062e-04  Grad: 2.7213  max=0.5825(module.vfe.pfn_layers.0.linear.weight)  min: -0.5529(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4812, loss_cls=0.0881, loss_bbox=0.5445, matched_ious=0.5689, loss_iou=0.0853, loss_iou_reg=0.2061, d_time=0.01(0.01), f_time=1.17(1.25), b_time=1.17(1.26)  Time cost: 36:54/00:03 [3:54:21/4:55:37]  Acc_iter 49250       Data time: 0.01(0.01)  Forward time: 1.17(1.25)  Batch time: 1.17(1.26)
2025-09-04 02:50:24,379   INFO  Train:   28/36 ( 78%) [1758/1759 (100%)]  Loss: 3.708 (2.92)  LR: 9.060e-04  Grad: 3.0159  max=0.3647(module.vfe.pfn_layers.0.linear.weight)  min: -0.5968(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5552, loss_cls=0.0970, loss_bbox=0.6363, matched_ious=0.5525, loss_iou=0.0855, loss_iou_reg=0.2170, d_time=0.00(0.01), f_time=0.69(1.25), b_time=0.70(1.26)  Time cost: 36:56/00:01 [3:54:23/4:55:30]  Acc_iter 49252       Data time: 0.00(0.01)  Forward time: 0.69(1.25)  Batch time: 0.70(1.26)

                                               [Aepochs:  47%|████▋     | 7/15 [3:54:23<4:48:23, 2162.97s/it]epochs:  47%|████▋     | 7/15 [3:54:23<4:48:23, 2162.97s/it]epochs:  47%|████▋     | 7/15 [3:54:23<4:48:23, 2162.98s/it]epochs:  47%|████▋     | 7/15 [3:54:23<4:48:23, 2162.97s/it]epochs:  47%|████▋     | 7/15 [3:54:23<4:48:23, 2162.99s/it]epochs:  47%|████▋     | 7/15 [3:54:23<4:48:23, 2162.99s/it]epochs:  47%|████▋     | 7/15 [3:54:23<4:48:23, 2163.00s/it]epochs:  47%|████▋     | 7/15 [3:54:24<4:48:23, 2162.97s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 02:50:28,431   INFO  Train:   29/36 ( 81%) [   0/1759 (  0%)]  Loss: 2.671 (2.67)  LR: 9.059e-04  Grad: 2.8968  max=0.7391(module.vfe.pfn_layers.0.linear.weight)  min: -0.5602(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5459, loss_cls=0.0831, loss_bbox=0.4458, matched_ious=0.5747, loss_iou=0.0762, loss_iou_reg=0.2059, d_time=1.05(1.05), f_time=2.01(2.01), b_time=3.05(3.05)  Time cost: 00:03/1:29:34 [3:54:27/11:56:37]  Acc_iter 49253       Data time: 1.05(1.05)  Forward time: 2.01(2.01)  Batch time: 3.05(3.05)
2025-09-04 02:51:27,790   INFO  Train:   29/36 ( 81%) [  47/1759 (  3%)]  Loss: 3.295 (2.83)  LR: 9.005e-04  Grad: 3.1091  max=0.2496(module.vfe.pfn_layers.0.linear.weight)  min: -1.3458(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4854, loss_cls=0.0856, loss_bbox=0.5438, matched_ious=0.5732, loss_iou=0.0871, loss_iou_reg=0.2057, d_time=0.01(0.03), f_time=1.26(1.27), b_time=1.26(1.30)  Time cost: 01:02/37:06 [3:55:26/5:03:56]  Acc_iter 49300       Data time: 0.01(0.03)  Forward time: 1.26(1.27)  Batch time: 1.26(1.30)
2025-09-04 02:52:33,860   INFO  Train:   29/36 ( 81%) [  97/1759 (  6%)]  Loss: 3.298 (2.84)  LR: 8.948e-04  Grad: 3.0015  max=0.4223(module.vfe.pfn_layers.0.linear.weight)  min: -0.3187(module.vfe.pfn_layers.0.norm.weight)  NaN: False  loss_hm=0.4917, loss_cls=0.0908, loss_bbox=0.5445, matched_ious=0.5699, loss_iou=0.0862, loss_iou_reg=0.2052, d_time=0.01(0.02), f_time=1.23(1.29), b_time=1.23(1.31)  Time cost: 02:08/36:18 [3:56:33/5:05:21]  Acc_iter 49350       Data time: 0.01(0.02)  Forward time: 1.23(1.29)  Batch time: 1.23(1.31)
2025-09-04 02:53:36,640   INFO  Train:   29/36 ( 81%) [ 147/1759 (  8%)]  Loss: 2.267 (2.84)  LR: 8.892e-04  Grad: 3.5507  max=0.9626(module.vfe.pfn_layers.0.linear.weight)  min: -1.0796(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4935, loss_cls=0.0878, loss_bbox=0.5354, matched_ious=0.5734, loss_iou=0.0853, loss_iou_reg=0.2055, d_time=0.01(0.01), f_time=1.19(1.28), b_time=1.20(1.29)  Time cost: 03:11/34:43 [3:57:35/4:59:55]  Acc_iter 49400       Data time: 0.01(0.01)  Forward time: 1.19(1.28)  Batch time: 1.20(1.29)
2025-09-04 02:54:41,213   INFO  Train:   29/36 ( 81%) [ 197/1759 ( 11%)]  Loss: 2.509 (2.86)  LR: 8.835e-04  Grad: 3.2604  max=0.6724(module.vfe.pfn_layers.0.linear.weight)  min: -0.9706(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4979, loss_cls=0.0902, loss_bbox=0.5820, matched_ious=0.5640, loss_iou=0.0871, loss_iou_reg=0.2091, d_time=0.01(0.01), f_time=1.22(1.28), b_time=1.22(1.29)  Time cost: 04:15/33:38 [3:58:40/4:58:48]  Acc_iter 49450       Data time: 0.01(0.01)  Forward time: 1.22(1.28)  Batch time: 1.22(1.29)
2025-09-04 02:55:44,127   INFO  Train:   29/36 ( 81%) [ 247/1759 ( 14%)]  Loss: 2.527 (2.87)  LR: 8.779e-04  Grad: 3.4965  max=0.8910(module.vfe.pfn_layers.0.linear.weight)  min: -0.9869(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5106, loss_cls=0.0932, loss_bbox=0.5738, matched_ious=0.5699, loss_iou=0.0858, loss_iou_reg=0.2042, d_time=0.01(0.01), f_time=1.31(1.28), b_time=1.31(1.29)  Time cost: 05:18/32:23 [3:59:43/4:56:09]  Acc_iter 49500       Data time: 0.01(0.01)  Forward time: 1.31(1.28)  Batch time: 1.31(1.29)
2025-09-04 02:56:47,488   INFO  Train:   29/36 ( 81%) [ 297/1759 ( 17%)]  Loss: 2.411 (2.87)  LR: 8.722e-04  Grad: 3.6954  max=1.0329(module.vfe.pfn_layers.0.linear.weight)  min: -0.8016(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4971, loss_cls=0.0839, loss_bbox=0.5789, matched_ious=0.5665, loss_iou=0.0843, loss_iou_reg=0.2056, d_time=0.01(0.01), f_time=1.27(1.27), b_time=1.28(1.28)  Time cost: 06:22/31:14 [4:00:46/4:54:23]  Acc_iter 49550       Data time: 0.01(0.01)  Forward time: 1.27(1.27)  Batch time: 1.28(1.28)
2025-09-04 02:57:50,355   INFO  Train:   29/36 ( 81%) [ 347/1759 ( 20%)]  Loss: 3.014 (2.87)  LR: 8.666e-04  Grad: 3.6405  max=0.8116(module.vfe.pfn_layers.0.linear.weight)  min: -0.4411(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4731, loss_cls=0.0848, loss_bbox=0.5595, matched_ious=0.5723, loss_iou=0.0856, loss_iou_reg=0.2047, d_time=0.01(0.01), f_time=1.26(1.27), b_time=1.26(1.28)  Time cost: 07:24/30:05 [4:01:49/4:52:29]  Acc_iter 49600       Data time: 0.01(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.28)
2025-09-04 02:58:53,580   INFO  Train:   29/36 ( 81%) [ 397/1759 ( 23%)]  Loss: 3.169 (2.87)  LR: 8.610e-04  Grad: 3.5874  max=0.6757(module.vfe.pfn_layers.0.linear.weight)  min: -0.5891(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4983, loss_cls=0.0887, loss_bbox=0.5640, matched_ious=0.5711, loss_iou=0.0861, loss_iou_reg=0.2047, d_time=0.01(0.01), f_time=1.18(1.27), b_time=1.19(1.28)  Time cost: 08:28/28:59 [4:02:52/4:51:01]  Acc_iter 49650       Data time: 0.01(0.01)  Forward time: 1.18(1.27)  Batch time: 1.19(1.28)
2025-09-04 02:59:55,668   INFO  Train:   29/36 ( 81%) [ 447/1759 ( 25%)]  Loss: 3.064 (2.86)  LR: 8.554e-04  Grad: 3.6160  max=0.1478(module.backbone_3d.cls_conv.3.bias)  min: -0.5981(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4814, loss_cls=0.0845, loss_bbox=0.5390, matched_ious=0.5754, loss_iou=0.0841, loss_iou_reg=0.2017, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 09:30/27:50 [4:03:54/4:49:04]  Acc_iter 49700       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-04 03:00:59,092   INFO  Train:   29/36 ( 81%) [ 497/1759 ( 28%)]  Loss: 2.880 (2.86)  LR: 8.498e-04  Grad: 3.8291  max=0.6274(module.vfe.pfn_layers.0.linear.weight)  min: -0.6339(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4856, loss_cls=0.0870, loss_bbox=0.5423, matched_ious=0.5767, loss_iou=0.0842, loss_iou_reg=0.2013, d_time=0.01(0.01), f_time=1.29(1.26), b_time=1.29(1.27)  Time cost: 10:33/26:45 [4:04:58/4:47:54]  Acc_iter 49750       Data time: 0.01(0.01)  Forward time: 1.29(1.26)  Batch time: 1.29(1.27)
2025-09-04 03:02:01,889   INFO  Train:   29/36 ( 81%) [ 547/1759 ( 31%)]  Loss: 2.795 (2.86)  LR: 8.442e-04  Grad: 3.8453  max=0.4858(module.vfe.pfn_layers.0.linear.weight)  min: -0.3102(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4837, loss_cls=0.0847, loss_bbox=0.5773, matched_ious=0.5740, loss_iou=0.0865, loss_iou_reg=0.2033, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.26(1.27)  Time cost: 11:36/25:40 [4:06:01/4:46:30]  Acc_iter 49800       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.27)
2025-09-04 03:03:05,881   INFO  Train:   29/36 ( 81%) [ 597/1759 ( 34%)]  Loss: 2.994 (2.87)  LR: 8.386e-04  Grad: 4.0299  max=0.8492(module.vfe.pfn_layers.0.linear.weight)  min: -0.2791(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5083, loss_cls=0.0913, loss_bbox=0.5917, matched_ious=0.5639, loss_iou=0.0888, loss_iou_reg=0.2074, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.23(1.27)  Time cost: 12:40/24:37 [4:07:05/4:45:36]  Acc_iter 49850       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.27)
2025-09-04 03:04:08,801   INFO  Train:   29/36 ( 81%) [ 647/1759 ( 37%)]  Loss: 2.871 (2.88)  LR: 8.331e-04  Grad: 4.1023  max=0.6558(module.vfe.pfn_layers.0.linear.weight)  min: -0.4957(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5126, loss_cls=0.0866, loss_bbox=0.6074, matched_ious=0.5693, loss_iou=0.0888, loss_iou_reg=0.2032, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.24(1.27)  Time cost: 13:43/23:33 [4:08:07/4:44:19]  Acc_iter 49900       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.27)
2025-09-04 03:05:11,755   INFO  Train:   29/36 ( 81%) [ 697/1759 ( 40%)]  Loss: 3.188 (2.87)  LR: 8.275e-04  Grad: 4.1538  max=0.2717(module.vfe.pfn_layers.0.linear.weight)  min: -0.7897(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4822, loss_cls=0.0876, loss_bbox=0.5464, matched_ious=0.5708, loss_iou=0.0858, loss_iou_reg=0.2042, d_time=0.01(0.01), f_time=1.35(1.26), b_time=1.36(1.27)  Time cost: 14:46/22:28 [4:09:10/4:43:04]  Acc_iter 49950       Data time: 0.01(0.01)  Forward time: 1.35(1.26)  Batch time: 1.36(1.27)
2025-09-04 03:06:14,855   INFO  Train:   29/36 ( 81%) [ 747/1759 ( 42%)]  Loss: 3.370 (2.87)  LR: 8.220e-04  Grad: 4.3541  max=0.3397(module.vfe.pfn_layers.0.norm.weight)  min: -0.3941(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4757, loss_cls=0.0843, loss_bbox=0.5657, matched_ious=0.5660, loss_iou=0.0866, loss_iou_reg=0.2050, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.23(1.27)  Time cost: 15:49/21:24 [4:10:14/4:41:54]  Acc_iter 50000       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.27)
2025-09-04 03:07:17,955   INFO  Train:   29/36 ( 81%) [ 797/1759 ( 45%)]  Loss: 3.022 (2.88)  LR: 8.165e-04  Grad: 4.7617  max=0.7899(module.vfe.pfn_layers.0.linear.weight)  min: -1.3559(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5090, loss_cls=0.0899, loss_bbox=0.5578, matched_ious=0.5692, loss_iou=0.0845, loss_iou_reg=0.2053, d_time=0.01(0.01), f_time=1.28(1.26), b_time=1.29(1.27)  Time cost: 16:52/20:20 [4:11:17/4:40:44]  Acc_iter 50050       Data time: 0.01(0.01)  Forward time: 1.28(1.26)  Batch time: 1.29(1.27)
2025-09-04 03:08:20,343   INFO  Train:   29/36 ( 81%) [ 847/1759 ( 48%)]  Loss: 2.609 (2.87)  LR: 8.110e-04  Grad: 4.4434  max=0.3361(module.vfe.pfn_layers.0.linear.weight)  min: -0.4888(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4717, loss_cls=0.0847, loss_bbox=0.5132, matched_ious=0.5731, loss_iou=0.0881, loss_iou_reg=0.2045, d_time=0.01(0.01), f_time=1.35(1.26), b_time=1.36(1.27)  Time cost: 17:54/19:16 [4:12:19/4:39:24]  Acc_iter 50100       Data time: 0.01(0.01)  Forward time: 1.35(1.26)  Batch time: 1.36(1.27)
2025-09-04 03:09:23,018   INFO  Train:   29/36 ( 81%) [ 897/1759 ( 51%)]  Loss: 2.827 (2.87)  LR: 8.055e-04  Grad: 4.6144  max=0.4358(module.vfe.pfn_layers.0.linear.weight)  min: -0.4840(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4812, loss_cls=0.0837, loss_bbox=0.5404, matched_ious=0.5820, loss_iou=0.0832, loss_iou_reg=0.1988, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.26(1.27)  Time cost: 18:57/18:12 [4:13:22/4:38:10]  Acc_iter 50150       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.27)
2025-09-04 03:10:25,464   INFO  Train:   29/36 ( 81%) [ 947/1759 ( 54%)]  Loss: 3.259 (2.86)  LR: 8.000e-04  Grad: 4.6771  max=0.2883(module.vfe.pfn_layers.0.linear.weight)  min: -0.6983(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4658, loss_cls=0.0813, loss_bbox=0.5223, matched_ious=0.5820, loss_iou=0.0857, loss_iou_reg=0.2020, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 20:00/17:07 [4:14:24/4:36:55]  Acc_iter 50200       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-04 03:11:28,446   INFO  Train:   29/36 ( 81%) [ 997/1759 ( 57%)]  Loss: 2.697 (2.86)  LR: 7.945e-04  Grad: 4.7515  max=0.3991(module.vfe.pfn_layers.0.linear.weight)  min: -0.4641(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4763, loss_cls=0.0860, loss_bbox=0.5440, matched_ious=0.5795, loss_iou=0.0839, loss_iou_reg=0.2018, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.25(1.27)  Time cost: 21:03/16:04 [4:15:27/4:35:47]  Acc_iter 50250       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.27)
2025-09-04 03:12:31,498   INFO  Train:   29/36 ( 81%) [1047/1759 ( 60%)]  Loss: 3.080 (2.86)  LR: 7.890e-04  Grad: 4.8159  max=0.6082(module.vfe.pfn_layers.0.linear.weight)  min: -0.3136(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4838, loss_cls=0.0857, loss_bbox=0.5637, matched_ious=0.5681, loss_iou=0.0878, loss_iou_reg=0.2071, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.22(1.27)  Time cost: 22:06/15:00 [4:16:30/4:34:41]  Acc_iter 50300       Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.27)
2025-09-04 03:13:36,924   INFO  Train:   29/36 ( 81%) [1097/1759 ( 62%)]  Loss: 2.310 (2.86)  LR: 7.836e-04  Grad: 4.9610  max=0.5533(module.vfe.pfn_layers.0.linear.weight)  min: -0.5786(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4776, loss_cls=0.0877, loss_bbox=0.5565, matched_ious=0.5711, loss_iou=0.0869, loss_iou_reg=0.2047, d_time=0.01(0.01), f_time=1.31(1.26), b_time=1.32(1.27)  Time cost: 23:11/13:58 [4:17:36/4:34:03]  Acc_iter 50350       Data time: 0.01(0.01)  Forward time: 1.31(1.26)  Batch time: 1.32(1.27)
2025-09-04 03:14:39,604   INFO  Train:   29/36 ( 81%) [1147/1759 ( 65%)]  Loss: 2.473 (2.86)  LR: 7.781e-04  Grad: 5.1154  max=0.6523(module.vfe.pfn_layers.0.linear.weight)  min: -0.3917(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4907, loss_cls=0.0846, loss_bbox=0.5557, matched_ious=0.5741, loss_iou=0.0861, loss_iou_reg=0.2037, d_time=0.01(0.01), f_time=1.20(1.26), b_time=1.20(1.27)  Time cost: 24:14/12:55 [4:18:38/4:32:52]  Acc_iter 50400       Data time: 0.01(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.27)
2025-09-04 03:15:42,215   INFO  Train:   29/36 ( 81%) [1197/1759 ( 68%)]  Loss: 2.981 (2.86)  LR: 7.727e-04  Grad: 5.1554  max=0.3572(module.vfe.pfn_layers.0.linear.weight)  min: -0.3845(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4959, loss_cls=0.0859, loss_bbox=0.5489, matched_ious=0.5749, loss_iou=0.0863, loss_iou_reg=0.2003, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.22(1.27)  Time cost: 25:16/11:51 [4:19:41/4:31:41]  Acc_iter 50450       Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.27)
2025-09-04 03:16:44,814   INFO  Train:   29/36 ( 81%) [1247/1759 ( 71%)]  Loss: 3.185 (2.87)  LR: 7.673e-04  Grad: 5.3035  max=0.3195(module.vfe.pfn_layers.0.linear.weight)  min: -0.4546(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5131, loss_cls=0.0893, loss_bbox=0.6178, matched_ious=0.5701, loss_iou=0.0856, loss_iou_reg=0.2039, d_time=0.01(0.01), f_time=1.34(1.26), b_time=1.35(1.27)  Time cost: 26:19/10:47 [4:20:43/4:30:31]  Acc_iter 50500       Data time: 0.01(0.01)  Forward time: 1.34(1.26)  Batch time: 1.35(1.27)
2025-09-04 03:17:47,920   INFO  Train:   29/36 ( 81%) [1297/1759 ( 74%)]  Loss: 3.500 (2.87)  LR: 7.619e-04  Grad: 5.5038  max=1.4020(module.vfe.pfn_layers.0.linear.weight)  min: -0.3304(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4819, loss_cls=0.0864, loss_bbox=0.5468, matched_ious=0.5743, loss_iou=0.0855, loss_iou_reg=0.2021, d_time=0.01(0.01), f_time=1.29(1.26), b_time=1.29(1.27)  Time cost: 27:22/09:44 [4:21:47/4:29:26]  Acc_iter 50550       Data time: 0.01(0.01)  Forward time: 1.29(1.26)  Batch time: 1.29(1.27)
2025-09-04 03:18:50,551   INFO  Train:   29/36 ( 81%) [1347/1759 ( 77%)]  Loss: 2.121 (2.87)  LR: 7.565e-04  Grad: 5.6024  max=0.7919(module.vfe.pfn_layers.0.linear.weight)  min: -0.8639(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5094, loss_cls=0.0892, loss_bbox=0.5620, matched_ious=0.5785, loss_iou=0.0843, loss_iou_reg=0.2011, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.23(1.26)  Time cost: 28:25/08:41 [4:22:49/4:28:16]  Acc_iter 50600       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.26)
2025-09-04 03:19:52,674   INFO  Train:   29/36 ( 81%) [1397/1759 ( 79%)]  Loss: 2.677 (2.87)  LR: 7.511e-04  Grad: 5.6068  max=0.4333(module.vfe.pfn_layers.0.linear.weight)  min: -0.5975(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4985, loss_cls=0.0862, loss_bbox=0.5431, matched_ious=0.5757, loss_iou=0.0862, loss_iou_reg=0.2016, d_time=0.01(0.01), f_time=1.20(1.26), b_time=1.21(1.26)  Time cost: 29:27/07:37 [4:23:51/4:27:03]  Acc_iter 50650       Data time: 0.01(0.01)  Forward time: 1.20(1.26)  Batch time: 1.21(1.26)
2025-09-04 03:20:55,104   INFO  Train:   29/36 ( 81%) [1447/1759 ( 82%)]  Loss: 3.006 (2.87)  LR: 7.457e-04  Grad: 5.6346  max=0.4359(module.vfe.pfn_layers.0.linear.weight)  min: -0.5163(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4912, loss_cls=0.0837, loss_bbox=0.5531, matched_ious=0.5726, loss_iou=0.0862, loss_iou_reg=0.2012, d_time=0.01(0.01), f_time=1.28(1.26), b_time=1.28(1.26)  Time cost: 30:29/06:34 [4:24:54/4:25:53]  Acc_iter 50700       Data time: 0.01(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.26)
2025-09-04 03:21:58,007   INFO  Train:   29/36 ( 81%) [1497/1759 ( 85%)]  Loss: 2.435 (2.87)  LR: 7.404e-04  Grad: 5.7430  max=0.4198(module.vfe.pfn_layers.0.linear.weight)  min: -0.3240(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4818, loss_cls=0.0855, loss_bbox=0.5609, matched_ious=0.5749, loss_iou=0.0848, loss_iou_reg=0.2015, d_time=0.01(0.01), f_time=1.19(1.26), b_time=1.19(1.26)  Time cost: 31:32/05:31 [4:25:57/4:24:47]  Acc_iter 50750       Data time: 0.01(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.26)
2025-09-04 03:23:00,289   INFO  Train:   29/36 ( 81%) [1547/1759 ( 88%)]  Loss: 2.835 (2.87)  LR: 7.350e-04  Grad: 5.9326  max=0.5591(module.vfe.pfn_layers.0.linear.weight)  min: -0.7858(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4934, loss_cls=0.0888, loss_bbox=0.5357, matched_ious=0.5721, loss_iou=0.0864, loss_iou_reg=0.2043, d_time=0.01(0.01), f_time=1.18(1.26), b_time=1.19(1.26)  Time cost: 32:34/04:27 [4:26:59/4:23:37]  Acc_iter 50800       Data time: 0.01(0.01)  Forward time: 1.18(1.26)  Batch time: 1.19(1.26)
2025-09-04 03:24:05,592   INFO  Train:   29/36 ( 81%) [1597/1759 ( 91%)]  Loss: 3.271 (2.86)  LR: 7.297e-04  Grad: 5.9223  max=0.4187(module.vfe.pfn_layers.0.linear.weight)  min: -0.2718(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4894, loss_cls=0.0844, loss_bbox=0.5473, matched_ious=0.5743, loss_iou=0.0858, loss_iou_reg=0.2024, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.27(1.26)  Time cost: 33:40/03:24 [4:28:04/4:22:51]  Acc_iter 50850       Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.27(1.26)
2025-09-04 03:25:08,466   INFO  Train:   29/36 ( 81%) [1647/1759 ( 94%)]  Loss: 3.306 (2.86)  LR: 7.244e-04  Grad: 6.0435  max=0.3420(module.vfe.pfn_layers.0.linear.weight)  min: -0.2823(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5018, loss_cls=0.0915, loss_bbox=0.5306, matched_ious=0.5791, loss_iou=0.0848, loss_iou_reg=0.2006, d_time=0.01(0.01), f_time=1.23(1.26), b_time=1.24(1.26)  Time cost: 34:43/02:21 [4:29:07/4:21:45]  Acc_iter 50900       Data time: 0.01(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.26)
2025-09-04 03:26:11,418   INFO  Train:   29/36 ( 81%) [1697/1759 ( 96%)]  Loss: 2.348 (2.86)  LR: 7.191e-04  Grad: 3.6524  max=2.4176(module.vfe.pfn_layers.0.linear.weight)  min: -0.8243(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4805, loss_cls=0.0854, loss_bbox=0.5315, matched_ious=0.5843, loss_iou=0.0833, loss_iou_reg=0.2003, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.25(1.26)  Time cost: 35:46/01:18 [4:30:10/4:20:40]  Acc_iter 50950       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.26)
2025-09-04 03:27:13,617   INFO  Train:   29/36 ( 81%) [1747/1759 ( 99%)]  Loss: 2.397 (2.86)  LR: 7.138e-04  Grad: 3.1207  max=0.5365(module.vfe.pfn_layers.0.linear.weight)  min: -0.8706(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4566, loss_cls=0.0823, loss_bbox=0.4985, matched_ious=0.5797, loss_iou=0.0844, loss_iou_reg=0.1999, d_time=0.01(0.01), f_time=1.15(1.26), b_time=1.16(1.26)  Time cost: 36:48/00:15 [4:31:12/4:19:30]  Acc_iter 51000       Data time: 0.01(0.01)  Forward time: 1.15(1.26)  Batch time: 1.16(1.26)
2025-09-04 03:27:26,675   INFO  Train:   29/36 ( 81%) [1758/1759 (100%)]  Loss: 3.012 (2.86)  LR: 7.126e-04  Grad: 3.3697  max=0.6457(module.backbone_3d.cls_conv.3.bias)  min: -0.4847(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5019, loss_cls=0.0826, loss_bbox=0.5699, matched_ious=0.5694, loss_iou=0.0889, loss_iou_reg=0.2075, d_time=0.01(0.01), f_time=0.74(1.26), b_time=0.75(1.26)  Time cost: 37:01/00:01 [4:31:25/4:19:10]  Acc_iter 51011       Data time: 0.01(0.01)  Forward time: 0.74(1.26)  Batch time: 0.75(1.26)

                                               [Aepochs:  53%|█████▎    | 8/15 [4:31:25<4:14:32, 2181.85s/it]epochs:  53%|█████▎    | 8/15 [4:31:26<4:14:32, 2181.86s/it]epochs:  53%|█████▎    | 8/15 [4:31:26<4:14:33, 2181.87s/it]epochs:  53%|█████▎    | 8/15 [4:31:26<4:14:33, 2181.88s/it]epochs:  53%|█████▎    | 8/15 [4:31:26<4:14:33, 2181.88s/it]epochs:  53%|█████▎    | 8/15 [4:31:26<4:14:33, 2181.87s/it]epochs:  53%|█████▎    | 8/15 [4:31:26<4:14:33, 2181.88s/it]epochs:  53%|█████▎    | 8/15 [4:31:26<4:14:32, 2181.86s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 03:27:30,835   INFO  Train:   30/36 ( 83%) [   0/1759 (  0%)]  Loss: 2.877 (2.88)  LR: 7.125e-04  Grad: 3.2050  max=0.2671(module.vfe.pfn_layers.0.linear.weight)  min: -0.6938(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6054, loss_cls=0.1121, loss_bbox=0.5597, matched_ious=0.5777, loss_iou=0.0856, loss_iou_reg=0.2203, d_time=0.98(0.98), f_time=2.18(2.18), b_time=3.16(3.16)  Time cost: 00:03/1:32:20 [4:31:29/10:46:22]  Acc_iter 51012       Data time: 0.98(0.98)  Forward time: 2.18(2.18)  Batch time: 3.16(3.16)
2025-09-04 03:28:19,117   INFO  Train:   30/36 ( 83%) [  38/1759 (  2%)]  Loss: 2.861 (2.81)  LR: 7.085e-04  Grad: 3.3247  max=1.1868(module.vfe.pfn_layers.0.linear.weight)  min: -0.5469(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4773, loss_cls=0.0854, loss_bbox=0.5226, matched_ious=0.5765, loss_iou=0.0848, loss_iou_reg=0.2040, d_time=0.01(0.03), f_time=1.21(1.29), b_time=1.21(1.32)  Time cost: 00:51/37:49 [4:32:18/4:29:48]  Acc_iter 51050       Data time: 0.01(0.03)  Forward time: 1.21(1.29)  Batch time: 1.21(1.32)
2025-09-04 03:29:21,573   INFO  Train:   30/36 ( 83%) [  88/1759 (  5%)]  Loss: 2.413 (2.83)  LR: 7.033e-04  Grad: 3.1834  max=0.5090(module.vfe.pfn_layers.0.linear.weight)  min: -0.1711(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4637, loss_cls=0.0823, loss_bbox=0.5526, matched_ious=0.5808, loss_iou=0.0845, loss_iou_reg=0.2001, d_time=0.01(0.02), f_time=1.32(1.26), b_time=1.32(1.28)  Time cost: 01:53/35:38 [4:33:20/4:20:43]  Acc_iter 51100       Data time: 0.01(0.02)  Forward time: 1.32(1.26)  Batch time: 1.32(1.28)
2025-09-04 03:30:25,042   INFO  Train:   30/36 ( 83%) [ 138/1759 (  8%)]  Loss: 2.711 (2.86)  LR: 6.980e-04  Grad: 3.3192  max=0.4161(module.vfe.pfn_layers.0.linear.weight)  min: -0.5814(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4943, loss_cls=0.0866, loss_bbox=0.5807, matched_ious=0.5675, loss_iou=0.0847, loss_iou_reg=0.2058, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.24(1.28)  Time cost: 02:57/34:28 [4:34:24/4:18:54]  Acc_iter 51150       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.28)
2025-09-04 03:31:27,969   INFO  Train:   30/36 ( 83%) [ 188/1759 ( 11%)]  Loss: 2.680 (2.86)  LR: 6.928e-04  Grad: 3.4945  max=0.4784(module.vfe.pfn_layers.0.linear.weight)  min: -0.5415(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4717, loss_cls=0.0796, loss_bbox=0.5531, matched_ious=0.5760, loss_iou=0.0850, loss_iou_reg=0.2013, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.21(1.27)  Time cost: 04:00/33:17 [4:35:27/4:16:55]  Acc_iter 51200       Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.27)
2025-09-04 03:32:31,116   INFO  Train:   30/36 ( 83%) [ 238/1759 ( 14%)]  Loss: 3.099 (2.87)  LR: 6.876e-04  Grad: 3.7720  max=0.6367(module.vfe.pfn_layers.0.linear.weight)  min: -0.5248(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4999, loss_cls=0.0888, loss_bbox=0.5636, matched_ious=0.5720, loss_iou=0.0848, loss_iou_reg=0.2052, d_time=0.01(0.01), f_time=1.29(1.26), b_time=1.30(1.27)  Time cost: 05:03/32:11 [4:36:30/4:15:30]  Acc_iter 51250       Data time: 0.01(0.01)  Forward time: 1.29(1.26)  Batch time: 1.30(1.27)
2025-09-04 03:33:34,781   INFO  Train:   30/36 ( 83%) [ 288/1759 ( 16%)]  Loss: 2.519 (2.87)  LR: 6.824e-04  Grad: 3.8506  max=0.6394(module.vfe.pfn_layers.0.linear.weight)  min: -0.5713(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4929, loss_cls=0.0900, loss_bbox=0.5461, matched_ious=0.5774, loss_iou=0.0839, loss_iou_reg=0.2006, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.27(1.27)  Time cost: 06:07/31:08 [4:37:33/4:14:34]  Acc_iter 51300       Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.27(1.27)
2025-09-04 03:34:39,725   INFO  Train:   30/36 ( 83%) [ 338/1759 ( 19%)]  Loss: 2.654 (2.87)  LR: 6.772e-04  Grad: 3.9208  max=0.7407(module.vfe.pfn_layers.0.linear.weight)  min: -0.8871(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4812, loss_cls=0.0845, loss_bbox=0.5766, matched_ious=0.5756, loss_iou=0.0862, loss_iou_reg=0.2051, d_time=0.01(0.01), f_time=1.21(1.27), b_time=1.22(1.27)  Time cost: 07:12/30:11 [4:38:38/4:14:21]  Acc_iter 51350       Data time: 0.01(0.01)  Forward time: 1.21(1.27)  Batch time: 1.22(1.27)
2025-09-04 03:35:42,045   INFO  Train:   30/36 ( 83%) [ 388/1759 ( 22%)]  Loss: 2.639 (2.86)  LR: 6.720e-04  Grad: 4.4115  max=0.8827(module.backbone_3d.cls_conv.3.bias)  min: -0.4303(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4649, loss_cls=0.0824, loss_bbox=0.5301, matched_ious=0.5755, loss_iou=0.0840, loss_iou_reg=0.2039, d_time=0.01(0.01), f_time=1.23(1.26), b_time=1.24(1.27)  Time cost: 08:14/29:02 [4:39:41/4:12:34]  Acc_iter 51400       Data time: 0.01(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.27)
2025-09-04 03:36:45,063   INFO  Train:   30/36 ( 83%) [ 438/1759 ( 25%)]  Loss: 2.933 (2.86)  LR: 6.668e-04  Grad: 4.1108  max=0.5846(module.vfe.pfn_layers.0.linear.weight)  min: -0.5152(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4703, loss_cls=0.0812, loss_bbox=0.5512, matched_ious=0.5785, loss_iou=0.0835, loss_iou_reg=0.2002, d_time=0.01(0.01), f_time=1.32(1.26), b_time=1.32(1.27)  Time cost: 09:17/27:57 [4:40:44/4:11:17]  Acc_iter 51450       Data time: 0.01(0.01)  Forward time: 1.32(1.26)  Batch time: 1.32(1.27)
2025-09-04 03:37:47,750   INFO  Train:   30/36 ( 83%) [ 488/1759 ( 28%)]  Loss: 2.923 (2.85)  LR: 6.617e-04  Grad: 4.1118  max=0.7202(module.vfe.pfn_layers.0.linear.weight)  min: -0.4385(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4768, loss_cls=0.0833, loss_bbox=0.5526, matched_ious=0.5766, loss_iou=0.0844, loss_iou_reg=0.2007, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 10:20/26:51 [4:41:46/4:09:54]  Acc_iter 51500       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-04 03:38:50,516   INFO  Train:   30/36 ( 83%) [ 538/1759 ( 31%)]  Loss: 2.647 (2.86)  LR: 6.565e-04  Grad: 4.2343  max=0.5001(module.vfe.pfn_layers.0.linear.weight)  min: -0.2306(module.backbone_3d.shared_conv.1.weight)  NaN: False  loss_hm=0.5160, loss_cls=0.0899, loss_bbox=0.5553, matched_ious=0.5697, loss_iou=0.0861, loss_iou_reg=0.2046, d_time=0.01(0.01), f_time=1.14(1.26), b_time=1.14(1.27)  Time cost: 11:22/25:46 [4:42:49/4:08:37]  Acc_iter 51550       Data time: 0.01(0.01)  Forward time: 1.14(1.26)  Batch time: 1.14(1.27)
2025-09-04 03:39:52,586   INFO  Train:   30/36 ( 83%) [ 588/1759 ( 33%)]  Loss: 2.651 (2.85)  LR: 6.514e-04  Grad: 4.3228  max=0.5485(module.vfe.pfn_layers.0.linear.weight)  min: -0.2412(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4830, loss_cls=0.0850, loss_bbox=0.5404, matched_ious=0.5801, loss_iou=0.0812, loss_iou_reg=0.2023, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.23(1.26)  Time cost: 12:24/24:40 [4:43:51/4:07:08]  Acc_iter 51600       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.26)
2025-09-04 03:40:55,177   INFO  Train:   30/36 ( 83%) [ 638/1759 ( 36%)]  Loss: 2.202 (2.84)  LR: 6.463e-04  Grad: 4.4993  max=0.4532(module.vfe.pfn_layers.0.linear.weight)  min: -0.9602(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4590, loss_cls=0.0797, loss_bbox=0.5139, matched_ious=0.5793, loss_iou=0.0856, loss_iou_reg=0.2014, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.25(1.26)  Time cost: 13:27/23:36 [4:44:54/4:05:53]  Acc_iter 51650       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.26)
2025-09-04 03:41:57,290   INFO  Train:   30/36 ( 83%) [ 688/1759 ( 39%)]  Loss: 3.681 (2.84)  LR: 6.412e-04  Grad: 4.5840  max=0.3887(module.vfe.pfn_layers.0.linear.weight)  min: -0.6262(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4758, loss_cls=0.0860, loss_bbox=0.5393, matched_ious=0.5775, loss_iou=0.0867, loss_iou_reg=0.2019, d_time=0.01(0.01), f_time=1.28(1.25), b_time=1.28(1.26)  Time cost: 14:29/22:31 [4:45:56/4:04:32]  Acc_iter 51700       Data time: 0.01(0.01)  Forward time: 1.28(1.25)  Batch time: 1.28(1.26)
2025-09-04 03:43:00,986   INFO  Train:   30/36 ( 83%) [ 738/1759 ( 42%)]  Loss: 3.431 (2.84)  LR: 6.361e-04  Grad: 4.6466  max=0.6771(module.vfe.pfn_layers.0.linear.weight)  min: -0.4765(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4612, loss_cls=0.0789, loss_bbox=0.5205, matched_ious=0.5855, loss_iou=0.0855, loss_iou_reg=0.1986, d_time=0.01(0.01), f_time=1.39(1.26), b_time=1.39(1.26)  Time cost: 15:33/21:29 [4:47:00/4:03:38]  Acc_iter 51750       Data time: 0.01(0.01)  Forward time: 1.39(1.26)  Batch time: 1.39(1.26)
2025-09-04 03:44:03,729   INFO  Train:   30/36 ( 83%) [ 788/1759 ( 45%)]  Loss: 2.960 (2.83)  LR: 6.311e-04  Grad: 4.7072  max=0.3507(module.vfe.pfn_layers.0.linear.weight)  min: -0.5534(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4745, loss_cls=0.0825, loss_bbox=0.5289, matched_ious=0.5820, loss_iou=0.0833, loss_iou_reg=0.1987, d_time=0.01(0.01), f_time=1.29(1.26), b_time=1.29(1.26)  Time cost: 16:36/20:25 [4:48:02/4:02:29]  Acc_iter 51800       Data time: 0.01(0.01)  Forward time: 1.29(1.26)  Batch time: 1.29(1.26)
2025-09-04 03:45:08,852   INFO  Train:   30/36 ( 83%) [ 838/1759 ( 48%)]  Loss: 2.277 (2.83)  LR: 6.260e-04  Grad: 3.2723  max=0.7568(module.vfe.pfn_layers.0.linear.weight)  min: -0.6493(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4614, loss_cls=0.0808, loss_bbox=0.5313, matched_ious=0.5746, loss_iou=0.0834, loss_iou_reg=0.2028, d_time=0.01(0.01), f_time=1.32(1.26), b_time=1.32(1.26)  Time cost: 17:41/19:24 [4:49:08/4:01:53]  Acc_iter 51850       Data time: 0.01(0.01)  Forward time: 1.32(1.26)  Batch time: 1.32(1.26)
2025-09-04 03:46:11,899   INFO  Train:   30/36 ( 83%) [ 888/1759 ( 50%)]  Loss: 2.099 (2.83)  LR: 6.210e-04  Grad: 3.3688  max=0.7883(module.vfe.pfn_layers.0.linear.weight)  min: -0.8402(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4515, loss_cls=0.0809, loss_bbox=0.5068, matched_ious=0.5800, loss_iou=0.0839, loss_iou_reg=0.2017, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.25(1.26)  Time cost: 18:44/18:21 [4:50:11/4:00:47]  Acc_iter 51900       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.26)
2025-09-04 03:47:14,398   INFO  Train:   30/36 ( 83%) [ 938/1759 ( 53%)]  Loss: 2.558 (2.82)  LR: 6.160e-04  Grad: 3.4927  max=0.8392(module.vfe.pfn_layers.0.linear.weight)  min: -0.5536(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4673, loss_cls=0.0818, loss_bbox=0.5317, matched_ious=0.5783, loss_iou=0.0848, loss_iou_reg=0.2034, d_time=0.01(0.01), f_time=1.20(1.26), b_time=1.20(1.26)  Time cost: 19:46/17:17 [4:51:13/3:59:35]  Acc_iter 51950       Data time: 0.01(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.26)
2025-09-04 03:48:17,749   INFO  Train:   30/36 ( 83%) [ 988/1759 ( 56%)]  Loss: 2.705 (2.82)  LR: 6.110e-04  Grad: 3.9235  max=0.9076(module.vfe.pfn_layers.0.linear.weight)  min: -0.8083(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4924, loss_cls=0.0870, loss_bbox=0.5114, matched_ious=0.5768, loss_iou=0.0855, loss_iou_reg=0.2032, d_time=0.01(0.01), f_time=1.29(1.26), b_time=1.30(1.26)  Time cost: 20:50/16:14 [4:52:16/3:58:34]  Acc_iter 52000       Data time: 0.01(0.01)  Forward time: 1.29(1.26)  Batch time: 1.30(1.26)
2025-09-04 03:49:20,536   INFO  Train:   30/36 ( 83%) [1038/1759 ( 59%)]  Loss: 2.601 (2.82)  LR: 6.060e-04  Grad: 3.6790  max=0.5523(module.vfe.pfn_layers.0.linear.weight)  min: -1.0540(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4592, loss_cls=0.0797, loss_bbox=0.5135, matched_ious=0.5817, loss_iou=0.0859, loss_iou_reg=0.2000, d_time=0.01(0.01), f_time=1.17(1.26), b_time=1.18(1.26)  Time cost: 21:52/15:11 [4:53:19/3:57:26]  Acc_iter 52050       Data time: 0.01(0.01)  Forward time: 1.17(1.26)  Batch time: 1.18(1.26)
2025-09-04 03:50:23,840   INFO  Train:   30/36 ( 83%) [1088/1759 ( 62%)]  Loss: 3.293 (2.82)  LR: 6.010e-04  Grad: 3.6970  max=0.6576(module.vfe.pfn_layers.0.linear.weight)  min: -0.1927(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4676, loss_cls=0.0828, loss_bbox=0.5530, matched_ious=0.5661, loss_iou=0.0895, loss_iou_reg=0.2096, d_time=0.01(0.01), f_time=1.14(1.26), b_time=1.15(1.26)  Time cost: 22:56/14:07 [4:54:23/3:56:24]  Acc_iter 52100       Data time: 0.01(0.01)  Forward time: 1.14(1.26)  Batch time: 1.15(1.26)
2025-09-04 03:51:26,282   INFO  Train:   30/36 ( 83%) [1138/1759 ( 65%)]  Loss: 2.332 (2.82)  LR: 5.961e-04  Grad: 3.7708  max=0.2941(module.vfe.pfn_layers.0.linear.weight)  min: -0.3630(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4773, loss_cls=0.0852, loss_bbox=0.5472, matched_ious=0.5810, loss_iou=0.0851, loss_iou_reg=0.1996, d_time=0.01(0.01), f_time=1.30(1.26), b_time=1.30(1.26)  Time cost: 23:58/13:04 [4:55:25/3:55:14]  Acc_iter 52150       Data time: 0.01(0.01)  Forward time: 1.30(1.26)  Batch time: 1.30(1.26)
2025-09-04 03:52:30,007   INFO  Train:   30/36 ( 83%) [1188/1759 ( 68%)]  Loss: 2.680 (2.82)  LR: 5.911e-04  Grad: 3.3017  max=0.4234(module.vfe.pfn_layers.0.linear.weight)  min: -0.4170(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4511, loss_cls=0.0781, loss_bbox=0.5047, matched_ious=0.5875, loss_iou=0.0840, loss_iou_reg=0.1996, d_time=0.14(0.01), f_time=2.15(1.26), b_time=2.28(1.26)  Time cost: 25:02/12:01 [4:56:29/3:54:16]  Acc_iter 52200       Data time: 0.14(0.01)  Forward time: 2.15(1.26)  Batch time: 2.28(1.26)
2025-09-04 03:53:33,173   INFO  Train:   30/36 ( 83%) [1238/1759 ( 70%)]  Loss: 2.301 (2.81)  LR: 5.862e-04  Grad: 3.5534  max=0.5870(module.vfe.pfn_layers.0.linear.weight)  min: -0.9355(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4515, loss_cls=0.0809, loss_bbox=0.5199, matched_ious=0.5832, loss_iou=0.0840, loss_iou_reg=0.1990, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.25(1.26)  Time cost: 26:05/10:58 [4:57:32/3:53:13]  Acc_iter 52250       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.26)
2025-09-04 03:54:35,908   INFO  Train:   30/36 ( 83%) [1288/1759 ( 73%)]  Loss: 2.784 (2.81)  LR: 5.813e-04  Grad: 3.5191  max=0.2467(module.vfe.pfn_layers.0.linear.weight)  min: -0.5965(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4654, loss_cls=0.0828, loss_bbox=0.5068, matched_ious=0.5808, loss_iou=0.0865, loss_iou_reg=0.1987, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.22(1.26)  Time cost: 27:08/09:54 [4:58:35/3:52:06]  Acc_iter 52300       Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.26)
2025-09-04 03:55:42,031   INFO  Train:   30/36 ( 83%) [1338/1759 ( 76%)]  Loss: 2.386 (2.81)  LR: 5.764e-04  Grad: 8.1157  max=5.1780(module.vfe.pfn_layers.0.linear.weight)  min: -1.6007(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4703, loss_cls=0.0851, loss_bbox=0.5298, matched_ious=0.5808, loss_iou=0.0827, loss_iou_reg=0.2005, d_time=0.01(0.01), f_time=1.17(1.26), b_time=1.17(1.27)  Time cost: 28:14/08:52 [4:59:41/3:51:27]  Acc_iter 52350       Data time: 0.01(0.01)  Forward time: 1.17(1.26)  Batch time: 1.17(1.27)
2025-09-04 03:56:45,279   INFO  Train:   30/36 ( 83%) [1388/1759 ( 79%)]  Loss: 2.696 (2.81)  LR: 5.715e-04  Grad: 3.8560  max=0.4848(module.vfe.pfn_layers.0.linear.weight)  min: -0.8853(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4775, loss_cls=0.0835, loss_bbox=0.5609, matched_ious=0.5782, loss_iou=0.0858, loss_iou_reg=0.2014, d_time=0.01(0.01), f_time=1.28(1.26), b_time=1.28(1.27)  Time cost: 29:17/07:49 [5:00:44/3:50:24]  Acc_iter 52400       Data time: 0.01(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.27)
2025-09-04 03:57:47,534   INFO  Train:   30/36 ( 83%) [1438/1759 ( 82%)]  Loss: 2.860 (2.81)  LR: 5.667e-04  Grad: 3.1442  max=0.4371(module.vfe.pfn_layers.0.linear.weight)  min: -1.1260(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4694, loss_cls=0.0794, loss_bbox=0.5326, matched_ious=0.5832, loss_iou=0.0825, loss_iou_reg=0.1994, d_time=0.00(0.01), f_time=1.29(1.26), b_time=1.30(1.26)  Time cost: 30:19/06:45 [5:01:46/3:49:13]  Acc_iter 52450       Data time: 0.00(0.01)  Forward time: 1.29(1.26)  Batch time: 1.30(1.26)
2025-09-04 03:58:50,397   INFO  Train:   30/36 ( 83%) [1488/1759 ( 85%)]  Loss: 2.445 (2.81)  LR: 5.618e-04  Grad: 3.2836  max=0.6333(module.vfe.pfn_layers.0.linear.weight)  min: -0.8344(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4706, loss_cls=0.0848, loss_bbox=0.5239, matched_ious=0.5811, loss_iou=0.0859, loss_iou_reg=0.2016, d_time=0.01(0.01), f_time=1.10(1.26), b_time=1.11(1.26)  Time cost: 31:22/05:42 [5:02:49/3:48:07]  Acc_iter 52500       Data time: 0.01(0.01)  Forward time: 1.10(1.26)  Batch time: 1.11(1.26)
2025-09-04 03:59:53,253   INFO  Train:   30/36 ( 83%) [1538/1759 ( 87%)]  Loss: 2.429 (2.80)  LR: 5.570e-04  Grad: 3.2506  max=0.3807(module.vfe.pfn_layers.0.linear.weight)  min: -0.5110(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4588, loss_cls=0.0847, loss_bbox=0.4932, matched_ious=0.5867, loss_iou=0.0831, loss_iou_reg=0.1990, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.18(1.26)  Time cost: 32:25/04:39 [5:03:52/3:47:01]  Acc_iter 52550       Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.18(1.26)
2025-09-04 04:00:55,513   INFO  Train:   30/36 ( 83%) [1588/1759 ( 90%)]  Loss: 2.881 (2.80)  LR: 5.522e-04  Grad: 3.5475  max=0.8649(module.vfe.pfn_layers.0.linear.weight)  min: -0.2841(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4729, loss_cls=0.0820, loss_bbox=0.5177, matched_ious=0.5849, loss_iou=0.0851, loss_iou_reg=0.1983, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.25(1.26)  Time cost: 33:27/03:36 [5:04:54/3:45:51]  Acc_iter 52600       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.26)
2025-09-04 04:01:57,963   INFO  Train:   30/36 ( 83%) [1638/1759 ( 93%)]  Loss: 2.529 (2.80)  LR: 5.474e-04  Grad: 3.6428  max=0.5671(module.vfe.pfn_layers.0.linear.weight)  min: -0.5077(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4869, loss_cls=0.0837, loss_bbox=0.5514, matched_ious=0.5819, loss_iou=0.0817, loss_iou_reg=0.1988, d_time=0.01(0.01), f_time=1.29(1.26), b_time=1.30(1.26)  Time cost: 34:30/02:32 [5:05:57/3:44:43]  Acc_iter 52650       Data time: 0.01(0.01)  Forward time: 1.29(1.26)  Batch time: 1.30(1.26)
2025-09-04 04:03:01,669   INFO  Train:   30/36 ( 83%) [1688/1759 ( 96%)]  Loss: 3.001 (2.80)  LR: 5.426e-04  Grad: 3.6514  max=0.1897(module.vfe.pfn_layers.0.linear.weight)  min: -0.6322(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4701, loss_cls=0.0831, loss_bbox=0.5176, matched_ious=0.5861, loss_iou=0.0836, loss_iou_reg=0.1964, d_time=0.01(0.01), f_time=1.16(1.26), b_time=1.17(1.26)  Time cost: 35:33/01:29 [5:07:00/3:43:44]  Acc_iter 52700       Data time: 0.01(0.01)  Forward time: 1.16(1.26)  Batch time: 1.17(1.26)
2025-09-04 04:04:03,465   INFO  Train:   30/36 ( 83%) [1738/1759 ( 99%)]  Loss: 2.741 (2.80)  LR: 5.378e-04  Grad: 3.8480  max=0.4866(module.vfe.pfn_layers.0.linear.weight)  min: -0.7563(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4933, loss_cls=0.0863, loss_bbox=0.5962, matched_ious=0.5782, loss_iou=0.0861, loss_iou_reg=0.2012, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.24(1.26)  Time cost: 36:35/00:26 [5:08:02/3:42:32]  Acc_iter 52750       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.26)
2025-09-04 04:04:27,661   INFO  Train:   30/36 ( 83%) [1758/1759 (100%)]  Loss: 3.997 (2.80)  LR: 5.359e-04  Grad: 3.9369  max=0.4948(module.vfe.pfn_layers.0.linear.weight)  min: -0.5568(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4666, loss_cls=0.0842, loss_bbox=0.5467, matched_ious=0.5750, loss_iou=0.0836, loss_iou_reg=0.2054, d_time=0.01(0.01), f_time=0.76(1.26), b_time=0.76(1.26)  Time cost: 36:59/00:01 [5:08:26/3:42:01]  Acc_iter 52770       Data time: 0.01(0.01)  Forward time: 0.76(1.26)  Batch time: 0.76(1.26)

                                               [Aepochs:  60%|██████    | 9/15 [5:08:27<3:39:24, 2194.08s/it]epochs:  60%|██████    | 9/15 [5:08:27<3:39:24, 2194.07s/it]epochs:  60%|██████    | 9/15 [5:08:27<3:39:24, 2194.09s/it]epochs:  60%|██████    | 9/15 [5:08:27<3:39:24, 2194.09s/it]epochs:  60%|██████    | 9/15 [5:08:27<3:39:24, 2194.09s/it]epochs:  60%|██████    | 9/15 [5:08:27<3:39:24, 2194.11s/it]epochs:  60%|██████    | 9/15 [5:08:27<3:39:24, 2194.10s/it]epochs:  60%|██████    | 9/15 [5:08:27<3:39:24, 2194.10s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 04:04:31,558   INFO  Train:   31/36 ( 86%) [   0/1759 (  0%)]  Loss: 2.275 (2.28)  LR: 5.358e-04  Grad: 3.8708  max=0.8974(module.vfe.pfn_layers.0.linear.weight)  min: -0.4843(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3847, loss_cls=0.0683, loss_bbox=0.3874, matched_ious=0.5791, loss_iou=0.0825, loss_iou_reg=0.2191, d_time=0.92(0.92), f_time=1.98(1.98), b_time=2.90(2.90)  Time cost: 00:02/1:22:50 [5:08:30/8:17:03]  Acc_iter 52771       Data time: 0.92(0.92)  Forward time: 1.98(1.98)  Batch time: 2.90(2.90)
2025-09-04 04:05:08,356   INFO  Train:   31/36 ( 86%) [  29/1759 (  2%)]  Loss: 2.765 (2.70)  LR: 5.331e-04  Grad: 4.2264  max=1.3700(module.vfe.pfn_layers.0.linear.weight)  min: -0.8808(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4534, loss_cls=0.0827, loss_bbox=0.5127, matched_ious=0.5897, loss_iou=0.0843, loss_iou_reg=0.1958, d_time=0.01(0.04), f_time=1.15(1.29), b_time=1.15(1.32)  Time cost: 00:39/38:05 [5:09:07/3:51:43]  Acc_iter 52800       Data time: 0.01(0.04)  Forward time: 1.15(1.29)  Batch time: 1.15(1.32)
2025-09-04 04:06:14,754   INFO  Train:   31/36 ( 86%) [  79/1759 (  4%)]  Loss: 2.542 (2.76)  LR: 5.283e-04  Grad: 4.1027  max=1.0912(module.vfe.pfn_layers.0.linear.weight)  min: -0.7176(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4625, loss_cls=0.0804, loss_bbox=0.5084, matched_ious=0.5837, loss_iou=0.0830, loss_iou_reg=0.1990, d_time=0.01(0.03), f_time=1.24(1.30), b_time=1.24(1.33)  Time cost: 01:46/37:06 [5:10:13/3:51:22]  Acc_iter 52850       Data time: 0.01(0.03)  Forward time: 1.24(1.30)  Batch time: 1.24(1.33)
2025-09-04 04:07:17,596   INFO  Train:   31/36 ( 86%) [ 129/1759 (  7%)]  Loss: 3.420 (2.78)  LR: 5.236e-04  Grad: 4.0101  max=0.3024(module.vfe.pfn_layers.0.linear.weight)  min: -0.5167(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4748, loss_cls=0.0833, loss_bbox=0.5276, matched_ious=0.5870, loss_iou=0.0847, loss_iou_reg=0.1972, d_time=0.01(0.02), f_time=1.17(1.28), b_time=1.18(1.30)  Time cost: 02:48/35:17 [5:11:16/3:45:41]  Acc_iter 52900       Data time: 0.01(0.02)  Forward time: 1.17(1.28)  Batch time: 1.18(1.30)
2025-09-04 04:08:19,577   INFO  Train:   31/36 ( 86%) [ 179/1759 ( 10%)]  Loss: 2.053 (2.77)  LR: 5.189e-04  Grad: 4.2206  max=0.2827(module.vfe.pfn_layers.0.linear.weight)  min: -0.9302(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4520, loss_cls=0.0794, loss_bbox=0.5260, matched_ious=0.5814, loss_iou=0.0850, loss_iou_reg=0.2008, d_time=0.01(0.02), f_time=1.13(1.27), b_time=1.14(1.28)  Time cost: 03:50/33:46 [5:12:18/3:41:45]  Acc_iter 52950       Data time: 0.01(0.02)  Forward time: 1.13(1.27)  Batch time: 1.14(1.28)
2025-09-04 04:09:21,970   INFO  Train:   31/36 ( 86%) [ 229/1759 ( 13%)]  Loss: 3.004 (2.76)  LR: 5.142e-04  Grad: 4.2293  max=0.2788(module.vfe.pfn_layers.0.linear.weight)  min: -0.4771(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4582, loss_cls=0.0804, loss_bbox=0.5186, matched_ious=0.5877, loss_iou=0.0817, loss_iou_reg=0.1982, d_time=0.01(0.01), f_time=1.35(1.26), b_time=1.36(1.28)  Time cost: 04:53/32:30 [5:13:21/3:39:24]  Acc_iter 53000       Data time: 0.01(0.01)  Forward time: 1.35(1.26)  Batch time: 1.36(1.28)
2025-09-04 04:10:25,113   INFO  Train:   31/36 ( 86%) [ 279/1759 ( 16%)]  Loss: 3.263 (2.77)  LR: 5.096e-04  Grad: 4.2773  max=0.1778(module.vfe.pfn_layers.0.linear.weight)  min: -0.1577(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=0.4586, loss_cls=0.0814, loss_bbox=0.5151, matched_ious=0.5797, loss_iou=0.0846, loss_iou_reg=0.1999, d_time=0.01(0.01), f_time=1.38(1.26), b_time=1.39(1.27)  Time cost: 05:56/31:23 [5:14:24/3:37:58]  Acc_iter 53050       Data time: 0.01(0.01)  Forward time: 1.38(1.26)  Batch time: 1.39(1.27)
2025-09-04 04:11:27,327   INFO  Train:   31/36 ( 86%) [ 329/1759 ( 19%)]  Loss: 2.159 (2.77)  LR: 5.049e-04  Grad: 4.6381  max=0.9427(module.vfe.pfn_layers.0.linear.weight)  min: -0.3547(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4642, loss_cls=0.0789, loss_bbox=0.5449, matched_ious=0.5806, loss_iou=0.0846, loss_iou_reg=0.1993, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 06:58/30:13 [5:15:26/3:36:10]  Acc_iter 53100       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-04 04:12:31,156   INFO  Train:   31/36 ( 86%) [ 379/1759 ( 22%)]  Loss: 2.628 (2.77)  LR: 5.003e-04  Grad: 4.5813  max=0.6213(module.vfe.pfn_layers.0.linear.weight)  min: -0.1636(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=0.4630, loss_cls=0.0839, loss_bbox=0.4938, matched_ious=0.5834, loss_iou=0.0832, loss_iou_reg=0.2004, d_time=0.01(0.01), f_time=1.19(1.26), b_time=1.20(1.27)  Time cost: 08:02/29:11 [5:16:30/3:35:17]  Acc_iter 53150       Data time: 0.01(0.01)  Forward time: 1.19(1.26)  Batch time: 1.20(1.27)
2025-09-04 04:13:33,590   INFO  Train:   31/36 ( 86%) [ 429/1759 ( 24%)]  Loss: 3.020 (2.78)  LR: 4.957e-04  Grad: 4.6473  max=0.3809(module.vfe.pfn_layers.0.linear.weight)  min: -0.5387(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5062, loss_cls=0.0867, loss_bbox=0.5460, matched_ious=0.5817, loss_iou=0.0860, loss_iou_reg=0.2004, d_time=0.01(0.01), f_time=1.18(1.26), b_time=1.18(1.27)  Time cost: 09:04/28:05 [5:17:32/3:33:49]  Acc_iter 53200       Data time: 0.01(0.01)  Forward time: 1.18(1.26)  Batch time: 1.18(1.27)
2025-09-04 04:14:36,209   INFO  Train:   31/36 ( 86%) [ 479/1759 ( 27%)]  Loss: 3.086 (2.77)  LR: 4.911e-04  Grad: 4.9423  max=0.5700(module.vfe.pfn_layers.0.linear.weight)  min: -0.5117(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4741, loss_cls=0.0814, loss_bbox=0.5434, matched_ious=0.5792, loss_iou=0.0850, loss_iou_reg=0.2005, d_time=0.01(0.01), f_time=1.31(1.25), b_time=1.31(1.27)  Time cost: 10:07/26:59 [5:18:35/3:32:30]  Acc_iter 53250       Data time: 0.01(0.01)  Forward time: 1.31(1.25)  Batch time: 1.31(1.27)
2025-09-04 04:15:38,708   INFO  Train:   31/36 ( 86%) [ 529/1759 ( 30%)]  Loss: 2.510 (2.77)  LR: 4.865e-04  Grad: 4.9076  max=0.5595(module.vfe.pfn_layers.0.linear.weight)  min: -0.7935(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4527, loss_cls=0.0772, loss_bbox=0.5192, matched_ious=0.5808, loss_iou=0.0854, loss_iou_reg=0.2020, d_time=0.01(0.01), f_time=1.21(1.25), b_time=1.22(1.26)  Time cost: 11:09/25:54 [5:19:37/3:31:12]  Acc_iter 53300       Data time: 0.01(0.01)  Forward time: 1.21(1.25)  Batch time: 1.22(1.26)
2025-09-04 04:16:43,208   INFO  Train:   31/36 ( 86%) [ 579/1759 ( 33%)]  Loss: 1.973 (2.77)  LR: 4.819e-04  Grad: 4.9185  max=0.5414(module.vfe.pfn_layers.0.linear.weight)  min: -0.5749(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4734, loss_cls=0.0832, loss_bbox=0.5201, matched_ious=0.5827, loss_iou=0.0857, loss_iou_reg=0.1988, d_time=0.01(0.01), f_time=1.27(1.26), b_time=1.27(1.27)  Time cost: 12:14/24:54 [5:20:42/3:30:31]  Acc_iter 53350       Data time: 0.01(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.27)
2025-09-04 04:17:45,720   INFO  Train:   31/36 ( 86%) [ 629/1759 ( 36%)]  Loss: 3.001 (2.77)  LR: 4.774e-04  Grad: 5.0879  max=0.5252(module.vfe.pfn_layers.0.linear.weight)  min: -0.3311(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4506, loss_cls=0.0787, loss_bbox=0.5088, matched_ious=0.5823, loss_iou=0.0839, loss_iou_reg=0.1991, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.23(1.27)  Time cost: 13:16/23:49 [5:21:44/3:29:15]  Acc_iter 53400       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.27)
2025-09-04 04:18:48,045   INFO  Train:   31/36 ( 86%) [ 679/1759 ( 39%)]  Loss: 2.774 (2.76)  LR: 4.729e-04  Grad: 5.4435  max=1.1543(module.vfe.pfn_layers.0.linear.weight)  min: -0.5239(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4321, loss_cls=0.0756, loss_bbox=0.5045, matched_ious=0.5837, loss_iou=0.0845, loss_iou_reg=0.2022, d_time=0.01(0.01), f_time=1.24(1.25), b_time=1.25(1.26)  Time cost: 14:19/22:44 [5:22:47/3:27:59]  Acc_iter 53450       Data time: 0.01(0.01)  Forward time: 1.24(1.25)  Batch time: 1.25(1.26)
2025-09-04 04:19:50,766   INFO  Train:   31/36 ( 86%) [ 729/1759 ( 41%)]  Loss: 2.927 (2.76)  LR: 4.684e-04  Grad: 5.3992  max=0.6949(module.vfe.pfn_layers.0.linear.weight)  min: -0.7741(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4582, loss_cls=0.0812, loss_bbox=0.5149, matched_ious=0.5829, loss_iou=0.0849, loss_iou_reg=0.2015, d_time=0.01(0.01), f_time=1.19(1.25), b_time=1.20(1.26)  Time cost: 15:22/21:40 [5:23:49/3:26:49]  Acc_iter 53500       Data time: 0.01(0.01)  Forward time: 1.19(1.25)  Batch time: 1.20(1.26)
2025-09-04 04:20:54,139   INFO  Train:   31/36 ( 86%) [ 779/1759 ( 44%)]  Loss: 2.952 (2.77)  LR: 4.639e-04  Grad: 5.1705  max=0.3927(module.vfe.pfn_layers.0.linear.weight)  min: -0.3242(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4710, loss_cls=0.0841, loss_bbox=0.5600, matched_ious=0.5684, loss_iou=0.0860, loss_iou_reg=0.2058, d_time=0.01(0.01), f_time=1.21(1.25), b_time=1.22(1.26)  Time cost: 16:25/20:38 [5:24:53/3:25:49]  Acc_iter 53550       Data time: 0.01(0.01)  Forward time: 1.21(1.25)  Batch time: 1.22(1.26)
2025-09-04 04:21:56,999   INFO  Train:   31/36 ( 86%) [ 829/1759 ( 47%)]  Loss: 3.285 (2.77)  LR: 4.594e-04  Grad: 5.3787  max=0.6137(module.vfe.pfn_layers.0.linear.weight)  min: -0.3868(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4524, loss_cls=0.0801, loss_bbox=0.5311, matched_ious=0.5808, loss_iou=0.0842, loss_iou_reg=0.1981, d_time=0.01(0.01), f_time=1.24(1.25), b_time=1.25(1.26)  Time cost: 17:28/19:34 [5:25:56/3:24:42]  Acc_iter 53600       Data time: 0.01(0.01)  Forward time: 1.24(1.25)  Batch time: 1.25(1.26)
2025-09-04 04:22:58,935   INFO  Train:   31/36 ( 86%) [ 879/1759 ( 50%)]  Loss: 2.822 (2.76)  LR: 4.549e-04  Grad: 5.5514  max=0.6634(module.vfe.pfn_layers.0.linear.weight)  min: -0.7695(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4546, loss_cls=0.0782, loss_bbox=0.5229, matched_ious=0.5870, loss_iou=0.0831, loss_iou_reg=0.1979, d_time=0.01(0.01), f_time=1.21(1.25), b_time=1.22(1.26)  Time cost: 18:30/18:30 [5:26:58/3:23:25]  Acc_iter 53650       Data time: 0.01(0.01)  Forward time: 1.21(1.25)  Batch time: 1.22(1.26)
2025-09-04 04:24:01,550   INFO  Train:   31/36 ( 86%) [ 929/1759 ( 53%)]  Loss: 2.526 (2.77)  LR: 4.505e-04  Grad: 5.6015  max=0.4402(module.vfe.pfn_layers.0.linear.weight)  min: -0.8241(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4692, loss_cls=0.0806, loss_bbox=0.5508, matched_ious=0.5828, loss_iou=0.0836, loss_iou_reg=0.2005, d_time=0.01(0.01), f_time=1.25(1.25), b_time=1.26(1.26)  Time cost: 19:32/17:26 [5:28:00/3:22:18]  Acc_iter 53700       Data time: 0.01(0.01)  Forward time: 1.25(1.25)  Batch time: 1.26(1.26)
2025-09-04 04:25:03,380   INFO  Train:   31/36 ( 86%) [ 979/1759 ( 56%)]  Loss: 2.719 (2.77)  LR: 4.461e-04  Grad: 5.6373  max=0.2862(module.vfe.pfn_layers.0.linear.weight)  min: -0.9722(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4818, loss_cls=0.0847, loss_bbox=0.5382, matched_ious=0.5907, loss_iou=0.0815, loss_iou_reg=0.1959, d_time=0.01(0.01), f_time=1.31(1.25), b_time=1.32(1.26)  Time cost: 20:34/16:22 [5:29:02/3:21:03]  Acc_iter 53750       Data time: 0.01(0.01)  Forward time: 1.31(1.25)  Batch time: 1.32(1.26)
2025-09-04 04:26:06,159   INFO  Train:   31/36 ( 86%) [1029/1759 ( 58%)]  Loss: 2.563 (2.77)  LR: 4.417e-04  Grad: 5.6202  max=0.4650(module.vfe.pfn_layers.0.linear.weight)  min: -0.2364(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4614, loss_cls=0.0799, loss_bbox=0.5284, matched_ious=0.5850, loss_iou=0.0838, loss_iou_reg=0.1996, d_time=0.01(0.01), f_time=1.25(1.25), b_time=1.25(1.26)  Time cost: 21:37/15:19 [5:30:05/3:19:58]  Acc_iter 53800       Data time: 0.01(0.01)  Forward time: 1.25(1.25)  Batch time: 1.25(1.26)
2025-09-04 04:27:11,037   INFO  Train:   31/36 ( 86%) [1079/1759 ( 61%)]  Loss: 2.325 (2.76)  LR: 4.373e-04  Grad: 5.8098  max=0.6948(module.vfe.pfn_layers.0.linear.weight)  min: -0.6124(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4393, loss_cls=0.0802, loss_bbox=0.4846, matched_ious=0.5790, loss_iou=0.0868, loss_iou_reg=0.2026, d_time=0.01(0.01), f_time=1.18(1.25), b_time=1.18(1.26)  Time cost: 22:42/14:17 [5:31:10/3:19:11]  Acc_iter 53850       Data time: 0.01(0.01)  Forward time: 1.18(1.25)  Batch time: 1.18(1.26)
2025-09-04 04:28:13,062   INFO  Train:   31/36 ( 86%) [1129/1759 ( 64%)]  Loss: 2.469 (2.76)  LR: 4.329e-04  Grad: 5.7251  max=0.4808(module.vfe.pfn_layers.0.linear.weight)  min: -0.3659(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4553, loss_cls=0.0801, loss_bbox=0.5013, matched_ious=0.5853, loss_iou=0.0830, loss_iou_reg=0.1985, d_time=0.01(0.01), f_time=1.21(1.25), b_time=1.22(1.26)  Time cost: 23:44/13:14 [5:32:12/3:17:59]  Acc_iter 53900       Data time: 0.01(0.01)  Forward time: 1.21(1.25)  Batch time: 1.22(1.26)
2025-09-04 04:29:15,735   INFO  Train:   31/36 ( 86%) [1179/1759 ( 67%)]  Loss: 2.449 (2.76)  LR: 4.286e-04  Grad: 5.9045  max=0.5675(module.vfe.pfn_layers.0.linear.weight)  min: -0.5245(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4531, loss_cls=0.0793, loss_bbox=0.5243, matched_ious=0.5952, loss_iou=0.0825, loss_iou_reg=0.1934, d_time=0.01(0.01), f_time=1.27(1.25), b_time=1.28(1.26)  Time cost: 24:47/12:10 [5:33:14/3:16:54]  Acc_iter 53950       Data time: 0.01(0.01)  Forward time: 1.27(1.25)  Batch time: 1.28(1.26)
2025-09-04 04:30:18,452   INFO  Train:   31/36 ( 86%) [1229/1759 ( 70%)]  Loss: 3.285 (2.75)  LR: 4.242e-04  Grad: 5.9949  max=0.3684(module.vfe.pfn_layers.0.linear.weight)  min: -0.4525(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4189, loss_cls=0.0757, loss_bbox=0.4728, matched_ious=0.5846, loss_iou=0.0833, loss_iou_reg=0.2002, d_time=0.01(0.01), f_time=1.32(1.25), b_time=1.33(1.26)  Time cost: 25:49/11:07 [5:34:17/3:15:48]  Acc_iter 54000       Data time: 0.01(0.01)  Forward time: 1.32(1.25)  Batch time: 1.33(1.26)
2025-09-04 04:31:21,581   INFO  Train:   31/36 ( 86%) [1279/1759 ( 73%)]  Loss: 3.218 (2.75)  LR: 4.199e-04  Grad: 6.0402  max=0.4703(module.vfe.pfn_layers.0.linear.weight)  min: -0.2456(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=0.4651, loss_cls=0.0833, loss_bbox=0.5417, matched_ious=0.5769, loss_iou=0.0847, loss_iou_reg=0.2026, d_time=0.01(0.01), f_time=1.27(1.25), b_time=1.28(1.26)  Time cost: 26:52/10:04 [5:35:20/3:14:46]  Acc_iter 54050       Data time: 0.01(0.01)  Forward time: 1.27(1.25)  Batch time: 1.28(1.26)
2025-09-04 04:32:24,017   INFO  Train:   31/36 ( 86%) [1329/1759 ( 76%)]  Loss: 2.966 (2.75)  LR: 4.156e-04  Grad: 6.1509  max=0.4524(module.vfe.pfn_layers.0.linear.weight)  min: -0.6565(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4464, loss_cls=0.0764, loss_bbox=0.4964, matched_ious=0.5951, loss_iou=0.0836, loss_iou_reg=0.1945, d_time=0.01(0.01), f_time=1.23(1.25), b_time=1.23(1.26)  Time cost: 27:55/09:01 [5:36:23/3:13:39]  Acc_iter 54100       Data time: 0.01(0.01)  Forward time: 1.23(1.25)  Batch time: 1.23(1.26)
2025-09-04 04:33:26,898   INFO  Train:   31/36 ( 86%) [1379/1759 ( 78%)]  Loss: 2.313 (2.75)  LR: 4.114e-04  Grad: 6.3298  max=0.4642(module.vfe.pfn_layers.0.linear.weight)  min: -0.5627(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4468, loss_cls=0.0760, loss_bbox=0.5245, matched_ious=0.5797, loss_iou=0.0880, loss_iou_reg=0.2024, d_time=0.01(0.01), f_time=1.30(1.25), b_time=1.31(1.26)  Time cost: 28:58/07:58 [5:37:26/3:12:36]  Acc_iter 54150       Data time: 0.01(0.01)  Forward time: 1.30(1.25)  Batch time: 1.31(1.26)
2025-09-04 04:34:29,671   INFO  Train:   31/36 ( 86%) [1429/1759 ( 81%)]  Loss: 3.190 (2.75)  LR: 4.071e-04  Grad: 6.2670  max=0.4472(module.vfe.pfn_layers.0.linear.weight)  min: -0.5878(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4588, loss_cls=0.0809, loss_bbox=0.5160, matched_ious=0.5824, loss_iou=0.0861, loss_iou_reg=0.2002, d_time=0.01(0.01), f_time=1.29(1.25), b_time=1.29(1.26)  Time cost: 30:00/06:55 [5:38:28/3:11:32]  Acc_iter 54200       Data time: 0.01(0.01)  Forward time: 1.29(1.25)  Batch time: 1.29(1.26)
2025-09-04 04:35:32,367   INFO  Train:   31/36 ( 86%) [1479/1759 ( 84%)]  Loss: 2.336 (2.75)  LR: 4.029e-04  Grad: 6.4094  max=0.3639(module.vfe.pfn_layers.0.linear.weight)  min: -0.7984(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4448, loss_cls=0.0796, loss_bbox=0.5094, matched_ious=0.5908, loss_iou=0.0830, loss_iou_reg=0.1956, d_time=0.01(0.01), f_time=1.14(1.25), b_time=1.14(1.26)  Time cost: 31:03/05:52 [5:39:31/3:10:27]  Acc_iter 54250       Data time: 0.01(0.01)  Forward time: 1.14(1.25)  Batch time: 1.14(1.26)
2025-09-04 04:36:34,771   INFO  Train:   31/36 ( 86%) [1529/1759 ( 87%)]  Loss: 2.609 (2.75)  LR: 3.986e-04  Grad: 6.5178  max=1.0890(module.vfe.pfn_layers.0.linear.weight)  min: -0.5557(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4620, loss_cls=0.0740, loss_bbox=0.5781, matched_ious=0.5883, loss_iou=0.0842, loss_iou_reg=0.1957, d_time=0.01(0.01), f_time=1.30(1.25), b_time=1.31(1.26)  Time cost: 32:06/04:49 [5:40:33/3:09:21]  Acc_iter 54300       Data time: 0.01(0.01)  Forward time: 1.30(1.25)  Batch time: 1.31(1.26)
2025-09-04 04:37:39,901   INFO  Train:   31/36 ( 86%) [1579/1759 ( 90%)]  Loss: 3.159 (2.75)  LR: 3.944e-04  Grad: 6.5292  max=0.3491(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.5180(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4448, loss_cls=0.0766, loss_bbox=0.4957, matched_ious=0.5918, loss_iou=0.0833, loss_iou_reg=0.1968, d_time=0.01(0.01), f_time=1.28(1.25), b_time=1.29(1.26)  Time cost: 33:11/03:46 [5:41:39/3:08:30]  Acc_iter 54350       Data time: 0.01(0.01)  Forward time: 1.28(1.25)  Batch time: 1.29(1.26)
2025-09-04 04:38:42,959   INFO  Train:   31/36 ( 86%) [1629/1759 ( 93%)]  Loss: 2.042 (2.75)  LR: 3.903e-04  Grad: 6.6774  max=0.6100(module.vfe.pfn_layers.0.linear.weight)  min: -0.7658(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4441, loss_cls=0.0781, loss_bbox=0.5118, matched_ious=0.5858, loss_iou=0.0827, loss_iou_reg=0.1979, d_time=0.01(0.01), f_time=1.28(1.25), b_time=1.29(1.26)  Time cost: 34:14/02:43 [5:42:42/3:07:27]  Acc_iter 54400       Data time: 0.01(0.01)  Forward time: 1.28(1.25)  Batch time: 1.29(1.26)
2025-09-04 04:39:46,440   INFO  Train:   31/36 ( 86%) [1679/1759 ( 95%)]  Loss: 2.239 (2.74)  LR: 3.861e-04  Grad: 6.1978  max=0.4408(module.vfe.pfn_layers.0.linear.weight)  min: -0.4134(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4488, loss_cls=0.0814, loss_bbox=0.4877, matched_ious=0.5876, loss_iou=0.0839, loss_iou_reg=0.1974, d_time=0.01(0.01), f_time=1.36(1.25), b_time=1.37(1.26)  Time cost: 35:17/01:40 [5:43:45/3:06:27]  Acc_iter 54450       Data time: 0.01(0.01)  Forward time: 1.36(1.25)  Batch time: 1.37(1.26)
2025-09-04 04:40:50,241   INFO  Train:   31/36 ( 86%) [1729/1759 ( 98%)]  Loss: 3.093 (2.75)  LR: 3.820e-04  Grad: 6.5050  max=0.5956(module.vfe.pfn_layers.0.linear.weight)  min: -1.2251(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4438, loss_cls=0.0789, loss_bbox=0.5180, matched_ious=0.5886, loss_iou=0.0828, loss_iou_reg=0.1977, d_time=0.01(0.01), f_time=1.26(1.25), b_time=1.27(1.26)  Time cost: 36:21/00:37 [5:44:49/3:05:28]  Acc_iter 54500       Data time: 0.01(0.01)  Forward time: 1.26(1.25)  Batch time: 1.27(1.26)
2025-09-04 04:41:26,649   INFO  Train:   31/36 ( 86%) [1758/1759 (100%)]  Loss: 2.923 (2.74)  LR: 3.796e-04  Grad: 6.7852  max=0.6002(module.vfe.pfn_layers.0.linear.weight)  min: -1.9039(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4598, loss_cls=0.0824, loss_bbox=0.4978, matched_ious=0.5873, loss_iou=0.0825, loss_iou_reg=0.1980, d_time=0.00(0.01), f_time=1.10(1.25), b_time=1.10(1.26)  Time cost: 36:57/00:01 [5:45:25/3:04:50]  Acc_iter 54529       Data time: 0.00(0.01)  Forward time: 1.10(1.25)  Batch time: 1.10(1.26)

                                               [Aepochs:  67%|██████▋   | 10/15 [5:45:25<3:03:28, 2201.77s/it]epochs:  67%|██████▋   | 10/15 [5:45:26<3:03:28, 2201.78s/it]epochs:  67%|██████▋   | 10/15 [5:45:26<3:03:28, 2201.78s/it]epochs:  67%|██████▋   | 10/15 [5:45:26<3:03:28, 2201.78s/it]epochs:  67%|██████▋   | 10/15 [5:45:26<3:03:28, 2201.78s/it]epochs:  67%|██████▋   | 10/15 [5:45:26<3:03:28, 2201.79s/it]epochs:  67%|██████▋   | 10/15 [5:45:26<3:03:28, 2201.78s/it]epochs:  67%|██████▋   | 10/15 [5:45:26<3:03:28, 2201.78s/it]2025-09-04 04:41:27,068   INFO  Disable augmentations: ['gt_sampling']

train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 04:41:30,739   INFO  Train:   32/36 ( 89%) [   0/1759 (  0%)]  Loss: 6.370 (6.37)  LR: 3.795e-04  Grad: 10.0000  max=3.9261(module.backbone_3d.cls_conv.3.bias)  min: -1.9854(module.backbone_3d.cls_conv.1.bias)  NaN: False  loss_hm=0.4066, loss_cls=0.1009, loss_bbox=0.2627, matched_ious=0.6348, loss_iou=0.0710, loss_iou_reg=0.1633, d_time=0.68(0.68), f_time=1.83(1.83), b_time=2.51(2.51)  Time cost: 00:02/1:13:10 [5:45:29/6:05:50]  Acc_iter 54530       Data time: 0.68(0.68)  Forward time: 1.83(1.83)  Batch time: 2.51(2.51)
2025-09-04 04:41:54,628   INFO  Train:   32/36 ( 89%) [  20/1759 (  1%)]  Loss: 2.761 (20.8)  LR: 3.778e-04  Grad: 10.0000  max=4.0472(module.backbone_3d.cls_conv.3.bias)  min: -2.2117(module.backbone_3d.cls_conv.1.bias)  NaN: False  loss_hm=0.5997, loss_cls=0.1444, loss_bbox=0.4876, matched_ious=0.5943, loss_iou=0.0908, loss_iou_reg=0.2028, d_time=0.01(0.04), f_time=1.22(1.22), b_time=1.23(1.26)  Time cost: 00:26/36:24 [5:45:53/3:03:45]  Acc_iter 54550       Data time: 0.01(0.04)  Forward time: 1.22(1.22)  Batch time: 1.23(1.26)
2025-09-04 04:42:53,515   INFO  Train:   32/36 ( 89%) [  70/1759 (  4%)]  Loss: 4.494 (9.74)  LR: 3.737e-04  Grad: 2.9819  max=0.3636(module.vfe.pfn_layers.0.linear.weight)  min: -0.4653(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4943, loss_cls=0.0963, loss_bbox=0.4389, matched_ious=0.5851, loss_iou=0.0880, loss_iou_reg=0.2061, d_time=0.01(0.02), f_time=1.21(1.19), b_time=1.21(1.20)  Time cost: 01:25/33:48 [5:46:52/2:54:38]  Acc_iter 54600       Data time: 0.01(0.02)  Forward time: 1.21(1.19)  Batch time: 1.21(1.20)
2025-09-04 04:43:51,815   INFO  Train:   32/36 ( 89%) [ 120/1759 (  7%)]  Loss: 3.149 (7.29)  LR: 3.696e-04  Grad: 3.7345  max=0.8389(module.vfe.pfn_layers.0.linear.weight)  min: -0.7446(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4667, loss_cls=0.0959, loss_bbox=0.4275, matched_ious=0.6023, loss_iou=0.0861, loss_iou_reg=0.2000, d_time=0.00(0.01), f_time=1.22(1.18), b_time=1.23(1.19)  Time cost: 02:23/32:24 [5:47:50/2:51:33]  Acc_iter 54650       Data time: 0.00(0.01)  Forward time: 1.22(1.18)  Batch time: 1.23(1.19)
2025-09-04 04:44:51,041   INFO  Train:   32/36 ( 89%) [ 170/1759 ( 10%)]  Loss: 4.736 (6.34)  LR: 3.656e-04  Grad: 3.1261  max=0.6502(module.vfe.pfn_layers.0.linear.weight)  min: -0.4073(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.5362, loss_cls=0.0991, loss_bbox=0.4865, matched_ious=0.5969, loss_iou=0.0869, loss_iou_reg=0.2003, d_time=0.01(0.01), f_time=1.18(1.18), b_time=1.19(1.19)  Time cost: 03:22/31:24 [5:48:50/2:50:28]  Acc_iter 54700       Data time: 0.01(0.01)  Forward time: 1.18(1.18)  Batch time: 1.19(1.19)
2025-09-04 04:45:49,893   INFO  Train:   32/36 ( 89%) [ 220/1759 ( 13%)]  Loss: 4.603 (5.77)  LR: 3.615e-04  Grad: 5.9553  max=3.3584(module.vfe.pfn_layers.0.linear.weight)  min: -2.1010(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5211, loss_cls=0.0970, loss_bbox=0.4857, matched_ious=0.5809, loss_iou=0.0927, loss_iou_reg=0.2123, d_time=0.01(0.01), f_time=1.15(1.18), b_time=1.16(1.18)  Time cost: 04:21/30:22 [5:49:49/2:49:12]  Acc_iter 54750       Data time: 0.01(0.01)  Forward time: 1.15(1.18)  Batch time: 1.16(1.18)
2025-09-04 04:46:49,026   INFO  Train:   32/36 ( 89%) [ 270/1759 ( 15%)]  Loss: 2.777 (5.38)  LR: 3.575e-04  Grad: 3.7665  max=0.3633(module.backbone_3d.cls_conv.3.bias)  min: -0.3350(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4681, loss_cls=0.0935, loss_bbox=0.4337, matched_ious=0.5958, loss_iou=0.0897, loss_iou_reg=0.2039, d_time=0.01(0.01), f_time=1.22(1.18), b_time=1.22(1.18)  Time cost: 05:20/29:22 [5:50:48/2:48:11]  Acc_iter 54800       Data time: 0.01(0.01)  Forward time: 1.22(1.18)  Batch time: 1.22(1.18)
2025-09-04 04:47:48,608   INFO  Train:   32/36 ( 89%) [ 320/1759 ( 18%)]  Loss: 2.496 (5.12)  LR: 3.535e-04  Grad: 4.7674  max=0.6112(module.vfe.pfn_layers.0.linear.weight)  min: -1.0940(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5081, loss_cls=0.1014, loss_bbox=0.4530, matched_ious=0.5731, loss_iou=0.0874, loss_iou_reg=0.2087, d_time=0.01(0.01), f_time=1.28(1.18), b_time=1.29(1.18)  Time cost: 06:20/28:25 [5:51:47/2:47:22]  Acc_iter 54850       Data time: 0.01(0.01)  Forward time: 1.28(1.18)  Batch time: 1.29(1.18)
2025-09-04 04:48:47,791   INFO  Train:   32/36 ( 89%) [ 370/1759 ( 21%)]  Loss: 4.154 (4.89)  LR: 3.495e-04  Grad: 3.9084  max=0.4860(module.vfe.pfn_layers.0.linear.weight)  min: -1.4471(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4713, loss_cls=0.0935, loss_bbox=0.4096, matched_ious=0.5891, loss_iou=0.0904, loss_iou_reg=0.2021, d_time=0.01(0.01), f_time=1.12(1.18), b_time=1.13(1.18)  Time cost: 07:19/27:25 [5:52:46/2:46:21]  Acc_iter 54900       Data time: 0.01(0.01)  Forward time: 1.12(1.18)  Batch time: 1.13(1.18)
2025-09-04 04:49:46,984   INFO  Train:   32/36 ( 89%) [ 420/1759 ( 24%)]  Loss: 4.040 (4.74)  LR: 3.455e-04  Grad: 3.4488  max=0.8658(module.vfe.pfn_layers.0.linear.weight)  min: -0.3660(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4703, loss_cls=0.0887, loss_bbox=0.4315, matched_ious=0.5972, loss_iou=0.0854, loss_iou_reg=0.1990, d_time=0.01(0.01), f_time=1.18(1.18), b_time=1.18(1.18)  Time cost: 08:18/26:26 [5:53:46/2:45:21]  Acc_iter 54950       Data time: 0.01(0.01)  Forward time: 1.18(1.18)  Batch time: 1.18(1.18)
2025-09-04 04:50:45,940   INFO  Train:   32/36 ( 89%) [ 470/1759 ( 27%)]  Loss: 2.805 (4.61)  LR: 3.416e-04  Grad: 4.1563  max=0.8205(module.vfe.pfn_layers.0.linear.weight)  min: -0.9372(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5009, loss_cls=0.0994, loss_bbox=0.4783, matched_ious=0.5905, loss_iou=0.0858, loss_iou_reg=0.2057, d_time=0.01(0.01), f_time=1.18(1.18), b_time=1.19(1.18)  Time cost: 09:17/25:26 [5:54:45/2:44:17]  Acc_iter 55000       Data time: 0.01(0.01)  Forward time: 1.18(1.18)  Batch time: 1.19(1.18)
2025-09-04 04:51:46,025   INFO  Train:   32/36 ( 89%) [ 520/1759 ( 30%)]  Loss: 2.896 (4.53)  LR: 3.377e-04  Grad: 4.1769  max=0.8346(module.vfe.pfn_layers.0.linear.weight)  min: -0.7133(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4977, loss_cls=0.0975, loss_bbox=0.4986, matched_ious=0.5848, loss_iou=0.0872, loss_iou_reg=0.2009, d_time=0.01(0.01), f_time=1.16(1.18), b_time=1.17(1.19)  Time cost: 10:17/24:29 [5:55:45/2:43:32]  Acc_iter 55050       Data time: 0.01(0.01)  Forward time: 1.16(1.18)  Batch time: 1.17(1.19)
2025-09-04 04:52:46,052   INFO  Train:   32/36 ( 89%) [ 570/1759 ( 32%)]  Loss: 6.751 (4.44)  LR: 3.337e-04  Grad: 4.4072  max=0.7522(module.backbone_3d.cls_conv.1.weight)  min: -0.6790(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.4619, loss_cls=0.0916, loss_bbox=0.4260, matched_ious=0.5901, loss_iou=0.0873, loss_iou_reg=0.2040, d_time=0.01(0.01), f_time=1.16(1.18), b_time=1.16(1.19)  Time cost: 11:17/23:31 [5:56:45/2:42:43]  Acc_iter 55100       Data time: 0.01(0.01)  Forward time: 1.16(1.18)  Batch time: 1.16(1.19)
2025-09-04 04:53:45,029   INFO  Train:   32/36 ( 89%) [ 620/1759 ( 35%)]  Loss: 2.509 (4.37)  LR: 3.299e-04  Grad: 4.8592  max=0.8715(module.vfe.pfn_layers.0.linear.weight)  min: -1.0814(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4857, loss_cls=0.1033, loss_bbox=0.4482, matched_ious=0.5911, loss_iou=0.0863, loss_iou_reg=0.1990, d_time=0.01(0.01), f_time=1.10(1.18), b_time=1.11(1.19)  Time cost: 12:16/22:31 [5:57:44/2:41:39]  Acc_iter 55150       Data time: 0.01(0.01)  Forward time: 1.10(1.18)  Batch time: 1.11(1.19)
2025-09-04 04:54:45,399   INFO  Train:   32/36 ( 89%) [ 670/1759 ( 38%)]  Loss: 3.995 (4.30)  LR: 3.260e-04  Grad: 5.3519  max=1.2928(module.vfe.pfn_layers.0.linear.weight)  min: -1.3684(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4759, loss_cls=0.0933, loss_bbox=0.4447, matched_ious=0.5818, loss_iou=0.0912, loss_iou_reg=0.2083, d_time=0.01(0.01), f_time=1.15(1.18), b_time=1.15(1.19)  Time cost: 13:17/21:33 [5:58:44/2:40:52]  Acc_iter 55200       Data time: 0.01(0.01)  Forward time: 1.15(1.18)  Batch time: 1.15(1.19)
2025-09-04 04:55:44,431   INFO  Train:   32/36 ( 89%) [ 720/1759 ( 41%)]  Loss: 2.097 (4.24)  LR: 3.221e-04  Grad: 4.4268  max=0.4729(module.vfe.pfn_layers.0.linear.weight)  min: -0.7301(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4747, loss_cls=0.0947, loss_bbox=0.4300, matched_ious=0.5950, loss_iou=0.0863, loss_iou_reg=0.2011, d_time=0.01(0.01), f_time=1.19(1.18), b_time=1.19(1.19)  Time cost: 14:16/20:33 [5:59:43/2:39:49]  Acc_iter 55250       Data time: 0.01(0.01)  Forward time: 1.19(1.18)  Batch time: 1.19(1.19)
2025-09-04 04:56:44,196   INFO  Train:   32/36 ( 89%) [ 770/1759 ( 44%)]  Loss: 3.391 (4.19)  LR: 3.183e-04  Grad: 4.6636  max=0.6262(module.vfe.pfn_layers.0.linear.weight)  min: -0.8094(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5068, loss_cls=0.0970, loss_bbox=0.4248, matched_ious=0.5948, loss_iou=0.0866, loss_iou_reg=0.2005, d_time=0.01(0.01), f_time=1.20(1.18), b_time=1.20(1.19)  Time cost: 15:15/19:34 [6:00:43/2:38:53]  Acc_iter 55300       Data time: 0.01(0.01)  Forward time: 1.20(1.18)  Batch time: 1.20(1.19)
2025-09-04 04:57:44,210   INFO  Train:   32/36 ( 89%) [ 820/1759 ( 47%)]  Loss: 4.120 (4.16)  LR: 3.145e-04  Grad: 5.1235  max=0.9508(module.vfe.pfn_layers.0.linear.weight)  min: -0.8225(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4865, loss_cls=0.0942, loss_bbox=0.4102, matched_ious=0.5861, loss_iou=0.0874, loss_iou_reg=0.2031, d_time=0.01(0.01), f_time=1.20(1.18), b_time=1.20(1.19)  Time cost: 16:15/18:36 [6:01:43/2:38:00]  Acc_iter 55350       Data time: 0.01(0.01)  Forward time: 1.20(1.18)  Batch time: 1.20(1.19)
2025-09-04 04:58:43,426   INFO  Train:   32/36 ( 89%) [ 870/1759 ( 49%)]  Loss: 3.117 (4.13)  LR: 3.107e-04  Grad: 3.1336  max=0.7315(module.vfe.pfn_layers.0.linear.weight)  min: -0.3713(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.5103, loss_cls=0.0933, loss_bbox=0.4701, matched_ious=0.5835, loss_iou=0.0897, loss_iou_reg=0.2036, d_time=0.00(0.01), f_time=1.15(1.18), b_time=1.15(1.19)  Time cost: 17:15/17:36 [6:02:42/2:36:58]  Acc_iter 55400       Data time: 0.00(0.01)  Forward time: 1.15(1.18)  Batch time: 1.15(1.19)
2025-09-04 04:59:43,043   INFO  Train:   32/36 ( 89%) [ 920/1759 ( 52%)]  Loss: 3.913 (4.10)  LR: 3.069e-04  Grad: 4.4201  max=1.2341(module.backbone_3d.cls_conv.1.weight)  min: -1.1509(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5229, loss_cls=0.1008, loss_bbox=0.4490, matched_ious=0.5994, loss_iou=0.0830, loss_iou_reg=0.1973, d_time=0.01(0.01), f_time=1.44(1.18), b_time=1.45(1.19)  Time cost: 18:14/16:37 [6:03:42/2:36:01]  Acc_iter 55450       Data time: 0.01(0.01)  Forward time: 1.44(1.18)  Batch time: 1.45(1.19)
2025-09-04 05:00:41,854   INFO  Train:   32/36 ( 89%) [ 970/1759 ( 55%)]  Loss: 4.283 (4.06)  LR: 3.032e-04  Grad: 3.8689  max=1.7699(module.vfe.pfn_layers.0.linear.weight)  min: -0.8165(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4528, loss_cls=0.0850, loss_bbox=0.3876, matched_ious=0.6059, loss_iou=0.0843, loss_iou_reg=0.1970, d_time=0.01(0.01), f_time=1.18(1.18), b_time=1.19(1.19)  Time cost: 19:13/15:37 [6:04:41/2:34:56]  Acc_iter 55500       Data time: 0.01(0.01)  Forward time: 1.18(1.18)  Batch time: 1.19(1.19)
2025-09-04 05:01:41,147   INFO  Train:   32/36 ( 89%) [1020/1759 ( 58%)]  Loss: 1.943 (4.03)  LR: 2.995e-04  Grad: 3.4183  max=0.6940(module.vfe.pfn_layers.0.linear.weight)  min: -0.5141(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4945, loss_cls=0.0979, loss_bbox=0.4413, matched_ious=0.5972, loss_iou=0.0841, loss_iou_reg=0.1980, d_time=0.01(0.01), f_time=1.22(1.18), b_time=1.22(1.19)  Time cost: 20:12/14:37 [6:05:40/2:33:56]  Acc_iter 55550       Data time: 0.01(0.01)  Forward time: 1.22(1.18)  Batch time: 1.22(1.19)
2025-09-04 05:02:40,801   INFO  Train:   32/36 ( 89%) [1070/1759 ( 61%)]  Loss: 1.929 (4.02)  LR: 2.958e-04  Grad: 4.3160  max=0.6451(module.vfe.pfn_layers.0.linear.weight)  min: -0.6625(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.4824, loss_cls=0.0943, loss_bbox=0.4099, matched_ious=0.6132, loss_iou=0.0821, loss_iou_reg=0.1919, d_time=0.01(0.01), f_time=1.10(1.18), b_time=1.10(1.19)  Time cost: 21:12/13:38 [6:06:39/2:32:58]  Acc_iter 55600       Data time: 0.01(0.01)  Forward time: 1.10(1.18)  Batch time: 1.10(1.19)
2025-09-04 05:03:40,634   INFO  Train:   32/36 ( 89%) [1120/1759 ( 64%)]  Loss: 2.727 (3.98)  LR: 2.921e-04  Grad: 4.2998  max=0.7601(module.backbone_3d.cls_conv.1.weight)  min: -0.5641(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4591, loss_cls=0.0878, loss_bbox=0.3984, matched_ious=0.6171, loss_iou=0.0839, loss_iou_reg=0.1891, d_time=0.01(0.01), f_time=1.13(1.18), b_time=1.13(1.19)  Time cost: 22:12/12:39 [6:07:39/2:32:02]  Acc_iter 55650       Data time: 0.01(0.01)  Forward time: 1.13(1.18)  Batch time: 1.13(1.19)
2025-09-04 05:04:39,891   INFO  Train:   32/36 ( 89%) [1170/1759 ( 67%)]  Loss: 3.059 (3.96)  LR: 2.884e-04  Grad: 4.7081  max=1.2051(module.vfe.pfn_layers.0.linear.weight)  min: -1.2028(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4873, loss_cls=0.1019, loss_bbox=0.4200, matched_ious=0.5966, loss_iou=0.0867, loss_iou_reg=0.1968, d_time=0.01(0.01), f_time=1.09(1.18), b_time=1.10(1.19)  Time cost: 23:11/11:39 [6:08:39/2:31:01]  Acc_iter 55700       Data time: 0.01(0.01)  Forward time: 1.09(1.18)  Batch time: 1.10(1.19)
2025-09-04 05:05:38,357   INFO  Train:   32/36 ( 89%) [1220/1759 ( 69%)]  Loss: 4.428 (3.94)  LR: 2.848e-04  Grad: 5.3708  max=0.9585(module.vfe.pfn_layers.0.linear.weight)  min: -1.7058(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4788, loss_cls=0.0955, loss_bbox=0.4158, matched_ious=0.5953, loss_iou=0.0851, loss_iou_reg=0.1983, d_time=0.01(0.01), f_time=1.19(1.18), b_time=1.19(1.19)  Time cost: 24:10/10:40 [6:09:37/2:29:56]  Acc_iter 55750       Data time: 0.01(0.01)  Forward time: 1.19(1.18)  Batch time: 1.19(1.19)
2025-09-04 05:06:37,132   INFO  Train:   32/36 ( 89%) [1270/1759 ( 72%)]  Loss: 3.767 (3.92)  LR: 2.811e-04  Grad: 5.4380  max=1.2364(module.vfe.pfn_layers.0.linear.weight)  min: -0.8432(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4612, loss_cls=0.0928, loss_bbox=0.4074, matched_ious=0.5932, loss_iou=0.0869, loss_iou_reg=0.2064, d_time=0.01(0.01), f_time=1.16(1.18), b_time=1.16(1.19)  Time cost: 25:08/09:40 [6:10:36/2:28:53]  Acc_iter 55800       Data time: 0.01(0.01)  Forward time: 1.16(1.18)  Batch time: 1.16(1.19)
2025-09-04 05:07:36,356   INFO  Train:   32/36 ( 89%) [1320/1759 ( 75%)]  Loss: 2.566 (3.91)  LR: 2.775e-04  Grad: 6.7277  max=3.2561(module.vfe.pfn_layers.0.linear.weight)  min: -1.0224(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4664, loss_cls=0.0922, loss_bbox=0.4457, matched_ious=0.5984, loss_iou=0.0866, loss_iou_reg=0.1975, d_time=0.01(0.01), f_time=1.21(1.18), b_time=1.22(1.19)  Time cost: 26:08/08:41 [6:11:35/2:27:53]  Acc_iter 55850       Data time: 0.01(0.01)  Forward time: 1.21(1.18)  Batch time: 1.22(1.19)
2025-09-04 05:08:35,731   INFO  Train:   32/36 ( 89%) [1370/1759 ( 78%)]  Loss: 3.668 (3.90)  LR: 2.739e-04  Grad: 5.1895  max=0.6961(module.backbone_3d.cls_conv.1.weight)  min: -0.5101(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4846, loss_cls=0.0866, loss_bbox=0.4267, matched_ious=0.6187, loss_iou=0.0847, loss_iou_reg=0.1885, d_time=0.01(0.01), f_time=1.19(1.18), b_time=1.19(1.19)  Time cost: 27:07/07:41 [6:12:34/2:26:54]  Acc_iter 55900       Data time: 0.01(0.01)  Forward time: 1.19(1.18)  Batch time: 1.19(1.19)
2025-09-04 05:09:35,088   INFO  Train:   32/36 ( 89%) [1420/1759 ( 81%)]  Loss: 3.517 (3.88)  LR: 2.704e-04  Grad: 4.9188  max=0.7996(module.vfe.pfn_layers.0.linear.weight)  min: -0.7905(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.4810, loss_cls=0.0909, loss_bbox=0.4513, matched_ious=0.6083, loss_iou=0.0854, loss_iou_reg=0.1934, d_time=0.01(0.01), f_time=1.49(1.18), b_time=1.50(1.19)  Time cost: 28:06/06:42 [6:13:34/2:25:54]  Acc_iter 55950       Data time: 0.01(0.01)  Forward time: 1.49(1.18)  Batch time: 1.50(1.19)
2025-09-04 05:10:34,052   INFO  Train:   32/36 ( 89%) [1470/1759 ( 84%)]  Loss: 4.625 (3.88)  LR: 2.668e-04  Grad: 3.9322  max=0.6052(module.vfe.pfn_layers.0.linear.weight)  min: -1.9654(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4820, loss_cls=0.0944, loss_bbox=0.4620, matched_ious=0.5890, loss_iou=0.0900, loss_iou_reg=0.2010, d_time=0.01(0.01), f_time=1.13(1.18), b_time=1.13(1.19)  Time cost: 29:05/05:42 [6:14:33/2:24:53]  Acc_iter 56000       Data time: 0.01(0.01)  Forward time: 1.13(1.18)  Batch time: 1.13(1.19)
2025-09-04 05:11:33,923   INFO  Train:   32/36 ( 89%) [1520/1759 ( 86%)]  Loss: 2.679 (3.87)  LR: 2.633e-04  Grad: 3.5299  max=1.1875(module.vfe.pfn_layers.0.linear.weight)  min: -0.6586(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4475, loss_cls=0.0927, loss_bbox=0.4340, matched_ious=0.5957, loss_iou=0.0865, loss_iou_reg=0.1998, d_time=0.01(0.01), f_time=1.21(1.18), b_time=1.21(1.19)  Time cost: 30:05/04:43 [6:15:33/2:23:56]  Acc_iter 56050       Data time: 0.01(0.01)  Forward time: 1.21(1.18)  Batch time: 1.21(1.19)
2025-09-04 05:12:33,472   INFO  Train:   32/36 ( 89%) [1570/1759 ( 89%)]  Loss: 2.368 (3.85)  LR: 2.598e-04  Grad: 3.4590  max=0.4724(module.vfe.pfn_layers.0.linear.weight)  min: -0.6053(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4533, loss_cls=0.0882, loss_bbox=0.3803, matched_ious=0.6058, loss_iou=0.0866, loss_iou_reg=0.1965, d_time=0.01(0.01), f_time=1.14(1.18), b_time=1.14(1.19)  Time cost: 31:05/03:44 [6:16:32/2:22:58]  Acc_iter 56100       Data time: 0.01(0.01)  Forward time: 1.14(1.18)  Batch time: 1.14(1.19)
2025-09-04 05:13:32,497   INFO  Train:   32/36 ( 89%) [1620/1759 ( 92%)]  Loss: 2.727 (3.84)  LR: 2.563e-04  Grad: 3.5053  max=0.5574(module.vfe.pfn_layers.0.linear.weight)  min: -1.2418(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4520, loss_cls=0.0908, loss_bbox=0.4104, matched_ious=0.5959, loss_iou=0.0861, loss_iou_reg=0.2000, d_time=0.01(0.01), f_time=1.12(1.18), b_time=1.12(1.19)  Time cost: 32:04/02:45 [6:17:31/2:21:57]  Acc_iter 56150       Data time: 0.01(0.01)  Forward time: 1.12(1.18)  Batch time: 1.12(1.19)
2025-09-04 05:14:31,307   INFO  Train:   32/36 ( 89%) [1670/1759 ( 95%)]  Loss: 3.531 (3.82)  LR: 2.529e-04  Grad: 3.4655  max=0.5792(module.vfe.pfn_layers.0.linear.weight)  min: -0.7231(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4638, loss_cls=0.0884, loss_bbox=0.4257, matched_ious=0.6067, loss_iou=0.0841, loss_iou_reg=0.1916, d_time=0.01(0.01), f_time=1.11(1.18), b_time=1.11(1.19)  Time cost: 33:03/01:45 [6:18:30/2:20:55]  Acc_iter 56200       Data time: 0.01(0.01)  Forward time: 1.11(1.18)  Batch time: 1.11(1.19)
2025-09-04 05:15:31,376   INFO  Train:   32/36 ( 89%) [1720/1759 ( 98%)]  Loss: 3.196 (3.81)  LR: 2.495e-04  Grad: 3.2462  max=0.7694(module.vfe.pfn_layers.0.linear.weight)  min: -0.9105(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5027, loss_cls=0.0951, loss_bbox=0.4223, matched_ious=0.5996, loss_iou=0.0894, loss_iou_reg=0.1990, d_time=0.01(0.01), f_time=1.17(1.18), b_time=1.17(1.19)  Time cost: 34:03/00:46 [6:19:30/2:19:59]  Acc_iter 56250       Data time: 0.01(0.01)  Forward time: 1.17(1.18)  Batch time: 1.17(1.19)
2025-09-04 05:16:15,160   INFO  Train:   32/36 ( 89%) [1758/1759 (100%)]  Loss: 5.332 (3.80)  LR: 2.469e-04  Grad: 4.0105  max=0.3509(module.backbone_3d.shared_conv.1.weight)  min: -1.1654(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4638, loss_cls=0.0943, loss_bbox=0.4226, matched_ious=0.5978, loss_iou=0.0856, loss_iou_reg=0.1988, d_time=0.01(0.01), f_time=0.71(1.18), b_time=0.72(1.19)  Time cost: 34:46/00:01 [6:20:14/2:19:08]  Acc_iter 56288       Data time: 0.01(0.01)  Forward time: 0.71(1.18)  Batch time: 0.72(1.19)

                                               [Aepochs:  73%|███████▎  | 11/15 [6:20:14<2:24:28, 2167.11s/it]epochs:  73%|███████▎  | 11/15 [6:20:14<2:24:28, 2167.11s/it]epochs:  73%|███████▎  | 11/15 [6:20:14<2:24:28, 2167.09s/it]epochs:  73%|███████▎  | 11/15 [6:20:14<2:24:28, 2167.12s/it]epochs:  73%|███████▎  | 11/15 [6:20:14<2:24:28, 2167.11s/it]epochs:  73%|███████▎  | 11/15 [6:20:14<2:24:28, 2167.12s/it]epochs:  73%|███████▎  | 11/15 [6:20:14<2:24:28, 2167.12s/it]epochs:  73%|███████▎  | 11/15 [6:20:14<2:24:28, 2167.13s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 05:16:18,483   INFO  Train:   33/36 ( 92%) [   0/1759 (  0%)]  Loss: 3.464 (3.46)  LR: 2.468e-04  Grad: 3.6681  max=0.5679(module.vfe.pfn_layers.0.linear.weight)  min: -0.5760(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5904, loss_cls=0.1034, loss_bbox=0.3452, matched_ious=0.6176, loss_iou=0.0869, loss_iou_reg=0.1802, d_time=0.70(0.70), f_time=1.64(1.64), b_time=2.33(2.33)  Time cost: 00:02/1:05:32 [6:20:17/4:22:08]  Acc_iter 56289       Data time: 0.70(0.70)  Forward time: 1.64(1.64)  Batch time: 2.33(2.33)
2025-09-04 05:16:31,951   INFO  Train:   33/36 ( 92%) [  11/1759 (  1%)]  Loss: 2.827 (3.56)  LR: 2.460e-04  Grad: 4.0031  max=1.0474(module.vfe.pfn_layers.0.linear.weight)  min: -0.4366(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4852, loss_cls=0.0899, loss_bbox=0.4290, matched_ious=0.5869, loss_iou=0.0908, loss_iou_reg=0.2089, d_time=0.01(0.07), f_time=1.19(1.25), b_time=1.20(1.32)  Time cost: 00:15/38:07 [6:20:31/2:33:14]  Acc_iter 56300       Data time: 0.01(0.07)  Forward time: 1.19(1.25)  Batch time: 1.20(1.32)
2025-09-04 05:17:32,122   INFO  Train:   33/36 ( 92%) [  61/1759 (  3%)]  Loss: 3.807 (3.63)  LR: 2.426e-04  Grad: 4.6593  max=0.8308(module.vfe.pfn_layers.0.linear.weight)  min: -0.9918(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4603, loss_cls=0.0893, loss_bbox=0.4608, matched_ious=0.6024, loss_iou=0.0878, loss_iou_reg=0.1992, d_time=0.01(0.02), f_time=1.19(1.20), b_time=1.20(1.23)  Time cost: 01:15/34:38 [6:21:31/2:22:16]  Acc_iter 56350       Data time: 0.01(0.02)  Forward time: 1.19(1.20)  Batch time: 1.20(1.23)
2025-09-04 05:18:31,999   INFO  Train:   33/36 ( 92%) [ 111/1759 (  6%)]  Loss: 4.419 (3.63)  LR: 2.393e-04  Grad: 4.0400  max=0.7536(module.vfe.pfn_layers.0.linear.weight)  min: -0.5875(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4690, loss_cls=0.0914, loss_bbox=0.4164, matched_ious=0.6106, loss_iou=0.0824, loss_iou_reg=0.1944, d_time=0.01(0.01), f_time=1.14(1.20), b_time=1.15(1.21)  Time cost: 02:15/33:17 [6:22:31/2:19:53]  Acc_iter 56400       Data time: 0.01(0.01)  Forward time: 1.14(1.20)  Batch time: 1.15(1.21)
2025-09-04 05:19:30,953   INFO  Train:   33/36 ( 92%) [ 161/1759 (  9%)]  Loss: 2.590 (3.59)  LR: 2.359e-04  Grad: 4.2802  max=0.8836(module.vfe.pfn_layers.0.linear.weight)  min: -0.8720(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5031, loss_cls=0.0913, loss_bbox=0.4443, matched_ious=0.6073, loss_iou=0.0851, loss_iou_reg=0.1916, d_time=0.01(0.01), f_time=1.16(1.19), b_time=1.17(1.20)  Time cost: 03:14/32:00 [6:23:30/2:17:43]  Acc_iter 56450       Data time: 0.01(0.01)  Forward time: 1.16(1.19)  Batch time: 1.17(1.20)
2025-09-04 05:20:29,303   INFO  Train:   33/36 ( 92%) [ 211/1759 ( 12%)]  Loss: 3.390 (3.59)  LR: 2.326e-04  Grad: 5.2258  max=1.5172(module.backbone_3d.cls_conv.1.weight)  min: -0.7455(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4799, loss_cls=0.0914, loss_bbox=0.4764, matched_ious=0.5934, loss_iou=0.0839, loss_iou_reg=0.2019, d_time=0.01(0.01), f_time=1.17(1.18), b_time=1.17(1.19)  Time cost: 04:13/30:47 [6:24:28/2:15:46]  Acc_iter 56500       Data time: 0.01(0.01)  Forward time: 1.17(1.18)  Batch time: 1.17(1.19)
2025-09-04 05:21:28,986   INFO  Train:   33/36 ( 92%) [ 261/1759 ( 15%)]  Loss: 4.388 (3.59)  LR: 2.293e-04  Grad: 4.6165  max=0.5952(module.vfe.pfn_layers.0.linear.weight)  min: -0.3275(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.5123, loss_cls=0.0955, loss_bbox=0.4369, matched_ious=0.5859, loss_iou=0.0896, loss_iou_reg=0.2050, d_time=0.01(0.01), f_time=1.20(1.18), b_time=1.20(1.19)  Time cost: 05:12/29:48 [6:25:28/2:14:47]  Acc_iter 56550       Data time: 0.01(0.01)  Forward time: 1.20(1.18)  Batch time: 1.20(1.19)
2025-09-04 05:22:28,612   INFO  Train:   33/36 ( 92%) [ 311/1759 ( 18%)]  Loss: 4.081 (3.61)  LR: 2.260e-04  Grad: 5.3100  max=1.4999(module.vfe.pfn_layers.0.linear.weight)  min: -0.8346(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4750, loss_cls=0.0894, loss_bbox=0.4693, matched_ious=0.6015, loss_iou=0.0860, loss_iou_reg=0.1958, d_time=0.01(0.01), f_time=1.12(1.18), b_time=1.12(1.19)  Time cost: 06:12/28:48 [6:26:27/2:13:46]  Acc_iter 56600       Data time: 0.01(0.01)  Forward time: 1.12(1.18)  Batch time: 1.12(1.19)
2025-09-04 05:23:27,956   INFO  Train:   33/36 ( 92%) [ 361/1759 ( 21%)]  Loss: 3.067 (3.56)  LR: 2.227e-04  Grad: 5.3330  max=0.7578(module.vfe.pfn_layers.0.linear.weight)  min: -1.2019(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4539, loss_cls=0.0885, loss_bbox=0.4276, matched_ious=0.6088, loss_iou=0.0843, loss_iou_reg=0.1936, d_time=0.02(0.01), f_time=1.17(1.18), b_time=1.19(1.19)  Time cost: 07:11/27:47 [6:27:27/2:12:40]  Acc_iter 56650       Data time: 0.02(0.01)  Forward time: 1.17(1.18)  Batch time: 1.19(1.19)
2025-09-04 05:24:27,456   INFO  Train:   33/36 ( 92%) [ 411/1759 ( 23%)]  Loss: 3.694 (3.56)  LR: 2.195e-04  Grad: 7.5117  max=3.7311(module.vfe.pfn_layers.0.linear.weight)  min: -3.0350(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4906, loss_cls=0.0863, loss_bbox=0.4748, matched_ious=0.6001, loss_iou=0.0867, loss_iou_reg=0.1990, d_time=0.01(0.01), f_time=1.35(1.18), b_time=1.36(1.19)  Time cost: 08:11/26:47 [6:28:26/2:11:38]  Acc_iter 56700       Data time: 0.01(0.01)  Forward time: 1.35(1.18)  Batch time: 1.36(1.19)
2025-09-04 05:25:26,150   INFO  Train:   33/36 ( 92%) [ 461/1759 ( 26%)]  Loss: 4.037 (3.54)  LR: 2.163e-04  Grad: 7.4123  max=1.1505(module.vfe.pfn_layers.0.linear.weight)  min: -3.9707(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4648, loss_cls=0.0855, loss_bbox=0.4066, matched_ious=0.6078, loss_iou=0.0825, loss_iou_reg=0.1896, d_time=0.01(0.01), f_time=1.19(1.18), b_time=1.19(1.19)  Time cost: 09:09/25:44 [6:29:25/2:10:26]  Acc_iter 56750       Data time: 0.01(0.01)  Forward time: 1.19(1.18)  Batch time: 1.19(1.19)
2025-09-04 05:26:25,865   INFO  Train:   33/36 ( 92%) [ 511/1759 ( 29%)]  Loss: 4.367 (3.54)  LR: 2.131e-04  Grad: 5.0341  max=0.5855(module.vfe.pfn_layers.0.linear.weight)  min: -0.7081(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5040, loss_cls=0.0932, loss_bbox=0.4545, matched_ious=0.5886, loss_iou=0.0875, loss_iou_reg=0.1981, d_time=0.01(0.01), f_time=1.24(1.18), b_time=1.24(1.19)  Time cost: 10:09/24:45 [6:30:25/2:09:29]  Acc_iter 56800       Data time: 0.01(0.01)  Forward time: 1.24(1.18)  Batch time: 1.24(1.19)
2025-09-04 05:27:24,996   INFO  Train:   33/36 ( 92%) [ 561/1759 ( 32%)]  Loss: 5.216 (3.53)  LR: 2.099e-04  Grad: 6.0672  max=1.2253(module.backbone_3d.cls_conv.3.bias)  min: -1.9311(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4811, loss_cls=0.0913, loss_bbox=0.4359, matched_ious=0.5924, loss_iou=0.0864, loss_iou_reg=0.2012, d_time=0.01(0.01), f_time=1.18(1.18), b_time=1.18(1.19)  Time cost: 11:08/23:45 [6:31:24/2:08:24]  Acc_iter 56850       Data time: 0.01(0.01)  Forward time: 1.18(1.18)  Batch time: 1.18(1.19)
2025-09-04 05:28:23,590   INFO  Train:   33/36 ( 92%) [ 611/1759 ( 35%)]  Loss: 3.306 (3.52)  LR: 2.068e-04  Grad: 4.4120  max=1.1980(module.vfe.pfn_layers.0.linear.weight)  min: -0.5965(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4690, loss_cls=0.0875, loss_bbox=0.4346, matched_ious=0.5958, loss_iou=0.0814, loss_iou_reg=0.1959, d_time=0.01(0.01), f_time=1.17(1.18), b_time=1.18(1.19)  Time cost: 12:07/22:44 [6:32:22/2:07:15]  Acc_iter 56900       Data time: 0.01(0.01)  Forward time: 1.17(1.18)  Batch time: 1.18(1.19)
2025-09-04 05:29:23,925   INFO  Train:   33/36 ( 92%) [ 661/1759 ( 38%)]  Loss: 2.965 (3.49)  LR: 2.036e-04  Grad: 4.8167  max=0.5023(module.vfe.pfn_layers.0.linear.weight)  min: -1.1549(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4412, loss_cls=0.0870, loss_bbox=0.4170, matched_ious=0.5988, loss_iou=0.0876, loss_iou_reg=0.2004, d_time=0.01(0.01), f_time=1.12(1.18), b_time=1.13(1.19)  Time cost: 13:07/21:46 [6:33:23/2:06:25]  Acc_iter 56950       Data time: 0.01(0.01)  Forward time: 1.12(1.18)  Batch time: 1.13(1.19)
2025-09-04 05:30:22,320   INFO  Train:   33/36 ( 92%) [ 711/1759 ( 40%)]  Loss: 3.381 (3.50)  LR: 2.005e-04  Grad: 5.0254  max=1.2400(module.backbone_3d.cls_conv.3.bias)  min: -0.8078(module.backbone_3d.cls_conv.1.bias)  NaN: False  loss_hm=0.4791, loss_cls=0.0913, loss_bbox=0.4244, matched_ious=0.6019, loss_iou=0.0863, loss_iou_reg=0.1959, d_time=0.01(0.01), f_time=1.06(1.18), b_time=1.07(1.19)  Time cost: 14:06/20:45 [6:34:21/2:05:16]  Acc_iter 57000       Data time: 0.01(0.01)  Forward time: 1.06(1.18)  Batch time: 1.07(1.19)
2025-09-04 05:31:21,458   INFO  Train:   33/36 ( 92%) [ 761/1759 ( 43%)]  Loss: 4.861 (3.48)  LR: 1.974e-04  Grad: 6.6126  max=1.5877(module.vfe.pfn_layers.0.linear.weight)  min: -1.4846(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4333, loss_cls=0.0879, loss_bbox=0.3623, matched_ious=0.6185, loss_iou=0.0833, loss_iou_reg=0.1887, d_time=0.01(0.01), f_time=1.14(1.18), b_time=1.15(1.19)  Time cost: 15:05/19:45 [6:35:20/2:04:14]  Acc_iter 57050       Data time: 0.01(0.01)  Forward time: 1.14(1.18)  Batch time: 1.15(1.19)
2025-09-04 05:32:20,606   INFO  Train:   33/36 ( 92%) [ 811/1759 ( 46%)]  Loss: 2.794 (3.47)  LR: 1.944e-04  Grad: 10.0000  max=3.7203(module.vfe.pfn_layers.0.linear.weight)  min: -5.9732(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4492, loss_cls=0.0885, loss_bbox=0.4120, matched_ious=0.6039, loss_iou=0.0826, loss_iou_reg=0.1960, d_time=0.01(0.01), f_time=1.24(1.18), b_time=1.25(1.19)  Time cost: 16:04/18:45 [6:36:19/2:03:13]  Acc_iter 57100       Data time: 0.01(0.01)  Forward time: 1.24(1.18)  Batch time: 1.25(1.19)
2025-09-04 05:33:19,539   INFO  Train:   33/36 ( 92%) [ 861/1759 ( 49%)]  Loss: 2.436 (3.47)  LR: 1.913e-04  Grad: 5.8238  max=1.4671(module.backbone_3d.cls_conv.3.bias)  min: -1.6225(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4456, loss_cls=0.0843, loss_bbox=0.3963, matched_ious=0.6133, loss_iou=0.0839, loss_iou_reg=0.1925, d_time=0.01(0.01), f_time=1.15(1.18), b_time=1.15(1.19)  Time cost: 17:03/17:46 [6:37:18/2:02:10]  Acc_iter 57150       Data time: 0.01(0.01)  Forward time: 1.15(1.18)  Batch time: 1.15(1.19)
2025-09-04 05:34:18,933   INFO  Train:   33/36 ( 92%) [ 911/1759 ( 52%)]  Loss: 3.895 (3.48)  LR: 1.883e-04  Grad: 4.5482  max=0.8767(module.vfe.pfn_layers.0.linear.weight)  min: -1.0041(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4810, loss_cls=0.0954, loss_bbox=0.4160, matched_ious=0.6077, loss_iou=0.0852, loss_iou_reg=0.1895, d_time=0.01(0.01), f_time=1.24(1.18), b_time=1.24(1.19)  Time cost: 18:02/16:46 [6:38:18/2:01:11]  Acc_iter 57200       Data time: 0.01(0.01)  Forward time: 1.24(1.18)  Batch time: 1.24(1.19)
2025-09-04 05:35:17,464   INFO  Train:   33/36 ( 92%) [ 961/1759 ( 55%)]  Loss: 2.862 (3.48)  LR: 1.853e-04  Grad: 4.5474  max=1.4073(module.vfe.pfn_layers.0.linear.weight)  min: -0.9948(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4798, loss_cls=0.0899, loss_bbox=0.4101, matched_ious=0.6168, loss_iou=0.0843, loss_iou_reg=0.1916, d_time=0.01(0.01), f_time=1.11(1.18), b_time=1.11(1.19)  Time cost: 19:01/15:46 [6:39:16/2:00:06]  Acc_iter 57250       Data time: 0.01(0.01)  Forward time: 1.11(1.18)  Batch time: 1.11(1.19)
2025-09-04 05:36:16,760   INFO  Train:   33/36 ( 92%) [1011/1759 ( 57%)]  Loss: 2.435 (3.48)  LR: 1.823e-04  Grad: 4.6137  max=0.7050(module.vfe.pfn_layers.0.linear.weight)  min: -0.6663(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4406, loss_cls=0.0911, loss_bbox=0.3971, matched_ious=0.6179, loss_iou=0.0862, loss_iou_reg=0.1938, d_time=0.01(0.01), f_time=1.29(1.18), b_time=1.29(1.19)  Time cost: 20:00/14:47 [6:40:15/1:59:07]  Acc_iter 57300       Data time: 0.01(0.01)  Forward time: 1.29(1.18)  Batch time: 1.29(1.19)
2025-09-04 05:37:16,858   INFO  Train:   33/36 ( 92%) [1061/1759 ( 60%)]  Loss: 2.937 (3.48)  LR: 1.794e-04  Grad: 5.7360  max=3.5492(module.vfe.pfn_layers.0.linear.weight)  min: -0.5650(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4920, loss_cls=0.0964, loss_bbox=0.4223, matched_ious=0.6034, loss_iou=0.0881, loss_iou_reg=0.2002, d_time=0.01(0.01), f_time=1.13(1.18), b_time=1.13(1.19)  Time cost: 21:00/13:48 [6:41:16/1:58:12]  Acc_iter 57350       Data time: 0.01(0.01)  Forward time: 1.13(1.18)  Batch time: 1.13(1.19)
2025-09-04 05:38:16,443   INFO  Train:   33/36 ( 92%) [1111/1759 ( 63%)]  Loss: 2.742 (3.47)  LR: 1.765e-04  Grad: 4.9506  max=1.1868(module.vfe.pfn_layers.0.linear.weight)  min: -0.5110(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4505, loss_cls=0.0920, loss_bbox=0.3965, matched_ious=0.5945, loss_iou=0.0876, loss_iou_reg=0.1959, d_time=0.01(0.01), f_time=1.15(1.18), b_time=1.15(1.19)  Time cost: 22:00/12:49 [6:42:15/1:57:14]  Acc_iter 57400       Data time: 0.01(0.01)  Forward time: 1.15(1.18)  Batch time: 1.15(1.19)
2025-09-04 05:39:16,288   INFO  Train:   33/36 ( 92%) [1161/1759 ( 66%)]  Loss: 3.101 (3.47)  LR: 1.736e-04  Grad: 7.7725  max=1.9931(module.backbone_3d.cls_conv.1.weight)  min: -1.8831(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4749, loss_cls=0.0903, loss_bbox=0.4492, matched_ious=0.5945, loss_iou=0.0872, loss_iou_reg=0.1994, d_time=0.01(0.01), f_time=1.19(1.18), b_time=1.19(1.19)  Time cost: 23:00/11:50 [6:43:15/1:56:17]  Acc_iter 57450       Data time: 0.01(0.01)  Forward time: 1.19(1.18)  Batch time: 1.19(1.19)
2025-09-04 05:40:15,811   INFO  Train:   33/36 ( 92%) [1211/1759 ( 69%)]  Loss: 4.407 (3.48)  LR: 1.707e-04  Grad: 5.0642  max=1.7285(module.vfe.pfn_layers.0.linear.weight)  min: -0.8963(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4992, loss_cls=0.0936, loss_bbox=0.5036, matched_ious=0.5939, loss_iou=0.0870, loss_iou_reg=0.1992, d_time=0.01(0.01), f_time=1.25(1.18), b_time=1.25(1.19)  Time cost: 23:59/10:50 [6:44:14/1:55:18]  Acc_iter 57500       Data time: 0.01(0.01)  Forward time: 1.25(1.18)  Batch time: 1.25(1.19)
2025-09-04 05:41:15,280   INFO  Train:   33/36 ( 92%) [1261/1759 ( 72%)]  Loss: 2.844 (3.47)  LR: 1.678e-04  Grad: 4.3071  max=0.9161(module.vfe.pfn_layers.0.linear.weight)  min: -0.7917(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4431, loss_cls=0.0800, loss_bbox=0.3902, matched_ious=0.6146, loss_iou=0.0809, loss_iou_reg=0.1930, d_time=0.01(0.01), f_time=1.17(1.18), b_time=1.18(1.19)  Time cost: 24:59/09:51 [6:45:14/1:54:19]  Acc_iter 57550       Data time: 0.01(0.01)  Forward time: 1.17(1.18)  Batch time: 1.18(1.19)
2025-09-04 05:42:13,896   INFO  Train:   33/36 ( 92%) [1311/1759 ( 75%)]  Loss: 3.993 (3.47)  LR: 1.650e-04  Grad: 8.7465  max=3.0188(module.vfe.pfn_layers.0.linear.weight)  min: -0.8884(module.backbone_3d.stage.embeddings.2.stem.1.bn1.weight)  NaN: False  loss_hm=0.4350, loss_cls=0.0813, loss_bbox=0.3845, matched_ious=0.6120, loss_iou=0.0854, loss_iou_reg=0.1962, d_time=0.01(0.01), f_time=1.20(1.18), b_time=1.21(1.19)  Time cost: 25:57/08:51 [6:46:13/1:53:16]  Acc_iter 57600       Data time: 0.01(0.01)  Forward time: 1.20(1.18)  Batch time: 1.21(1.19)
2025-09-04 05:43:13,494   INFO  Train:   33/36 ( 92%) [1361/1759 ( 77%)]  Loss: 1.567 (3.46)  LR: 1.622e-04  Grad: 4.6269  max=0.8755(module.vfe.pfn_layers.0.linear.weight)  min: -0.5041(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4719, loss_cls=0.0910, loss_bbox=0.4198, matched_ious=0.6063, loss_iou=0.0876, loss_iou_reg=0.1986, d_time=0.01(0.01), f_time=1.17(1.18), b_time=1.18(1.19)  Time cost: 26:57/07:52 [6:47:12/1:52:18]  Acc_iter 57650       Data time: 0.01(0.01)  Forward time: 1.17(1.18)  Batch time: 1.18(1.19)
2025-09-04 05:44:13,378   INFO  Train:   33/36 ( 92%) [1411/1759 ( 80%)]  Loss: 2.304 (3.45)  LR: 1.594e-04  Grad: 3.6819  max=0.7005(module.backbone_3d.cls_conv.3.bias)  min: -0.9473(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4523, loss_cls=0.0861, loss_bbox=0.3873, matched_ious=0.6023, loss_iou=0.0862, loss_iou_reg=0.1994, d_time=0.01(0.01), f_time=1.19(1.18), b_time=1.20(1.19)  Time cost: 27:57/06:53 [6:48:12/1:51:21]  Acc_iter 57700       Data time: 0.01(0.01)  Forward time: 1.19(1.18)  Batch time: 1.20(1.19)
2025-09-04 05:45:11,830   INFO  Train:   33/36 ( 92%) [1461/1759 ( 83%)]  Loss: 3.277 (3.45)  LR: 1.566e-04  Grad: 4.2009  max=0.4992(module.backbone_3d.cls_conv.1.bias)  min: -0.8442(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.4418, loss_cls=0.0834, loss_bbox=0.4246, matched_ious=0.6090, loss_iou=0.0854, loss_iou_reg=0.1970, d_time=0.01(0.01), f_time=1.09(1.18), b_time=1.10(1.19)  Time cost: 28:55/05:53 [6:49:10/1:50:18]  Acc_iter 57750       Data time: 0.01(0.01)  Forward time: 1.09(1.18)  Batch time: 1.10(1.19)
2025-09-04 05:46:10,586   INFO  Train:   33/36 ( 92%) [1511/1759 ( 86%)]  Loss: 2.133 (3.45)  LR: 1.538e-04  Grad: 4.7474  max=0.7830(module.vfe.pfn_layers.0.linear.weight)  min: -2.6607(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4859, loss_cls=0.0953, loss_bbox=0.4401, matched_ious=0.6160, loss_iou=0.0853, loss_iou_reg=0.1971, d_time=0.01(0.01), f_time=1.13(1.18), b_time=1.14(1.19)  Time cost: 29:54/04:54 [6:50:09/1:49:16]  Acc_iter 57800       Data time: 0.01(0.01)  Forward time: 1.13(1.18)  Batch time: 1.14(1.19)
2025-09-04 05:47:09,622   INFO  Train:   33/36 ( 92%) [1561/1759 ( 89%)]  Loss: 2.621 (3.45)  LR: 1.511e-04  Grad: 4.4182  max=0.6367(module.vfe.pfn_layers.0.linear.weight)  min: -0.9174(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4440, loss_cls=0.0881, loss_bbox=0.4036, matched_ious=0.6035, loss_iou=0.0867, loss_iou_reg=0.1982, d_time=0.01(0.01), f_time=1.17(1.18), b_time=1.17(1.19)  Time cost: 30:53/03:54 [6:51:08/1:48:16]  Acc_iter 57850       Data time: 0.01(0.01)  Forward time: 1.17(1.18)  Batch time: 1.17(1.19)
2025-09-04 05:48:08,480   INFO  Train:   33/36 ( 92%) [1611/1759 ( 92%)]  Loss: 2.907 (3.44)  LR: 1.484e-04  Grad: 5.1038  max=0.7925(module.backbone_3d.cls_conv.3.bias)  min: -1.1754(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4369, loss_cls=0.0859, loss_bbox=0.3963, matched_ious=0.6206, loss_iou=0.0855, loss_iou_reg=0.1923, d_time=0.01(0.01), f_time=1.26(1.18), b_time=1.26(1.19)  Time cost: 31:52/02:55 [6:52:07/1:47:15]  Acc_iter 57900       Data time: 0.01(0.01)  Forward time: 1.26(1.18)  Batch time: 1.26(1.19)
2025-09-04 05:49:08,087   INFO  Train:   33/36 ( 92%) [1661/1759 ( 94%)]  Loss: 3.070 (3.44)  LR: 1.457e-04  Grad: 5.2530  max=0.9428(module.vfe.pfn_layers.0.linear.weight)  min: -0.9027(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4318, loss_cls=0.0835, loss_bbox=0.4199, matched_ious=0.6167, loss_iou=0.0866, loss_iou_reg=0.1965, d_time=0.01(0.01), f_time=1.15(1.18), b_time=1.16(1.19)  Time cost: 32:51/01:56 [6:53:07/1:46:17]  Acc_iter 57950       Data time: 0.01(0.01)  Forward time: 1.15(1.18)  Batch time: 1.16(1.19)
2025-09-04 05:50:07,206   INFO  Train:   33/36 ( 92%) [1711/1759 ( 97%)]  Loss: 4.246 (3.43)  LR: 1.431e-04  Grad: 5.2817  max=1.3729(module.vfe.pfn_layers.0.linear.weight)  min: -0.6956(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4615, loss_cls=0.0850, loss_bbox=0.4130, matched_ious=0.6129, loss_iou=0.0821, loss_iou_reg=0.1895, d_time=0.00(0.01), f_time=1.27(1.18), b_time=1.28(1.19)  Time cost: 33:50/00:56 [6:54:06/1:45:17]  Acc_iter 58000       Data time: 0.00(0.01)  Forward time: 1.27(1.18)  Batch time: 1.28(1.19)
2025-09-04 05:51:02,422   INFO  Train:   33/36 ( 92%) [1758/1759 (100%)]  Loss: 2.097 (3.43)  LR: 1.406e-04  Grad: 5.4578  max=0.7886(module.vfe.pfn_layers.0.linear.weight)  min: -0.8256(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4642, loss_cls=0.0913, loss_bbox=0.3906, matched_ious=0.6206, loss_iou=0.0818, loss_iou_reg=0.1876, d_time=0.01(0.01), f_time=0.69(1.18), b_time=0.70(1.19)  Time cost: 34:46/00:01 [6:55:01/1:44:19]  Acc_iter 58047       Data time: 0.01(0.01)  Forward time: 0.69(1.18)  Batch time: 0.70(1.19)

                                               [Aepochs:  80%|████████  | 12/15 [6:55:01<1:47:08, 2142.81s/it]epochs:  80%|████████  | 12/15 [6:55:01<1:47:08, 2142.80s/it]epochs:  80%|████████  | 12/15 [6:55:01<1:47:08, 2142.82s/it]epochs:  80%|████████  | 12/15 [6:55:01<1:47:08, 2142.81s/it]epochs:  80%|████████  | 12/15 [6:55:01<1:47:08, 2142.81s/it]epochs:  80%|████████  | 12/15 [6:55:01<1:47:08, 2142.82s/it]epochs:  80%|████████  | 12/15 [6:55:01<1:47:08, 2142.82s/it]epochs:  80%|████████  | 12/15 [6:55:02<1:47:08, 2142.82s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 05:51:05,884   INFO  Train:   34/36 ( 94%) [   0/1759 (  0%)]  Loss: 3.288 (3.29)  LR: 1.406e-04  Grad: 5.3685  max=0.9889(module.backbone_3d.cls_conv.1.weight)  min: -0.4507(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5633, loss_cls=0.0693, loss_bbox=0.4001, matched_ious=0.5584, loss_iou=0.1039, loss_iou_reg=0.2252, d_time=0.63(0.63), f_time=1.85(1.85), b_time=2.48(2.48)  Time cost: 00:02/1:10:15 [6:55:05/3:30:45]  Acc_iter 58048       Data time: 0.63(0.63)  Forward time: 1.85(1.85)  Batch time: 2.48(2.48)
2025-09-04 05:51:08,799   INFO  Train:   34/36 ( 94%) [   2/1759 (  0%)]  Loss: 3.898 (3.47)  LR: 1.405e-04  Grad: 6.1829  max=2.4761(module.vfe.pfn_layers.0.linear.weight)  min: -1.5367(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3907, loss_cls=0.0613, loss_bbox=0.4328, matched_ious=0.6261, loss_iou=0.0797, loss_iou_reg=0.1942, d_time=0.01(0.22), f_time=1.60(1.58), b_time=1.60(1.80)  Time cost: 00:05/51:53 [6:55:07/2:35:48]  Acc_iter 58050       Data time: 0.01(0.22)  Forward time: 1.60(1.58)  Batch time: 1.60(1.80)
2025-09-04 05:52:09,063   INFO  Train:   34/36 ( 94%) [  52/1759 (  3%)]  Loss: 3.204 (3.25)  LR: 1.378e-04  Grad: 5.9196  max=1.5377(module.vfe.pfn_layers.0.linear.weight)  min: -1.0849(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4418, loss_cls=0.0857, loss_bbox=0.4051, matched_ious=0.6025, loss_iou=0.0861, loss_iou_reg=0.1993, d_time=0.01(0.02), f_time=1.15(1.22), b_time=1.15(1.24)  Time cost: 01:05/35:12 [6:56:08/1:47:45]  Acc_iter 58100       Data time: 0.01(0.02)  Forward time: 1.15(1.22)  Batch time: 1.15(1.24)
2025-09-04 05:53:08,783   INFO  Train:   34/36 ( 94%) [ 102/1759 (  6%)]  Loss: 3.962 (3.40)  LR: 1.353e-04  Grad: 5.8403  max=0.5780(module.vfe.pfn_layers.0.linear.weight)  min: -1.0206(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4480, loss_cls=0.0899, loss_bbox=0.4325, matched_ious=0.6016, loss_iou=0.0827, loss_iou_reg=0.1955, d_time=0.01(0.01), f_time=1.23(1.20), b_time=1.24(1.22)  Time cost: 02:05/33:35 [6:57:07/1:44:55]  Acc_iter 58150       Data time: 0.01(0.01)  Forward time: 1.23(1.20)  Batch time: 1.24(1.22)
2025-09-04 05:54:07,893   INFO  Train:   34/36 ( 94%) [ 152/1759 (  9%)]  Loss: 4.053 (3.37)  LR: 1.327e-04  Grad: 4.4821  max=0.6165(module.vfe.pfn_layers.0.linear.weight)  min: -1.0781(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4423, loss_cls=0.0827, loss_bbox=0.4285, matched_ious=0.6151, loss_iou=0.0845, loss_iou_reg=0.1966, d_time=0.01(0.01), f_time=1.17(1.19), b_time=1.18(1.21)  Time cost: 03:04/32:16 [6:58:07/1:42:57]  Acc_iter 58200       Data time: 0.01(0.01)  Forward time: 1.17(1.19)  Batch time: 1.18(1.21)
2025-09-04 05:55:06,609   INFO  Train:   34/36 ( 94%) [ 202/1759 ( 11%)]  Loss: 2.952 (3.40)  LR: 1.302e-04  Grad: 5.3473  max=0.8965(module.vfe.pfn_layers.0.linear.weight)  min: -1.1845(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4839, loss_cls=0.0876, loss_bbox=0.4343, matched_ious=0.6259, loss_iou=0.0827, loss_iou_reg=0.1868, d_time=0.01(0.01), f_time=1.26(1.19), b_time=1.27(1.20)  Time cost: 04:03/31:04 [6:59:05/1:41:18]  Acc_iter 58250       Data time: 0.01(0.01)  Forward time: 1.26(1.19)  Batch time: 1.27(1.20)
2025-09-04 05:56:06,131   INFO  Train:   34/36 ( 94%) [ 252/1759 ( 14%)]  Loss: 2.787 (3.38)  LR: 1.277e-04  Grad: 5.0131  max=0.9127(module.vfe.pfn_layers.0.linear.weight)  min: -0.8570(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4344, loss_cls=0.0811, loss_bbox=0.4057, matched_ious=0.6062, loss_iou=0.0822, loss_iou_reg=0.1978, d_time=0.04(0.01), f_time=1.60(1.19), b_time=1.63(1.20)  Time cost: 05:02/30:02 [7:00:05/1:40:11]  Acc_iter 58300       Data time: 0.04(0.01)  Forward time: 1.60(1.19)  Batch time: 1.63(1.20)
2025-09-04 05:57:05,501   INFO  Train:   34/36 ( 94%) [ 302/1759 ( 17%)]  Loss: 2.533 (3.38)  LR: 1.252e-04  Grad: 5.4414  max=0.5791(module.vfe.pfn_layers.0.linear.weight)  min: -0.6390(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.4508, loss_cls=0.0864, loss_bbox=0.4097, matched_ious=0.6030, loss_iou=0.0844, loss_iou_reg=0.1997, d_time=0.01(0.01), f_time=1.16(1.19), b_time=1.16(1.20)  Time cost: 06:02/29:00 [7:01:04/1:39:04]  Acc_iter 58350       Data time: 0.01(0.01)  Forward time: 1.16(1.19)  Batch time: 1.16(1.20)
2025-09-04 05:58:05,756   INFO  Train:   34/36 ( 94%) [ 352/1759 ( 20%)]  Loss: 3.549 (3.38)  LR: 1.227e-04  Grad: 5.8244  max=0.7304(module.backbone_3d.cls_conv.1.weight)  min: -0.6352(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.4727, loss_cls=0.0927, loss_bbox=0.4020, matched_ious=0.6028, loss_iou=0.0881, loss_iou_reg=0.2003, d_time=0.01(0.01), f_time=1.13(1.19), b_time=1.14(1.20)  Time cost: 07:02/28:03 [7:02:04/1:38:11]  Acc_iter 58400       Data time: 0.01(0.01)  Forward time: 1.13(1.19)  Batch time: 1.14(1.20)
2025-09-04 05:59:05,548   INFO  Train:   34/36 ( 94%) [ 402/1759 ( 23%)]  Loss: 3.117 (3.38)  LR: 1.202e-04  Grad: 4.7575  max=0.4701(module.vfe.pfn_layers.0.linear.weight)  min: -0.6237(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4379, loss_cls=0.0819, loss_bbox=0.3743, matched_ious=0.6166, loss_iou=0.0809, loss_iou_reg=0.1898, d_time=0.01(0.01), f_time=1.14(1.19), b_time=1.14(1.20)  Time cost: 08:02/27:03 [7:03:04/1:37:11]  Acc_iter 58450       Data time: 0.01(0.01)  Forward time: 1.14(1.19)  Batch time: 1.14(1.20)
2025-09-04 06:00:04,297   INFO  Train:   34/36 ( 94%) [ 452/1759 ( 26%)]  Loss: 5.266 (3.38)  LR: 1.178e-04  Grad: 5.3373  max=1.3692(module.vfe.pfn_layers.0.linear.weight)  min: -0.4543(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4542, loss_cls=0.0867, loss_bbox=0.4012, matched_ious=0.6073, loss_iou=0.0857, loss_iou_reg=0.1982, d_time=0.01(0.01), f_time=1.11(1.19), b_time=1.11(1.19)  Time cost: 09:00/26:00 [7:04:03/1:36:00]  Acc_iter 58500       Data time: 0.01(0.01)  Forward time: 1.11(1.19)  Batch time: 1.11(1.19)
2025-09-04 06:01:03,852   INFO  Train:   34/36 ( 94%) [ 502/1759 ( 29%)]  Loss: 4.308 (3.38)  LR: 1.154e-04  Grad: 9.1703  max=2.2326(module.backbone_3d.cls_conv.3.bias)  min: -2.3926(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4766, loss_cls=0.0904, loss_bbox=0.4356, matched_ious=0.6144, loss_iou=0.0870, loss_iou_reg=0.1935, d_time=0.01(0.01), f_time=1.12(1.18), b_time=1.13(1.19)  Time cost: 10:00/25:00 [7:05:03/1:34:59]  Acc_iter 58550       Data time: 0.01(0.01)  Forward time: 1.12(1.18)  Batch time: 1.13(1.19)
2025-09-04 06:02:02,598   INFO  Train:   34/36 ( 94%) [ 552/1759 ( 31%)]  Loss: 3.907 (3.39)  LR: 1.131e-04  Grad: 5.1481  max=0.5509(module.vfe.pfn_layers.0.linear.weight)  min: -0.6674(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4213, loss_cls=0.0824, loss_bbox=0.3984, matched_ious=0.6272, loss_iou=0.0820, loss_iou_reg=0.1881, d_time=0.01(0.01), f_time=1.19(1.18), b_time=1.20(1.19)  Time cost: 10:59/23:58 [7:06:01/1:33:51]  Acc_iter 58600       Data time: 0.01(0.01)  Forward time: 1.19(1.18)  Batch time: 1.20(1.19)
2025-09-04 06:03:01,233   INFO  Train:   34/36 ( 94%) [ 602/1759 ( 34%)]  Loss: 1.834 (3.39)  LR: 1.107e-04  Grad: 6.2524  max=1.3339(module.vfe.pfn_layers.0.linear.weight)  min: -2.2851(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4248, loss_cls=0.0822, loss_bbox=0.4373, matched_ious=0.6038, loss_iou=0.0869, loss_iou_reg=0.1982, d_time=0.01(0.01), f_time=1.14(1.18), b_time=1.14(1.19)  Time cost: 11:57/22:57 [7:07:00/1:32:44]  Acc_iter 58650       Data time: 0.01(0.01)  Forward time: 1.14(1.18)  Batch time: 1.14(1.19)
2025-09-04 06:04:00,155   INFO  Train:   34/36 ( 94%) [ 652/1759 ( 37%)]  Loss: 3.960 (3.38)  LR: 1.084e-04  Grad: 4.9164  max=0.6830(module.vfe.pfn_layers.0.linear.weight)  min: -1.2775(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4347, loss_cls=0.0803, loss_bbox=0.4265, matched_ious=0.5981, loss_iou=0.0854, loss_iou_reg=0.2023, d_time=0.01(0.01), f_time=1.18(1.18), b_time=1.19(1.19)  Time cost: 12:56/21:56 [7:07:59/1:31:40]  Acc_iter 58700       Data time: 0.01(0.01)  Forward time: 1.18(1.18)  Batch time: 1.19(1.19)
2025-09-04 06:04:59,196   INFO  Train:   34/36 ( 94%) [ 702/1759 ( 40%)]  Loss: 2.921 (3.37)  LR: 1.061e-04  Grad: 4.3899  max=1.3863(module.vfe.pfn_layers.0.linear.weight)  min: -0.8663(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4514, loss_cls=0.0871, loss_bbox=0.3741, matched_ious=0.5995, loss_iou=0.0861, loss_iou_reg=0.1949, d_time=0.01(0.01), f_time=1.10(1.18), b_time=1.10(1.19)  Time cost: 13:55/20:56 [7:08:58/1:30:38]  Acc_iter 58750       Data time: 0.01(0.01)  Forward time: 1.10(1.18)  Batch time: 1.10(1.19)
2025-09-04 06:05:59,318   INFO  Train:   34/36 ( 94%) [ 752/1759 ( 43%)]  Loss: 2.780 (3.37)  LR: 1.038e-04  Grad: 4.8877  max=0.1832(module.backbone_3d.cls_conv.1.weight)  min: -1.9929(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4565, loss_cls=0.0840, loss_bbox=0.4195, matched_ious=0.6138, loss_iou=0.0825, loss_iou_reg=0.1891, d_time=0.01(0.01), f_time=1.18(1.18), b_time=1.19(1.19)  Time cost: 14:55/19:58 [7:09:58/1:29:43]  Acc_iter 58800       Data time: 0.01(0.01)  Forward time: 1.18(1.18)  Batch time: 1.19(1.19)
2025-09-04 06:06:58,926   INFO  Train:   34/36 ( 94%) [ 802/1759 ( 46%)]  Loss: 4.269 (3.37)  LR: 1.015e-04  Grad: 4.3139  max=0.5411(module.vfe.pfn_layers.0.linear.weight)  min: -0.2797(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4131, loss_cls=0.0761, loss_bbox=0.3849, matched_ious=0.6171, loss_iou=0.0835, loss_iou_reg=0.1871, d_time=0.01(0.01), f_time=1.09(1.18), b_time=1.10(1.19)  Time cost: 15:55/18:58 [7:10:58/1:28:44]  Acc_iter 58850       Data time: 0.01(0.01)  Forward time: 1.09(1.18)  Batch time: 1.10(1.19)
2025-09-04 06:07:58,290   INFO  Train:   34/36 ( 94%) [ 852/1759 ( 48%)]  Loss: 3.503 (3.36)  LR: 9.931e-05  Grad: 3.1476  max=0.3931(module.vfe.pfn_layers.0.linear.weight)  min: -0.4255(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4511, loss_cls=0.0802, loss_bbox=0.3943, matched_ious=0.6213, loss_iou=0.0807, loss_iou_reg=0.1911, d_time=0.00(0.01), f_time=1.16(1.18), b_time=1.16(1.19)  Time cost: 16:54/17:59 [7:11:57/1:27:44]  Acc_iter 58900       Data time: 0.00(0.01)  Forward time: 1.16(1.18)  Batch time: 1.16(1.19)
2025-09-04 06:08:57,233   INFO  Train:   34/36 ( 94%) [ 902/1759 ( 51%)]  Loss: 5.299 (3.36)  LR: 9.711e-05  Grad: 4.1905  max=0.6694(module.vfe.pfn_layers.0.linear.weight)  min: -0.5812(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4581, loss_cls=0.0940, loss_bbox=0.4187, matched_ious=0.6075, loss_iou=0.0853, loss_iou_reg=0.1998, d_time=0.01(0.01), f_time=1.14(1.18), b_time=1.15(1.19)  Time cost: 17:53/16:59 [7:12:56/1:26:42]  Acc_iter 58950       Data time: 0.01(0.01)  Forward time: 1.14(1.18)  Batch time: 1.15(1.19)
2025-09-04 06:09:56,501   INFO  Train:   34/36 ( 94%) [ 952/1759 ( 54%)]  Loss: 2.774 (3.34)  LR: 9.492e-05  Grad: 5.2627  max=1.9645(module.vfe.pfn_layers.0.linear.weight)  min: -0.8297(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4323, loss_cls=0.0833, loss_bbox=0.3800, matched_ious=0.6036, loss_iou=0.0871, loss_iou_reg=0.1937, d_time=0.01(0.01), f_time=1.12(1.18), b_time=1.13(1.19)  Time cost: 18:53/15:59 [7:13:55/1:25:41]  Acc_iter 59000       Data time: 0.01(0.01)  Forward time: 1.12(1.18)  Batch time: 1.13(1.19)
2025-09-04 06:10:56,203   INFO  Train:   34/36 ( 94%) [1002/1759 ( 57%)]  Loss: 3.309 (3.34)  LR: 9.277e-05  Grad: 4.7532  max=0.5204(module.vfe.pfn_layers.0.linear.weight)  min: -1.1037(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4177, loss_cls=0.0831, loss_bbox=0.4053, matched_ious=0.5979, loss_iou=0.0837, loss_iou_reg=0.1972, d_time=0.01(0.01), f_time=1.13(1.18), b_time=1.14(1.19)  Time cost: 19:52/15:00 [7:14:55/1:24:43]  Acc_iter 59050       Data time: 0.01(0.01)  Forward time: 1.13(1.18)  Batch time: 1.14(1.19)
2025-09-04 06:11:55,738   INFO  Train:   34/36 ( 94%) [1052/1759 ( 60%)]  Loss: 2.822 (3.33)  LR: 9.063e-05  Grad: 5.6696  max=0.9438(module.backbone_3d.cls_conv.1.weight)  min: -0.8312(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4015, loss_cls=0.0743, loss_bbox=0.3755, matched_ious=0.6137, loss_iou=0.0811, loss_iou_reg=0.1893, d_time=0.01(0.01), f_time=1.27(1.18), b_time=1.28(1.19)  Time cost: 20:52/14:00 [7:15:54/1:23:44]  Acc_iter 59100       Data time: 0.01(0.01)  Forward time: 1.27(1.18)  Batch time: 1.28(1.19)
2025-09-04 06:12:55,023   INFO  Train:   34/36 ( 94%) [1102/1759 ( 63%)]  Loss: 3.722 (3.33)  LR: 8.852e-05  Grad: 5.7984  max=1.3489(module.vfe.pfn_layers.0.linear.weight)  min: -3.3786(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4124, loss_cls=0.0781, loss_bbox=0.4554, matched_ious=0.6131, loss_iou=0.0824, loss_iou_reg=0.1889, d_time=0.01(0.01), f_time=1.16(1.18), b_time=1.17(1.19)  Time cost: 21:51/13:01 [7:16:54/1:22:44]  Acc_iter 59150       Data time: 0.01(0.01)  Forward time: 1.16(1.18)  Batch time: 1.17(1.19)
2025-09-04 06:13:54,340   INFO  Train:   34/36 ( 94%) [1152/1759 ( 65%)]  Loss: 6.438 (3.33)  LR: 8.643e-05  Grad: 5.0297  max=1.2378(module.vfe.pfn_layers.0.linear.weight)  min: -0.4266(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.4457, loss_cls=0.0853, loss_bbox=0.4017, matched_ious=0.6053, loss_iou=0.0810, loss_iou_reg=0.1913, d_time=0.00(0.01), f_time=1.21(1.18), b_time=1.22(1.19)  Time cost: 22:50/12:01 [7:17:53/1:21:44]  Acc_iter 59200       Data time: 0.00(0.01)  Forward time: 1.21(1.18)  Batch time: 1.22(1.19)
2025-09-04 06:14:53,509   INFO  Train:   34/36 ( 94%) [1202/1759 ( 68%)]  Loss: 2.432 (3.33)  LR: 8.437e-05  Grad: 5.1760  max=0.3616(module.vfe.pfn_layers.0.linear.weight)  min: -0.4787(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4443, loss_cls=0.0847, loss_bbox=0.4040, matched_ious=0.6064, loss_iou=0.0852, loss_iou_reg=0.1951, d_time=0.01(0.01), f_time=1.26(1.18), b_time=1.26(1.19)  Time cost: 23:50/11:02 [7:18:52/1:20:44]  Acc_iter 59250       Data time: 0.01(0.01)  Forward time: 1.26(1.18)  Batch time: 1.26(1.19)
2025-09-04 06:15:52,831   INFO  Train:   34/36 ( 94%) [1252/1759 ( 71%)]  Loss: 4.765 (3.33)  LR: 8.233e-05  Grad: 5.4778  max=0.7267(module.backbone_3d.cls_conv.1.weight)  min: -0.9579(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4576, loss_cls=0.0909, loss_bbox=0.4063, matched_ious=0.6121, loss_iou=0.0823, loss_iou_reg=0.1893, d_time=0.01(0.01), f_time=1.14(1.18), b_time=1.14(1.19)  Time cost: 24:49/10:02 [7:19:51/1:19:44]  Acc_iter 59300       Data time: 0.01(0.01)  Forward time: 1.14(1.18)  Batch time: 1.14(1.19)
2025-09-04 06:16:52,281   INFO  Train:   34/36 ( 94%) [1302/1759 ( 74%)]  Loss: 4.010 (3.33)  LR: 8.032e-05  Grad: 6.6473  max=1.5841(module.backbone_3d.cls_conv.1.weight)  min: -1.7797(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4172, loss_cls=0.0901, loss_bbox=0.3729, matched_ious=0.6057, loss_iou=0.0812, loss_iou_reg=0.1913, d_time=0.01(0.01), f_time=1.10(1.18), b_time=1.10(1.19)  Time cost: 25:48/09:03 [7:20:51/1:18:44]  Acc_iter 59350       Data time: 0.01(0.01)  Forward time: 1.10(1.18)  Batch time: 1.10(1.19)
2025-09-04 06:17:52,265   INFO  Train:   34/36 ( 94%) [1352/1759 ( 77%)]  Loss: 2.719 (3.33)  LR: 7.833e-05  Grad: 5.7523  max=0.5434(module.vfe.pfn_layers.0.linear.weight)  min: -0.7342(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4351, loss_cls=0.0894, loss_bbox=0.3797, matched_ious=0.6023, loss_iou=0.0876, loss_iou_reg=0.1987, d_time=0.01(0.01), f_time=1.21(1.18), b_time=1.22(1.19)  Time cost: 26:48/08:03 [7:21:51/1:17:47]  Acc_iter 59400       Data time: 0.01(0.01)  Forward time: 1.21(1.18)  Batch time: 1.22(1.19)
2025-09-04 06:18:51,737   INFO  Train:   34/36 ( 94%) [1402/1759 ( 80%)]  Loss: 3.195 (3.33)  LR: 7.637e-05  Grad: 5.7101  max=0.6644(module.vfe.pfn_layers.0.linear.weight)  min: -0.4494(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4632, loss_cls=0.0926, loss_bbox=0.4391, matched_ious=0.6038, loss_iou=0.0869, loss_iou_reg=0.1995, d_time=0.01(0.01), f_time=1.16(1.18), b_time=1.17(1.19)  Time cost: 27:48/07:04 [7:22:50/1:16:47]  Acc_iter 59450       Data time: 0.01(0.01)  Forward time: 1.16(1.18)  Batch time: 1.17(1.19)
2025-09-04 06:19:50,831   INFO  Train:   34/36 ( 94%) [1452/1759 ( 83%)]  Loss: 3.766 (3.32)  LR: 7.442e-05  Grad: 6.7859  max=2.2137(module.vfe.pfn_layers.0.linear.weight)  min: -0.6526(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.4556, loss_cls=0.0835, loss_bbox=0.4041, matched_ious=0.6135, loss_iou=0.0867, loss_iou_reg=0.1940, d_time=0.01(0.01), f_time=1.13(1.18), b_time=1.14(1.19)  Time cost: 28:47/06:04 [7:23:49/1:15:47]  Acc_iter 59500       Data time: 0.01(0.01)  Forward time: 1.13(1.18)  Batch time: 1.14(1.19)
2025-09-04 06:20:49,743   INFO  Train:   34/36 ( 94%) [1502/1759 ( 85%)]  Loss: 3.416 (3.33)  LR: 7.251e-05  Grad: 7.3582  max=1.4703(module.vfe.pfn_layers.0.linear.weight)  min: -2.9771(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4722, loss_cls=0.0870, loss_bbox=0.4122, matched_ious=0.6119, loss_iou=0.0840, loss_iou_reg=0.1915, d_time=0.01(0.01), f_time=1.12(1.18), b_time=1.12(1.19)  Time cost: 29:46/05:05 [7:24:48/1:14:46]  Acc_iter 59550       Data time: 0.01(0.01)  Forward time: 1.12(1.18)  Batch time: 1.12(1.19)
2025-09-04 06:21:48,169   INFO  Train:   34/36 ( 94%) [1552/1759 ( 88%)]  Loss: 3.522 (3.34)  LR: 7.062e-05  Grad: 6.8437  max=0.8857(module.backbone_3d.cls_conv.1.weight)  min: -1.0968(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4725, loss_cls=0.0863, loss_bbox=0.4346, matched_ious=0.6158, loss_iou=0.0834, loss_iou_reg=0.1918, d_time=0.01(0.01), f_time=1.07(1.18), b_time=1.08(1.19)  Time cost: 30:44/04:05 [7:25:47/1:13:44]  Acc_iter 59600       Data time: 0.01(0.01)  Forward time: 1.07(1.18)  Batch time: 1.08(1.19)
2025-09-04 06:22:47,587   INFO  Train:   34/36 ( 94%) [1602/1759 ( 91%)]  Loss: 3.842 (3.34)  LR: 6.875e-05  Grad: 7.2697  max=1.7991(module.vfe.pfn_layers.0.linear.weight)  min: -2.7573(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4244, loss_cls=0.0839, loss_bbox=0.3937, matched_ious=0.6115, loss_iou=0.0843, loss_iou_reg=0.1939, d_time=0.01(0.01), f_time=1.23(1.18), b_time=1.24(1.19)  Time cost: 31:44/03:06 [7:26:46/1:12:45]  Acc_iter 59650       Data time: 0.01(0.01)  Forward time: 1.23(1.18)  Batch time: 1.24(1.19)
2025-09-04 06:23:47,005   INFO  Train:   34/36 ( 94%) [1652/1759 ( 94%)]  Loss: 2.969 (3.34)  LR: 6.691e-05  Grad: 6.2586  max=0.6667(module.vfe.pfn_layers.0.linear.weight)  min: -0.4055(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4607, loss_cls=0.0891, loss_bbox=0.4360, matched_ious=0.5982, loss_iou=0.0838, loss_iou_reg=0.1973, d_time=0.01(0.01), f_time=1.24(1.18), b_time=1.24(1.19)  Time cost: 32:43/02:07 [7:27:46/1:11:45]  Acc_iter 59700       Data time: 0.01(0.01)  Forward time: 1.24(1.18)  Batch time: 1.24(1.19)
2025-09-04 06:24:46,680   INFO  Train:   34/36 ( 94%) [1702/1759 ( 97%)]  Loss: 2.942 (3.34)  LR: 6.509e-05  Grad: 6.5230  max=0.9552(module.vfe.pfn_layers.0.linear.weight)  min: -0.6727(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4582, loss_cls=0.0913, loss_bbox=0.4095, matched_ious=0.6078, loss_iou=0.0894, loss_iou_reg=0.2000, d_time=0.00(0.01), f_time=1.22(1.18), b_time=1.23(1.19)  Time cost: 33:43/01:07 [7:28:45/1:10:47]  Acc_iter 59750       Data time: 0.00(0.01)  Forward time: 1.22(1.18)  Batch time: 1.23(1.19)
2025-09-04 06:25:46,121   INFO  Train:   34/36 ( 94%) [1752/1759 (100%)]  Loss: 6.671 (3.34)  LR: 6.329e-05  Grad: 6.8702  max=1.0173(module.vfe.pfn_layers.0.linear.weight)  min: -1.1144(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4356, loss_cls=0.0908, loss_bbox=0.3956, matched_ious=0.6120, loss_iou=0.0828, loss_iou_reg=0.1908, d_time=0.01(0.01), f_time=1.18(1.18), b_time=1.19(1.19)  Time cost: 34:42/00:08 [7:29:45/1:09:47]  Acc_iter 59800       Data time: 0.01(0.01)  Forward time: 1.18(1.18)  Batch time: 1.19(1.19)
2025-09-04 06:25:52,651   INFO  Train:   34/36 ( 94%) [1758/1759 (100%)]  Loss: 3.514 (3.34)  LR: 6.308e-05  Grad: 8.8225  max=3.2843(module.vfe.pfn_layers.0.linear.weight)  min: -1.4090(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5120, loss_cls=0.1003, loss_bbox=0.3587, matched_ious=0.6335, loss_iou=0.0826, loss_iou_reg=0.2001, d_time=0.01(0.01), f_time=0.70(1.18), b_time=0.70(1.19)  Time cost: 34:49/00:01 [7:29:51/1:09:39]  Acc_iter 59806       Data time: 0.01(0.01)  Forward time: 0.70(1.18)  Batch time: 0.70(1.19)

                                               [Aepochs:  87%|████████▋ | 13/15 [7:29:51<1:10:53, 2126.86s/it]epochs:  87%|████████▋ | 13/15 [7:29:51<1:10:53, 2126.88s/it]epochs:  87%|████████▋ | 13/15 [7:29:51<1:10:53, 2126.87s/it]epochs:  87%|████████▋ | 13/15 [7:29:52<1:10:53, 2126.91s/it]epochs:  87%|████████▋ | 13/15 [7:29:52<1:10:53, 2126.89s/it]epochs:  87%|████████▋ | 13/15 [7:29:52<1:10:53, 2126.91s/it]epochs:  87%|████████▋ | 13/15 [7:29:52<1:10:53, 2126.90s/it]epochs:  87%|████████▋ | 13/15 [7:29:52<1:10:53, 2126.90s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 06:25:56,193   INFO  Train:   35/36 ( 97%) [   0/1759 (  0%)]  Loss: 4.299 (4.30)  LR: 6.304e-05  Grad: 7.1749  max=1.3748(module.vfe.pfn_layers.0.linear.weight)  min: -1.2363(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6619, loss_cls=0.0925, loss_bbox=1.0452, matched_ious=0.6251, loss_iou=0.0896, loss_iou_reg=0.1654, d_time=0.76(0.76), f_time=1.80(1.80), b_time=2.56(2.56)  Time cost: 00:02/1:11:11 [7:29:55/2:22:22]  Acc_iter 59807       Data time: 0.76(0.76)  Forward time: 1.80(1.80)  Batch time: 2.56(2.56)
2025-09-04 06:26:48,275   INFO  Train:   35/36 ( 97%) [  43/1759 (  2%)]  Loss: 2.876 (3.45)  LR: 6.152e-05  Grad: 5.8236  max=0.5321(module.vfe.pfn_layers.0.linear.weight)  min: -0.3446(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4745, loss_cls=0.0886, loss_bbox=0.4147, matched_ious=0.5970, loss_iou=0.0876, loss_iou_reg=0.2015, d_time=0.01(0.02), f_time=1.11(1.22), b_time=1.11(1.24)  Time cost: 00:54/35:25 [7:30:47/1:11:45]  Acc_iter 59850       Data time: 0.01(0.02)  Forward time: 1.11(1.22)  Batch time: 1.11(1.24)
2025-09-04 06:27:48,415   INFO  Train:   35/36 ( 97%) [  93/1759 (  5%)]  Loss: 2.336 (3.57)  LR: 5.978e-05  Grad: 5.2704  max=0.6587(module.backbone_3d.cls_conv.1.weight)  min: -2.1967(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4452, loss_cls=0.0888, loss_bbox=0.4159, matched_ious=0.6032, loss_iou=0.0857, loss_iou_reg=0.1988, d_time=0.01(0.02), f_time=1.19(1.21), b_time=1.20(1.22)  Time cost: 01:54/33:51 [7:31:47/1:09:37]  Acc_iter 59900       Data time: 0.01(0.02)  Forward time: 1.19(1.21)  Batch time: 1.20(1.22)
2025-09-04 06:28:48,322   INFO  Train:   35/36 ( 97%) [ 143/1759 (  8%)]  Loss: 2.963 (3.53)  LR: 5.806e-05  Grad: 4.7817  max=0.9929(module.vfe.pfn_layers.0.linear.weight)  min: -0.2690(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4765, loss_cls=0.0873, loss_bbox=0.4432, matched_ious=0.6071, loss_iou=0.0848, loss_iou_reg=0.1955, d_time=0.01(0.01), f_time=1.18(1.20), b_time=1.19(1.21)  Time cost: 02:54/32:38 [7:32:47/1:08:11]  Acc_iter 59950       Data time: 0.01(0.01)  Forward time: 1.18(1.20)  Batch time: 1.19(1.21)
2025-09-04 06:29:48,471   INFO  Train:   35/36 ( 97%) [ 193/1759 ( 11%)]  Loss: 2.255 (3.45)  LR: 5.636e-05  Grad: 5.6539  max=0.6112(module.vfe.pfn_layers.0.linear.weight)  min: -0.8689(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.4447, loss_cls=0.0840, loss_bbox=0.4028, matched_ious=0.6212, loss_iou=0.0820, loss_iou_reg=0.1895, d_time=0.01(0.01), f_time=1.23(1.20), b_time=1.24(1.21)  Time cost: 03:54/31:34 [7:33:47/1:07:02]  Acc_iter 60000       Data time: 0.01(0.01)  Forward time: 1.23(1.20)  Batch time: 1.24(1.21)
2025-09-04 06:30:47,954   INFO  Train:   35/36 ( 97%) [ 243/1759 ( 14%)]  Loss: 3.464 (3.46)  LR: 5.469e-05  Grad: 6.5151  max=0.9793(module.vfe.pfn_layers.0.linear.weight)  min: -0.7319(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.4900, loss_cls=0.0966, loss_bbox=0.4456, matched_ious=0.6147, loss_iou=0.0833, loss_iou_reg=0.1909, d_time=0.01(0.01), f_time=1.17(1.20), b_time=1.18(1.21)  Time cost: 04:54/30:27 [7:34:47/1:05:48]  Acc_iter 60050       Data time: 0.01(0.01)  Forward time: 1.17(1.20)  Batch time: 1.18(1.21)
2025-09-04 06:31:46,997   INFO  Train:   35/36 ( 97%) [ 293/1759 ( 17%)]  Loss: 4.120 (3.47)  LR: 5.304e-05  Grad: 6.2380  max=1.6779(module.vfe.pfn_layers.0.linear.weight)  min: -0.3931(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4381, loss_cls=0.0816, loss_bbox=0.4270, matched_ious=0.6198, loss_iou=0.0854, loss_iou_reg=0.1921, d_time=0.01(0.01), f_time=1.24(1.19), b_time=1.25(1.20)  Time cost: 05:53/29:21 [7:35:46/1:04:34]  Acc_iter 60100       Data time: 0.01(0.01)  Forward time: 1.24(1.19)  Batch time: 1.25(1.20)
2025-09-04 06:32:46,590   INFO  Train:   35/36 ( 97%) [ 343/1759 ( 19%)]  Loss: 3.324 (3.41)  LR: 5.142e-05  Grad: 5.8638  max=0.6983(module.vfe.pfn_layers.0.linear.weight)  min: -0.8606(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4275, loss_cls=0.0784, loss_bbox=0.3902, matched_ious=0.6122, loss_iou=0.0855, loss_iou_reg=0.1984, d_time=0.01(0.01), f_time=1.20(1.19), b_time=1.21(1.20)  Time cost: 06:52/28:19 [7:36:45/1:03:30]  Acc_iter 60150       Data time: 0.01(0.01)  Forward time: 1.20(1.19)  Batch time: 1.21(1.20)
2025-09-04 06:33:45,975   INFO  Train:   35/36 ( 97%) [ 393/1759 ( 22%)]  Loss: 3.068 (3.44)  LR: 4.983e-05  Grad: 6.3259  max=0.7698(module.backbone_3d.cls_conv.1.weight)  min: -1.2212(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4830, loss_cls=0.0967, loss_bbox=0.4169, matched_ious=0.5984, loss_iou=0.0881, loss_iou_reg=0.2020, d_time=0.01(0.01), f_time=1.21(1.19), b_time=1.21(1.20)  Time cost: 07:52/27:17 [7:37:45/1:02:25]  Acc_iter 60200       Data time: 0.01(0.01)  Forward time: 1.21(1.19)  Batch time: 1.21(1.20)
2025-09-04 06:34:44,731   INFO  Train:   35/36 ( 97%) [ 443/1759 ( 25%)]  Loss: 2.974 (3.42)  LR: 4.825e-05  Grad: 4.4065  max=1.7029(module.vfe.pfn_layers.0.linear.weight)  min: -0.7646(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4680, loss_cls=0.0808, loss_bbox=0.4028, matched_ious=0.6163, loss_iou=0.0852, loss_iou_reg=0.1920, d_time=0.01(0.01), f_time=1.14(1.19), b_time=1.15(1.20)  Time cost: 08:50/26:13 [7:38:43/1:01:17]  Acc_iter 60250       Data time: 0.01(0.01)  Forward time: 1.14(1.19)  Batch time: 1.15(1.20)
2025-09-04 06:35:43,798   INFO  Train:   35/36 ( 97%) [ 493/1759 ( 28%)]  Loss: 4.282 (3.40)  LR: 4.671e-05  Grad: 7.3712  max=2.7060(module.vfe.pfn_layers.0.linear.weight)  min: -3.4939(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4469, loss_cls=0.0900, loss_bbox=0.3736, matched_ious=0.6139, loss_iou=0.0823, loss_iou_reg=0.1931, d_time=0.01(0.01), f_time=1.11(1.19), b_time=1.12(1.19)  Time cost: 09:50/25:12 [7:39:42/1:00:13]  Acc_iter 60300       Data time: 0.01(0.01)  Forward time: 1.11(1.19)  Batch time: 1.12(1.19)
2025-09-04 06:36:42,937   INFO  Train:   35/36 ( 97%) [ 543/1759 ( 31%)]  Loss: 3.158 (3.39)  LR: 4.518e-05  Grad: 5.4581  max=0.8263(module.backbone_3d.cls_conv.3.bias)  min: -1.4531(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4376, loss_cls=0.0802, loss_bbox=0.4076, matched_ious=0.6210, loss_iou=0.0790, loss_iou_reg=0.1897, d_time=0.01(0.01), f_time=1.26(1.18), b_time=1.26(1.19)  Time cost: 10:49/24:11 [7:40:42/59:10]  Acc_iter 60350       Data time: 0.01(0.01)  Forward time: 1.26(1.18)  Batch time: 1.26(1.19)
2025-09-04 06:37:42,712   INFO  Train:   35/36 ( 97%) [ 593/1759 ( 34%)]  Loss: 5.032 (3.39)  LR: 4.369e-05  Grad: 5.1990  max=0.8096(module.vfe.pfn_layers.0.linear.weight)  min: -0.6495(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4535, loss_cls=0.0869, loss_bbox=0.4068, matched_ious=0.6004, loss_iou=0.0854, loss_iou_reg=0.1948, d_time=0.01(0.01), f_time=1.21(1.18), b_time=1.22(1.19)  Time cost: 11:48/23:11 [7:41:41/58:11]  Acc_iter 60400       Data time: 0.01(0.01)  Forward time: 1.21(1.18)  Batch time: 1.22(1.19)
2025-09-04 06:38:41,889   INFO  Train:   35/36 ( 97%) [ 643/1759 ( 37%)]  Loss: 3.968 (3.39)  LR: 4.221e-05  Grad: 5.9327  max=1.3707(module.vfe.pfn_layers.0.linear.weight)  min: -1.3852(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4261, loss_cls=0.0844, loss_bbox=0.3838, matched_ious=0.6034, loss_iou=0.0855, loss_iou_reg=0.1971, d_time=0.01(0.01), f_time=1.20(1.18), b_time=1.21(1.19)  Time cost: 12:48/22:11 [7:42:41/57:09]  Acc_iter 60450       Data time: 0.01(0.01)  Forward time: 1.20(1.18)  Batch time: 1.21(1.19)
2025-09-04 06:39:41,040   INFO  Train:   35/36 ( 97%) [ 693/1759 ( 39%)]  Loss: 3.358 (3.39)  LR: 4.076e-05  Grad: 5.6185  max=0.7004(module.backbone_3d.cls_conv.1.weight)  min: -0.6428(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4730, loss_cls=0.0951, loss_bbox=0.3854, matched_ious=0.6098, loss_iou=0.0868, loss_iou_reg=0.2009, d_time=0.01(0.01), f_time=1.16(1.18), b_time=1.17(1.19)  Time cost: 13:47/21:10 [7:43:40/56:07]  Acc_iter 60500       Data time: 0.01(0.01)  Forward time: 1.16(1.18)  Batch time: 1.17(1.19)
2025-09-04 06:40:39,814   INFO  Train:   35/36 ( 97%) [ 743/1759 ( 42%)]  Loss: 2.382 (3.38)  LR: 3.934e-05  Grad: 4.7677  max=0.6382(module.backbone_3d.cls_conv.1.weight)  min: -0.7580(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4377, loss_cls=0.0817, loss_bbox=0.4206, matched_ious=0.6052, loss_iou=0.0883, loss_iou_reg=0.1976, d_time=0.01(0.01), f_time=1.14(1.18), b_time=1.15(1.19)  Time cost: 14:46/20:09 [7:44:38/55:04]  Acc_iter 60550       Data time: 0.01(0.01)  Forward time: 1.14(1.18)  Batch time: 1.15(1.19)
2025-09-04 06:41:39,271   INFO  Train:   35/36 ( 97%) [ 793/1759 ( 45%)]  Loss: 3.528 (3.38)  LR: 3.794e-05  Grad: 6.5544  max=1.4437(module.vfe.pfn_layers.0.linear.weight)  min: -1.5464(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4338, loss_cls=0.0811, loss_bbox=0.4039, matched_ious=0.6095, loss_iou=0.0836, loss_iou_reg=0.1987, d_time=0.01(0.01), f_time=1.27(1.18), b_time=1.28(1.19)  Time cost: 15:45/19:10 [7:45:38/54:04]  Acc_iter 60600       Data time: 0.01(0.01)  Forward time: 1.27(1.18)  Batch time: 1.28(1.19)
2025-09-04 06:42:39,989   INFO  Train:   35/36 ( 97%) [ 843/1759 ( 48%)]  Loss: 2.795 (3.37)  LR: 3.657e-05  Grad: 4.5816  max=1.9686(module.vfe.pfn_layers.0.linear.weight)  min: -0.9180(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4366, loss_cls=0.0851, loss_bbox=0.3667, matched_ious=0.6250, loss_iou=0.0854, loss_iou_reg=0.1884, d_time=0.01(0.01), f_time=1.18(1.18), b_time=1.18(1.19)  Time cost: 16:46/18:12 [7:46:39/53:09]  Acc_iter 60650       Data time: 0.01(0.01)  Forward time: 1.18(1.18)  Batch time: 1.18(1.19)
2025-09-04 06:43:38,229   INFO  Train:   35/36 ( 97%) [ 893/1759 ( 51%)]  Loss: 3.301 (3.37)  LR: 3.522e-05  Grad: 4.6947  max=0.9123(module.vfe.pfn_layers.0.linear.weight)  min: -0.6714(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4405, loss_cls=0.0849, loss_bbox=0.3922, matched_ious=0.6058, loss_iou=0.0864, loss_iou_reg=0.1935, d_time=0.01(0.01), f_time=1.25(1.18), b_time=1.26(1.19)  Time cost: 17:44/17:11 [7:47:37/52:05]  Acc_iter 60700       Data time: 0.01(0.01)  Forward time: 1.25(1.18)  Batch time: 1.26(1.19)
2025-09-04 06:44:37,787   INFO  Train:   35/36 ( 97%) [ 943/1759 ( 54%)]  Loss: 5.082 (3.37)  LR: 3.390e-05  Grad: 5.3180  max=0.5409(module.backbone_3d.cls_conv.1.weight)  min: -0.7090(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.4409, loss_cls=0.0810, loss_bbox=0.3944, matched_ious=0.6122, loss_iou=0.0895, loss_iou_reg=0.1989, d_time=0.01(0.01), f_time=1.12(1.18), b_time=1.13(1.19)  Time cost: 18:44/16:11 [7:48:36/51:06]  Acc_iter 60750       Data time: 0.01(0.01)  Forward time: 1.12(1.18)  Batch time: 1.13(1.19)
2025-09-04 06:45:36,913   INFO  Train:   35/36 ( 97%) [ 993/1759 ( 56%)]  Loss: 3.769 (3.36)  LR: 3.260e-05  Grad: 5.5544  max=1.7125(module.vfe.pfn_layers.0.linear.weight)  min: -0.8216(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4470, loss_cls=0.0891, loss_bbox=0.3932, matched_ious=0.6021, loss_iou=0.0817, loss_iou_reg=0.1926, d_time=0.01(0.01), f_time=1.16(1.18), b_time=1.16(1.19)  Time cost: 19:43/15:11 [7:49:36/50:05]  Acc_iter 60800       Data time: 0.01(0.01)  Forward time: 1.16(1.18)  Batch time: 1.16(1.19)
2025-09-04 06:46:36,177   INFO  Train:   35/36 ( 97%) [1043/1759 ( 59%)]  Loss: 3.374 (3.36)  LR: 3.133e-05  Grad: 5.1664  max=0.5905(module.vfe.pfn_layers.0.linear.weight)  min: -0.6825(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4145, loss_cls=0.0803, loss_bbox=0.3858, matched_ious=0.6163, loss_iou=0.0795, loss_iou_reg=0.1926, d_time=0.01(0.01), f_time=1.18(1.18), b_time=1.19(1.19)  Time cost: 20:42/14:12 [7:50:35/49:05]  Acc_iter 60850       Data time: 0.01(0.01)  Forward time: 1.18(1.18)  Batch time: 1.19(1.19)
2025-09-04 06:47:35,905   INFO  Train:   35/36 ( 97%) [1093/1759 ( 62%)]  Loss: 1.536 (3.35)  LR: 3.008e-05  Grad: 5.7184  max=0.3921(module.vfe.pfn_layers.0.linear.weight)  min: -0.4669(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.3921, loss_cls=0.0716, loss_bbox=0.3742, matched_ious=0.6192, loss_iou=0.0780, loss_iou_reg=0.1849, d_time=0.01(0.01), f_time=1.29(1.18), b_time=1.29(1.19)  Time cost: 21:42/13:12 [7:51:35/48:06]  Acc_iter 60900       Data time: 0.01(0.01)  Forward time: 1.29(1.18)  Batch time: 1.29(1.19)
2025-09-04 06:48:34,869   INFO  Train:   35/36 ( 97%) [1143/1759 ( 65%)]  Loss: 2.326 (3.35)  LR: 2.886e-05  Grad: 4.7716  max=0.6805(module.backbone_3d.cls_conv.3.weight)  min: -0.8822(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4356, loss_cls=0.0889, loss_bbox=0.4121, matched_ious=0.6019, loss_iou=0.0850, loss_iou_reg=0.1963, d_time=0.01(0.01), f_time=1.10(1.18), b_time=1.10(1.19)  Time cost: 22:41/12:12 [7:52:34/47:05]  Acc_iter 60950       Data time: 0.01(0.01)  Forward time: 1.10(1.18)  Batch time: 1.10(1.19)
2025-09-04 06:49:33,393   INFO  Train:   35/36 ( 97%) [1193/1759 ( 68%)]  Loss: 3.703 (3.35)  LR: 2.766e-05  Grad: 4.6635  max=0.4990(module.backbone_3d.cls_conv.1.bias)  min: -1.1886(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4566, loss_cls=0.0870, loss_bbox=0.4236, matched_ious=0.5901, loss_iou=0.0869, loss_iou_reg=0.1995, d_time=0.01(0.01), f_time=1.10(1.18), b_time=1.11(1.19)  Time cost: 23:39/11:12 [7:53:32/46:04]  Acc_iter 61000       Data time: 0.01(0.01)  Forward time: 1.10(1.18)  Batch time: 1.11(1.19)
2025-09-04 06:50:33,103   INFO  Train:   35/36 ( 97%) [1243/1759 ( 71%)]  Loss: 4.515 (3.35)  LR: 2.649e-05  Grad: 5.5789  max=1.6182(module.vfe.pfn_layers.0.linear.weight)  min: -0.9476(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4571, loss_cls=0.0827, loss_bbox=0.4117, matched_ious=0.6086, loss_iou=0.0831, loss_iou_reg=0.1949, d_time=0.01(0.01), f_time=1.16(1.18), b_time=1.16(1.19)  Time cost: 24:39/10:13 [7:54:32/45:05]  Acc_iter 61050       Data time: 0.01(0.01)  Forward time: 1.16(1.18)  Batch time: 1.16(1.19)
2025-09-04 06:51:32,178   INFO  Train:   35/36 ( 97%) [1293/1759 ( 74%)]  Loss: 4.055 (3.36)  LR: 2.534e-05  Grad: 6.0213  max=0.4805(module.backbone_3d.cls_conv.3.bias)  min: -1.1342(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4692, loss_cls=0.0910, loss_bbox=0.4472, matched_ious=0.5938, loss_iou=0.0837, loss_iou_reg=0.1954, d_time=0.01(0.01), f_time=1.11(1.18), b_time=1.12(1.19)  Time cost: 25:38/09:14 [7:55:31/44:05]  Acc_iter 61100       Data time: 0.01(0.01)  Forward time: 1.11(1.18)  Batch time: 1.12(1.19)
2025-09-04 06:52:31,845   INFO  Train:   35/36 ( 97%) [1343/1759 ( 76%)]  Loss: 3.992 (3.35)  LR: 2.422e-05  Grad: 5.6123  max=0.7651(module.vfe.pfn_layers.0.linear.weight)  min: -0.7453(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4529, loss_cls=0.0821, loss_bbox=0.3916, matched_ious=0.6164, loss_iou=0.0849, loss_iou_reg=0.1897, d_time=0.01(0.01), f_time=1.15(1.18), b_time=1.15(1.19)  Time cost: 26:38/08:14 [7:56:31/43:06]  Acc_iter 61150       Data time: 0.01(0.01)  Forward time: 1.15(1.18)  Batch time: 1.15(1.19)
2025-09-04 06:53:30,434   INFO  Train:   35/36 ( 97%) [1393/1759 ( 79%)]  Loss: 4.967 (3.35)  LR: 2.312e-05  Grad: 5.7583  max=0.6814(module.vfe.pfn_layers.0.linear.weight)  min: -1.0171(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4181, loss_cls=0.0843, loss_bbox=0.3667, matched_ious=0.6084, loss_iou=0.0858, loss_iou_reg=0.1921, d_time=0.01(0.01), f_time=1.11(1.18), b_time=1.11(1.19)  Time cost: 27:36/07:14 [7:57:29/42:05]  Acc_iter 61200       Data time: 0.01(0.01)  Forward time: 1.11(1.18)  Batch time: 1.11(1.19)
2025-09-04 06:54:29,378   INFO  Train:   35/36 ( 97%) [1443/1759 ( 82%)]  Loss: 3.342 (3.34)  LR: 2.205e-05  Grad: 7.3353  max=2.0294(module.vfe.pfn_layers.0.linear.weight)  min: -0.8882(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4364, loss_cls=0.0865, loss_bbox=0.4168, matched_ious=0.6120, loss_iou=0.0868, loss_iou_reg=0.1981, d_time=0.01(0.01), f_time=1.20(1.18), b_time=1.20(1.19)  Time cost: 28:35/06:15 [7:58:28/41:05]  Acc_iter 61250       Data time: 0.01(0.01)  Forward time: 1.20(1.18)  Batch time: 1.20(1.19)
2025-09-04 06:55:28,520   INFO  Train:   35/36 ( 97%) [1493/1759 ( 85%)]  Loss: 3.385 (3.34)  LR: 2.101e-05  Grad: 7.5861  max=1.2960(module.vfe.pfn_layers.0.linear.weight)  min: -2.0183(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4350, loss_cls=0.0847, loss_bbox=0.3763, matched_ious=0.6123, loss_iou=0.0831, loss_iou_reg=0.1922, d_time=0.01(0.01), f_time=1.09(1.18), b_time=1.10(1.19)  Time cost: 29:34/05:15 [7:59:27/40:05]  Acc_iter 61300       Data time: 0.01(0.01)  Forward time: 1.09(1.18)  Batch time: 1.10(1.19)
2025-09-04 06:56:28,128   INFO  Train:   35/36 ( 97%) [1543/1759 ( 88%)]  Loss: 4.354 (3.35)  LR: 1.999e-05  Grad: 6.7918  max=0.6062(module.vfe.pfn_layers.0.linear.weight)  min: -0.9032(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4522, loss_cls=0.0863, loss_bbox=0.3987, matched_ious=0.5943, loss_iou=0.0896, loss_iou_reg=0.2025, d_time=0.01(0.01), f_time=1.32(1.18), b_time=1.33(1.19)  Time cost: 30:34/04:16 [8:00:27/39:06]  Acc_iter 61350       Data time: 0.01(0.01)  Forward time: 1.32(1.18)  Batch time: 1.33(1.19)
2025-09-04 06:57:27,684   INFO  Train:   35/36 ( 97%) [1593/1759 ( 91%)]  Loss: 4.293 (3.35)  LR: 1.899e-05  Grad: 7.2371  max=0.5948(module.vfe.pfn_layers.0.linear.weight)  min: -0.8428(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.4677, loss_cls=0.0802, loss_bbox=0.4436, matched_ious=0.6029, loss_iou=0.0882, loss_iou_reg=0.1983, d_time=0.01(0.01), f_time=1.14(1.18), b_time=1.14(1.19)  Time cost: 31:33/03:17 [8:01:26/38:07]  Acc_iter 61400       Data time: 0.01(0.01)  Forward time: 1.14(1.18)  Batch time: 1.14(1.19)
2025-09-04 06:58:27,517   INFO  Train:   35/36 ( 97%) [1643/1759 ( 93%)]  Loss: 2.027 (3.34)  LR: 1.802e-05  Grad: 7.1619  max=0.8727(module.vfe.pfn_layers.0.linear.weight)  min: -0.7987(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4529, loss_cls=0.0862, loss_bbox=0.4194, matched_ious=0.6080, loss_iou=0.0835, loss_iou_reg=0.1947, d_time=0.01(0.01), f_time=1.20(1.18), b_time=1.21(1.19)  Time cost: 32:33/02:17 [8:02:26/37:08]  Acc_iter 61450       Data time: 0.01(0.01)  Forward time: 1.20(1.18)  Batch time: 1.21(1.19)
2025-09-04 06:59:26,946   INFO  Train:   35/36 ( 97%) [1693/1759 ( 96%)]  Loss: 4.878 (3.35)  LR: 1.708e-05  Grad: 7.5572  max=0.7499(module.vfe.pfn_layers.0.linear.weight)  min: -0.7341(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4466, loss_cls=0.0863, loss_bbox=0.3919, matched_ious=0.6160, loss_iou=0.0867, loss_iou_reg=0.1939, d_time=0.09(0.01), f_time=1.20(1.18), b_time=1.30(1.19)  Time cost: 33:33/01:18 [8:03:26/36:08]  Acc_iter 61500       Data time: 0.09(0.01)  Forward time: 1.20(1.18)  Batch time: 1.30(1.19)
2025-09-04 07:00:26,309   INFO  Train:   35/36 ( 97%) [1743/1759 ( 99%)]  Loss: 3.466 (3.35)  LR: 1.616e-05  Grad: 7.7056  max=1.0111(module.vfe.pfn_layers.0.linear.weight)  min: -3.9605(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5066, loss_cls=0.0947, loss_bbox=0.5076, matched_ious=0.6241, loss_iou=0.0813, loss_iou_reg=0.1842, d_time=0.01(0.01), f_time=1.19(1.18), b_time=1.20(1.19)  Time cost: 34:32/00:19 [8:04:25/35:09]  Acc_iter 61550       Data time: 0.01(0.01)  Forward time: 1.19(1.18)  Batch time: 1.20(1.19)
2025-09-04 07:00:43,019   INFO  Train:   35/36 ( 97%) [1758/1759 (100%)]  Loss: 3.262 (3.36)  LR: 1.588e-05  Grad: 6.5720  max=0.9204(module.vfe.pfn_layers.0.linear.weight)  min: -1.3541(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4430, loss_cls=0.0913, loss_bbox=0.4434, matched_ious=0.5765, loss_iou=0.0801, loss_iou_reg=0.1969, d_time=0.00(0.01), f_time=0.61(1.18), b_time=0.62(1.19)  Time cost: 34:49/00:01 [8:04:42/34:50]  Acc_iter 61565       Data time: 0.00(0.01)  Forward time: 0.61(1.18)  Batch time: 0.62(1.19)

                                               [Aepochs:  93%|█████████▎| 14/15 [8:04:42<35:15, 2115.84s/it]  epochs:  93%|█████████▎| 14/15 [8:04:42<35:15, 2115.83s/it]  epochs:  93%|█████████▎| 14/15 [8:04:42<35:15, 2115.86s/it]  epochs:  93%|█████████▎| 14/15 [8:04:42<35:15, 2115.87s/it]  epochs:  93%|█████████▎| 14/15 [8:04:42<35:15, 2115.85s/it]  epochs:  93%|█████████▎| 14/15 [8:04:42<35:15, 2115.86s/it]  epochs:  93%|█████████▎| 14/15 [8:04:42<35:15, 2115.87s/it]  epochs:  93%|█████████▎| 14/15 [8:04:42<35:15, 2115.91s/it]  
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 07:00:46,536   INFO  Train:   36/36 (100%) [   0/1759 (  0%)]  Loss: 3.542 (3.54)  LR: 1.587e-05  Grad: 5.9565  max=0.5425(module.vfe.pfn_layers.0.linear.weight)  min: -0.5895(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4687, loss_cls=0.0848, loss_bbox=0.4150, matched_ious=0.5818, loss_iou=0.0843, loss_iou_reg=0.2021, d_time=0.75(0.75), f_time=1.77(1.77), b_time=2.52(2.52)  Time cost: 00:02/1:06:31 [8:04:45/1:06:31]  Acc_iter 61566       Data time: 0.75(0.75)  Forward time: 1.77(1.77)  Batch time: 2.52(2.52)
2025-09-04 07:01:26,886   INFO  Train:   36/36 (100%) [  34/1759 (  2%)]  Loss: 2.286 (3.39)  LR: 1.526e-05  Grad: 6.2365  max=0.8920(module.vfe.pfn_layers.0.linear.weight)  min: -0.3757(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4579, loss_cls=0.0918, loss_bbox=0.4320, matched_ious=0.6134, loss_iou=0.0838, loss_iou_reg=0.1985, d_time=0.01(0.03), f_time=1.19(1.20), b_time=1.20(1.22)  Time cost: 00:42/35:00 [8:05:26/35:00]  Acc_iter 61600       Data time: 0.01(0.03)  Forward time: 1.19(1.20)  Batch time: 1.20(1.22)
2025-09-04 07:02:27,164   INFO  Train:   36/36 (100%) [  84/1759 (  5%)]  Loss: 4.025 (3.34)  LR: 1.439e-05  Grad: 6.8476  max=1.6846(module.vfe.pfn_layers.0.linear.weight)  min: -2.7320(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4344, loss_cls=0.0863, loss_bbox=0.3926, matched_ious=0.6105, loss_iou=0.0863, loss_iou_reg=0.1955, d_time=0.00(0.02), f_time=1.14(1.20), b_time=1.15(1.21)  Time cost: 01:42/33:47 [8:06:26/33:47]  Acc_iter 61650       Data time: 0.00(0.02)  Forward time: 1.14(1.20)  Batch time: 1.15(1.21)
2025-09-04 07:03:26,303   INFO  Train:   36/36 (100%) [ 134/1759 (  8%)]  Loss: 2.072 (3.35)  LR: 1.355e-05  Grad: 3.7830  max=0.9509(module.vfe.pfn_layers.0.linear.weight)  min: -0.6078(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4428, loss_cls=0.0889, loss_bbox=0.4129, matched_ious=0.6047, loss_iou=0.0864, loss_iou_reg=0.1969, d_time=0.01(0.01), f_time=1.20(1.19), b_time=1.21(1.20)  Time cost: 02:42/32:30 [8:07:25/32:30]  Acc_iter 61700       Data time: 0.01(0.01)  Forward time: 1.20(1.19)  Batch time: 1.21(1.20)
2025-09-04 07:04:26,242   INFO  Train:   36/36 (100%) [ 184/1759 ( 10%)]  Loss: 3.083 (3.25)  LR: 1.273e-05  Grad: 5.2145  max=1.4659(module.vfe.pfn_layers.0.linear.weight)  min: -1.3583(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3734, loss_cls=0.0761, loss_bbox=0.3586, matched_ious=0.6284, loss_iou=0.0816, loss_iou_reg=0.1870, d_time=0.01(0.01), f_time=1.39(1.19), b_time=1.40(1.20)  Time cost: 03:41/31:29 [8:08:25/31:29]  Acc_iter 61750       Data time: 0.01(0.01)  Forward time: 1.39(1.19)  Batch time: 1.40(1.20)
2025-09-04 07:05:27,519   INFO  Train:   36/36 (100%) [ 234/1759 ( 13%)]  Loss: 2.161 (3.23)  LR: 1.194e-05  Grad: 4.9929  max=0.8192(module.vfe.pfn_layers.0.linear.weight)  min: -0.6695(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4325, loss_cls=0.0816, loss_bbox=0.3974, matched_ious=0.6188, loss_iou=0.0833, loss_iou_reg=0.1897, d_time=0.01(0.01), f_time=1.20(1.20), b_time=1.20(1.21)  Time cost: 04:43/30:38 [8:09:26/30:38]  Acc_iter 61800       Data time: 0.01(0.01)  Forward time: 1.20(1.20)  Batch time: 1.20(1.21)
2025-09-04 07:06:26,532   INFO  Train:   36/36 (100%) [ 284/1759 ( 16%)]  Loss: 1.340 (3.23)  LR: 1.117e-05  Grad: 6.1062  max=1.3693(module.vfe.pfn_layers.0.linear.weight)  min: -1.5797(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.3957, loss_cls=0.0790, loss_bbox=0.3633, matched_ious=0.6165, loss_iou=0.0838, loss_iou_reg=0.1912, d_time=0.01(0.01), f_time=1.09(1.19), b_time=1.10(1.20)  Time cost: 05:42/29:31 [8:10:25/29:31]  Acc_iter 61850       Data time: 0.01(0.01)  Forward time: 1.09(1.19)  Batch time: 1.10(1.20)
2025-09-04 07:07:26,464   INFO  Train:   36/36 (100%) [ 334/1759 ( 19%)]  Loss: 4.388 (3.25)  LR: 1.043e-05  Grad: 5.8473  max=0.5897(module.backbone_3d.cls_conv.1.weight)  min: -0.8153(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4283, loss_cls=0.0849, loss_bbox=0.4020, matched_ious=0.6173, loss_iou=0.0821, loss_iou_reg=0.1894, d_time=0.01(0.01), f_time=1.18(1.19), b_time=1.19(1.20)  Time cost: 06:42/28:30 [8:11:25/28:30]  Acc_iter 61900       Data time: 0.01(0.01)  Forward time: 1.18(1.19)  Batch time: 1.19(1.20)
2025-09-04 07:08:25,856   INFO  Train:   36/36 (100%) [ 384/1759 ( 22%)]  Loss: 2.544 (3.23)  LR: 9.714e-06  Grad: 6.3267  max=0.8506(module.vfe.pfn_layers.0.linear.weight)  min: -0.9326(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4000, loss_cls=0.0778, loss_bbox=0.3546, matched_ious=0.6288, loss_iou=0.0814, loss_iou_reg=0.1879, d_time=0.01(0.01), f_time=1.22(1.19), b_time=1.23(1.20)  Time cost: 07:41/27:28 [8:12:25/27:28]  Acc_iter 61950       Data time: 0.01(0.01)  Forward time: 1.22(1.19)  Batch time: 1.23(1.20)
2025-09-04 07:09:26,277   INFO  Train:   36/36 (100%) [ 434/1759 ( 25%)]  Loss: 2.991 (3.23)  LR: 9.023e-06  Grad: 6.8222  max=0.7743(module.vfe.pfn_layers.0.linear.weight)  min: -1.5863(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4335, loss_cls=0.0793, loss_bbox=0.3765, matched_ious=0.6169, loss_iou=0.0826, loss_iou_reg=0.1907, d_time=0.01(0.01), f_time=1.38(1.19), b_time=1.39(1.20)  Time cost: 08:42/26:30 [8:13:25/26:30]  Acc_iter 62000       Data time: 0.01(0.01)  Forward time: 1.38(1.19)  Batch time: 1.39(1.20)
2025-09-04 07:10:25,836   INFO  Train:   36/36 (100%) [ 484/1759 ( 28%)]  Loss: 3.777 (3.25)  LR: 8.358e-06  Grad: 7.0493  max=0.7143(module.vfe.pfn_layers.0.linear.weight)  min: -1.7151(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4824, loss_cls=0.0915, loss_bbox=0.4214, matched_ious=0.5987, loss_iou=0.0846, loss_iou_reg=0.1964, d_time=0.01(0.01), f_time=1.06(1.19), b_time=1.07(1.20)  Time cost: 09:41/25:28 [8:14:24/25:28]  Acc_iter 62050       Data time: 0.01(0.01)  Forward time: 1.06(1.19)  Batch time: 1.07(1.20)
2025-09-04 07:11:25,674   INFO  Train:   36/36 (100%) [ 534/1759 ( 30%)]  Loss: 2.213 (3.25)  LR: 7.718e-06  Grad: 5.4004  max=0.3397(module.vfe.pfn_layers.0.linear.weight)  min: -0.4710(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4315, loss_cls=0.0857, loss_bbox=0.4085, matched_ious=0.6131, loss_iou=0.0860, loss_iou_reg=0.1902, d_time=0.01(0.01), f_time=1.17(1.19), b_time=1.18(1.20)  Time cost: 10:41/24:28 [8:15:24/24:28]  Acc_iter 62100       Data time: 0.01(0.01)  Forward time: 1.17(1.19)  Batch time: 1.18(1.20)
2025-09-04 07:12:24,246   INFO  Train:   36/36 (100%) [ 584/1759 ( 33%)]  Loss: 3.637 (3.25)  LR: 7.104e-06  Grad: 5.6185  max=0.4234(module.vfe.pfn_layers.0.linear.weight)  min: -0.5520(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4216, loss_cls=0.0797, loss_bbox=0.4144, matched_ious=0.6224, loss_iou=0.0839, loss_iou_reg=0.1896, d_time=0.01(0.01), f_time=1.17(1.19), b_time=1.18(1.20)  Time cost: 11:39/23:25 [8:16:23/23:25]  Acc_iter 62150       Data time: 0.01(0.01)  Forward time: 1.17(1.19)  Batch time: 1.18(1.20)
2025-09-04 07:13:24,013   INFO  Train:   36/36 (100%) [ 634/1759 ( 36%)]  Loss: 3.401 (3.25)  LR: 6.515e-06  Grad: 5.6295  max=0.5830(module.backbone_3d.cls_conv.1.weight)  min: -0.6737(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4073, loss_cls=0.0766, loss_bbox=0.4257, matched_ious=0.5974, loss_iou=0.0870, loss_iou_reg=0.2024, d_time=0.01(0.01), f_time=1.14(1.19), b_time=1.15(1.20)  Time cost: 12:39/22:26 [8:17:23/22:26]  Acc_iter 62200       Data time: 0.01(0.01)  Forward time: 1.14(1.19)  Batch time: 1.15(1.20)
2025-09-04 07:14:23,437   INFO  Train:   36/36 (100%) [ 684/1759 ( 39%)]  Loss: 2.672 (3.24)  LR: 5.952e-06  Grad: 6.0386  max=1.1893(module.vfe.pfn_layers.0.linear.weight)  min: -0.8288(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4312, loss_cls=0.0802, loss_bbox=0.4023, matched_ious=0.6058, loss_iou=0.0855, loss_iou_reg=0.1980, d_time=0.01(0.01), f_time=1.20(1.19), b_time=1.20(1.20)  Time cost: 13:39/21:25 [8:18:22/21:25]  Acc_iter 62250       Data time: 0.01(0.01)  Forward time: 1.20(1.19)  Batch time: 1.20(1.20)
2025-09-04 07:15:23,254   INFO  Train:   36/36 (100%) [ 734/1759 ( 42%)]  Loss: 4.057 (3.25)  LR: 5.414e-06  Grad: 5.5544  max=0.3330(module.backbone_3d.cls_conv.1.weight)  min: -0.6825(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4290, loss_cls=0.0828, loss_bbox=0.4141, matched_ious=0.6082, loss_iou=0.0865, loss_iou_reg=0.1998, d_time=0.01(0.01), f_time=1.16(1.19), b_time=1.17(1.20)  Time cost: 14:38/20:25 [8:19:22/20:25]  Acc_iter 62300       Data time: 0.01(0.01)  Forward time: 1.16(1.19)  Batch time: 1.17(1.20)
2025-09-04 07:16:22,645   INFO  Train:   36/36 (100%) [ 784/1759 ( 45%)]  Loss: 2.491 (3.25)  LR: 4.902e-06  Grad: 6.9101  max=1.2090(module.backbone_3d.cls_conv.3.bias)  min: -1.3578(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4792, loss_cls=0.0826, loss_bbox=0.4349, matched_ious=0.6174, loss_iou=0.0835, loss_iou_reg=0.1925, d_time=0.01(0.01), f_time=1.13(1.19), b_time=1.13(1.20)  Time cost: 15:38/19:25 [8:20:21/19:25]  Acc_iter 62350       Data time: 0.01(0.01)  Forward time: 1.13(1.19)  Batch time: 1.13(1.20)
2025-09-04 07:17:22,321   INFO  Train:   36/36 (100%) [ 834/1759 ( 47%)]  Loss: 3.151 (3.27)  LR: 4.415e-06  Grad: 5.3707  max=0.9209(module.vfe.pfn_layers.0.linear.weight)  min: -0.5005(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4369, loss_cls=0.0856, loss_bbox=0.3992, matched_ious=0.6175, loss_iou=0.0848, loss_iou_reg=0.1910, d_time=0.01(0.01), f_time=1.27(1.19), b_time=1.27(1.20)  Time cost: 16:38/18:25 [8:21:21/18:25]  Acc_iter 62400       Data time: 0.01(0.01)  Forward time: 1.27(1.19)  Batch time: 1.27(1.20)
2025-09-04 07:18:21,301   INFO  Train:   36/36 (100%) [ 884/1759 ( 50%)]  Loss: 4.429 (3.28)  LR: 3.954e-06  Grad: 7.5357  max=1.1526(module.vfe.pfn_layers.0.linear.weight)  min: -1.7427(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4245, loss_cls=0.0802, loss_bbox=0.4082, matched_ious=0.6042, loss_iou=0.0857, loss_iou_reg=0.1973, d_time=0.00(0.01), f_time=1.14(1.19), b_time=1.15(1.19)  Time cost: 17:37/17:25 [8:22:20/17:25]  Acc_iter 62450       Data time: 0.00(0.01)  Forward time: 1.14(1.19)  Batch time: 1.15(1.19)
2025-09-04 07:19:21,126   INFO  Train:   36/36 (100%) [ 934/1759 ( 53%)]  Loss: 2.410 (3.27)  LR: 3.519e-06  Grad: 6.2069  max=0.4522(module.vfe.pfn_layers.0.linear.weight)  min: -0.5146(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4585, loss_cls=0.0840, loss_bbox=0.4207, matched_ious=0.6070, loss_iou=0.0879, loss_iou_reg=0.2001, d_time=0.01(0.01), f_time=1.18(1.19), b_time=1.19(1.19)  Time cost: 18:36/16:25 [8:23:20/16:25]  Acc_iter 62500       Data time: 0.01(0.01)  Forward time: 1.18(1.19)  Batch time: 1.19(1.19)
2025-09-04 07:20:21,272   INFO  Train:   36/36 (100%) [ 984/1759 ( 56%)]  Loss: 2.800 (3.28)  LR: 3.109e-06  Grad: 6.5956  max=0.7419(module.vfe.pfn_layers.0.linear.weight)  min: -0.7411(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4398, loss_cls=0.0813, loss_bbox=0.3730, matched_ious=0.6167, loss_iou=0.0863, loss_iou_reg=0.1894, d_time=0.01(0.01), f_time=1.25(1.19), b_time=1.26(1.20)  Time cost: 19:37/15:26 [8:24:20/15:26]  Acc_iter 62550       Data time: 0.01(0.01)  Forward time: 1.25(1.19)  Batch time: 1.26(1.20)
2025-09-04 07:21:20,876   INFO  Train:   36/36 (100%) [1034/1759 ( 59%)]  Loss: 4.979 (3.29)  LR: 2.724e-06  Grad: 7.2212  max=0.4667(module.backbone_3d.cls_conv.1.bias)  min: -0.6123(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.4943, loss_cls=0.0904, loss_bbox=0.4300, matched_ious=0.6029, loss_iou=0.0878, loss_iou_reg=0.1966, d_time=0.01(0.01), f_time=1.21(1.19), b_time=1.22(1.20)  Time cost: 20:36/14:26 [8:25:20/14:26]  Acc_iter 62600       Data time: 0.01(0.01)  Forward time: 1.21(1.19)  Batch time: 1.22(1.20)
2025-09-04 07:22:20,863   INFO  Train:   36/36 (100%) [1084/1759 ( 62%)]  Loss: 3.629 (3.29)  LR: 2.366e-06  Grad: 6.3535  max=1.2481(module.vfe.pfn_layers.0.linear.weight)  min: -0.5471(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4709, loss_cls=0.0882, loss_bbox=0.4374, matched_ious=0.6006, loss_iou=0.0839, loss_iou_reg=0.1948, d_time=0.01(0.01), f_time=1.19(1.19), b_time=1.20(1.20)  Time cost: 21:36/13:26 [8:26:20/13:26]  Acc_iter 62650       Data time: 0.01(0.01)  Forward time: 1.19(1.19)  Batch time: 1.20(1.20)
2025-09-04 07:23:19,866   INFO  Train:   36/36 (100%) [1134/1759 ( 64%)]  Loss: 2.918 (3.29)  LR: 2.032e-06  Grad: 8.2616  max=1.6115(module.vfe.pfn_layers.0.linear.weight)  min: -1.2970(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.4482, loss_cls=0.0841, loss_bbox=0.4180, matched_ious=0.6040, loss_iou=0.0890, loss_iou_reg=0.1951, d_time=0.01(0.01), f_time=1.28(1.19), b_time=1.29(1.19)  Time cost: 22:35/12:26 [8:27:19/12:26]  Acc_iter 62700       Data time: 0.01(0.01)  Forward time: 1.28(1.19)  Batch time: 1.29(1.19)
2025-09-04 07:24:20,334   INFO  Train:   36/36 (100%) [1184/1759 ( 67%)]  Loss: 2.266 (3.29)  LR: 1.725e-06  Grad: 5.2607  max=0.4168(module.vfe.pfn_layers.0.linear.weight)  min: -0.4971(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4268, loss_cls=0.0825, loss_bbox=0.3858, matched_ious=0.6246, loss_iou=0.0829, loss_iou_reg=0.1857, d_time=0.01(0.01), f_time=1.22(1.19), b_time=1.22(1.20)  Time cost: 23:36/11:27 [8:28:19/11:27]  Acc_iter 62750       Data time: 0.01(0.01)  Forward time: 1.22(1.19)  Batch time: 1.22(1.20)
2025-09-04 07:25:20,254   INFO  Train:   36/36 (100%) [1234/1759 ( 70%)]  Loss: 3.025 (3.29)  LR: 1.443e-06  Grad: 6.1947  max=0.5892(module.vfe.pfn_layers.0.linear.weight)  min: -1.7813(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4427, loss_cls=0.0844, loss_bbox=0.4157, matched_ious=0.6137, loss_iou=0.0845, loss_iou_reg=0.1922, d_time=0.01(0.01), f_time=1.06(1.19), b_time=1.06(1.20)  Time cost: 24:35/10:27 [8:29:19/10:27]  Acc_iter 62800       Data time: 0.01(0.01)  Forward time: 1.06(1.19)  Batch time: 1.06(1.20)
2025-09-04 07:26:19,809   INFO  Train:   36/36 (100%) [1284/1759 ( 73%)]  Loss: 4.638 (3.28)  LR: 1.187e-06  Grad: 6.6787  max=1.1465(module.vfe.pfn_layers.0.linear.weight)  min: -0.9671(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4390, loss_cls=0.0834, loss_bbox=0.4053, matched_ious=0.6069, loss_iou=0.0848, loss_iou_reg=0.1961, d_time=0.01(0.01), f_time=1.13(1.19), b_time=1.14(1.20)  Time cost: 25:35/09:27 [8:30:18/09:27]  Acc_iter 62850       Data time: 0.01(0.01)  Forward time: 1.13(1.19)  Batch time: 1.14(1.20)
2025-09-04 07:27:20,180   INFO  Train:   36/36 (100%) [1334/1759 ( 76%)]  Loss: 3.434 (3.29)  LR: 9.561e-07  Grad: 6.9421  max=0.7928(module.vfe.pfn_layers.0.linear.weight)  min: -0.6849(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4386, loss_cls=0.0811, loss_bbox=0.4111, matched_ious=0.6055, loss_iou=0.0844, loss_iou_reg=0.1972, d_time=0.01(0.01), f_time=1.09(1.19), b_time=1.09(1.20)  Time cost: 26:35/08:28 [8:31:19/08:28]  Acc_iter 62900       Data time: 0.01(0.01)  Forward time: 1.09(1.19)  Batch time: 1.09(1.20)
2025-09-04 07:28:18,845   INFO  Train:   36/36 (100%) [1384/1759 ( 79%)]  Loss: 3.587 (3.28)  LR: 7.510e-07  Grad: 7.4761  max=0.8655(module.vfe.pfn_layers.0.linear.weight)  min: -1.0151(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4314, loss_cls=0.0876, loss_bbox=0.4004, matched_ious=0.6152, loss_iou=0.0853, loss_iou_reg=0.1911, d_time=0.01(0.01), f_time=1.17(1.19), b_time=1.17(1.19)  Time cost: 27:34/07:27 [8:32:18/07:27]  Acc_iter 62950       Data time: 0.01(0.01)  Forward time: 1.17(1.19)  Batch time: 1.17(1.19)
2025-09-04 07:29:17,212   INFO  Train:   36/36 (100%) [1434/1759 ( 82%)]  Loss: 2.845 (3.28)  LR: 5.716e-07  Grad: 8.1553  max=1.0565(module.vfe.pfn_layers.0.linear.weight)  min: -1.6807(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4302, loss_cls=0.0786, loss_bbox=0.4164, matched_ious=0.6107, loss_iou=0.0842, loss_iou_reg=0.1975, d_time=0.01(0.01), f_time=1.14(1.19), b_time=1.15(1.19)  Time cost: 28:32/06:27 [8:33:16/06:27]  Acc_iter 63000       Data time: 0.01(0.01)  Forward time: 1.14(1.19)  Batch time: 1.15(1.19)
2025-09-04 07:30:16,468   INFO  Train:   36/36 (100%) [1484/1759 ( 84%)]  Loss: 3.787 (3.28)  LR: 4.177e-07  Grad: 7.2015  max=0.6263(module.vfe.pfn_layers.0.linear.weight)  min: -0.8312(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4280, loss_cls=0.0878, loss_bbox=0.3945, matched_ious=0.6048, loss_iou=0.0862, loss_iou_reg=0.1981, d_time=0.01(0.01), f_time=1.08(1.19), b_time=1.09(1.19)  Time cost: 29:32/05:28 [8:34:15/05:28]  Acc_iter 63050       Data time: 0.01(0.01)  Forward time: 1.08(1.19)  Batch time: 1.09(1.19)
2025-09-04 07:31:15,905   INFO  Train:   36/36 (100%) [1534/1759 ( 87%)]  Loss: 1.872 (3.29)  LR: 2.896e-07  Grad: 3.9159  max=0.5960(module.vfe.pfn_layers.0.linear.weight)  min: -0.6454(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4762, loss_cls=0.0930, loss_bbox=0.4179, matched_ious=0.6075, loss_iou=0.0831, loss_iou_reg=0.1937, d_time=0.01(0.01), f_time=1.31(1.19), b_time=1.32(1.19)  Time cost: 30:31/04:28 [8:35:15/04:28]  Acc_iter 63100       Data time: 0.01(0.01)  Forward time: 1.31(1.19)  Batch time: 1.32(1.19)
2025-09-04 07:32:16,112   INFO  Train:   36/36 (100%) [1584/1759 ( 90%)]  Loss: 4.971 (3.29)  LR: 1.870e-07  Grad: 4.7258  max=1.0810(module.vfe.pfn_layers.0.linear.weight)  min: -0.5455(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4632, loss_cls=0.0881, loss_bbox=0.3974, matched_ious=0.6018, loss_iou=0.0855, loss_iou_reg=0.1970, d_time=0.01(0.01), f_time=1.33(1.19), b_time=1.34(1.19)  Time cost: 31:31/03:28 [8:36:15/03:28]  Acc_iter 63150       Data time: 0.01(0.01)  Forward time: 1.33(1.19)  Batch time: 1.34(1.19)
2025-09-04 07:33:13,948   INFO  Train:   36/36 (100%) [1634/1759 ( 93%)]  Loss: 2.351 (3.29)  LR: 1.101e-07  Grad: 4.3496  max=0.8873(module.vfe.pfn_layers.0.linear.weight)  min: -0.3532(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4213, loss_cls=0.0848, loss_bbox=0.3725, matched_ious=0.6079, loss_iou=0.0860, loss_iou_reg=0.1986, d_time=0.01(0.01), f_time=1.09(1.19), b_time=1.09(1.19)  Time cost: 32:29/02:29 [8:37:13/02:29]  Acc_iter 63200       Data time: 0.01(0.01)  Forward time: 1.09(1.19)  Batch time: 1.09(1.19)
2025-09-04 07:34:13,334   INFO  Train:   36/36 (100%) [1684/1759 ( 96%)]  Loss: 2.727 (3.29)  LR: 5.884e-08  Grad: 5.4056  max=0.5434(module.backbone_3d.cls_conv.1.weight)  min: -0.7799(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4665, loss_cls=0.0901, loss_bbox=0.4251, matched_ious=0.6007, loss_iou=0.0881, loss_iou_reg=0.1997, d_time=0.00(0.01), f_time=1.26(1.18), b_time=1.27(1.19)  Time cost: 33:29/01:29 [8:38:12/01:29]  Acc_iter 63250       Data time: 0.00(0.01)  Forward time: 1.26(1.18)  Batch time: 1.27(1.19)
2025-09-04 07:35:13,099   INFO  Train:   36/36 (100%) [1734/1759 ( 99%)]  Loss: 4.254 (3.29)  LR: 3.320e-08  Grad: 6.0485  max=0.8364(module.vfe.pfn_layers.0.linear.weight)  min: -0.6560(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4412, loss_cls=0.0853, loss_bbox=0.3899, matched_ious=0.6232, loss_iou=0.0818, loss_iou_reg=0.1886, d_time=0.01(0.01), f_time=1.09(1.18), b_time=1.10(1.19)  Time cost: 34:28/00:29 [8:39:12/00:29]  Acc_iter 63300       Data time: 0.01(0.01)  Forward time: 1.09(1.18)  Batch time: 1.10(1.19)
2025-09-04 07:35:40,475   INFO  Train:   36/36 (100%) [1758/1759 (100%)]  Loss: 5.293 (3.29)  LR: 3.001e-08  Grad: 7.1111  max=1.1838(module.vfe.pfn_layers.0.linear.weight)  min: -1.7050(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.4341, loss_cls=0.0792, loss_bbox=0.3967, matched_ious=0.6026, loss_iou=0.0868, loss_iou_reg=0.1967, d_time=0.07(0.01), f_time=0.71(1.18), b_time=0.78(1.19)  Time cost: 34:56/00:01 [8:39:39/00:01]  Acc_iter 63324       Data time: 0.07(0.01)  Forward time: 0.71(1.18)  Batch time: 0.78(1.19)
train:   0%|          | 0/1759 [34:56<?, ?it/s]
epochs: 100%|██████████| 15/15 [8:39:39<00:00, 2110.31s/it]                                                           epochs: 100%|██████████| 15/15 [8:39:39<00:00, 2110.29s/it]                                                           epochs: 100%|██████████| 15/15 [8:39:39<00:00, 2110.30s/it]                                                           epochs: 100%|██████████| 15/15 [8:39:39<00:00, 2110.31s/it]                                                           epochs: 100%|██████████| 15/15 [8:39:39<00:00, 2110.31s/it]                                                           epochs: 100%|██████████| 15/15 [8:39:39<00:00, 2110.32s/it]                                                           epochs: 100%|██████████| 15/15 [8:39:39<00:00, 2110.32s/it]                                                           epochs: 100%|██████████| 15/15 [8:39:40<00:00, 2110.29s/it]epochs: 100%|██████████| 15/15 [8:39:40<00:00, 2078.67s/it]
2025-09-04 07:35:41,041   INFO  **********************End training sparse_models/sparse_former_base(default)**********************



2025-09-04 07:35:41,042   INFO  **********************Start evaluation sparse_models/sparse_former_base(default)**********************
2025-09-04 07:35:41,042   INFO  Loading NuScenes dataset
2025-09-04 07:35:41,445   INFO  Total samples for NuScenes dataset: 6019
2025-09-04 07:35:41,451   INFO  ==> Loading parameters from checkpoint /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_.0diff_dense_hm_.1lr_.1wd/ckpt/checkpoint_epoch_36.pth to CPU
2025-09-04 07:35:41,861   INFO  ==> Checkpoint trained from version: pcdet+0.6.0+8694e7f
2025-09-04 07:35:41,898   INFO  ==> Done (loaded 877/877)
2025-09-04 07:35:41,907   INFO  *************** EPOCH 36 EVALUATION *****************
eval:   0%|          | 0/377 [00:00<?, ?it/s]eval:   0%|          | 0/377 [00:01<?, ?it/s, recall_0.3=(0, 59) / 69]eval:   0%|          | 1/377 [00:01<09:33,  1.53s/it, recall_0.3=(0, 59) / 69]eval:   0%|          | 1/377 [00:01<09:33,  1.53s/it, recall_0.3=(0, 121) / 141]eval:   1%|          | 2/377 [00:01<05:08,  1.21it/s, recall_0.3=(0, 121) / 141]eval:   1%|          | 2/377 [00:02<05:08,  1.21it/s, recall_0.3=(0, 144) / 171]eval:   1%|          | 3/377 [00:02<03:44,  1.67it/s, recall_0.3=(0, 144) / 171]eval:   1%|          | 3/377 [00:02<03:44,  1.67it/s, recall_0.3=(0, 151) / 178]eval:   1%|          | 4/377 [00:02<03:14,  1.92it/s, recall_0.3=(0, 151) / 178]eval:   1%|          | 4/377 [00:03<03:14,  1.92it/s, recall_0.3=(0, 154) / 181]eval:   1%|▏         | 5/377 [00:03<02:59,  2.07it/s, recall_0.3=(0, 154) / 181]eval:   1%|▏         | 5/377 [00:03<02:59,  2.07it/s, recall_0.3=(0, 156) / 187]eval:   2%|▏         | 6/377 [00:03<02:40,  2.31it/s, recall_0.3=(0, 156) / 187]eval:   2%|▏         | 6/377 [00:03<02:40,  2.31it/s, recall_0.3=(0, 176) / 209]eval:   2%|▏         | 7/377 [00:03<02:35,  2.38it/s, recall_0.3=(0, 176) / 209]eval:   2%|▏         | 7/377 [00:04<02:35,  2.38it/s, recall_0.3=(0, 187) / 220]eval:   2%|▏         | 8/377 [00:04<02:25,  2.54it/s, recall_0.3=(0, 187) / 220]eval:   2%|▏         | 8/377 [00:04<02:25,  2.54it/s, recall_0.3=(0, 190) / 223]eval:   2%|▏         | 9/377 [00:04<02:19,  2.64it/s, recall_0.3=(0, 190) / 223]eval:   2%|▏         | 9/377 [00:04<02:19,  2.64it/s, recall_0.3=(0, 197) / 233]eval:   3%|▎         | 10/377 [00:04<02:23,  2.55it/s, recall_0.3=(0, 197) / 233]eval:   3%|▎         | 10/377 [00:05<02:23,  2.55it/s, recall_0.3=(0, 203) / 242]eval:   3%|▎         | 11/377 [00:05<02:17,  2.65it/s, recall_0.3=(0, 203) / 242]eval:   3%|▎         | 11/377 [00:05<02:17,  2.65it/s, recall_0.3=(0, 223) / 267]eval:   3%|▎         | 12/377 [00:05<02:13,  2.73it/s, recall_0.3=(0, 223) / 267]eval:   3%|▎         | 12/377 [00:05<02:13,  2.73it/s, recall_0.3=(0, 243) / 291]eval:   3%|▎         | 13/377 [00:05<02:07,  2.86it/s, recall_0.3=(0, 243) / 291]eval:   3%|▎         | 13/377 [00:06<02:07,  2.86it/s, recall_0.3=(0, 282) / 334]eval:   4%|▎         | 14/377 [00:06<02:04,  2.92it/s, recall_0.3=(0, 282) / 334]eval:   4%|▎         | 14/377 [00:06<02:04,  2.92it/s, recall_0.3=(0, 321) / 379]eval:   4%|▍         | 15/377 [00:06<02:04,  2.91it/s, recall_0.3=(0, 321) / 379]eval:   4%|▍         | 15/377 [00:06<02:04,  2.91it/s, recall_0.3=(0, 377) / 457]eval:   4%|▍         | 16/377 [00:06<02:03,  2.93it/s, recall_0.3=(0, 377) / 457]eval:   4%|▍         | 16/377 [00:07<02:03,  2.93it/s, recall_0.3=(0, 475) / 570]eval:   5%|▍         | 17/377 [00:07<02:03,  2.92it/s, recall_0.3=(0, 475) / 570]eval:   5%|▍         | 17/377 [00:07<02:03,  2.92it/s, recall_0.3=(0, 604) / 731]eval:   5%|▍         | 18/377 [00:07<01:59,  3.00it/s, recall_0.3=(0, 604) / 731]eval:   5%|▍         | 18/377 [00:07<01:59,  3.00it/s, recall_0.3=(0, 747) / 886]eval:   5%|▌         | 19/377 [00:07<01:58,  3.02it/s, recall_0.3=(0, 747) / 886]eval:   5%|▌         | 19/377 [00:08<01:58,  3.02it/s, recall_0.3=(0, 811) / 966]eval:   5%|▌         | 20/377 [00:08<01:56,  3.07it/s, recall_0.3=(0, 811) / 966]eval:   5%|▌         | 20/377 [00:08<01:56,  3.07it/s, recall_0.3=(0, 838) / 1009]eval:   6%|▌         | 21/377 [00:08<01:55,  3.07it/s, recall_0.3=(0, 838) / 1009]eval:   6%|▌         | 21/377 [00:08<01:55,  3.07it/s, recall_0.3=(0, 871) / 1051]eval:   6%|▌         | 22/377 [00:08<02:01,  2.91it/s, recall_0.3=(0, 871) / 1051]eval:   6%|▌         | 22/377 [00:09<02:01,  2.91it/s, recall_0.3=(0, 926) / 1132]eval:   6%|▌         | 23/377 [00:09<02:00,  2.95it/s, recall_0.3=(0, 926) / 1132]eval:   6%|▌         | 23/377 [00:09<02:00,  2.95it/s, recall_0.3=(0, 1020) / 1242]eval:   6%|▋         | 24/377 [00:09<02:00,  2.93it/s, recall_0.3=(0, 1020) / 1242]eval:   6%|▋         | 24/377 [00:09<02:00,  2.93it/s, recall_0.3=(0, 1046) / 1274]eval:   7%|▋         | 25/377 [00:09<02:02,  2.88it/s, recall_0.3=(0, 1046) / 1274]eval:   7%|▋         | 25/377 [00:10<02:02,  2.88it/s, recall_0.3=(0, 1062) / 1296]eval:   7%|▋         | 26/377 [00:10<01:58,  2.97it/s, recall_0.3=(0, 1062) / 1296]eval:   7%|▋         | 26/377 [00:10<01:58,  2.97it/s, recall_0.3=(0, 1082) / 1317]eval:   7%|▋         | 27/377 [00:10<01:57,  2.99it/s, recall_0.3=(0, 1082) / 1317]eval:   7%|▋         | 27/377 [00:10<01:57,  2.99it/s, recall_0.3=(0, 1099) / 1339]eval:   7%|▋         | 28/377 [00:10<01:56,  3.00it/s, recall_0.3=(0, 1099) / 1339]eval:   7%|▋         | 28/377 [00:11<01:56,  3.00it/s, recall_0.3=(0, 1120) / 1369]eval:   8%|▊         | 29/377 [00:11<01:59,  2.92it/s, recall_0.3=(0, 1120) / 1369]eval:   8%|▊         | 29/377 [00:11<01:59,  2.92it/s, recall_0.3=(0, 1141) / 1402]eval:   8%|▊         | 30/377 [00:11<02:00,  2.89it/s, recall_0.3=(0, 1141) / 1402]eval:   8%|▊         | 30/377 [00:11<02:00,  2.89it/s, recall_0.3=(0, 1163) / 1427]eval:   8%|▊         | 31/377 [00:11<01:52,  3.08it/s, recall_0.3=(0, 1163) / 1427]eval:   8%|▊         | 31/377 [00:12<01:52,  3.08it/s, recall_0.3=(0, 1200) / 1475]eval:   8%|▊         | 32/377 [00:12<01:47,  3.19it/s, recall_0.3=(0, 1200) / 1475]eval:   8%|▊         | 32/377 [00:12<01:47,  3.19it/s, recall_0.3=(0, 1239) / 1538]eval:   9%|▉         | 33/377 [00:12<01:45,  3.26it/s, recall_0.3=(0, 1239) / 1538]eval:   9%|▉         | 33/377 [00:12<01:45,  3.26it/s, recall_0.3=(0, 1301) / 1613]eval:   9%|▉         | 34/377 [00:12<01:51,  3.08it/s, recall_0.3=(0, 1301) / 1613]eval:   9%|▉         | 34/377 [00:13<01:51,  3.08it/s, recall_0.3=(0, 1349) / 1675]eval:   9%|▉         | 35/377 [00:13<01:54,  2.98it/s, recall_0.3=(0, 1349) / 1675]eval:   9%|▉         | 35/377 [00:13<01:54,  2.98it/s, recall_0.3=(0, 1364) / 1696]eval:  10%|▉         | 36/377 [00:13<01:55,  2.94it/s, recall_0.3=(0, 1364) / 1696]eval:  10%|▉         | 36/377 [00:13<01:55,  2.94it/s, recall_0.3=(0, 1391) / 1727]eval:  10%|▉         | 37/377 [00:13<01:57,  2.88it/s, recall_0.3=(0, 1391) / 1727]eval:  10%|▉         | 37/377 [00:14<01:57,  2.88it/s, recall_0.3=(0, 1419) / 1759]eval:  10%|█         | 38/377 [00:14<01:52,  3.02it/s, recall_0.3=(0, 1419) / 1759]eval:  10%|█         | 38/377 [00:14<01:52,  3.02it/s, recall_0.3=(0, 1458) / 1804]eval:  10%|█         | 39/377 [00:14<01:47,  3.16it/s, recall_0.3=(0, 1458) / 1804]eval:  10%|█         | 39/377 [00:14<01:47,  3.16it/s, recall_0.3=(0, 1493) / 1849]eval:  11%|█         | 40/377 [00:14<01:42,  3.29it/s, recall_0.3=(0, 1493) / 1849]eval:  11%|█         | 40/377 [00:15<01:42,  3.29it/s, recall_0.3=(0, 1530) / 1891]eval:  11%|█         | 41/377 [00:15<01:44,  3.21it/s, recall_0.3=(0, 1530) / 1891]eval:  11%|█         | 41/377 [00:15<01:44,  3.21it/s, recall_0.3=(0, 1627) / 1997]eval:  11%|█         | 42/377 [00:15<01:47,  3.11it/s, recall_0.3=(0, 1627) / 1997]eval:  11%|█         | 42/377 [00:15<01:47,  3.11it/s, recall_0.3=(0, 1723) / 2112]eval:  11%|█▏        | 43/377 [00:15<01:47,  3.12it/s, recall_0.3=(0, 1723) / 2112]eval:  11%|█▏        | 43/377 [00:16<01:47,  3.12it/s, recall_0.3=(0, 1787) / 2194]eval:  12%|█▏        | 44/377 [00:16<01:44,  3.18it/s, recall_0.3=(0, 1787) / 2194]eval:  12%|█▏        | 44/377 [00:16<01:44,  3.18it/s, recall_0.3=(0, 1831) / 2254]eval:  12%|█▏        | 45/377 [00:16<01:46,  3.12it/s, recall_0.3=(0, 1831) / 2254]eval:  12%|█▏        | 45/377 [00:16<01:46,  3.12it/s, recall_0.3=(0, 1876) / 2303]eval:  12%|█▏        | 46/377 [00:16<01:46,  3.11it/s, recall_0.3=(0, 1876) / 2303]eval:  12%|█▏        | 46/377 [00:17<01:46,  3.11it/s, recall_0.3=(0, 1900) / 2330]eval:  12%|█▏        | 47/377 [00:17<01:47,  3.08it/s, recall_0.3=(0, 1900) / 2330]eval:  12%|█▏        | 47/377 [00:17<01:47,  3.08it/s, recall_0.3=(0, 1942) / 2375]eval:  13%|█▎        | 48/377 [00:17<01:46,  3.10it/s, recall_0.3=(0, 1942) / 2375]eval:  13%|█▎        | 48/377 [00:17<01:46,  3.10it/s, recall_0.3=(0, 2006) / 2448]eval:  13%|█▎        | 49/377 [00:17<01:44,  3.14it/s, recall_0.3=(0, 2006) / 2448]eval:  13%|█▎        | 49/377 [00:17<01:44,  3.14it/s, recall_0.3=(0, 2103) / 2563]eval:  13%|█▎        | 50/377 [00:17<01:44,  3.13it/s, recall_0.3=(0, 2103) / 2563]eval:  13%|█▎        | 50/377 [00:18<01:44,  3.13it/s, recall_0.3=(0, 2175) / 2649]eval:  14%|█▎        | 51/377 [00:18<01:40,  3.24it/s, recall_0.3=(0, 2175) / 2649]eval:  14%|█▎        | 51/377 [00:18<01:40,  3.24it/s, recall_0.3=(0, 2247) / 2736]eval:  14%|█▍        | 52/377 [00:18<01:37,  3.32it/s, recall_0.3=(0, 2247) / 2736]eval:  14%|█▍        | 52/377 [00:18<01:37,  3.32it/s, recall_0.3=(0, 2321) / 2827]eval:  14%|█▍        | 53/377 [00:18<01:35,  3.38it/s, recall_0.3=(0, 2321) / 2827]eval:  14%|█▍        | 53/377 [00:19<01:35,  3.38it/s, recall_0.3=(0, 2426) / 2957]eval:  14%|█▍        | 54/377 [00:19<01:37,  3.31it/s, recall_0.3=(0, 2426) / 2957]eval:  14%|█▍        | 54/377 [00:19<01:37,  3.31it/s, recall_0.3=(0, 2500) / 3041]eval:  15%|█▍        | 55/377 [00:19<01:38,  3.26it/s, recall_0.3=(0, 2500) / 3041]eval:  15%|█▍        | 55/377 [00:19<01:38,  3.26it/s, recall_0.3=(0, 2578) / 3122]eval:  15%|█▍        | 56/377 [00:19<01:40,  3.19it/s, recall_0.3=(0, 2578) / 3122]eval:  15%|█▍        | 56/377 [00:20<01:40,  3.19it/s, recall_0.3=(0, 2647) / 3203]eval:  15%|█▌        | 57/377 [00:20<01:37,  3.28it/s, recall_0.3=(0, 2647) / 3203]eval:  15%|█▌        | 57/377 [00:20<01:37,  3.28it/s, recall_0.3=(0, 2697) / 3259]eval:  15%|█▌        | 58/377 [00:20<01:38,  3.24it/s, recall_0.3=(0, 2697) / 3259]eval:  15%|█▌        | 58/377 [00:20<01:38,  3.24it/s, recall_0.3=(0, 2776) / 3351]eval:  16%|█▌        | 59/377 [00:20<01:37,  3.25it/s, recall_0.3=(0, 2776) / 3351]eval:  16%|█▌        | 59/377 [00:20<01:37,  3.25it/s, recall_0.3=(0, 2860) / 3448]eval:  16%|█▌        | 60/377 [00:20<01:39,  3.20it/s, recall_0.3=(0, 2860) / 3448]eval:  16%|█▌        | 60/377 [00:21<01:39,  3.20it/s, recall_0.3=(0, 2925) / 3524]eval:  16%|█▌        | 61/377 [00:21<01:37,  3.23it/s, recall_0.3=(0, 2925) / 3524]eval:  16%|█▌        | 61/377 [00:21<01:37,  3.23it/s, recall_0.3=(0, 2991) / 3606]eval:  16%|█▋        | 62/377 [00:21<01:39,  3.18it/s, recall_0.3=(0, 2991) / 3606]eval:  16%|█▋        | 62/377 [00:21<01:39,  3.18it/s, recall_0.3=(0, 3067) / 3701]eval:  17%|█▋        | 63/377 [00:21<01:40,  3.13it/s, recall_0.3=(0, 3067) / 3701]eval:  17%|█▋        | 63/377 [00:22<01:40,  3.13it/s, recall_0.3=(0, 3207) / 3863]eval:  17%|█▋        | 64/377 [00:22<01:39,  3.14it/s, recall_0.3=(0, 3207) / 3863]eval:  17%|█▋        | 64/377 [00:22<01:39,  3.14it/s, recall_0.3=(0, 3341) / 4019]eval:  17%|█▋        | 65/377 [00:22<01:36,  3.23it/s, recall_0.3=(0, 3341) / 4019]eval:  17%|█▋        | 65/377 [00:22<01:36,  3.23it/s, recall_0.3=(0, 3445) / 4163]eval:  18%|█▊        | 66/377 [00:22<01:37,  3.19it/s, recall_0.3=(0, 3445) / 4163]eval:  18%|█▊        | 66/377 [00:23<01:37,  3.19it/s, recall_0.3=(0, 3535) / 4263]eval:  18%|█▊        | 67/377 [00:23<01:35,  3.25it/s, recall_0.3=(0, 3535) / 4263]eval:  18%|█▊        | 67/377 [00:23<01:35,  3.25it/s, recall_0.3=(0, 3623) / 4365]eval:  18%|█▊        | 68/377 [00:23<01:37,  3.17it/s, recall_0.3=(0, 3623) / 4365]eval:  18%|█▊        | 68/377 [00:23<01:37,  3.17it/s, recall_0.3=(0, 3680) / 4433]eval:  18%|█▊        | 69/377 [00:23<01:40,  3.07it/s, recall_0.3=(0, 3680) / 4433]eval:  18%|█▊        | 69/377 [00:24<01:40,  3.07it/s, recall_0.3=(0, 3737) / 4502]eval:  19%|█▊        | 70/377 [00:24<01:39,  3.08it/s, recall_0.3=(0, 3737) / 4502]eval:  19%|█▊        | 70/377 [00:24<01:39,  3.08it/s, recall_0.3=(0, 3827) / 4609]eval:  19%|█▉        | 71/377 [00:24<01:37,  3.13it/s, recall_0.3=(0, 3827) / 4609]eval:  19%|█▉        | 71/377 [00:24<01:37,  3.13it/s, recall_0.3=(0, 3912) / 4706]eval:  19%|█▉        | 72/377 [00:24<01:38,  3.09it/s, recall_0.3=(0, 3912) / 4706]eval:  19%|█▉        | 72/377 [00:25<01:38,  3.09it/s, recall_0.3=(0, 3971) / 4779]eval:  19%|█▉        | 73/377 [00:25<01:36,  3.15it/s, recall_0.3=(0, 3971) / 4779]eval:  19%|█▉        | 73/377 [00:25<01:36,  3.15it/s, recall_0.3=(0, 4043) / 4876]eval:  20%|█▉        | 74/377 [00:25<01:38,  3.08it/s, recall_0.3=(0, 4043) / 4876]eval:  20%|█▉        | 74/377 [00:25<01:38,  3.08it/s, recall_0.3=(0, 4105) / 4954]eval:  20%|█▉        | 75/377 [00:25<01:40,  3.01it/s, recall_0.3=(0, 4105) / 4954]eval:  20%|█▉        | 75/377 [00:26<01:40,  3.01it/s, recall_0.3=(0, 4152) / 5010]eval:  20%|██        | 76/377 [00:26<01:41,  2.97it/s, recall_0.3=(0, 4152) / 5010]eval:  20%|██        | 76/377 [00:26<01:41,  2.97it/s, recall_0.3=(0, 4190) / 5058]eval:  20%|██        | 77/377 [00:26<01:42,  2.91it/s, recall_0.3=(0, 4190) / 5058]eval:  20%|██        | 77/377 [00:26<01:42,  2.91it/s, recall_0.3=(0, 4216) / 5084]eval:  21%|██        | 78/377 [00:26<01:43,  2.90it/s, recall_0.3=(0, 4216) / 5084]eval:  21%|██        | 78/377 [00:27<01:43,  2.90it/s, recall_0.3=(0, 4238) / 5106]eval:  21%|██        | 79/377 [00:27<01:43,  2.88it/s, recall_0.3=(0, 4238) / 5106]eval:  21%|██        | 79/377 [00:27<01:43,  2.88it/s, recall_0.3=(0, 4252) / 5127]eval:  21%|██        | 80/377 [00:27<01:41,  2.92it/s, recall_0.3=(0, 4252) / 5127]eval:  21%|██        | 80/377 [00:27<01:41,  2.92it/s, recall_0.3=(0, 4278) / 5159]eval:  21%|██▏       | 81/377 [00:27<01:39,  2.96it/s, recall_0.3=(0, 4278) / 5159]eval:  21%|██▏       | 81/377 [00:28<01:39,  2.96it/s, recall_0.3=(0, 4314) / 5196]eval:  22%|██▏       | 82/377 [00:28<01:39,  2.98it/s, recall_0.3=(0, 4314) / 5196]eval:  22%|██▏       | 82/377 [00:28<01:39,  2.98it/s, recall_0.3=(0, 4352) / 5247]eval:  22%|██▏       | 83/377 [00:28<01:41,  2.90it/s, recall_0.3=(0, 4352) / 5247]eval:  22%|██▏       | 83/377 [00:28<01:41,  2.90it/s, recall_0.3=(0, 4392) / 5300]eval:  22%|██▏       | 84/377 [00:28<01:41,  2.89it/s, recall_0.3=(0, 4392) / 5300]eval:  22%|██▏       | 84/377 [00:29<01:41,  2.89it/s, recall_0.3=(0, 4418) / 5333]eval:  23%|██▎       | 85/377 [00:29<01:41,  2.88it/s, recall_0.3=(0, 4418) / 5333]eval:  23%|██▎       | 85/377 [00:29<01:41,  2.88it/s, recall_0.3=(0, 4474) / 5400]eval:  23%|██▎       | 86/377 [00:29<01:41,  2.86it/s, recall_0.3=(0, 4474) / 5400]eval:  23%|██▎       | 86/377 [00:29<01:41,  2.86it/s, recall_0.3=(0, 4531) / 5468]eval:  23%|██▎       | 87/377 [00:29<01:40,  2.87it/s, recall_0.3=(0, 4531) / 5468]eval:  23%|██▎       | 87/377 [00:30<01:40,  2.87it/s, recall_0.3=(0, 4542) / 5482]eval:  23%|██▎       | 88/377 [00:30<01:37,  2.96it/s, recall_0.3=(0, 4542) / 5482]eval:  23%|██▎       | 88/377 [00:30<01:37,  2.96it/s, recall_0.3=(0, 4557) / 5500]eval:  24%|██▎       | 89/377 [00:30<01:37,  2.95it/s, recall_0.3=(0, 4557) / 5500]eval:  24%|██▎       | 89/377 [00:30<01:37,  2.95it/s, recall_0.3=(0, 4572) / 5517]eval:  24%|██▍       | 90/377 [00:30<01:34,  3.02it/s, recall_0.3=(0, 4572) / 5517]eval:  24%|██▍       | 90/377 [00:31<01:34,  3.02it/s, recall_0.3=(0, 4586) / 5538]eval:  24%|██▍       | 91/377 [00:31<01:35,  3.00it/s, recall_0.3=(0, 4586) / 5538]eval:  24%|██▍       | 91/377 [00:31<01:35,  3.00it/s, recall_0.3=(0, 4605) / 5559]eval:  24%|██▍       | 92/377 [00:31<01:40,  2.84it/s, recall_0.3=(0, 4605) / 5559]eval:  24%|██▍       | 92/377 [00:32<01:40,  2.84it/s, recall_0.3=(0, 4651) / 5610]eval:  25%|██▍       | 93/377 [00:32<01:39,  2.85it/s, recall_0.3=(0, 4651) / 5610]eval:  25%|██▍       | 93/377 [00:32<01:39,  2.85it/s, recall_0.3=(0, 4660) / 5623]eval:  25%|██▍       | 94/377 [00:32<01:39,  2.86it/s, recall_0.3=(0, 4660) / 5623]eval:  25%|██▍       | 94/377 [00:32<01:39,  2.86it/s, recall_0.3=(0, 4714) / 5684]eval:  25%|██▌       | 95/377 [00:32<01:39,  2.83it/s, recall_0.3=(0, 4714) / 5684]eval:  25%|██▌       | 95/377 [00:33<01:39,  2.83it/s, recall_0.3=(0, 4842) / 5830]eval:  25%|██▌       | 96/377 [00:33<01:40,  2.80it/s, recall_0.3=(0, 4842) / 5830]eval:  25%|██▌       | 96/377 [00:33<01:40,  2.80it/s, recall_0.3=(0, 4939) / 5948]eval:  26%|██▌       | 97/377 [00:33<01:39,  2.83it/s, recall_0.3=(0, 4939) / 5948]eval:  26%|██▌       | 97/377 [00:33<01:39,  2.83it/s, recall_0.3=(0, 5004) / 6035]eval:  26%|██▌       | 98/377 [00:33<01:35,  2.91it/s, recall_0.3=(0, 5004) / 6035]eval:  26%|██▌       | 98/377 [00:34<01:35,  2.91it/s, recall_0.3=(0, 5066) / 6134]eval:  26%|██▋       | 99/377 [00:34<01:34,  2.94it/s, recall_0.3=(0, 5066) / 6134]eval:  26%|██▋       | 99/377 [00:34<01:34,  2.94it/s, recall_0.3=(0, 5166) / 6269]eval:  27%|██▋       | 100/377 [00:34<01:35,  2.89it/s, recall_0.3=(0, 5166) / 6269]eval:  27%|██▋       | 100/377 [00:34<01:35,  2.89it/s, recall_0.3=(0, 5311) / 6452]eval:  27%|██▋       | 101/377 [00:34<01:36,  2.85it/s, recall_0.3=(0, 5311) / 6452]eval:  27%|██▋       | 101/377 [00:35<01:36,  2.85it/s, recall_0.3=(0, 5368) / 6535]eval:  27%|██▋       | 102/377 [00:35<01:34,  2.92it/s, recall_0.3=(0, 5368) / 6535]eval:  27%|██▋       | 102/377 [00:35<01:34,  2.92it/s, recall_0.3=(0, 5438) / 6624]eval:  27%|██▋       | 103/377 [00:35<01:33,  2.93it/s, recall_0.3=(0, 5438) / 6624]eval:  27%|██▋       | 103/377 [00:35<01:33,  2.93it/s, recall_0.3=(0, 5494) / 6691]eval:  28%|██▊       | 104/377 [00:35<01:32,  2.95it/s, recall_0.3=(0, 5494) / 6691]eval:  28%|██▊       | 104/377 [00:36<01:32,  2.95it/s, recall_0.3=(0, 5543) / 6747]eval:  28%|██▊       | 105/377 [00:36<01:31,  2.97it/s, recall_0.3=(0, 5543) / 6747]eval:  28%|██▊       | 105/377 [00:36<01:31,  2.97it/s, recall_0.3=(0, 5586) / 6794]eval:  28%|██▊       | 106/377 [00:36<01:31,  2.97it/s, recall_0.3=(0, 5586) / 6794]eval:  28%|██▊       | 106/377 [00:36<01:31,  2.97it/s, recall_0.3=(0, 5612) / 6825]eval:  28%|██▊       | 107/377 [00:36<01:34,  2.85it/s, recall_0.3=(0, 5612) / 6825]eval:  28%|██▊       | 107/377 [00:37<01:34,  2.85it/s, recall_0.3=(0, 5671) / 6892]eval:  29%|██▊       | 108/377 [00:37<01:28,  3.04it/s, recall_0.3=(0, 5671) / 6892]eval:  29%|██▊       | 108/377 [00:37<01:28,  3.04it/s, recall_0.3=(0, 5716) / 6946]eval:  29%|██▉       | 109/377 [00:37<01:28,  3.02it/s, recall_0.3=(0, 5716) / 6946]eval:  29%|██▉       | 109/377 [00:37<01:28,  3.02it/s, recall_0.3=(0, 5753) / 7002]eval:  29%|██▉       | 110/377 [00:37<01:32,  2.89it/s, recall_0.3=(0, 5753) / 7002]eval:  29%|██▉       | 110/377 [00:38<01:32,  2.89it/s, recall_0.3=(0, 5788) / 7038]eval:  29%|██▉       | 111/377 [00:38<01:28,  3.00it/s, recall_0.3=(0, 5788) / 7038]eval:  29%|██▉       | 111/377 [00:38<01:28,  3.00it/s, recall_0.3=(0, 5833) / 7092]eval:  30%|██▉       | 112/377 [00:38<01:24,  3.13it/s, recall_0.3=(0, 5833) / 7092]eval:  30%|██▉       | 112/377 [00:38<01:24,  3.13it/s, recall_0.3=(0, 5903) / 7180]eval:  30%|██▉       | 113/377 [00:38<01:23,  3.17it/s, recall_0.3=(0, 5903) / 7180]eval:  30%|██▉       | 113/377 [00:39<01:23,  3.17it/s, recall_0.3=(0, 5954) / 7252]eval:  30%|███       | 114/377 [00:39<01:25,  3.07it/s, recall_0.3=(0, 5954) / 7252]eval:  30%|███       | 114/377 [00:39<01:25,  3.07it/s, recall_0.3=(0, 6045) / 7347]eval:  31%|███       | 115/377 [00:39<01:23,  3.13it/s, recall_0.3=(0, 6045) / 7347]eval:  31%|███       | 115/377 [00:39<01:23,  3.13it/s, recall_0.3=(0, 6138) / 7448]eval:  31%|███       | 116/377 [00:39<01:24,  3.07it/s, recall_0.3=(0, 6138) / 7448]eval:  31%|███       | 116/377 [00:40<01:24,  3.07it/s, recall_0.3=(0, 6219) / 7540]eval:  31%|███       | 117/377 [00:40<01:25,  3.04it/s, recall_0.3=(0, 6219) / 7540]eval:  31%|███       | 117/377 [00:40<01:25,  3.04it/s, recall_0.3=(0, 6318) / 7660]eval:  31%|███▏      | 118/377 [00:40<01:23,  3.11it/s, recall_0.3=(0, 6318) / 7660]eval:  31%|███▏      | 118/377 [00:40<01:23,  3.11it/s, recall_0.3=(0, 6417) / 7780]eval:  32%|███▏      | 119/377 [00:40<01:22,  3.14it/s, recall_0.3=(0, 6417) / 7780]eval:  32%|███▏      | 119/377 [00:41<01:22,  3.14it/s, recall_0.3=(0, 6545) / 7929]eval:  32%|███▏      | 120/377 [00:41<01:22,  3.12it/s, recall_0.3=(0, 6545) / 7929]eval:  32%|███▏      | 120/377 [00:41<01:22,  3.12it/s, recall_0.3=(0, 6680) / 8090]eval:  32%|███▏      | 121/377 [00:41<01:26,  2.97it/s, recall_0.3=(0, 6680) / 8090]eval:  32%|███▏      | 121/377 [00:41<01:26,  2.97it/s, recall_0.3=(0, 6719) / 8133]eval:  32%|███▏      | 122/377 [00:41<01:26,  2.94it/s, recall_0.3=(0, 6719) / 8133]eval:  32%|███▏      | 122/377 [00:42<01:26,  2.94it/s, recall_0.3=(0, 6726) / 8140]eval:  33%|███▎      | 123/377 [00:42<01:27,  2.89it/s, recall_0.3=(0, 6726) / 8140]eval:  33%|███▎      | 123/377 [00:42<01:27,  2.89it/s, recall_0.3=(0, 6743) / 8157]eval:  33%|███▎      | 124/377 [00:42<01:33,  2.72it/s, recall_0.3=(0, 6743) / 8157]eval:  33%|███▎      | 124/377 [00:42<01:33,  2.72it/s, recall_0.3=(0, 6821) / 8257]eval:  33%|███▎      | 125/377 [00:42<01:31,  2.74it/s, recall_0.3=(0, 6821) / 8257]eval:  33%|███▎      | 125/377 [00:43<01:31,  2.74it/s, recall_0.3=(0, 6901) / 8350]eval:  33%|███▎      | 126/377 [00:43<01:30,  2.78it/s, recall_0.3=(0, 6901) / 8350]eval:  33%|███▎      | 126/377 [00:43<01:30,  2.78it/s, recall_0.3=(0, 6953) / 8415]eval:  34%|███▎      | 127/377 [00:43<01:28,  2.82it/s, recall_0.3=(0, 6953) / 8415]eval:  34%|███▎      | 127/377 [00:43<01:28,  2.82it/s, recall_0.3=(0, 6996) / 8470]eval:  34%|███▍      | 128/377 [00:43<01:26,  2.88it/s, recall_0.3=(0, 6996) / 8470]eval:  34%|███▍      | 128/377 [00:44<01:26,  2.88it/s, recall_0.3=(0, 7057) / 8541]eval:  34%|███▍      | 129/377 [00:44<01:25,  2.91it/s, recall_0.3=(0, 7057) / 8541]eval:  34%|███▍      | 129/377 [00:44<01:25,  2.91it/s, recall_0.3=(0, 7104) / 8599]eval:  34%|███▍      | 130/377 [00:44<01:25,  2.89it/s, recall_0.3=(0, 7104) / 8599]eval:  34%|███▍      | 130/377 [00:44<01:25,  2.89it/s, recall_0.3=(0, 7133) / 8633]eval:  35%|███▍      | 131/377 [00:44<01:25,  2.87it/s, recall_0.3=(0, 7133) / 8633]eval:  35%|███▍      | 131/377 [00:45<01:25,  2.87it/s, recall_0.3=(0, 7180) / 8696]eval:  35%|███▌      | 132/377 [00:45<01:23,  2.93it/s, recall_0.3=(0, 7180) / 8696]eval:  35%|███▌      | 132/377 [00:45<01:23,  2.93it/s, recall_0.3=(0, 7235) / 8762]eval:  35%|███▌      | 133/377 [00:45<01:22,  2.94it/s, recall_0.3=(0, 7235) / 8762]eval:  35%|███▌      | 133/377 [00:45<01:22,  2.94it/s, recall_0.3=(0, 7306) / 8854]eval:  36%|███▌      | 134/377 [00:45<01:21,  2.99it/s, recall_0.3=(0, 7306) / 8854]eval:  36%|███▌      | 134/377 [00:46<01:21,  2.99it/s, recall_0.3=(0, 7360) / 8927]eval:  36%|███▌      | 135/377 [00:46<01:17,  3.13it/s, recall_0.3=(0, 7360) / 8927]eval:  36%|███▌      | 135/377 [00:46<01:17,  3.13it/s, recall_0.3=(0, 7428) / 9004]eval:  36%|███▌      | 136/377 [00:46<01:18,  3.06it/s, recall_0.3=(0, 7428) / 9004]eval:  36%|███▌      | 136/377 [00:46<01:18,  3.06it/s, recall_0.3=(0, 7467) / 9063]eval:  36%|███▋      | 137/377 [00:46<01:19,  3.03it/s, recall_0.3=(0, 7467) / 9063]eval:  36%|███▋      | 137/377 [00:47<01:19,  3.03it/s, recall_0.3=(0, 7520) / 9141]eval:  37%|███▋      | 138/377 [00:47<01:21,  2.93it/s, recall_0.3=(0, 7520) / 9141]eval:  37%|███▋      | 138/377 [00:47<01:21,  2.93it/s, recall_0.3=(0, 7559) / 9191]eval:  37%|███▋      | 139/377 [00:47<01:21,  2.93it/s, recall_0.3=(0, 7559) / 9191]eval:  37%|███▋      | 139/377 [00:47<01:21,  2.93it/s, recall_0.3=(0, 7593) / 9234]eval:  37%|███▋      | 140/377 [00:47<01:20,  2.95it/s, recall_0.3=(0, 7593) / 9234]eval:  37%|███▋      | 140/377 [00:48<01:20,  2.95it/s, recall_0.3=(0, 7640) / 9285]eval:  37%|███▋      | 141/377 [00:48<01:20,  2.93it/s, recall_0.3=(0, 7640) / 9285]eval:  37%|███▋      | 141/377 [00:48<01:20,  2.93it/s, recall_0.3=(0, 7678) / 9326]eval:  38%|███▊      | 142/377 [00:48<01:21,  2.89it/s, recall_0.3=(0, 7678) / 9326]eval:  38%|███▊      | 142/377 [00:48<01:21,  2.89it/s, recall_0.3=(0, 7731) / 9388]eval:  38%|███▊      | 143/377 [00:48<01:17,  3.00it/s, recall_0.3=(0, 7731) / 9388]eval:  38%|███▊      | 143/377 [00:49<01:17,  3.00it/s, recall_0.3=(0, 7782) / 9448]eval:  38%|███▊      | 144/377 [00:49<01:13,  3.15it/s, recall_0.3=(0, 7782) / 9448]eval:  38%|███▊      | 144/377 [00:49<01:13,  3.15it/s, recall_0.3=(0, 7830) / 9508]eval:  38%|███▊      | 145/377 [00:49<01:11,  3.24it/s, recall_0.3=(0, 7830) / 9508]eval:  38%|███▊      | 145/377 [00:49<01:11,  3.24it/s, recall_0.3=(0, 7885) / 9572]eval:  39%|███▊      | 146/377 [00:49<01:09,  3.34it/s, recall_0.3=(0, 7885) / 9572]eval:  39%|███▊      | 146/377 [00:50<01:09,  3.34it/s, recall_0.3=(0, 7940) / 9633]eval:  39%|███▉      | 147/377 [00:50<01:09,  3.32it/s, recall_0.3=(0, 7940) / 9633]eval:  39%|███▉      | 147/377 [00:50<01:09,  3.32it/s, recall_0.3=(0, 7997) / 9705]eval:  39%|███▉      | 148/377 [00:50<01:10,  3.23it/s, recall_0.3=(0, 7997) / 9705]eval:  39%|███▉      | 148/377 [00:50<01:10,  3.23it/s, recall_0.3=(0, 8073) / 9805]eval:  40%|███▉      | 149/377 [00:50<01:11,  3.18it/s, recall_0.3=(0, 8073) / 9805]eval:  40%|███▉      | 149/377 [00:51<01:11,  3.18it/s, recall_0.3=(0, 8127) / 9868]eval:  40%|███▉      | 150/377 [00:51<01:12,  3.14it/s, recall_0.3=(0, 8127) / 9868]eval:  40%|███▉      | 150/377 [00:51<01:12,  3.14it/s, recall_0.3=(0, 8152) / 9896]eval:  40%|████      | 151/377 [00:51<01:13,  3.07it/s, recall_0.3=(0, 8152) / 9896]eval:  40%|████      | 151/377 [00:51<01:13,  3.07it/s, recall_0.3=(0, 8169) / 9915]eval:  40%|████      | 152/377 [00:51<01:10,  3.20it/s, recall_0.3=(0, 8169) / 9915]eval:  40%|████      | 152/377 [00:51<01:10,  3.20it/s, recall_0.3=(0, 8197) / 9949]eval:  41%|████      | 153/377 [00:51<01:07,  3.30it/s, recall_0.3=(0, 8197) / 9949]eval:  41%|████      | 153/377 [00:52<01:07,  3.30it/s, recall_0.3=(0, 8218) / 9973]eval:  41%|████      | 154/377 [00:52<01:05,  3.40it/s, recall_0.3=(0, 8218) / 9973]eval:  41%|████      | 154/377 [00:52<01:05,  3.40it/s, recall_0.3=(0, 8244) / 10000]eval:  41%|████      | 155/377 [00:52<01:04,  3.46it/s, recall_0.3=(0, 8244) / 10000]eval:  41%|████      | 155/377 [00:52<01:04,  3.46it/s, recall_0.3=(0, 8284) / 10051]eval:  41%|████▏     | 156/377 [00:52<01:06,  3.31it/s, recall_0.3=(0, 8284) / 10051]eval:  41%|████▏     | 156/377 [00:53<01:06,  3.31it/s, recall_0.3=(0, 8330) / 10104]eval:  42%|████▏     | 157/377 [00:53<01:05,  3.35it/s, recall_0.3=(0, 8330) / 10104]eval:  42%|████▏     | 157/377 [00:53<01:05,  3.35it/s, recall_0.3=(0, 8372) / 10155]eval:  42%|████▏     | 158/377 [00:53<01:06,  3.29it/s, recall_0.3=(0, 8372) / 10155]eval:  42%|████▏     | 158/377 [00:53<01:06,  3.29it/s, recall_0.3=(0, 8420) / 10210]eval:  42%|████▏     | 159/377 [00:53<01:08,  3.18it/s, recall_0.3=(0, 8420) / 10210]eval:  42%|████▏     | 159/377 [00:54<01:08,  3.18it/s, recall_0.3=(0, 8458) / 10254]eval:  42%|████▏     | 160/377 [00:54<01:10,  3.07it/s, recall_0.3=(0, 8458) / 10254]eval:  42%|████▏     | 160/377 [00:54<01:10,  3.07it/s, recall_0.3=(0, 8486) / 10294]eval:  43%|████▎     | 161/377 [00:54<01:08,  3.15it/s, recall_0.3=(0, 8486) / 10294]eval:  43%|████▎     | 161/377 [00:54<01:08,  3.15it/s, recall_0.3=(0, 8520) / 10340]eval:  43%|████▎     | 162/377 [00:54<01:06,  3.24it/s, recall_0.3=(0, 8520) / 10340]eval:  43%|████▎     | 162/377 [00:55<01:06,  3.24it/s, recall_0.3=(0, 8563) / 10394]eval:  43%|████▎     | 163/377 [00:55<01:04,  3.32it/s, recall_0.3=(0, 8563) / 10394]eval:  43%|████▎     | 163/377 [00:55<01:04,  3.32it/s, recall_0.3=(0, 8614) / 10456]eval:  44%|████▎     | 164/377 [00:55<01:04,  3.30it/s, recall_0.3=(0, 8614) / 10456]eval:  44%|████▎     | 164/377 [00:55<01:04,  3.30it/s, recall_0.3=(0, 8702) / 10551]eval:  44%|████▍     | 165/377 [00:55<01:07,  3.12it/s, recall_0.3=(0, 8702) / 10551]eval:  44%|████▍     | 165/377 [00:56<01:07,  3.12it/s, recall_0.3=(0, 8796) / 10664]eval:  44%|████▍     | 166/377 [00:56<01:09,  3.05it/s, recall_0.3=(0, 8796) / 10664]eval:  44%|████▍     | 166/377 [00:56<01:09,  3.05it/s, recall_0.3=(0, 8864) / 10765]eval:  44%|████▍     | 167/377 [00:56<01:08,  3.04it/s, recall_0.3=(0, 8864) / 10765]eval:  44%|████▍     | 167/377 [00:56<01:08,  3.04it/s, recall_0.3=(0, 8921) / 10833]eval:  45%|████▍     | 168/377 [00:56<01:08,  3.05it/s, recall_0.3=(0, 8921) / 10833]eval:  45%|████▍     | 168/377 [00:57<01:08,  3.05it/s, recall_0.3=(0, 8972) / 10897]eval:  45%|████▍     | 169/377 [00:57<01:06,  3.13it/s, recall_0.3=(0, 8972) / 10897]eval:  45%|████▍     | 169/377 [00:57<01:06,  3.13it/s, recall_0.3=(0, 9023) / 10957]eval:  45%|████▌     | 170/377 [00:57<01:03,  3.25it/s, recall_0.3=(0, 9023) / 10957]eval:  45%|████▌     | 170/377 [00:57<01:03,  3.25it/s, recall_0.3=(0, 9075) / 11016]eval:  45%|████▌     | 171/377 [00:57<01:01,  3.33it/s, recall_0.3=(0, 9075) / 11016]eval:  45%|████▌     | 171/377 [00:57<01:01,  3.33it/s, recall_0.3=(0, 9134) / 11089]eval:  46%|████▌     | 172/377 [00:57<01:03,  3.22it/s, recall_0.3=(0, 9134) / 11089]eval:  46%|████▌     | 172/377 [00:58<01:03,  3.22it/s, recall_0.3=(0, 9195) / 11160]eval:  46%|████▌     | 173/377 [00:58<01:05,  3.13it/s, recall_0.3=(0, 9195) / 11160]eval:  46%|████▌     | 173/377 [00:58<01:05,  3.13it/s, recall_0.3=(0, 9254) / 11238]eval:  46%|████▌     | 174/377 [00:58<01:05,  3.10it/s, recall_0.3=(0, 9254) / 11238]eval:  46%|████▌     | 174/377 [00:58<01:05,  3.10it/s, recall_0.3=(0, 9291) / 11286]eval:  46%|████▋     | 175/377 [00:58<01:04,  3.11it/s, recall_0.3=(0, 9291) / 11286]eval:  46%|████▋     | 175/377 [00:59<01:04,  3.11it/s, recall_0.3=(0, 9331) / 11331]eval:  47%|████▋     | 176/377 [00:59<01:04,  3.13it/s, recall_0.3=(0, 9331) / 11331]eval:  47%|████▋     | 176/377 [00:59<01:04,  3.13it/s, recall_0.3=(0, 9377) / 11384]eval:  47%|████▋     | 177/377 [00:59<01:04,  3.11it/s, recall_0.3=(0, 9377) / 11384]eval:  47%|████▋     | 177/377 [00:59<01:04,  3.11it/s, recall_0.3=(0, 9443) / 11465]eval:  47%|████▋     | 178/377 [00:59<01:03,  3.13it/s, recall_0.3=(0, 9443) / 11465]eval:  47%|████▋     | 178/377 [01:00<01:03,  3.13it/s, recall_0.3=(0, 9493) / 11527]eval:  47%|████▋     | 179/377 [01:00<01:01,  3.21it/s, recall_0.3=(0, 9493) / 11527]eval:  47%|████▋     | 179/377 [01:00<01:01,  3.21it/s, recall_0.3=(0, 9537) / 11579]eval:  48%|████▊     | 180/377 [01:00<01:00,  3.24it/s, recall_0.3=(0, 9537) / 11579]eval:  48%|████▊     | 180/377 [01:00<01:00,  3.24it/s, recall_0.3=(0, 9563) / 11608]eval:  48%|████▊     | 181/377 [01:00<01:00,  3.25it/s, recall_0.3=(0, 9563) / 11608]eval:  48%|████▊     | 181/377 [01:01<01:00,  3.25it/s, recall_0.3=(0, 9582) / 11633]eval:  48%|████▊     | 182/377 [01:01<00:58,  3.34it/s, recall_0.3=(0, 9582) / 11633]eval:  48%|████▊     | 182/377 [01:01<00:58,  3.34it/s, recall_0.3=(0, 9618) / 11676]eval:  49%|████▊     | 183/377 [01:01<00:57,  3.40it/s, recall_0.3=(0, 9618) / 11676]eval:  49%|████▊     | 183/377 [01:01<00:57,  3.40it/s, recall_0.3=(0, 9682) / 11769]eval:  49%|████▉     | 184/377 [01:01<00:57,  3.34it/s, recall_0.3=(0, 9682) / 11769]eval:  49%|████▉     | 184/377 [01:01<00:57,  3.34it/s, recall_0.3=(0, 9736) / 11844]eval:  49%|████▉     | 185/377 [01:01<00:58,  3.26it/s, recall_0.3=(0, 9736) / 11844]eval:  49%|████▉     | 185/377 [01:02<00:58,  3.26it/s, recall_0.3=(0, 9749) / 11858]eval:  49%|████▉     | 186/377 [01:02<00:58,  3.25it/s, recall_0.3=(0, 9749) / 11858]eval:  49%|████▉     | 186/377 [01:02<00:58,  3.25it/s, recall_0.3=(0, 9773) / 11888]eval:  50%|████▉     | 187/377 [01:02<00:59,  3.20it/s, recall_0.3=(0, 9773) / 11888]eval:  50%|████▉     | 187/377 [01:02<00:59,  3.20it/s, recall_0.3=(0, 9786) / 11901]eval:  50%|████▉     | 188/377 [01:02<00:57,  3.29it/s, recall_0.3=(0, 9786) / 11901]eval:  50%|████▉     | 188/377 [01:03<00:57,  3.29it/s, recall_0.3=(0, 9798) / 11915]eval:  50%|█████     | 189/377 [01:03<00:57,  3.29it/s, recall_0.3=(0, 9798) / 11915]eval:  50%|█████     | 189/377 [01:03<00:57,  3.29it/s, recall_0.3=(0, 9816) / 11941]eval:  50%|█████     | 190/377 [01:03<00:54,  3.40it/s, recall_0.3=(0, 9816) / 11941]eval:  50%|█████     | 190/377 [01:03<00:54,  3.40it/s, recall_0.3=(0, 9846) / 11975]eval:  51%|█████     | 191/377 [01:03<00:55,  3.33it/s, recall_0.3=(0, 9846) / 11975]eval:  51%|█████     | 191/377 [01:04<00:55,  3.33it/s, recall_0.3=(0, 9873) / 12013]eval:  51%|█████     | 192/377 [01:04<00:55,  3.32it/s, recall_0.3=(0, 9873) / 12013]eval:  51%|█████     | 192/377 [01:04<00:55,  3.32it/s, recall_0.3=(0, 9948) / 12122]eval:  51%|█████     | 193/377 [01:04<00:54,  3.36it/s, recall_0.3=(0, 9948) / 12122]eval:  51%|█████     | 193/377 [01:04<00:54,  3.36it/s, recall_0.3=(0, 10021) / 12218]eval:  51%|█████▏    | 194/377 [01:04<00:55,  3.31it/s, recall_0.3=(0, 10021) / 12218]eval:  51%|█████▏    | 194/377 [01:04<00:55,  3.31it/s, recall_0.3=(0, 10095) / 12301]eval:  52%|█████▏    | 195/377 [01:04<00:53,  3.38it/s, recall_0.3=(0, 10095) / 12301]eval:  52%|█████▏    | 195/377 [01:05<00:53,  3.38it/s, recall_0.3=(0, 10171) / 12386]eval:  52%|█████▏    | 196/377 [01:05<00:52,  3.42it/s, recall_0.3=(0, 10171) / 12386]eval:  52%|█████▏    | 196/377 [01:05<00:52,  3.42it/s, recall_0.3=(0, 10248) / 12472]eval:  52%|█████▏    | 197/377 [01:05<00:51,  3.48it/s, recall_0.3=(0, 10248) / 12472]eval:  52%|█████▏    | 197/377 [01:05<00:51,  3.48it/s, recall_0.3=(0, 10288) / 12527]eval:  53%|█████▎    | 198/377 [01:05<00:51,  3.45it/s, recall_0.3=(0, 10288) / 12527]eval:  53%|█████▎    | 198/377 [01:06<00:51,  3.45it/s, recall_0.3=(0, 10313) / 12559]eval:  53%|█████▎    | 199/377 [01:06<00:52,  3.40it/s, recall_0.3=(0, 10313) / 12559]eval:  53%|█████▎    | 199/377 [01:06<00:52,  3.40it/s, recall_0.3=(0, 10347) / 12600]eval:  53%|█████▎    | 200/377 [01:06<00:52,  3.37it/s, recall_0.3=(0, 10347) / 12600]eval:  53%|█████▎    | 200/377 [01:06<00:52,  3.37it/s, recall_0.3=(0, 10395) / 12658]eval:  53%|█████▎    | 201/377 [01:06<00:54,  3.25it/s, recall_0.3=(0, 10395) / 12658]eval:  53%|█████▎    | 201/377 [01:07<00:54,  3.25it/s, recall_0.3=(0, 10436) / 12707]eval:  54%|█████▎    | 202/377 [01:07<00:54,  3.19it/s, recall_0.3=(0, 10436) / 12707]eval:  54%|█████▎    | 202/377 [01:07<00:54,  3.19it/s, recall_0.3=(0, 10485) / 12764]eval:  54%|█████▍    | 203/377 [01:07<00:54,  3.20it/s, recall_0.3=(0, 10485) / 12764]eval:  54%|█████▍    | 203/377 [01:07<00:54,  3.20it/s, recall_0.3=(0, 10519) / 12814]eval:  54%|█████▍    | 204/377 [01:07<00:55,  3.11it/s, recall_0.3=(0, 10519) / 12814]eval:  54%|█████▍    | 204/377 [01:08<00:55,  3.11it/s, recall_0.3=(0, 10534) / 12832]eval:  54%|█████▍    | 205/377 [01:08<00:55,  3.07it/s, recall_0.3=(0, 10534) / 12832]eval:  54%|█████▍    | 205/377 [01:08<00:55,  3.07it/s, recall_0.3=(0, 10558) / 12860]eval:  55%|█████▍    | 206/377 [01:08<00:52,  3.23it/s, recall_0.3=(0, 10558) / 12860]eval:  55%|█████▍    | 206/377 [01:08<00:52,  3.23it/s, recall_0.3=(0, 10584) / 12899]eval:  55%|█████▍    | 207/377 [01:08<00:50,  3.36it/s, recall_0.3=(0, 10584) / 12899]eval:  55%|█████▍    | 207/377 [01:08<00:50,  3.36it/s, recall_0.3=(0, 10611) / 12935]eval:  55%|█████▌    | 208/377 [01:08<00:49,  3.44it/s, recall_0.3=(0, 10611) / 12935]eval:  55%|█████▌    | 208/377 [01:09<00:49,  3.44it/s, recall_0.3=(0, 10643) / 12978]eval:  55%|█████▌    | 209/377 [01:09<00:47,  3.52it/s, recall_0.3=(0, 10643) / 12978]eval:  55%|█████▌    | 209/377 [01:09<00:47,  3.52it/s, recall_0.3=(0, 10676) / 13023]eval:  56%|█████▌    | 210/377 [01:09<00:46,  3.57it/s, recall_0.3=(0, 10676) / 13023]eval:  56%|█████▌    | 210/377 [01:09<00:46,  3.57it/s, recall_0.3=(0, 10711) / 13067]eval:  56%|█████▌    | 211/377 [01:09<00:45,  3.63it/s, recall_0.3=(0, 10711) / 13067]eval:  56%|█████▌    | 211/377 [01:09<00:45,  3.63it/s, recall_0.3=(0, 10745) / 13113]eval:  56%|█████▌    | 212/377 [01:09<00:45,  3.66it/s, recall_0.3=(0, 10745) / 13113]eval:  56%|█████▌    | 212/377 [01:10<00:45,  3.66it/s, recall_0.3=(0, 10779) / 13154]eval:  56%|█████▋    | 213/377 [01:10<00:44,  3.68it/s, recall_0.3=(0, 10779) / 13154]eval:  56%|█████▋    | 213/377 [01:10<00:44,  3.68it/s, recall_0.3=(0, 10805) / 13190]eval:  57%|█████▋    | 214/377 [01:10<00:44,  3.70it/s, recall_0.3=(0, 10805) / 13190]eval:  57%|█████▋    | 214/377 [01:10<00:44,  3.70it/s, recall_0.3=(0, 10832) / 13228]eval:  57%|█████▋    | 215/377 [01:10<00:43,  3.71it/s, recall_0.3=(0, 10832) / 13228]eval:  57%|█████▋    | 215/377 [01:11<00:43,  3.71it/s, recall_0.3=(0, 10853) / 13257]eval:  57%|█████▋    | 216/377 [01:11<00:44,  3.61it/s, recall_0.3=(0, 10853) / 13257]eval:  57%|█████▋    | 216/377 [01:11<00:44,  3.61it/s, recall_0.3=(0, 10879) / 13292]eval:  58%|█████▊    | 217/377 [01:11<00:44,  3.60it/s, recall_0.3=(0, 10879) / 13292]eval:  58%|█████▊    | 217/377 [01:11<00:44,  3.60it/s, recall_0.3=(0, 10914) / 13328]eval:  58%|█████▊    | 218/377 [01:11<00:44,  3.60it/s, recall_0.3=(0, 10914) / 13328]eval:  58%|█████▊    | 218/377 [01:11<00:44,  3.60it/s, recall_0.3=(0, 10953) / 13371]eval:  58%|█████▊    | 219/377 [01:11<00:43,  3.60it/s, recall_0.3=(0, 10953) / 13371]eval:  58%|█████▊    | 219/377 [01:12<00:43,  3.60it/s, recall_0.3=(0, 10996) / 13418]eval:  58%|█████▊    | 220/377 [01:12<00:43,  3.60it/s, recall_0.3=(0, 10996) / 13418]eval:  58%|█████▊    | 220/377 [01:12<00:43,  3.60it/s, recall_0.3=(0, 11049) / 13473]eval:  59%|█████▊    | 221/377 [01:12<00:43,  3.59it/s, recall_0.3=(0, 11049) / 13473]eval:  59%|█████▊    | 221/377 [01:12<00:43,  3.59it/s, recall_0.3=(0, 11101) / 13526]eval:  59%|█████▉    | 222/377 [01:12<00:43,  3.57it/s, recall_0.3=(0, 11101) / 13526]eval:  59%|█████▉    | 222/377 [01:13<00:43,  3.57it/s, recall_0.3=(0, 11146) / 13574]eval:  59%|█████▉    | 223/377 [01:13<00:43,  3.58it/s, recall_0.3=(0, 11146) / 13574]eval:  59%|█████▉    | 223/377 [01:13<00:43,  3.58it/s, recall_0.3=(0, 11188) / 13622]eval:  59%|█████▉    | 224/377 [01:13<00:43,  3.48it/s, recall_0.3=(0, 11188) / 13622]eval:  59%|█████▉    | 224/377 [01:13<00:43,  3.48it/s, recall_0.3=(0, 11217) / 13655]eval:  60%|█████▉    | 225/377 [01:13<00:46,  3.25it/s, recall_0.3=(0, 11217) / 13655]eval:  60%|█████▉    | 225/377 [01:13<00:46,  3.25it/s, recall_0.3=(0, 11246) / 13691]eval:  60%|█████▉    | 226/377 [01:13<00:46,  3.24it/s, recall_0.3=(0, 11246) / 13691]eval:  60%|█████▉    | 226/377 [01:14<00:46,  3.24it/s, recall_0.3=(0, 11298) / 13756]eval:  60%|██████    | 227/377 [01:14<00:48,  3.08it/s, recall_0.3=(0, 11298) / 13756]eval:  60%|██████    | 227/377 [01:14<00:48,  3.08it/s, recall_0.3=(0, 11336) / 13809]eval:  60%|██████    | 228/377 [01:14<00:49,  2.99it/s, recall_0.3=(0, 11336) / 13809]eval:  60%|██████    | 228/377 [01:14<00:49,  2.99it/s, recall_0.3=(0, 11367) / 13860]eval:  61%|██████    | 229/377 [01:14<00:47,  3.10it/s, recall_0.3=(0, 11367) / 13860]eval:  61%|██████    | 229/377 [01:15<00:47,  3.10it/s, recall_0.3=(0, 11396) / 13908]eval:  61%|██████    | 230/377 [01:15<00:45,  3.20it/s, recall_0.3=(0, 11396) / 13908]eval:  61%|██████    | 230/377 [01:15<00:45,  3.20it/s, recall_0.3=(0, 11455) / 13972]eval:  61%|██████▏   | 231/377 [01:15<00:46,  3.14it/s, recall_0.3=(0, 11455) / 13972]eval:  61%|██████▏   | 231/377 [01:15<00:46,  3.14it/s, recall_0.3=(0, 11509) / 14030]eval:  62%|██████▏   | 232/377 [01:15<00:46,  3.12it/s, recall_0.3=(0, 11509) / 14030]eval:  62%|██████▏   | 232/377 [01:16<00:46,  3.12it/s, recall_0.3=(0, 11561) / 14092]eval:  62%|██████▏   | 233/377 [01:16<00:47,  3.01it/s, recall_0.3=(0, 11561) / 14092]eval:  62%|██████▏   | 233/377 [01:16<00:47,  3.01it/s, recall_0.3=(0, 11615) / 14161]eval:  62%|██████▏   | 234/377 [01:16<00:49,  2.90it/s, recall_0.3=(0, 11615) / 14161]eval:  62%|██████▏   | 234/377 [01:17<00:49,  2.90it/s, recall_0.3=(0, 11663) / 14220]eval:  62%|██████▏   | 235/377 [01:17<00:50,  2.81it/s, recall_0.3=(0, 11663) / 14220]eval:  62%|██████▏   | 235/377 [01:17<00:50,  2.81it/s, recall_0.3=(0, 11684) / 14256]eval:  63%|██████▎   | 236/377 [01:17<00:50,  2.79it/s, recall_0.3=(0, 11684) / 14256]eval:  63%|██████▎   | 236/377 [01:17<00:50,  2.79it/s, recall_0.3=(0, 11702) / 14276]eval:  63%|██████▎   | 237/377 [01:17<00:50,  2.75it/s, recall_0.3=(0, 11702) / 14276]eval:  63%|██████▎   | 237/377 [01:18<00:50,  2.75it/s, recall_0.3=(0, 11712) / 14289]eval:  63%|██████▎   | 238/377 [01:18<00:50,  2.76it/s, recall_0.3=(0, 11712) / 14289]eval:  63%|██████▎   | 238/377 [01:18<00:50,  2.76it/s, recall_0.3=(0, 11722) / 14303]eval:  63%|██████▎   | 239/377 [01:18<00:49,  2.79it/s, recall_0.3=(0, 11722) / 14303]eval:  63%|██████▎   | 239/377 [01:18<00:49,  2.79it/s, recall_0.3=(0, 11733) / 14319]eval:  64%|██████▎   | 240/377 [01:18<00:47,  2.88it/s, recall_0.3=(0, 11733) / 14319]eval:  64%|██████▎   | 240/377 [01:19<00:47,  2.88it/s, recall_0.3=(0, 11753) / 14345]eval:  64%|██████▍   | 241/377 [01:19<00:46,  2.92it/s, recall_0.3=(0, 11753) / 14345]eval:  64%|██████▍   | 241/377 [01:19<00:46,  2.92it/s, recall_0.3=(0, 11780) / 14380]eval:  64%|██████▍   | 242/377 [01:19<00:48,  2.81it/s, recall_0.3=(0, 11780) / 14380]eval:  64%|██████▍   | 242/377 [01:19<00:48,  2.81it/s, recall_0.3=(0, 11804) / 14407]eval:  64%|██████▍   | 243/377 [01:19<00:48,  2.74it/s, recall_0.3=(0, 11804) / 14407]eval:  64%|██████▍   | 243/377 [01:20<00:48,  2.74it/s, recall_0.3=(0, 11819) / 14428]eval:  65%|██████▍   | 244/377 [01:20<00:48,  2.72it/s, recall_0.3=(0, 11819) / 14428]eval:  65%|██████▍   | 244/377 [01:20<00:48,  2.72it/s, recall_0.3=(0, 11832) / 14450]eval:  65%|██████▍   | 245/377 [01:20<00:49,  2.66it/s, recall_0.3=(0, 11832) / 14450]eval:  65%|██████▍   | 245/377 [01:21<00:49,  2.66it/s, recall_0.3=(0, 11850) / 14474]eval:  65%|██████▌   | 246/377 [01:21<00:48,  2.69it/s, recall_0.3=(0, 11850) / 14474]eval:  65%|██████▌   | 246/377 [01:21<00:48,  2.69it/s, recall_0.3=(0, 11859) / 14486]eval:  66%|██████▌   | 247/377 [01:21<00:47,  2.76it/s, recall_0.3=(0, 11859) / 14486]eval:  66%|██████▌   | 247/377 [01:21<00:47,  2.76it/s, recall_0.3=(0, 11876) / 14511]eval:  66%|██████▌   | 248/377 [01:21<00:46,  2.77it/s, recall_0.3=(0, 11876) / 14511]eval:  66%|██████▌   | 248/377 [01:22<00:46,  2.77it/s, recall_0.3=(0, 11888) / 14526]eval:  66%|██████▌   | 249/377 [01:22<00:46,  2.76it/s, recall_0.3=(0, 11888) / 14526]eval:  66%|██████▌   | 249/377 [01:22<00:46,  2.76it/s, recall_0.3=(0, 11908) / 14555]eval:  66%|██████▋   | 250/377 [01:22<00:46,  2.73it/s, recall_0.3=(0, 11908) / 14555]eval:  66%|██████▋   | 250/377 [01:22<00:46,  2.73it/s, recall_0.3=(0, 11956) / 14610]eval:  67%|██████▋   | 251/377 [01:22<00:43,  2.92it/s, recall_0.3=(0, 11956) / 14610]eval:  67%|██████▋   | 251/377 [01:23<00:43,  2.92it/s, recall_0.3=(0, 12023) / 14684]eval:  67%|██████▋   | 252/377 [01:23<00:41,  3.03it/s, recall_0.3=(0, 12023) / 14684]eval:  67%|██████▋   | 252/377 [01:23<00:41,  3.03it/s, recall_0.3=(0, 12077) / 14747]eval:  67%|██████▋   | 253/377 [01:23<00:39,  3.14it/s, recall_0.3=(0, 12077) / 14747]eval:  67%|██████▋   | 253/377 [01:23<00:39,  3.14it/s, recall_0.3=(0, 12127) / 14817]eval:  67%|██████▋   | 254/377 [01:23<00:39,  3.14it/s, recall_0.3=(0, 12127) / 14817]eval:  67%|██████▋   | 254/377 [01:24<00:39,  3.14it/s, recall_0.3=(0, 12156) / 14851]eval:  68%|██████▊   | 255/377 [01:24<00:39,  3.10it/s, recall_0.3=(0, 12156) / 14851]eval:  68%|██████▊   | 255/377 [01:24<00:39,  3.10it/s, recall_0.3=(0, 12174) / 14877]eval:  68%|██████▊   | 256/377 [01:24<00:39,  3.07it/s, recall_0.3=(0, 12174) / 14877]eval:  68%|██████▊   | 256/377 [01:24<00:39,  3.07it/s, recall_0.3=(0, 12224) / 14936]eval:  68%|██████▊   | 257/377 [01:24<00:38,  3.08it/s, recall_0.3=(0, 12224) / 14936]eval:  68%|██████▊   | 257/377 [01:25<00:38,  3.08it/s, recall_0.3=(0, 12280) / 15004]eval:  68%|██████▊   | 258/377 [01:25<00:38,  3.09it/s, recall_0.3=(0, 12280) / 15004]eval:  68%|██████▊   | 258/377 [01:25<00:38,  3.09it/s, recall_0.3=(0, 12309) / 15040]eval:  69%|██████▊   | 259/377 [01:25<00:39,  3.01it/s, recall_0.3=(0, 12309) / 15040]eval:  69%|██████▊   | 259/377 [01:25<00:39,  3.01it/s, recall_0.3=(0, 12329) / 15069]eval:  69%|██████▉   | 260/377 [01:25<00:38,  3.05it/s, recall_0.3=(0, 12329) / 15069]eval:  69%|██████▉   | 260/377 [01:25<00:38,  3.05it/s, recall_0.3=(0, 12349) / 15091]eval:  69%|██████▉   | 261/377 [01:25<00:37,  3.13it/s, recall_0.3=(0, 12349) / 15091]eval:  69%|██████▉   | 261/377 [01:26<00:37,  3.13it/s, recall_0.3=(0, 12367) / 15114]eval:  69%|██████▉   | 262/377 [01:26<00:36,  3.11it/s, recall_0.3=(0, 12367) / 15114]eval:  69%|██████▉   | 262/377 [01:26<00:36,  3.11it/s, recall_0.3=(0, 12413) / 15166]eval:  70%|██████▉   | 263/377 [01:26<00:39,  2.92it/s, recall_0.3=(0, 12413) / 15166]eval:  70%|██████▉   | 263/377 [01:27<00:39,  2.92it/s, recall_0.3=(0, 12455) / 15224]eval:  70%|███████   | 264/377 [01:27<00:38,  2.90it/s, recall_0.3=(0, 12455) / 15224]eval:  70%|███████   | 264/377 [01:27<00:38,  2.90it/s, recall_0.3=(0, 12488) / 15265]eval:  70%|███████   | 265/377 [01:27<00:37,  2.98it/s, recall_0.3=(0, 12488) / 15265]eval:  70%|███████   | 265/377 [01:27<00:37,  2.98it/s, recall_0.3=(0, 12520) / 15304]eval:  71%|███████   | 266/377 [01:27<00:35,  3.10it/s, recall_0.3=(0, 12520) / 15304]eval:  71%|███████   | 266/377 [01:27<00:35,  3.10it/s, recall_0.3=(0, 12573) / 15371]eval:  71%|███████   | 267/377 [01:27<00:34,  3.14it/s, recall_0.3=(0, 12573) / 15371]eval:  71%|███████   | 267/377 [01:28<00:34,  3.14it/s, recall_0.3=(0, 12627) / 15435]eval:  71%|███████   | 268/377 [01:28<00:34,  3.19it/s, recall_0.3=(0, 12627) / 15435]eval:  71%|███████   | 268/377 [01:28<00:34,  3.19it/s, recall_0.3=(0, 12672) / 15492]eval:  71%|███████▏  | 269/377 [01:28<00:33,  3.19it/s, recall_0.3=(0, 12672) / 15492]eval:  71%|███████▏  | 269/377 [01:28<00:33,  3.19it/s, recall_0.3=(0, 12706) / 15528]eval:  72%|███████▏  | 270/377 [01:28<00:33,  3.17it/s, recall_0.3=(0, 12706) / 15528]eval:  72%|███████▏  | 270/377 [01:29<00:33,  3.17it/s, recall_0.3=(0, 12734) / 15559]eval:  72%|███████▏  | 271/377 [01:29<00:33,  3.14it/s, recall_0.3=(0, 12734) / 15559]eval:  72%|███████▏  | 271/377 [01:29<00:33,  3.14it/s, recall_0.3=(0, 12781) / 15618]eval:  72%|███████▏  | 272/377 [01:29<00:32,  3.23it/s, recall_0.3=(0, 12781) / 15618]eval:  72%|███████▏  | 272/377 [01:29<00:32,  3.23it/s, recall_0.3=(0, 12817) / 15668]eval:  72%|███████▏  | 273/377 [01:29<00:32,  3.24it/s, recall_0.3=(0, 12817) / 15668]eval:  72%|███████▏  | 273/377 [01:30<00:32,  3.24it/s, recall_0.3=(0, 12898) / 15760]eval:  73%|███████▎  | 274/377 [01:30<00:32,  3.21it/s, recall_0.3=(0, 12898) / 15760]eval:  73%|███████▎  | 274/377 [01:30<00:32,  3.21it/s, recall_0.3=(0, 13036) / 15920]eval:  73%|███████▎  | 275/377 [01:30<00:31,  3.23it/s, recall_0.3=(0, 13036) / 15920]eval:  73%|███████▎  | 275/377 [01:30<00:31,  3.23it/s, recall_0.3=(0, 13120) / 16017]eval:  73%|███████▎  | 276/377 [01:30<00:31,  3.23it/s, recall_0.3=(0, 13120) / 16017]eval:  73%|███████▎  | 276/377 [01:31<00:31,  3.23it/s, recall_0.3=(0, 13141) / 16047]eval:  73%|███████▎  | 277/377 [01:31<00:31,  3.21it/s, recall_0.3=(0, 13141) / 16047]eval:  73%|███████▎  | 277/377 [01:31<00:31,  3.21it/s, recall_0.3=(0, 13167) / 16073]eval:  74%|███████▎  | 278/377 [01:31<00:31,  3.15it/s, recall_0.3=(0, 13167) / 16073]eval:  74%|███████▎  | 278/377 [01:31<00:31,  3.15it/s, recall_0.3=(0, 13203) / 16122]eval:  74%|███████▍  | 279/377 [01:31<00:31,  3.14it/s, recall_0.3=(0, 13203) / 16122]eval:  74%|███████▍  | 279/377 [01:32<00:31,  3.14it/s, recall_0.3=(0, 13251) / 16175]eval:  74%|███████▍  | 280/377 [01:32<00:30,  3.17it/s, recall_0.3=(0, 13251) / 16175]eval:  74%|███████▍  | 280/377 [01:32<00:30,  3.17it/s, recall_0.3=(0, 13320) / 16252]eval:  75%|███████▍  | 281/377 [01:32<00:30,  3.15it/s, recall_0.3=(0, 13320) / 16252]eval:  75%|███████▍  | 281/377 [01:32<00:30,  3.15it/s, recall_0.3=(0, 13422) / 16360]eval:  75%|███████▍  | 282/377 [01:32<00:30,  3.07it/s, recall_0.3=(0, 13422) / 16360]eval:  75%|███████▍  | 282/377 [01:33<00:30,  3.07it/s, recall_0.3=(0, 13504) / 16447]eval:  75%|███████▌  | 283/377 [01:33<00:31,  3.02it/s, recall_0.3=(0, 13504) / 16447]eval:  75%|███████▌  | 283/377 [01:33<00:31,  3.02it/s, recall_0.3=(0, 13523) / 16470]eval:  75%|███████▌  | 284/377 [01:33<00:31,  2.93it/s, recall_0.3=(0, 13523) / 16470]eval:  75%|███████▌  | 284/377 [01:33<00:31,  2.93it/s, recall_0.3=(0, 13549) / 16500]eval:  76%|███████▌  | 285/377 [01:33<00:31,  2.97it/s, recall_0.3=(0, 13549) / 16500]eval:  76%|███████▌  | 285/377 [01:34<00:31,  2.97it/s, recall_0.3=(0, 13570) / 16523]eval:  76%|███████▌  | 286/377 [01:34<00:30,  2.99it/s, recall_0.3=(0, 13570) / 16523]eval:  76%|███████▌  | 286/377 [01:34<00:30,  2.99it/s, recall_0.3=(0, 13574) / 16527]eval:  76%|███████▌  | 287/377 [01:34<00:30,  2.97it/s, recall_0.3=(0, 13574) / 16527]eval:  76%|███████▌  | 287/377 [01:34<00:30,  2.97it/s, recall_0.3=(0, 13575) / 16529]eval:  76%|███████▋  | 288/377 [01:34<00:30,  2.93it/s, recall_0.3=(0, 13575) / 16529]eval:  76%|███████▋  | 288/377 [01:35<00:30,  2.93it/s, recall_0.3=(0, 13585) / 16540]eval:  77%|███████▋  | 289/377 [01:35<00:30,  2.89it/s, recall_0.3=(0, 13585) / 16540]eval:  77%|███████▋  | 289/377 [01:35<00:30,  2.89it/s, recall_0.3=(0, 13595) / 16553]eval:  77%|███████▋  | 290/377 [01:35<00:29,  2.94it/s, recall_0.3=(0, 13595) / 16553]eval:  77%|███████▋  | 290/377 [01:35<00:29,  2.94it/s, recall_0.3=(0, 13612) / 16571]eval:  77%|███████▋  | 291/377 [01:35<00:28,  3.03it/s, recall_0.3=(0, 13612) / 16571]eval:  77%|███████▋  | 291/377 [01:36<00:28,  3.03it/s, recall_0.3=(0, 13623) / 16584]eval:  77%|███████▋  | 292/377 [01:36<00:28,  2.97it/s, recall_0.3=(0, 13623) / 16584]eval:  77%|███████▋  | 292/377 [01:36<00:28,  2.97it/s, recall_0.3=(0, 13635) / 16597]eval:  78%|███████▊  | 293/377 [01:36<00:28,  2.94it/s, recall_0.3=(0, 13635) / 16597]eval:  78%|███████▊  | 293/377 [01:36<00:28,  2.94it/s, recall_0.3=(0, 13650) / 16615]eval:  78%|███████▊  | 294/377 [01:36<00:29,  2.86it/s, recall_0.3=(0, 13650) / 16615]eval:  78%|███████▊  | 294/377 [01:37<00:29,  2.86it/s, recall_0.3=(0, 13666) / 16631]eval:  78%|███████▊  | 295/377 [01:37<00:28,  2.85it/s, recall_0.3=(0, 13666) / 16631]eval:  78%|███████▊  | 295/377 [01:37<00:28,  2.85it/s, recall_0.3=(0, 13675) / 16641]eval:  79%|███████▊  | 296/377 [01:37<00:28,  2.86it/s, recall_0.3=(0, 13675) / 16641]eval:  79%|███████▊  | 296/377 [01:37<00:28,  2.86it/s, recall_0.3=(0, 13687) / 16659]eval:  79%|███████▉  | 297/377 [01:37<00:28,  2.80it/s, recall_0.3=(0, 13687) / 16659]eval:  79%|███████▉  | 297/377 [01:38<00:28,  2.80it/s, recall_0.3=(0, 13693) / 16671]eval:  79%|███████▉  | 298/377 [01:38<00:28,  2.76it/s, recall_0.3=(0, 13693) / 16671]eval:  79%|███████▉  | 298/377 [01:38<00:28,  2.76it/s, recall_0.3=(0, 13700) / 16682]eval:  79%|███████▉  | 299/377 [01:38<00:28,  2.73it/s, recall_0.3=(0, 13700) / 16682]eval:  79%|███████▉  | 299/377 [01:38<00:28,  2.73it/s, recall_0.3=(0, 13726) / 16712]eval:  80%|███████▉  | 300/377 [01:38<00:27,  2.81it/s, recall_0.3=(0, 13726) / 16712]eval:  80%|███████▉  | 300/377 [01:39<00:27,  2.81it/s, recall_0.3=(0, 13751) / 16738]eval:  80%|███████▉  | 301/377 [01:39<00:25,  2.92it/s, recall_0.3=(0, 13751) / 16738]eval:  80%|███████▉  | 301/377 [01:39<00:25,  2.92it/s, recall_0.3=(0, 13767) / 16756]eval:  80%|████████  | 302/377 [01:39<00:25,  3.00it/s, recall_0.3=(0, 13767) / 16756]eval:  80%|████████  | 302/377 [01:39<00:25,  3.00it/s, recall_0.3=(0, 13781) / 16774]eval:  80%|████████  | 303/377 [01:39<00:25,  2.92it/s, recall_0.3=(0, 13781) / 16774]eval:  80%|████████  | 303/377 [01:40<00:25,  2.92it/s, recall_0.3=(0, 13797) / 16791]eval:  81%|████████  | 304/377 [01:40<00:25,  2.91it/s, recall_0.3=(0, 13797) / 16791]eval:  81%|████████  | 304/377 [01:40<00:25,  2.91it/s, recall_0.3=(0, 13816) / 16811]eval:  81%|████████  | 305/377 [01:40<00:24,  2.94it/s, recall_0.3=(0, 13816) / 16811]eval:  81%|████████  | 305/377 [01:40<00:24,  2.94it/s, recall_0.3=(0, 13838) / 16838]eval:  81%|████████  | 306/377 [01:40<00:23,  2.98it/s, recall_0.3=(0, 13838) / 16838]eval:  81%|████████  | 306/377 [01:41<00:23,  2.98it/s, recall_0.3=(0, 13859) / 16865]eval:  81%|████████▏ | 307/377 [01:41<00:22,  3.05it/s, recall_0.3=(0, 13859) / 16865]eval:  81%|████████▏ | 307/377 [01:41<00:22,  3.05it/s, recall_0.3=(0, 13887) / 16900]eval:  82%|████████▏ | 308/377 [01:41<00:22,  3.01it/s, recall_0.3=(0, 13887) / 16900]eval:  82%|████████▏ | 308/377 [01:41<00:22,  3.01it/s, recall_0.3=(0, 13924) / 16944]eval:  82%|████████▏ | 309/377 [01:41<00:21,  3.10it/s, recall_0.3=(0, 13924) / 16944]eval:  82%|████████▏ | 309/377 [01:42<00:21,  3.10it/s, recall_0.3=(0, 13985) / 17014]eval:  82%|████████▏ | 310/377 [01:42<00:21,  3.07it/s, recall_0.3=(0, 13985) / 17014]eval:  82%|████████▏ | 310/377 [01:42<00:21,  3.07it/s, recall_0.3=(0, 14019) / 17054]eval:  82%|████████▏ | 311/377 [01:42<00:22,  2.97it/s, recall_0.3=(0, 14019) / 17054]eval:  82%|████████▏ | 311/377 [01:42<00:22,  2.97it/s, recall_0.3=(0, 14030) / 17068]eval:  83%|████████▎ | 312/377 [01:42<00:22,  2.93it/s, recall_0.3=(0, 14030) / 17068]eval:  83%|████████▎ | 312/377 [01:43<00:22,  2.93it/s, recall_0.3=(0, 14033) / 17071]eval:  83%|████████▎ | 313/377 [01:43<00:21,  3.00it/s, recall_0.3=(0, 14033) / 17071]eval:  83%|████████▎ | 313/377 [01:43<00:21,  3.00it/s, recall_0.3=(0, 14035) / 17080]eval:  83%|████████▎ | 314/377 [01:43<00:20,  3.02it/s, recall_0.3=(0, 14035) / 17080]eval:  83%|████████▎ | 314/377 [01:43<00:20,  3.02it/s, recall_0.3=(0, 14057) / 17103]eval:  84%|████████▎ | 315/377 [01:43<00:20,  2.99it/s, recall_0.3=(0, 14057) / 17103]eval:  84%|████████▎ | 315/377 [01:44<00:20,  2.99it/s, recall_0.3=(0, 14092) / 17139]eval:  84%|████████▍ | 316/377 [01:44<00:20,  3.03it/s, recall_0.3=(0, 14092) / 17139]eval:  84%|████████▍ | 316/377 [01:44<00:20,  3.03it/s, recall_0.3=(0, 14134) / 17187]eval:  84%|████████▍ | 317/377 [01:44<00:20,  2.99it/s, recall_0.3=(0, 14134) / 17187]eval:  84%|████████▍ | 317/377 [01:44<00:20,  2.99it/s, recall_0.3=(0, 14186) / 17242]eval:  84%|████████▍ | 318/377 [01:44<00:19,  3.10it/s, recall_0.3=(0, 14186) / 17242]eval:  84%|████████▍ | 318/377 [01:45<00:19,  3.10it/s, recall_0.3=(0, 14208) / 17267]eval:  85%|████████▍ | 319/377 [01:45<00:18,  3.17it/s, recall_0.3=(0, 14208) / 17267]eval:  85%|████████▍ | 319/377 [01:45<00:18,  3.17it/s, recall_0.3=(0, 14217) / 17284]eval:  85%|████████▍ | 320/377 [01:45<00:18,  3.00it/s, recall_0.3=(0, 14217) / 17284]eval:  85%|████████▍ | 320/377 [01:45<00:18,  3.00it/s, recall_0.3=(0, 14234) / 17306]eval:  85%|████████▌ | 321/377 [01:45<00:19,  2.84it/s, recall_0.3=(0, 14234) / 17306]eval:  85%|████████▌ | 321/377 [01:46<00:19,  2.84it/s, recall_0.3=(0, 14266) / 17363]eval:  85%|████████▌ | 322/377 [01:46<00:19,  2.80it/s, recall_0.3=(0, 14266) / 17363]eval:  85%|████████▌ | 322/377 [01:46<00:19,  2.80it/s, recall_0.3=(0, 14320) / 17430]eval:  86%|████████▌ | 323/377 [01:46<00:18,  2.86it/s, recall_0.3=(0, 14320) / 17430]eval:  86%|████████▌ | 323/377 [01:47<00:18,  2.86it/s, recall_0.3=(0, 14381) / 17508]eval:  86%|████████▌ | 324/377 [01:47<00:18,  2.90it/s, recall_0.3=(0, 14381) / 17508]eval:  86%|████████▌ | 324/377 [01:47<00:18,  2.90it/s, recall_0.3=(0, 14457) / 17599]eval:  86%|████████▌ | 325/377 [01:47<00:17,  3.03it/s, recall_0.3=(0, 14457) / 17599]eval:  86%|████████▌ | 325/377 [01:47<00:17,  3.03it/s, recall_0.3=(0, 14521) / 17682]eval:  86%|████████▋ | 326/377 [01:47<00:17,  2.96it/s, recall_0.3=(0, 14521) / 17682]eval:  86%|████████▋ | 326/377 [01:47<00:17,  2.96it/s, recall_0.3=(0, 14549) / 17714]eval:  87%|████████▋ | 327/377 [01:47<00:16,  3.02it/s, recall_0.3=(0, 14549) / 17714]eval:  87%|████████▋ | 327/377 [01:48<00:16,  3.02it/s, recall_0.3=(0, 14579) / 17747]eval:  87%|████████▋ | 328/377 [01:48<00:15,  3.10it/s, recall_0.3=(0, 14579) / 17747]eval:  87%|████████▋ | 328/377 [01:48<00:15,  3.10it/s, recall_0.3=(0, 14609) / 17785]eval:  87%|████████▋ | 329/377 [01:48<00:15,  3.12it/s, recall_0.3=(0, 14609) / 17785]eval:  87%|████████▋ | 329/377 [01:48<00:15,  3.12it/s, recall_0.3=(0, 14646) / 17841]eval:  88%|████████▊ | 330/377 [01:48<00:15,  3.07it/s, recall_0.3=(0, 14646) / 17841]eval:  88%|████████▊ | 330/377 [01:49<00:15,  3.07it/s, recall_0.3=(0, 14688) / 17905]eval:  88%|████████▊ | 331/377 [01:49<00:14,  3.10it/s, recall_0.3=(0, 14688) / 17905]eval:  88%|████████▊ | 331/377 [01:49<00:14,  3.10it/s, recall_0.3=(0, 14729) / 17958]eval:  88%|████████▊ | 332/377 [01:49<00:14,  3.11it/s, recall_0.3=(0, 14729) / 17958]eval:  88%|████████▊ | 332/377 [01:49<00:14,  3.11it/s, recall_0.3=(0, 14800) / 18040]eval:  88%|████████▊ | 333/377 [01:49<00:13,  3.19it/s, recall_0.3=(0, 14800) / 18040]eval:  88%|████████▊ | 333/377 [01:50<00:13,  3.19it/s, recall_0.3=(0, 14876) / 18127]eval:  89%|████████▊ | 334/377 [01:50<00:13,  3.24it/s, recall_0.3=(0, 14876) / 18127]eval:  89%|████████▊ | 334/377 [01:50<00:13,  3.24it/s, recall_0.3=(0, 14941) / 18205]eval:  89%|████████▉ | 335/377 [01:50<00:13,  3.22it/s, recall_0.3=(0, 14941) / 18205]eval:  89%|████████▉ | 335/377 [01:50<00:13,  3.22it/s, recall_0.3=(0, 14976) / 18247]eval:  89%|████████▉ | 336/377 [01:50<00:12,  3.26it/s, recall_0.3=(0, 14976) / 18247]eval:  89%|████████▉ | 336/377 [01:51<00:12,  3.26it/s, recall_0.3=(0, 15000) / 18275]eval:  89%|████████▉ | 337/377 [01:51<00:12,  3.29it/s, recall_0.3=(0, 15000) / 18275]eval:  89%|████████▉ | 337/377 [01:51<00:12,  3.29it/s, recall_0.3=(0, 15039) / 18322]eval:  90%|████████▉ | 338/377 [01:51<00:12,  3.17it/s, recall_0.3=(0, 15039) / 18322]eval:  90%|████████▉ | 338/377 [01:51<00:12,  3.17it/s, recall_0.3=(0, 15089) / 18373]eval:  90%|████████▉ | 339/377 [01:51<00:12,  3.06it/s, recall_0.3=(0, 15089) / 18373]eval:  90%|████████▉ | 339/377 [01:52<00:12,  3.06it/s, recall_0.3=(0, 15097) / 18382]eval:  90%|█████████ | 340/377 [01:52<00:11,  3.15it/s, recall_0.3=(0, 15097) / 18382]eval:  90%|█████████ | 340/377 [01:52<00:11,  3.15it/s, recall_0.3=(0, 15106) / 18391]eval:  90%|█████████ | 341/377 [01:52<00:11,  3.23it/s, recall_0.3=(0, 15106) / 18391]eval:  90%|█████████ | 341/377 [01:52<00:11,  3.23it/s, recall_0.3=(0, 15109) / 18395]eval:  91%|█████████ | 342/377 [01:52<00:10,  3.22it/s, recall_0.3=(0, 15109) / 18395]eval:  91%|█████████ | 342/377 [01:53<00:10,  3.22it/s, recall_0.3=(0, 15110) / 18396]eval:  91%|█████████ | 343/377 [01:53<00:11,  3.08it/s, recall_0.3=(0, 15110) / 18396]eval:  91%|█████████ | 343/377 [01:53<00:11,  3.08it/s, recall_0.3=(0, 15110) / 18396]eval:  91%|█████████ | 344/377 [01:53<00:10,  3.14it/s, recall_0.3=(0, 15110) / 18396]eval:  91%|█████████ | 344/377 [01:53<00:10,  3.14it/s, recall_0.3=(0, 15112) / 18398]eval:  92%|█████████▏| 345/377 [01:53<00:10,  3.19it/s, recall_0.3=(0, 15112) / 18398]eval:  92%|█████████▏| 345/377 [01:53<00:10,  3.19it/s, recall_0.3=(0, 15115) / 18402]eval:  92%|█████████▏| 346/377 [01:53<00:09,  3.17it/s, recall_0.3=(0, 15115) / 18402]eval:  92%|█████████▏| 346/377 [01:54<00:09,  3.17it/s, recall_0.3=(0, 15119) / 18407]eval:  92%|█████████▏| 347/377 [01:54<00:09,  3.09it/s, recall_0.3=(0, 15119) / 18407]eval:  92%|█████████▏| 347/377 [01:54<00:09,  3.09it/s, recall_0.3=(0, 15126) / 18419]eval:  92%|█████████▏| 348/377 [01:54<00:09,  3.08it/s, recall_0.3=(0, 15126) / 18419]eval:  92%|█████████▏| 348/377 [01:54<00:09,  3.08it/s, recall_0.3=(0, 15142) / 18446]eval:  93%|█████████▎| 349/377 [01:54<00:09,  3.09it/s, recall_0.3=(0, 15142) / 18446]eval:  93%|█████████▎| 349/377 [01:55<00:09,  3.09it/s, recall_0.3=(0, 15144) / 18448]eval:  93%|█████████▎| 350/377 [01:55<00:09,  2.98it/s, recall_0.3=(0, 15144) / 18448]eval:  93%|█████████▎| 350/377 [01:55<00:09,  2.98it/s, recall_0.3=(0, 15146) / 18451]eval:  93%|█████████▎| 351/377 [01:55<00:08,  2.96it/s, recall_0.3=(0, 15146) / 18451]eval:  93%|█████████▎| 351/377 [01:55<00:08,  2.96it/s, recall_0.3=(0, 15149) / 18454]eval:  93%|█████████▎| 352/377 [01:55<00:08,  2.92it/s, recall_0.3=(0, 15149) / 18454]eval:  93%|█████████▎| 352/377 [01:56<00:08,  2.92it/s, recall_0.3=(0, 15157) / 18464]eval:  94%|█████████▎| 353/377 [01:56<00:08,  2.98it/s, recall_0.3=(0, 15157) / 18464]eval:  94%|█████████▎| 353/377 [01:56<00:08,  2.98it/s, recall_0.3=(0, 15163) / 18472]eval:  94%|█████████▍| 354/377 [01:56<00:07,  2.89it/s, recall_0.3=(0, 15163) / 18472]eval:  94%|█████████▍| 354/377 [01:57<00:07,  2.89it/s, recall_0.3=(0, 15167) / 18476]eval:  94%|█████████▍| 355/377 [01:57<00:07,  2.86it/s, recall_0.3=(0, 15167) / 18476]eval:  94%|█████████▍| 355/377 [01:57<00:07,  2.86it/s, recall_0.3=(0, 15171) / 18480]eval:  94%|█████████▍| 356/377 [01:57<00:07,  2.89it/s, recall_0.3=(0, 15171) / 18480]eval:  94%|█████████▍| 356/377 [01:57<00:07,  2.89it/s, recall_0.3=(0, 15189) / 18498]eval:  95%|█████████▍| 357/377 [01:57<00:06,  2.89it/s, recall_0.3=(0, 15189) / 18498]eval:  95%|█████████▍| 357/377 [01:58<00:06,  2.89it/s, recall_0.3=(0, 15197) / 18509]eval:  95%|█████████▍| 358/377 [01:58<00:06,  2.91it/s, recall_0.3=(0, 15197) / 18509]eval:  95%|█████████▍| 358/377 [01:58<00:06,  2.91it/s, recall_0.3=(0, 15203) / 18518]eval:  95%|█████████▌| 359/377 [01:58<00:06,  2.93it/s, recall_0.3=(0, 15203) / 18518]eval:  95%|█████████▌| 359/377 [01:58<00:06,  2.93it/s, recall_0.3=(0, 15217) / 18533]eval:  95%|█████████▌| 360/377 [01:58<00:05,  2.95it/s, recall_0.3=(0, 15217) / 18533]eval:  95%|█████████▌| 360/377 [01:59<00:05,  2.95it/s, recall_0.3=(0, 15228) / 18545]eval:  96%|█████████▌| 361/377 [01:59<00:05,  3.06it/s, recall_0.3=(0, 15228) / 18545]eval:  96%|█████████▌| 361/377 [01:59<00:05,  3.06it/s, recall_0.3=(0, 15238) / 18559]eval:  96%|█████████▌| 362/377 [01:59<00:04,  3.16it/s, recall_0.3=(0, 15238) / 18559]eval:  96%|█████████▌| 362/377 [01:59<00:04,  3.16it/s, recall_0.3=(0, 15247) / 18568]eval:  96%|█████████▋| 363/377 [01:59<00:04,  3.19it/s, recall_0.3=(0, 15247) / 18568]eval:  96%|█████████▋| 363/377 [01:59<00:04,  3.19it/s, recall_0.3=(0, 15268) / 18589]eval:  97%|█████████▋| 364/377 [01:59<00:04,  3.18it/s, recall_0.3=(0, 15268) / 18589]eval:  97%|█████████▋| 364/377 [02:00<00:04,  3.18it/s, recall_0.3=(0, 15290) / 18613]eval:  97%|█████████▋| 365/377 [02:00<00:03,  3.03it/s, recall_0.3=(0, 15290) / 18613]eval:  97%|█████████▋| 365/377 [02:00<00:03,  3.03it/s, recall_0.3=(0, 15316) / 18641]eval:  97%|█████████▋| 366/377 [02:00<00:03,  2.92it/s, recall_0.3=(0, 15316) / 18641]eval:  97%|█████████▋| 366/377 [02:01<00:03,  2.92it/s, recall_0.3=(0, 15339) / 18665]eval:  97%|█████████▋| 367/377 [02:01<00:03,  2.89it/s, recall_0.3=(0, 15339) / 18665]eval:  97%|█████████▋| 367/377 [02:01<00:03,  2.89it/s, recall_0.3=(0, 15353) / 18681]eval:  98%|█████████▊| 368/377 [02:01<00:03,  2.85it/s, recall_0.3=(0, 15353) / 18681]eval:  98%|█████████▊| 368/377 [02:01<00:03,  2.85it/s, recall_0.3=(0, 15360) / 18690]eval:  98%|█████████▊| 369/377 [02:01<00:02,  2.82it/s, recall_0.3=(0, 15360) / 18690]eval:  98%|█████████▊| 369/377 [02:02<00:02,  2.82it/s, recall_0.3=(0, 15375) / 18706]eval:  98%|█████████▊| 370/377 [02:02<00:02,  2.79it/s, recall_0.3=(0, 15375) / 18706]eval:  98%|█████████▊| 370/377 [02:02<00:02,  2.79it/s, recall_0.3=(0, 15381) / 18715]eval:  98%|█████████▊| 371/377 [02:02<00:02,  2.84it/s, recall_0.3=(0, 15381) / 18715]eval:  98%|█████████▊| 371/377 [02:02<00:02,  2.84it/s, recall_0.3=(0, 15390) / 18725]eval:  99%|█████████▊| 372/377 [02:02<00:01,  2.90it/s, recall_0.3=(0, 15390) / 18725]eval:  99%|█████████▊| 372/377 [02:03<00:01,  2.90it/s, recall_0.3=(0, 15400) / 18737]eval:  99%|█████████▉| 373/377 [02:03<00:01,  2.94it/s, recall_0.3=(0, 15400) / 18737]eval:  99%|█████████▉| 373/377 [02:03<00:01,  2.94it/s, recall_0.3=(0, 15408) / 18749]eval:  99%|█████████▉| 374/377 [02:03<00:00,  3.01it/s, recall_0.3=(0, 15408) / 18749]eval:  99%|█████████▉| 374/377 [02:03<00:00,  3.01it/s, recall_0.3=(0, 15435) / 18776]eval:  99%|█████████▉| 375/377 [02:03<00:00,  3.19it/s, recall_0.3=(0, 15435) / 18776]eval:  99%|█████████▉| 375/377 [02:04<00:00,  3.19it/s, recall_0.3=(0, 15467) / 18809]eval: 100%|█████████▉| 376/377 [02:04<00:00,  3.27it/s, recall_0.3=(0, 15467) / 18809]eval: 100%|█████████▉| 376/377 [02:04<00:00,  3.27it/s, recall_0.3=(0, 15483) / 18825]eval: 100%|██████████| 377/377 [02:04<00:00,  3.65it/s, recall_0.3=(0, 15483) / 18825]eval: 100%|██████████| 377/377 [02:04<00:00,  3.03it/s, recall_0.3=(0, 15483) / 18825]
2025-09-04 07:37:53,165   INFO  *************** Performance of EPOCH 36 *****************
2025-09-04 07:37:53,165   INFO  Generate label finished(sec_per_example: 0.0218 second).
2025-09-04 07:37:53,165   INFO  recall_roi_0.3: 0.000000
2025-09-04 07:37:53,165   INFO  recall_rcnn_0.3: 0.825382
2025-09-04 07:37:53,165   INFO  recall_roi_0.5: 0.000000
2025-09-04 07:37:53,165   INFO  recall_rcnn_0.5: 0.635504
2025-09-04 07:37:53,165   INFO  recall_roi_0.7: 0.000000
2025-09-04 07:37:53,165   INFO  recall_rcnn_0.7: 0.318828
2025-09-04 07:37:53,168   INFO  Average predicted number of objects(6019 samples): 211.391
======
Loading NuScenes tables for version v1.0-trainval...
32 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 30.4 seconds.
======
Reverse indexing ...
Done reverse indexing in 6.4 seconds.
======
2025-09-04 07:40:56,644   INFO  The predictions of NuScenes have been saved to /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_.0diff_dense_hm_.1lr_.1wd/eval/eval_with_train/epoch_36/val/final_result/data/results_nusc.json
Initializing nuScenes detection evaluation
Loaded results from /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_.0diff_dense_hm_.1lr_.1wd/eval/eval_with_train/epoch_36/val/final_result/data/results_nusc.json. Found detections for 6019 samples.
Loading annotations for val split from nuScenes version: v1.0-trainval
  0%|          | 0/6019 [00:00<?, ?it/s]  3%|▎         | 207/6019 [00:00<00:02, 2066.41it/s]  7%|▋         | 414/6019 [00:00<00:05, 943.23it/s]   9%|▉         | 540/6019 [00:00<00:05, 1024.59it/s] 11%|█         | 665/6019 [00:00<00:05, 1046.28it/s] 13%|█▎        | 784/6019 [00:00<00:05, 964.87it/s]  15%|█▍        | 890/6019 [00:00<00:06, 824.23it/s] 16%|█▋        | 980/6019 [00:01<00:06, 785.06it/s] 18%|█▊        | 1064/6019 [00:01<00:07, 639.79it/s] 19%|█▉        | 1136/6019 [00:01<00:07, 655.37it/s] 20%|██        | 1208/6019 [00:01<00:07, 669.75it/s] 22%|██▏       | 1354/6019 [00:01<00:05, 866.76it/s] 25%|██▌       | 1511/6019 [00:01<00:04, 1048.77it/s] 27%|██▋       | 1624/6019 [00:01<00:05, 750.69it/s]  29%|██▊       | 1716/6019 [00:02<00:05, 778.67it/s] 30%|███       | 1811/6019 [00:02<00:05, 817.33it/s] 32%|███▏      | 1903/6019 [00:02<00:05, 745.38it/s] 33%|███▎      | 1986/6019 [00:02<00:05, 735.66it/s] 34%|███▍      | 2065/6019 [00:02<00:05, 703.36it/s] 36%|███▌      | 2139/6019 [00:02<00:05, 706.22it/s] 37%|███▋      | 2213/6019 [00:02<00:05, 659.46it/s] 38%|███▊      | 2304/6019 [00:02<00:05, 721.52it/s] 40%|███▉      | 2379/6019 [00:02<00:05, 688.85it/s] 42%|████▏     | 2501/6019 [00:03<00:04, 829.36it/s] 43%|████▎     | 2616/6019 [00:03<00:03, 915.09it/s] 45%|████▌     | 2711/6019 [00:03<00:04, 803.80it/s] 46%|████▋     | 2796/6019 [00:03<00:04, 786.27it/s] 48%|████▊     | 2890/6019 [00:03<00:03, 825.78it/s] 50%|████▉     | 2980/6019 [00:03<00:03, 846.00it/s] 51%|█████▏    | 3098/6019 [00:03<00:03, 938.47it/s] 53%|█████▎    | 3194/6019 [00:03<00:03, 917.62it/s] 55%|█████▍    | 3295/6019 [00:03<00:02, 943.72it/s] 56%|█████▋    | 3391/6019 [00:04<00:02, 945.55it/s] 58%|█████▊    | 3514/6019 [00:04<00:02, 1026.23it/s] 60%|██████    | 3618/6019 [00:04<00:02, 1019.10it/s] 62%|██████▏   | 3721/6019 [00:04<00:02, 979.05it/s]  64%|██████▍   | 3880/6019 [00:04<00:01, 1153.45it/s] 67%|██████▋   | 4028/6019 [00:04<00:01, 1242.80it/s] 69%|██████▉   | 4154/6019 [00:04<00:01, 1124.12it/s] 71%|███████   | 4270/6019 [00:04<00:01, 1130.50it/s] 73%|███████▎  | 4386/6019 [00:04<00:01, 997.77it/s]  75%|███████▍  | 4490/6019 [00:05<00:01, 921.66it/s] 77%|███████▋  | 4621/6019 [00:05<00:01, 1019.85it/s] 82%|████████▏ | 4906/6019 [00:05<00:00, 1505.11it/s] 84%|████████▍ | 5066/6019 [00:05<00:00, 1483.91it/s] 87%|████████▋ | 5221/6019 [00:05<00:00, 1253.60it/s] 89%|████████▉ | 5357/6019 [00:05<00:00, 1077.71it/s] 95%|█████████▍| 5702/6019 [00:05<00:00, 1621.33it/s]100%|█████████▉| 6016/6019 [00:05<00:00, 1994.28it/s]100%|██████████| 6019/6019 [00:05<00:00, 1005.60it/s]
Loaded ground truth annotations for 6019 samples.
Filtering predictions
=> Original number of boxes: 1272364
=> After distance based filtering: 839808
=> After LIDAR points based filtering: 839808
=> After bike rack filtering: 838918
Filtering ground truth annotations
=> Original number of boxes: 187528
=> After distance based filtering: 134565
=> After LIDAR points based filtering: 121871
=> After bike rack filtering: 121861
Accumulating metric data...
Calculating metrics...
Saving metrics to: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_.0diff_dense_hm_.1lr_.1wd/eval/eval_with_train/epoch_36/val/final_result/data
mAP: 0.6715
mATE: 0.2755
mASE: 0.2443
mAOE: 0.2663
mAVE: 0.2629
mAAE: 0.1887
NDS: 0.7120
Eval time: 77.2s

Per-class results:
Object Class	AP	ATE	ASE	AOE	AVE	AAE
car	0.880	0.166	0.144	0.088	0.263	0.188
truck	0.586	0.315	0.186	0.083	0.260	0.232
bus	0.789	0.275	0.154	0.049	0.427	0.235
trailer	0.432	0.556	0.215	0.474	0.219	0.160
construction_vehicle	0.264	0.696	0.420	0.761	0.132	0.317
pedestrian	0.884	0.124	0.269	0.345	0.218	0.086
motorcycle	0.771	0.179	0.224	0.219	0.398	0.276
bicycle	0.594	0.147	0.251	0.323	0.186	0.017
traffic_cone	0.783	0.109	0.322	nan	nan	nan
barrier	0.734	0.189	0.259	0.054	nan	nan
2025-09-04 07:43:14,881   INFO  ----------------Nuscene detection_cvpr_2019 results-----------------
***car error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.17, 0.14, 0.09, 0.26, 0.19 | 80.13, 88.45, 91.09, 92.23 | mean AP: 0.8797132696971546
***truck error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.31, 0.19, 0.08, 0.26, 0.23 | 39.41, 58.75, 65.95, 70.14 | mean AP: 0.5856312332094751
***construction_vehicle error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.70, 0.42, 0.76, 0.13, 0.32 | 4.51, 16.58, 35.15, 49.23 | mean AP: 0.26366600928871725
***bus error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.28, 0.15, 0.05, 0.43, 0.24 | 59.44, 78.58, 87.59, 89.97 | mean AP: 0.7889225103495368
***trailer error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.56, 0.22, 0.47, 0.22, 0.16 | 12.25, 35.65, 58.06, 66.72 | mean AP: 0.43172385453024364
***barrier error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.19, 0.26, 0.05, nan, nan | 65.10, 73.55, 76.90, 77.86 | mean AP: 0.7335161734335667
***motorcycle error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.18, 0.22, 0.22, 0.40, 0.28 | 69.15, 78.90, 79.97, 80.34 | mean AP: 0.7708811131980676
***bicycle error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.15, 0.25, 0.32, 0.19, 0.02 | 57.38, 59.76, 60.10, 60.46 | mean AP: 0.5942577736767752
***pedestrian error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.12, 0.27, 0.34, 0.22, 0.09 | 86.81, 88.05, 88.95, 89.99 | mean AP: 0.8844865820519231
***traffic_cone error@trans, scale, orient, vel, attr | AP@0.5, 1.0, 2.0, 4.0
0.11, 0.32, nan, nan, nan | 76.06, 77.29, 78.64, 81.08 | mean AP: 0.7826924825980335
--------------average performance-------------
trans_err:	 0.2755
scale_err:	 0.2443
orient_err:	 0.2663
vel_err:	 0.2629
attr_err:	 0.1887
mAP:	 0.6715
NDS:	 0.7120

2025-09-04 07:43:14,882   INFO  Result is saved to /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_.0diff_dense_hm_.1lr_.1wd/eval/eval_with_train/epoch_36/val
2025-09-04 07:43:14,882   INFO  ****************Evaluation done.*****************
2025-09-04 07:43:14,913   INFO  Epoch 36 has been evaluated
Wait 30 seconds for next check (progress: 0.0 / 0 minutes): /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_.0diff_dense_hm_.1lr_.1wd/ckpt 2025-09-04 07:43:44,945   INFO  **********************End evaluation sparse_models/sparse_former_base(default)**********************
