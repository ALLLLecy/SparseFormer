/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
2025-09-04 09:34:00,084   INFO  **********************Start logging**********************
2025-09-04 09:34:00,084   INFO  CUDA_VISIBLE_DEVICES=ALL
2025-09-04 09:34:00,084   INFO  Training in distributed mode : total_batch_size: 16
2025-09-04 09:34:00,084   INFO  cfg_file         cfgs/sparse_models/sparse_former_base.yaml
2025-09-04 09:34:00,085   INFO  batch_size       2
2025-09-04 09:34:00,085   INFO  epochs           36
2025-09-04 09:34:00,085   INFO  workers          12
2025-09-04 09:34:00,085   INFO  extra_tag        default
2025-09-04 09:34:00,085   INFO  ckpt             None
2025-09-04 09:34:00,085   INFO  pretrained_model None
2025-09-04 09:34:00,085   INFO  launcher         pytorch
2025-09-04 09:34:00,085   INFO  tcp_port         18888
2025-09-04 09:34:00,085   INFO  sync_bn          True
2025-09-04 09:34:00,085   INFO  fix_random_seed  False
2025-09-04 09:34:00,085   INFO  ckpt_save_interval 1
2025-09-04 09:34:00,085   INFO  local_rank       0
2025-09-04 09:34:00,085   INFO  max_ckpt_save_num 30
2025-09-04 09:34:00,085   INFO  merge_all_iters_to_one_epoch False
2025-09-04 09:34:00,085   INFO  set_cfgs         None
2025-09-04 09:34:00,085   INFO  max_waiting_mins 0
2025-09-04 09:34:00,085   INFO  start_epoch      0
2025-09-04 09:34:00,086   INFO  num_epochs_to_eval 0
2025-09-04 09:34:00,086   INFO  save_to_file     False
2025-09-04 09:34:00,086   INFO  use_tqdm_to_record False
2025-09-04 09:34:00,086   INFO  logger_iter_interval 50
2025-09-04 09:34:00,086   INFO  ckpt_save_time_interval 300
2025-09-04 09:34:00,086   INFO  wo_gpu_stat      True
2025-09-04 09:34:00,086   INFO  use_amp          False
2025-09-04 09:34:00,086   INFO  eval_map         False
2025-09-04 09:34:00,086   INFO  dataset          nuscenes
2025-09-04 09:34:00,086   INFO  root_dir         /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2
2025-09-04 09:34:00,086   INFO  output_dir       /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_shuflle_.0diff_dense_hm_.3lr_.3wd_copy
2025-09-04 09:34:00,086   INFO  cfg.ROOT_DIR: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2
2025-09-04 09:34:00,086   INFO  cfg.LOCAL_RANK: 0
2025-09-04 09:34:00,086   INFO  cfg.CLASS_NAMES: ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']
2025-09-04 09:34:00,086   INFO  ----------- DATA_CONFIG -----------
2025-09-04 09:34:00,086   INFO  cfg.DATA_CONFIG.DATASET: NuScenesDataset
2025-09-04 09:34:00,086   INFO  cfg.DATA_CONFIG.DATA_PATH: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/data/nuscenes
2025-09-04 09:34:00,086   INFO  cfg.DATA_CONFIG.VERSION: v1.0-trainval
2025-09-04 09:34:00,086   INFO  cfg.DATA_CONFIG.MAX_SWEEPS: 10
2025-09-04 09:34:00,086   INFO  cfg.DATA_CONFIG.PRED_VELOCITY: True
2025-09-04 09:34:00,086   INFO  cfg.DATA_CONFIG.SET_NAN_VELOCITY_TO_ZEROS: True
2025-09-04 09:34:00,086   INFO  cfg.DATA_CONFIG.FILTER_MIN_POINTS_IN_GT: 1
2025-09-04 09:34:00,087   INFO  ----------- DATA_SPLIT -----------
2025-09-04 09:34:00,087   INFO  cfg.DATA_CONFIG.DATA_SPLIT.train: train
2025-09-04 09:34:00,087   INFO  cfg.DATA_CONFIG.DATA_SPLIT.test: val
2025-09-04 09:34:00,087   INFO  ----------- INFO_PATH -----------
2025-09-04 09:34:00,087   INFO  cfg.DATA_CONFIG.INFO_PATH.train: ['nuscenes_infos_10sweeps_train.pkl']
2025-09-04 09:34:00,087   INFO  cfg.DATA_CONFIG.INFO_PATH.test: ['nuscenes_infos_10sweeps_val.pkl']
2025-09-04 09:34:00,087   INFO  cfg.DATA_CONFIG.POINT_CLOUD_RANGE: [-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]
2025-09-04 09:34:00,087   INFO  cfg.DATA_CONFIG.BALANCED_RESAMPLING: False
2025-09-04 09:34:00,087   INFO  ----------- DATA_AUGMENTOR -----------
2025-09-04 09:34:00,087   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.DISABLE_AUG_LIST: ['placeholder']
2025-09-04 09:34:00,087   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.AUG_CONFIG_LIST: [{'NAME': 'gt_sampling', 'DB_INFO_PATH': ['nuscenes_dbinfos_10sweeps_withvelo.pkl'], 'USE_SHARED_MEMORY': True, 'DB_DATA_PATH': ['nuscenes_10sweeps_withvelo_lidar.npy'], 'PREPARE': {'filter_by_min_points': ['car:5', 'truck:5', 'construction_vehicle:5', 'bus:5', 'trailer:5', 'barrier:5', 'motorcycle:5', 'bicycle:5', 'pedestrian:5', 'traffic_cone:5']}, 'SAMPLE_GROUPS': ['car:2', 'truck:3', 'construction_vehicle:7', 'bus:4', 'trailer:6', 'barrier:2', 'motorcycle:6', 'bicycle:6', 'pedestrian:2', 'traffic_cone:2'], 'NUM_POINT_FEATURES': 5, 'DATABASE_WITH_FAKELIDAR': False, 'REMOVE_EXTRA_WIDTH': [0.0, 0.0, 0.0], 'LIMIT_WHOLE_SCENE': True}, {'NAME': 'random_world_flip', 'ALONG_AXIS_LIST': ['x', 'y']}, {'NAME': 'random_world_rotation', 'WORLD_ROT_ANGLE': [-0.78539816, 0.78539816]}, {'NAME': 'random_world_scaling', 'WORLD_SCALE_RANGE': [0.9, 1.1]}, {'NAME': 'random_world_translation', 'NOISE_TRANSLATE_STD': [0.5, 0.5, 0.5]}]
2025-09-04 09:34:00,088   INFO  ----------- POINT_FEATURE_ENCODING -----------
2025-09-04 09:34:00,088   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.encoding_type: absolute_coordinates_encoding
2025-09-04 09:34:00,088   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.used_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-09-04 09:34:00,088   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.src_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-09-04 09:34:00,088   INFO  cfg.DATA_CONFIG.DATA_PROCESSOR: [{'NAME': 'mask_points_and_boxes_outside_range', 'REMOVE_OUTSIDE_BOXES': True}, {'NAME': 'shuffle_points', 'SHUFFLE_ENABLED': {'train': True, 'test': True}}, {'NAME': 'transform_points_to_voxels_placeholder', 'VOXEL_SIZE': [0.3, 0.3, 0.2]}]
2025-09-04 09:34:00,088   INFO  cfg.DATA_CONFIG._BASE_CONFIG_: cfgs/dataset_configs/nuscenes_dataset.yaml
2025-09-04 09:34:00,088   INFO  ----------- SAMPLED_INTERVAL -----------
2025-09-04 09:34:00,088   INFO  cfg.DATA_CONFIG.SAMPLED_INTERVAL.train: 1
2025-09-04 09:34:00,089   INFO  cfg.DATA_CONFIG.SAMPLED_INTERVAL.test: 1
2025-09-04 09:34:00,089   INFO  ----------- MODEL -----------
2025-09-04 09:34:00,089   INFO  cfg.MODEL.NAME: TransFusion
2025-09-04 09:34:00,089   INFO  ----------- VFE -----------
2025-09-04 09:34:00,089   INFO  cfg.MODEL.VFE.NAME: DynamicVoxelVFE
2025-09-04 09:34:00,089   INFO  cfg.MODEL.VFE.WITH_DISTANCE: False
2025-09-04 09:34:00,089   INFO  cfg.MODEL.VFE.USE_ABSLOTE_XYZ: True
2025-09-04 09:34:00,089   INFO  cfg.MODEL.VFE.USE_NORM: True
2025-09-04 09:34:00,089   INFO  cfg.MODEL.VFE.NUM_FILTERS: [64, 64]
2025-09-04 09:34:00,089   INFO  ----------- BACKBONE_3D -----------
2025-09-04 09:34:00,089   INFO  cfg.MODEL.BACKBONE_3D.NAME: FSHNet_nusc
2025-09-04 09:34:00,089   INFO  cfg.MODEL.BACKBONE_3D.FEATURE_DIM: 128
2025-09-04 09:34:00,089   INFO  ----------- SPENCODER -----------
2025-09-04 09:34:00,089   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.DIM: 3
2025-09-04 09:34:00,089   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.NUM_LAYERS: 3
2025-09-04 09:34:00,089   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SHUFFLE: True
2025-09-04 09:34:00,090   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.ORDERS: [['z', 'z-trans'], ['hilbert', 'hilbert-trans'], ['x', 'y']]
2025-09-04 09:34:00,090   INFO  ----------- SMSA -----------
2025-09-04 09:34:00,090   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.EMBED_DIM: 128
2025-09-04 09:34:00,090   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DEPTH: 8
2025-09-04 09:34:00,090   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.NUM_LEVELS: 3
2025-09-04 09:34:00,090   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.WINDOW_SHAPE: [9, 9, 5]
2025-09-04 09:34:00,090   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DROP_PATH: 0.2
2025-09-04 09:34:00,090   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.SPATIAL_ENHANCE: True
2025-09-04 09:34:00,090   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.FUSED_ADD_NORM: True
2025-09-04 09:34:00,090   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.RMS_NORM: True
2025-09-04 09:34:00,090   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.NORM_EPSILON: 1e-05
2025-09-04 09:34:00,090   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.RESIDUAL_IN_FP32: True
2025-09-04 09:34:00,090   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DIFF_COEF: 0.0
2025-09-04 09:34:00,090   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DIFF_KERNEL: [7, 5, 3]
2025-09-04 09:34:00,090   INFO  cfg.MODEL.BACKBONE_3D.AFD_FEATURE_DIM: 128
2025-09-04 09:34:00,090   INFO  cfg.MODEL.BACKBONE_3D.AFD_NUM_LAYERS: 4
2025-09-04 09:34:00,090   INFO  cfg.MODEL.BACKBONE_3D.AFD_NUM_SBB: [2, 1, 1]
2025-09-04 09:34:00,091   INFO  cfg.MODEL.BACKBONE_3D.AFD_DOWN_STRIDE: [1, 2, 2]
2025-09-04 09:34:00,091   INFO  cfg.MODEL.BACKBONE_3D.AFD_DOWN_KERNEL_SIZE: [3, 3, 3]
2025-09-04 09:34:00,091   INFO  cfg.MODEL.BACKBONE_3D.AFD: True
2025-09-04 09:34:00,091   INFO  cfg.MODEL.BACKBONE_3D.FG_THRESHOLD: 0.2
2025-09-04 09:34:00,091   INFO  cfg.MODEL.BACKBONE_3D.FEATMAP_STRIDE: 2
2025-09-04 09:34:00,091   INFO  cfg.MODEL.BACKBONE_3D.DETACH_FEATURE: True
2025-09-04 09:34:00,091   INFO  cfg.MODEL.BACKBONE_3D.GREOUP_POOLING_KERNEL_SIZE: [9, 15, 5, 5]
2025-09-04 09:34:00,091   INFO  cfg.MODEL.BACKBONE_3D.GROUP_CLASS_NAMES: [['car', 'truck', 'construction_vehicle'], ['bus', 'trailer'], ['barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']]
2025-09-04 09:34:00,091   INFO  ----------- DENSE_HEAD -----------
2025-09-04 09:34:00,091   INFO  cfg.MODEL.DENSE_HEAD.CLASS_AGNOSTIC: False
2025-09-04 09:34:00,091   INFO  cfg.MODEL.DENSE_HEAD.NAME: TransFusionHead
2025-09-04 09:34:00,091   INFO  cfg.MODEL.DENSE_HEAD.QUERY_RADIUS: 20
2025-09-04 09:34:00,091   INFO  cfg.MODEL.DENSE_HEAD.QUERY_LOCAL: True
2025-09-04 09:34:00,091   INFO  cfg.MODEL.DENSE_HEAD.USE_BIAS_BEFORE_NORM: True
2025-09-04 09:34:00,091   INFO  cfg.MODEL.DENSE_HEAD.INPUT_FEATURES: 128
2025-09-04 09:34:00,091   INFO  cfg.MODEL.DENSE_HEAD.NUM_PROPOSALS: 600
2025-09-04 09:34:00,091   INFO  cfg.MODEL.DENSE_HEAD.HIDDEN_CHANNEL: 128
2025-09-04 09:34:00,092   INFO  cfg.MODEL.DENSE_HEAD.NUM_CLASSES: 10
2025-09-04 09:34:00,092   INFO  cfg.MODEL.DENSE_HEAD.NUM_HEADS: 8
2025-09-04 09:34:00,092   INFO  cfg.MODEL.DENSE_HEAD.NMS_KERNEL_SIZE: 3
2025-09-04 09:34:00,092   INFO  cfg.MODEL.DENSE_HEAD.FFN_CHANNEL: 256
2025-09-04 09:34:00,092   INFO  cfg.MODEL.DENSE_HEAD.DROPOUT: 0.1
2025-09-04 09:34:00,092   INFO  cfg.MODEL.DENSE_HEAD.BN_MOMENTUM: 0.1
2025-09-04 09:34:00,092   INFO  cfg.MODEL.DENSE_HEAD.ACTIVATION: relu
2025-09-04 09:34:00,092   INFO  cfg.MODEL.DENSE_HEAD.NUM_HM_CONV: 2
2025-09-04 09:34:00,092   INFO  ----------- SEPARATE_HEAD_CFG -----------
2025-09-04 09:34:00,092   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_ORDER: ['center', 'height', 'dim', 'rot', 'vel', 'iou']
2025-09-04 09:34:00,092   INFO  ----------- HEAD_DICT -----------
2025-09-04 09:34:00,092   INFO  ----------- center -----------
2025-09-04 09:34:00,092   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.out_channels: 2
2025-09-04 09:34:00,092   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.num_conv: 2
2025-09-04 09:34:00,092   INFO  ----------- height -----------
2025-09-04 09:34:00,092   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.height.out_channels: 1
2025-09-04 09:34:00,093   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.height.num_conv: 2
2025-09-04 09:34:00,093   INFO  ----------- dim -----------
2025-09-04 09:34:00,093   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.out_channels: 3
2025-09-04 09:34:00,093   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.num_conv: 2
2025-09-04 09:34:00,093   INFO  ----------- rot -----------
2025-09-04 09:34:00,093   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.out_channels: 2
2025-09-04 09:34:00,093   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.num_conv: 2
2025-09-04 09:34:00,093   INFO  ----------- vel -----------
2025-09-04 09:34:00,093   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.out_channels: 2
2025-09-04 09:34:00,093   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.num_conv: 2
2025-09-04 09:34:00,093   INFO  ----------- iou -----------
2025-09-04 09:34:00,093   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.iou.out_channels: 1
2025-09-04 09:34:00,093   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.iou.num_conv: 2
2025-09-04 09:34:00,093   INFO  ----------- TARGET_ASSIGNER_CONFIG -----------
2025-09-04 09:34:00,093   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.FEATURE_MAP_STRIDE: 2
2025-09-04 09:34:00,093   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.DATASET: nuScenes
2025-09-04 09:34:00,093   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.GAUSSIAN_OVERLAP: 0.1
2025-09-04 09:34:00,094   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.MIN_RADIUS: 2
2025-09-04 09:34:00,094   INFO  ----------- HUNGARIAN_ASSIGNER -----------
2025-09-04 09:34:00,094   INFO  ----------- cls_cost -----------
2025-09-04 09:34:00,094   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.gamma: 2.0
2025-09-04 09:34:00,094   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.alpha: 0.25
2025-09-04 09:34:00,094   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.weight: 0.15
2025-09-04 09:34:00,094   INFO  ----------- reg_cost -----------
2025-09-04 09:34:00,094   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.reg_cost.weight: 0.25
2025-09-04 09:34:00,094   INFO  ----------- iou_cost -----------
2025-09-04 09:34:00,094   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.iou_cost.weight: 0.25
2025-09-04 09:34:00,094   INFO  ----------- LOSS_CONFIG -----------
2025-09-04 09:34:00,094   INFO  ----------- LOSS_WEIGHTS -----------
2025-09-04 09:34:00,094   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.cls_weight: 1.0
2025-09-04 09:34:00,094   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.bbox_weight: 0.25
2025-09-04 09:34:00,094   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.hm_weight: 1.0
2025-09-04 09:34:00,094   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.iou_weight: 0.5
2025-09-04 09:34:00,095   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.iou_reg_weight: 0.5
2025-09-04 09:34:00,095   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.code_weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2]
2025-09-04 09:34:00,095   INFO  ----------- LOSS_CLS -----------
2025-09-04 09:34:00,095   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.use_sigmoid: True
2025-09-04 09:34:00,095   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.gamma: 2.0
2025-09-04 09:34:00,095   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.alpha: 0.25
2025-09-04 09:34:00,095   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_IOU: True
2025-09-04 09:34:00,095   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_IOU_REG: True
2025-09-04 09:34:00,095   INFO  ----------- POST_PROCESSING -----------
2025-09-04 09:34:00,095   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.SCORE_THRESH: 0.0
2025-09-04 09:34:00,095   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.POST_CENTER_RANGE: [-61.2, -61.2, -10.0, 61.2, 61.2, 10.0]
2025-09-04 09:34:00,095   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.USE_IOU_TO_RECTIFY_SCORE: True
2025-09-04 09:34:00,095   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.IOU_RECTIFIER: [0.5]
2025-09-04 09:34:00,095   INFO  ----------- NMS_CONFIG -----------
2025-09-04 09:34:00,095   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_TYPE: nms_gpu
2025-09-04 09:34:00,095   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_THRESH: 0.2
2025-09-04 09:34:00,095   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_PRE_MAXSIZE: 1000
2025-09-04 09:34:00,095   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_POST_MAXSIZE: 100
2025-09-04 09:34:00,095   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.SCORE_THRES: 0.0
2025-09-04 09:34:00,095   INFO  ----------- POST_PROCESSING -----------
2025-09-04 09:34:00,096   INFO  cfg.MODEL.POST_PROCESSING.RECALL_THRESH_LIST: [0.3, 0.5, 0.7]
2025-09-04 09:34:00,096   INFO  cfg.MODEL.POST_PROCESSING.SCORE_THRESH: 0.1
2025-09-04 09:34:00,096   INFO  cfg.MODEL.POST_PROCESSING.OUTPUT_RAW_SCORE: False
2025-09-04 09:34:00,096   INFO  cfg.MODEL.POST_PROCESSING.EVAL_METRIC: kitti
2025-09-04 09:34:00,096   INFO  ----------- NMS_CONFIG -----------
2025-09-04 09:34:00,096   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.MULTI_CLASSES_NMS: True
2025-09-04 09:34:00,096   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_TYPE: nms_gpu
2025-09-04 09:34:00,096   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_THRESH: 0.2
2025-09-04 09:34:00,096   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_PRE_MAXSIZE: 1000
2025-09-04 09:34:00,096   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_POST_MAXSIZE: 83
2025-09-04 09:34:00,096   INFO  ----------- OPTIMIZATION -----------
2025-09-04 09:34:00,096   INFO  cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU: 2
2025-09-04 09:34:00,096   INFO  cfg.OPTIMIZATION.NUM_EPOCHS: 36
2025-09-04 09:34:00,096   INFO  cfg.OPTIMIZATION.OPTIMIZER: adam_onecycle
2025-09-04 09:34:00,096   INFO  cfg.OPTIMIZATION.LR: 0.003
2025-09-04 09:34:00,096   INFO  cfg.OPTIMIZATION.WEIGHT_DECAY: 0.03
2025-09-04 09:34:00,096   INFO  cfg.OPTIMIZATION.MOMENTUM: 0.9
2025-09-04 09:34:00,096   INFO  cfg.OPTIMIZATION.MOMS: [0.95, 0.85]
2025-09-04 09:34:00,096   INFO  cfg.OPTIMIZATION.PCT_START: 0.4
2025-09-04 09:34:00,096   INFO  cfg.OPTIMIZATION.DIV_FACTOR: 10
2025-09-04 09:34:00,097   INFO  cfg.OPTIMIZATION.DECAY_STEP_LIST: [35, 45]
2025-09-04 09:34:00,097   INFO  cfg.OPTIMIZATION.LR_DECAY: 0.1
2025-09-04 09:34:00,097   INFO  cfg.OPTIMIZATION.LR_CLIP: 1e-07
2025-09-04 09:34:00,097   INFO  cfg.OPTIMIZATION.LR_WARMUP: False
2025-09-04 09:34:00,097   INFO  cfg.OPTIMIZATION.WARMUP_EPOCH: 1
2025-09-04 09:34:00,097   INFO  cfg.OPTIMIZATION.GRAD_NORM_CLIP: 10
2025-09-04 09:34:00,097   INFO  cfg.OPTIMIZATION.LOSS_SCALE_FP16: 32
2025-09-04 09:34:00,097   INFO  ----------- HOOK -----------
2025-09-04 09:34:00,097   INFO  ----------- DisableAugmentationHook -----------
2025-09-04 09:34:00,097   INFO  cfg.HOOK.DisableAugmentationHook.DISABLE_AUG_LIST: ['gt_sampling']
2025-09-04 09:34:00,097   INFO  cfg.HOOK.DisableAugmentationHook.NUM_LAST_EPOCHS: 5
2025-09-04 09:34:00,097   INFO  cfg.TAG: sparse_former_base
2025-09-04 09:34:00,097   INFO  cfg.EXP_GROUP_PATH: sparse_models
2025-09-04 09:34:00,097   INFO  cfg.OUTPUT_DIR: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_shuflle_.0diff_dense_hm_.3lr_.3wd_copy
2025-09-04 09:34:00,112   INFO  ----------- Create dataloader & network & optimizer -----------
2025-09-04 09:34:05,619   INFO  Database filter by min points car: 339949 => 294532
2025-09-04 09:34:05,634   INFO  Database filter by min points truck: 65262 => 60344
2025-09-04 09:34:05,636   INFO  Database filter by min points construction_vehicle: 11050 => 10589
2025-09-04 09:34:05,637   INFO  Database filter by min points bus: 12286 => 11619
2025-09-04 09:34:05,640   INFO  Database filter by min points trailer: 19202 => 17934
2025-09-04 09:34:05,652   INFO  Database filter by min points barrier: 107507 => 101993
2025-09-04 09:34:05,655   INFO  Database filter by min points motorcycle: 8846 => 8055
2025-09-04 09:34:05,656   INFO  Database filter by min points bicycle: 8185 => 7531
2025-09-04 09:34:05,673   INFO  Database filter by min points pedestrian: 161928 => 148520
2025-09-04 09:34:05,681   INFO  Database filter by min points traffic_cone: 62964 => 55504
2025-09-04 09:34:05,681   INFO  Loading GT database to shared memory
eflops102:23:23 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:23:23 [0] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:23:23 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:23:23 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:23:23 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.18.5+cuda11.8
eflops102:26:26 [3] NCCL INFO cudaDriverVersion 12050
eflops102:24:24 [1] NCCL INFO cudaDriverVersion 12050
eflops102:26:26 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:24:24 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:27:27 [4] NCCL INFO cudaDriverVersion 12050
eflops102:27:27 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:30:30 [7] NCCL INFO cudaDriverVersion 12050
eflops102:30:30 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:29:29 [6] NCCL INFO cudaDriverVersion 12050
eflops102:25:25 [2] NCCL INFO cudaDriverVersion 12050
eflops102:29:29 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:25:25 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:28:28 [5] NCCL INFO cudaDriverVersion 12050
eflops102:28:28 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:26:26 [3] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:24:24 [1] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:27:27 [4] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:26:26 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:30:30 [7] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:26:26 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:27:27 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:27:27 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:24:24 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:24:24 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:30:30 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:30:30 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:25:25 [2] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:29:29 [6] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:25:25 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:25:25 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:28:28 [5] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:29:29 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:29:29 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:28:28 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:28:28 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:23:99 [0] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:23:99 [0] NCCL INFO P2P plugin IBext
eflops102:23:99 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:23:99 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:23:99 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:23:99 [0] NCCL INFO NET/IB : No device found.
eflops102:23:99 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:23:99 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:23:99 [0] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:23:99 [0] NCCL INFO Using network Socket
eflops102:27:100 [4] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:27:100 [4] NCCL INFO P2P plugin IBext
eflops102:27:100 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:25:102 [2] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:25:102 [2] NCCL INFO P2P plugin IBext
eflops102:25:102 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:27:100 [4] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:27:100 [4] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:27:100 [4] NCCL INFO NET/IB : No device found.
eflops102:27:100 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:27:100 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:25:102 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:25:102 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:25:102 [2] NCCL INFO NET/IB : No device found.
eflops102:25:102 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:25:102 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:27:100 [4] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:27:100 [4] NCCL INFO Using network Socket
eflops102:25:102 [2] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:25:102 [2] NCCL INFO Using network Socket
eflops102:26:101 [3] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:26:101 [3] NCCL INFO P2P plugin IBext
eflops102:26:101 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:26:101 [3] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:26:101 [3] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:26:101 [3] NCCL INFO NET/IB : No device found.
eflops102:26:101 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:26:101 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:26:101 [3] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:26:101 [3] NCCL INFO Using network Socket
eflops102:29:106 [6] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:29:106 [6] NCCL INFO P2P plugin IBext
eflops102:29:106 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:29:106 [6] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:29:106 [6] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:29:106 [6] NCCL INFO NET/IB : No device found.
eflops102:29:106 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:29:106 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:29:106 [6] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:29:106 [6] NCCL INFO Using network Socket
eflops102:30:103 [7] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:30:103 [7] NCCL INFO P2P plugin IBext
eflops102:30:103 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:30:103 [7] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:30:103 [7] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:30:103 [7] NCCL INFO NET/IB : No device found.
eflops102:30:103 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:30:103 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:30:103 [7] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:30:103 [7] NCCL INFO Using network Socket
eflops102:28:104 [5] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:28:104 [5] NCCL INFO P2P plugin IBext
eflops102:28:104 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:28:104 [5] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:28:104 [5] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:28:104 [5] NCCL INFO NET/IB : No device found.
eflops102:28:104 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:28:104 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:24:105 [1] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:24:105 [1] NCCL INFO P2P plugin IBext
eflops102:24:105 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:28:104 [5] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:28:104 [5] NCCL INFO Using network Socket

eflops102:24:105 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:24:105 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:24:105 [1] NCCL INFO NET/IB : No device found.
eflops102:24:105 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:24:105 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:24:105 [1] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:24:105 [1] NCCL INFO Using network Socket
eflops102:24:105 [1] NCCL INFO comm 0x10af6a60 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 50000 commId 0x967c1b492fcfc119 - Init START
eflops102:23:99 [0] NCCL INFO comm 0x10a7ff40 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 4f000 commId 0x967c1b492fcfc119 - Init START
eflops102:29:106 [6] NCCL INFO comm 0x11588cf0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a0000 commId 0x967c1b492fcfc119 - Init START
eflops102:25:102 [2] NCCL INFO comm 0x10c854b0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 53000 commId 0x967c1b492fcfc119 - Init START
eflops102:30:103 [7] NCCL INFO comm 0x1110f330 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a4000 commId 0x967c1b492fcfc119 - Init START
eflops102:26:101 [3] NCCL INFO comm 0x116a0630 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 57000 commId 0x967c1b492fcfc119 - Init START
eflops102:28:104 [5] NCCL INFO comm 0x11082470 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 9d000 commId 0x967c1b492fcfc119 - Init START
eflops102:27:100 [4] NCCL INFO comm 0x110ae420 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9c000 commId 0x967c1b492fcfc119 - Init START
eflops102:23:99 [0] NCCL INFO Setting affinity for GPU 0 to 0fffff,ff000000,0fffffff
eflops102:30:103 [7] NCCL INFO Setting affinity for GPU 7 to ffff,fff00000,00ffffff,f0000000
eflops102:25:102 [2] NCCL INFO Setting affinity for GPU 2 to 0fffff,ff000000,0fffffff
eflops102:29:106 [6] NCCL INFO Setting affinity for GPU 6 to ffff,fff00000,00ffffff,f0000000
eflops102:28:104 [5] NCCL INFO Setting affinity for GPU 5 to ffff,fff00000,00ffffff,f0000000
eflops102:27:100 [4] NCCL INFO Setting affinity for GPU 4 to ffff,fff00000,00ffffff,f0000000
eflops102:24:105 [1] NCCL INFO Setting affinity for GPU 1 to 0fffff,ff000000,0fffffff
eflops102:26:101 [3] NCCL INFO Setting affinity for GPU 3 to 0fffff,ff000000,0fffffff
eflops102:25:102 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
eflops102:25:102 [2] NCCL INFO P2P Chunksize set to 131072
eflops102:26:101 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
eflops102:30:103 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
eflops102:28:104 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
eflops102:23:99 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
eflops102:27:100 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
eflops102:24:105 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
eflops102:26:101 [3] NCCL INFO P2P Chunksize set to 131072
eflops102:30:103 [7] NCCL INFO P2P Chunksize set to 131072
eflops102:28:104 [5] NCCL INFO P2P Chunksize set to 131072
eflops102:23:99 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
eflops102:27:100 [4] NCCL INFO P2P Chunksize set to 131072
eflops102:24:105 [1] NCCL INFO P2P Chunksize set to 131072
eflops102:23:99 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
eflops102:23:99 [0] NCCL INFO P2P Chunksize set to 131072
eflops102:29:106 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5
eflops102:29:106 [6] NCCL INFO P2P Chunksize set to 131072
eflops102:25:102 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
eflops102:28:104 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC
eflops102:24:105 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
eflops102:29:106 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC
eflops102:30:103 [7] NCCL INFO Channel 00 : 7[7] -> 0[0] via SHM/direct/direct
eflops102:26:101 [3] NCCL INFO Channel 00 : 3[3] -> 4[4] via SHM/direct/direct
eflops102:25:102 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
eflops102:24:105 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
eflops102:28:104 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC
eflops102:30:103 [7] NCCL INFO Channel 01 : 7[7] -> 0[0] via SHM/direct/direct
eflops102:26:101 [3] NCCL INFO Channel 01 : 3[3] -> 4[4] via SHM/direct/direct
eflops102:29:106 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC
eflops102:23:99 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
eflops102:27:100 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
eflops102:23:99 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
eflops102:27:100 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
eflops102:25:102 [2] NCCL INFO Connected all rings
eflops102:29:106 [6] NCCL INFO Connected all rings
eflops102:25:102 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
eflops102:29:106 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC
eflops102:25:102 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
eflops102:29:106 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC
eflops102:23:99 [0] NCCL INFO Connected all rings
eflops102:24:105 [1] NCCL INFO Connected all rings
eflops102:27:100 [4] NCCL INFO Connected all rings
eflops102:26:101 [3] NCCL INFO Connected all rings
eflops102:30:103 [7] NCCL INFO Connected all rings
eflops102:24:105 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
eflops102:30:103 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC
eflops102:28:104 [5] NCCL INFO Connected all rings
eflops102:24:105 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
eflops102:30:103 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC
eflops102:27:100 [4] NCCL INFO Channel 00 : 4[4] -> 3[3] via SHM/direct/direct
eflops102:27:100 [4] NCCL INFO Channel 01 : 4[4] -> 3[3] via SHM/direct/direct
eflops102:28:104 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
eflops102:23:99 [0] NCCL INFO Connected all trees
eflops102:23:99 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:23:99 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:30:103 [7] NCCL INFO Connected all trees
eflops102:30:103 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:30:103 [7] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:28:104 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
eflops102:24:105 [1] NCCL INFO Connected all trees
eflops102:24:105 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:24:105 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:29:106 [6] NCCL INFO Connected all trees
eflops102:29:106 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:29:106 [6] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:28:104 [5] NCCL INFO Connected all trees
eflops102:28:104 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:28:104 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:26:101 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
eflops102:26:101 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
eflops102:25:102 [2] NCCL INFO Connected all trees
eflops102:25:102 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:25:102 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:26:101 [3] NCCL INFO Connected all trees
eflops102:26:101 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:26:101 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:27:100 [4] NCCL INFO Connected all trees
eflops102:27:100 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:27:100 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:28:104 [5] NCCL INFO comm 0x11082470 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 9d000 commId 0x967c1b492fcfc119 - Init COMPLETE
eflops102:30:103 [7] NCCL INFO comm 0x1110f330 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a4000 commId 0x967c1b492fcfc119 - Init COMPLETE
eflops102:24:105 [1] NCCL INFO comm 0x10af6a60 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 50000 commId 0x967c1b492fcfc119 - Init COMPLETE
eflops102:26:101 [3] NCCL INFO comm 0x116a0630 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 57000 commId 0x967c1b492fcfc119 - Init COMPLETE
eflops102:29:106 [6] NCCL INFO comm 0x11588cf0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a0000 commId 0x967c1b492fcfc119 - Init COMPLETE
eflops102:25:102 [2] NCCL INFO comm 0x10c854b0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 53000 commId 0x967c1b492fcfc119 - Init COMPLETE
eflops102:23:99 [0] NCCL INFO comm 0x10a7ff40 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 4f000 commId 0x967c1b492fcfc119 - Init COMPLETE
eflops102:27:100 [4] NCCL INFO comm 0x110ae420 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9c000 commId 0x967c1b492fcfc119 - Init COMPLETE
2025-09-04 09:34:24,371   INFO  GT database has been saved to shared memory
2025-09-04 09:34:24,502   INFO  Loading NuScenes dataset
2025-09-04 09:34:26,245   INFO  Total samples for NuScenes dataset: 28130
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
epochs:   0%|          | 0/36 [00:00<?, ?it/s]2025-09-04 09:34:27,491   INFO  ----------- Model TransFusion created, param count: 24443458 -----------
epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]2025-09-04 09:34:27,491   INFO  DistributedDataParallel(
  (module): TransFusion(
    (vfe): DynamicVoxelVFE(
      (pfn_layers): ModuleList(
        (0): PFNLayerV2(
          (linear): Linear(in_features=11, out_features=32, bias=False)
          (norm): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
        )
        (1): PFNLayerV2(
          (linear): Linear(in_features=64, out_features=64, bias=False)
          (norm): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
        )
      )
    )
    (backbone_3d): FSHNet_nusc(
      (cpe): SparseSequential(
        (0): SparseSequential(
          (0): SubMConv3d(64, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (2): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
      )
      (down1): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (down2): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (down3): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (stage): SPEncoder(
        (blocks): ModuleList(
          (0): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): ZOrderSerialization()
            )
          )
          (1): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): HilbertSerialization()
            )
          )
          (2): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): FlattenWindowsSerialization()
            )
          )
        )
        (embeddings): ModuleList(
          (0-2): 3 x MSSubConvEmbeddingLearned(
            (stem): ModuleList(
              (0-2): 3 x SparseBasicBlock(
                (conv1): SubMConv3d(3, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
              )
            )
          )
        )
      )
      (app): AttnPillarPool(
        (query_func): SparseMaxPool3d(kernel_size=[10, 1, 1], stride=[10, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], algo=ConvAlgo.MaskImplicitGemm)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (pos_embedding): Embedding(10, 128)
      )
      (cls_conv): SparseSequential(
        (0): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): SubMConv2d(128, 3, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
      )
      (afd_layers): ModuleList(
        (0-3): 4 x SEDLayer(
          (encoder): ModuleList(
            (0): SparseSequential(
              (0): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
              (1): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
            )
            (1-2): 2 x SparseSequential(
              (0): SparseSequential(
                (0): SparseConv2d(128, 128, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
            )
          )
          (decoder): ModuleList(
            (0-1): 2 x SparseSequential(
              (0): SparseInverseConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
              (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (decoder_norm): ModuleList(
            (0-1): 2 x SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (shared_conv): SparseSequential(
        (0): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (map_to_bev_module): None
    (pfe): None
    (backbone_2d): None
    (dense_head): TransFusionHead(
      (loss_cls): SigmoidFocalClassificationLoss()
      (loss_bbox): L1Loss()
      (loss_heatmap): GaussianFocalLoss()
      (shared_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (heatmap_head): Sequential(
        (0): BasicBlock2D(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): Conv2d(128, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (class_encoding): Conv1d(10, 128, kernel_size=(1,), stride=(1,))
      (decoder): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_posembed): PositionEmbeddingLearned(
          (position_embedding_head): Sequential(
            (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
        )
        (cross_posembed): PositionEmbeddingLearned(
          (position_embedding_head): Sequential(
            (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
        )
      )
      (prediction_head): SeparateHead_Transfusion(
        (center): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (height): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
        (dim): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 3, kernel_size=(1,), stride=(1,))
        )
        (rot): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (vel): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (iou): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
        (heatmap): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 10, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (point_head): None
    (roi_head): None
  )
)
epochs:   0%|          | 0/36 [00:00<?, ?it/s]2025-09-04 09:34:27,497   INFO  **********************Start training sparse_models/sparse_former_base(default)**********************
epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
2025-09-04 09:34:43,029   INFO  Train:    1/36 (  3%) [   0/1759 (  0%)]  Loss: 1674. (1.67e+03)  LR: 3.000e-04  Grad: 10.0000  max=0.3004(module.dense_head.heatmap_head.1.bias)  min: -0.1844(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1637.4417, loss_cls=18.3865, loss_bbox=9.1933, matched_ious=0.0040, loss_iou=0.3153, loss_iou_reg=0.4888, d_time=1.84(1.84), f_time=10.51(10.51), b_time=12.35(12.35)  Time cost: 00:12/6:08:24 [00:15/221:02:57]  Acc_iter 1           Data time: 1.84(1.84)  Forward time: 10.51(10.51)  Batch time: 12.35(12.35)
2025-09-04 09:35:46,228   INFO  Train:    1/36 (  3%) [  49/1759 (  3%)]  Loss: 16.88 (161.)  LR: 3.000e-04  Grad: 10.0000  max=1.3331(module.dense_head.prediction_head.heatmap.1.bias)  min: -0.2916(module.dense_head.prediction_head.heatmap.0.1.bias)  NaN: False  loss_hm=102.2492, loss_cls=12.2809, loss_bbox=9.6023, matched_ious=0.0058, loss_iou=0.2410, loss_iou_reg=0.2742, d_time=0.00(0.06), f_time=1.26(1.45), b_time=1.26(1.51)  Time cost: 01:15/43:11 [01:18/26:38:02]  Acc_iter 50          Data time: 0.00(0.06)  Forward time: 1.26(1.45)  Batch time: 1.26(1.51)
2025-09-04 09:36:49,974   INFO  Train:    1/36 (  3%) [  99/1759 (  6%)]  Loss: 12.97 (88.7)  LR: 3.001e-04  Grad: 7.6530  max=0.9538(module.dense_head.prediction_head.heatmap.1.bias)  min: -0.2417(module.dense_head.prediction_head.heatmap.0.1.bias)  NaN: False  loss_hm=2.8366, loss_cls=4.7162, loss_bbox=4.5972, matched_ious=0.0439, loss_iou=0.1233, loss_iou_reg=0.4424, d_time=0.00(0.03), f_time=1.24(1.36), b_time=1.24(1.39)  Time cost: 02:19/38:35 [02:22/24:30:05]  Acc_iter 100         Data time: 0.00(0.03)  Forward time: 1.24(1.36)  Batch time: 1.24(1.39)
2025-09-04 09:37:53,795   INFO  Train:    1/36 (  3%) [ 149/1759 (  8%)]  Loss: 11.49 (62.9)  LR: 3.002e-04  Grad: 5.7732  max=0.4294(module.dense_head.prediction_head.heatmap.1.bias)  min: -0.1130(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=2.5336, loss_cls=1.6741, loss_bbox=3.0647, matched_ious=0.0931, loss_iou=0.0993, loss_iou_reg=0.4186, d_time=0.01(0.02), f_time=1.42(1.33), b_time=1.43(1.35)  Time cost: 03:23/36:22 [03:26/23:47:17]  Acc_iter 150         Data time: 0.01(0.02)  Forward time: 1.42(1.33)  Batch time: 1.43(1.35)
2025-09-04 09:38:56,885   INFO  Train:    1/36 (  3%) [ 199/1759 ( 11%)]  Loss: 9.361 (49.8)  LR: 3.004e-04  Grad: 5.4104  max=0.1855(module.dense_head.prediction_head.heatmap.1.bias)  min: -0.1423(module.dense_head.heatmap_head.1.weight)  NaN: False  loss_hm=2.4131, loss_cls=0.9186, loss_bbox=2.8583, matched_ious=0.1137, loss_iou=0.1051, loss_iou_reg=0.4000, d_time=0.01(0.02), f_time=1.14(1.31), b_time=1.16(1.33)  Time cost: 04:26/34:38 [04:29/23:21:30]  Acc_iter 200         Data time: 0.01(0.02)  Forward time: 1.14(1.31)  Batch time: 1.16(1.33)
2025-09-04 09:40:00,092   INFO  Train:    1/36 (  3%) [ 249/1759 ( 14%)]  Loss: 9.439 (41.7)  LR: 3.006e-04  Grad: 5.3513  max=0.1282(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2758(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=2.2731, loss_cls=0.7254, loss_bbox=2.5064, matched_ious=0.1432, loss_iou=0.1129, loss_iou_reg=0.3869, d_time=0.00(0.02), f_time=1.23(1.30), b_time=1.23(1.32)  Time cost: 05:29/33:10 [05:32/23:06:05]  Acc_iter 250         Data time: 0.00(0.02)  Forward time: 1.23(1.30)  Batch time: 1.23(1.32)
2025-09-04 09:41:05,329   INFO  Train:    1/36 (  3%) [ 299/1759 ( 17%)]  Loss: 8.735 (36.2)  LR: 3.009e-04  Grad: 5.6962  max=0.1652(module.dense_head.heatmap_head.1.weight)  min: -0.1261(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=2.1390, loss_cls=0.6145, loss_bbox=2.3162, matched_ious=0.1679, loss_iou=0.1094, loss_iou_reg=0.3786, d_time=0.00(0.02), f_time=1.39(1.30), b_time=1.39(1.32)  Time cost: 06:34/32:01 [06:37/23:02:34]  Acc_iter 300         Data time: 0.00(0.02)  Forward time: 1.39(1.30)  Batch time: 1.39(1.32)
2025-09-04 09:42:09,173   INFO  Train:    1/36 (  3%) [ 349/1759 ( 20%)]  Loss: 7.518 (32.2)  LR: 3.013e-04  Grad: 6.3904  max=0.1661(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2354(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=2.0462, loss_cls=0.5798, loss_bbox=2.0979, matched_ious=0.1991, loss_iou=0.1111, loss_iou_reg=0.3603, d_time=0.01(0.01), f_time=1.14(1.30), b_time=1.15(1.31)  Time cost: 07:38/30:47 [07:41/22:55:35]  Acc_iter 350         Data time: 0.01(0.01)  Forward time: 1.14(1.30)  Batch time: 1.15(1.31)
2025-09-04 09:43:12,346   INFO  Train:    1/36 (  3%) [ 399/1759 ( 23%)]  Loss: 7.147 (29.2)  LR: 3.017e-04  Grad: 6.9362  max=0.2775(module.backbone_3d.cls_conv.3.bias)  min: -0.1721(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.9269, loss_cls=0.5398, loss_bbox=1.9943, matched_ious=0.2149, loss_iou=0.1104, loss_iou_reg=0.3537, d_time=0.01(0.01), f_time=1.26(1.29), b_time=1.27(1.30)  Time cost: 08:41/29:34 [08:44/22:48:18]  Acc_iter 400         Data time: 0.01(0.01)  Forward time: 1.26(1.29)  Batch time: 1.27(1.30)
2025-09-04 09:44:15,490   INFO  Train:    1/36 (  3%) [ 449/1759 ( 26%)]  Loss: 7.167 (26.8)  LR: 3.021e-04  Grad: 7.9209  max=0.2662(module.dense_head.heatmap_head.1.weight)  min: -0.4618(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.8416, loss_cls=0.5228, loss_bbox=1.8384, matched_ious=0.2376, loss_iou=0.1123, loss_iou_reg=0.3434, d_time=0.00(0.01), f_time=1.28(1.29), b_time=1.28(1.30)  Time cost: 09:45/28:23 [09:47/22:42:21]  Acc_iter 450         Data time: 0.00(0.01)  Forward time: 1.28(1.29)  Batch time: 1.28(1.30)
2025-09-04 09:45:18,674   INFO  Train:    1/36 (  3%) [ 499/1759 ( 28%)]  Loss: 8.233 (24.9)  LR: 3.026e-04  Grad: 8.1996  max=0.3163(module.backbone_3d.cls_conv.3.bias)  min: -0.3063(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.7685, loss_cls=0.5008, loss_bbox=1.7916, matched_ious=0.2560, loss_iou=0.1093, loss_iou_reg=0.3347, d_time=0.00(0.01), f_time=1.29(1.28), b_time=1.29(1.30)  Time cost: 10:48/27:13 [10:51/22:37:27]  Acc_iter 500         Data time: 0.00(0.01)  Forward time: 1.29(1.28)  Batch time: 1.29(1.30)
