/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
2025-09-04 09:30:56,662   INFO  **********************Start logging**********************
2025-09-04 09:30:56,662   INFO  CUDA_VISIBLE_DEVICES=ALL
2025-09-04 09:30:56,662   INFO  Training in distributed mode : total_batch_size: 16
2025-09-04 09:30:56,662   INFO  cfg_file         cfgs/sparse_models/sparse_former_base.yaml
2025-09-04 09:30:56,663   INFO  batch_size       2
2025-09-04 09:30:56,663   INFO  epochs           36
2025-09-04 09:30:56,663   INFO  workers          12
2025-09-04 09:30:56,663   INFO  extra_tag        default
2025-09-04 09:30:56,663   INFO  ckpt             None
2025-09-04 09:30:56,663   INFO  pretrained_model None
2025-09-04 09:30:56,663   INFO  launcher         pytorch
2025-09-04 09:30:56,663   INFO  tcp_port         18888
2025-09-04 09:30:56,663   INFO  sync_bn          True
2025-09-04 09:30:56,663   INFO  fix_random_seed  False
2025-09-04 09:30:56,663   INFO  ckpt_save_interval 1
2025-09-04 09:30:56,663   INFO  local_rank       0
2025-09-04 09:30:56,663   INFO  max_ckpt_save_num 30
2025-09-04 09:30:56,663   INFO  merge_all_iters_to_one_epoch False
2025-09-04 09:30:56,663   INFO  set_cfgs         None
2025-09-04 09:30:56,663   INFO  max_waiting_mins 0
2025-09-04 09:30:56,663   INFO  start_epoch      0
2025-09-04 09:30:56,663   INFO  num_epochs_to_eval 0
2025-09-04 09:30:56,663   INFO  save_to_file     False
2025-09-04 09:30:56,663   INFO  use_tqdm_to_record False
2025-09-04 09:30:56,663   INFO  logger_iter_interval 50
2025-09-04 09:30:56,663   INFO  ckpt_save_time_interval 300
2025-09-04 09:30:56,663   INFO  wo_gpu_stat      True
2025-09-04 09:30:56,663   INFO  use_amp          False
2025-09-04 09:30:56,663   INFO  eval_map         False
2025-09-04 09:30:56,664   INFO  dataset          nuscenes
2025-09-04 09:30:56,664   INFO  root_dir         /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2
2025-09-04 09:30:56,664   INFO  output_dir       /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_shuflle_.0diff_dense_hm_.3lr_.3wd_copy
2025-09-04 09:30:56,664   INFO  cfg.ROOT_DIR: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2
2025-09-04 09:30:56,664   INFO  cfg.LOCAL_RANK: 0
2025-09-04 09:30:56,664   INFO  cfg.CLASS_NAMES: ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']
2025-09-04 09:30:56,664   INFO  ----------- DATA_CONFIG -----------
2025-09-04 09:30:56,664   INFO  cfg.DATA_CONFIG.DATASET: NuScenesDataset
2025-09-04 09:30:56,664   INFO  cfg.DATA_CONFIG.DATA_PATH: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/data/nuscenes
2025-09-04 09:30:56,664   INFO  cfg.DATA_CONFIG.VERSION: v1.0-trainval
2025-09-04 09:30:56,664   INFO  cfg.DATA_CONFIG.MAX_SWEEPS: 10
2025-09-04 09:30:56,664   INFO  cfg.DATA_CONFIG.PRED_VELOCITY: True
2025-09-04 09:30:56,664   INFO  cfg.DATA_CONFIG.SET_NAN_VELOCITY_TO_ZEROS: True
2025-09-04 09:30:56,664   INFO  cfg.DATA_CONFIG.FILTER_MIN_POINTS_IN_GT: 1
2025-09-04 09:30:56,664   INFO  ----------- DATA_SPLIT -----------
2025-09-04 09:30:56,664   INFO  cfg.DATA_CONFIG.DATA_SPLIT.train: train
2025-09-04 09:30:56,664   INFO  cfg.DATA_CONFIG.DATA_SPLIT.test: val
2025-09-04 09:30:56,664   INFO  ----------- INFO_PATH -----------
2025-09-04 09:30:56,664   INFO  cfg.DATA_CONFIG.INFO_PATH.train: ['nuscenes_infos_10sweeps_train.pkl']
2025-09-04 09:30:56,664   INFO  cfg.DATA_CONFIG.INFO_PATH.test: ['nuscenes_infos_10sweeps_val.pkl']
2025-09-04 09:30:56,664   INFO  cfg.DATA_CONFIG.POINT_CLOUD_RANGE: [-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]
2025-09-04 09:30:56,664   INFO  cfg.DATA_CONFIG.BALANCED_RESAMPLING: False
2025-09-04 09:30:56,664   INFO  ----------- DATA_AUGMENTOR -----------
2025-09-04 09:30:56,664   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.DISABLE_AUG_LIST: ['placeholder']
2025-09-04 09:30:56,664   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.AUG_CONFIG_LIST: [{'NAME': 'gt_sampling', 'DB_INFO_PATH': ['nuscenes_dbinfos_10sweeps_withvelo.pkl'], 'USE_SHARED_MEMORY': True, 'DB_DATA_PATH': ['nuscenes_10sweeps_withvelo_lidar.npy'], 'PREPARE': {'filter_by_min_points': ['car:5', 'truck:5', 'construction_vehicle:5', 'bus:5', 'trailer:5', 'barrier:5', 'motorcycle:5', 'bicycle:5', 'pedestrian:5', 'traffic_cone:5']}, 'SAMPLE_GROUPS': ['car:2', 'truck:3', 'construction_vehicle:7', 'bus:4', 'trailer:6', 'barrier:2', 'motorcycle:6', 'bicycle:6', 'pedestrian:2', 'traffic_cone:2'], 'NUM_POINT_FEATURES': 5, 'DATABASE_WITH_FAKELIDAR': False, 'REMOVE_EXTRA_WIDTH': [0.0, 0.0, 0.0], 'LIMIT_WHOLE_SCENE': True}, {'NAME': 'random_world_flip', 'ALONG_AXIS_LIST': ['x', 'y']}, {'NAME': 'random_world_rotation', 'WORLD_ROT_ANGLE': [-0.78539816, 0.78539816]}, {'NAME': 'random_world_scaling', 'WORLD_SCALE_RANGE': [0.9, 1.1]}, {'NAME': 'random_world_translation', 'NOISE_TRANSLATE_STD': [0.5, 0.5, 0.5]}]
2025-09-04 09:30:56,694   INFO  ----------- POINT_FEATURE_ENCODING -----------
2025-09-04 09:30:56,694   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.encoding_type: absolute_coordinates_encoding
2025-09-04 09:30:56,694   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.used_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-09-04 09:30:56,694   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.src_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-09-04 09:30:56,694   INFO  cfg.DATA_CONFIG.DATA_PROCESSOR: [{'NAME': 'mask_points_and_boxes_outside_range', 'REMOVE_OUTSIDE_BOXES': True}, {'NAME': 'shuffle_points', 'SHUFFLE_ENABLED': {'train': True, 'test': True}}, {'NAME': 'transform_points_to_voxels_placeholder', 'VOXEL_SIZE': [0.3, 0.3, 0.2]}]
2025-09-04 09:30:56,695   INFO  cfg.DATA_CONFIG._BASE_CONFIG_: cfgs/dataset_configs/nuscenes_dataset.yaml
2025-09-04 09:30:56,695   INFO  ----------- SAMPLED_INTERVAL -----------
2025-09-04 09:30:56,695   INFO  cfg.DATA_CONFIG.SAMPLED_INTERVAL.train: 1
2025-09-04 09:30:56,695   INFO  cfg.DATA_CONFIG.SAMPLED_INTERVAL.test: 1
2025-09-04 09:30:56,695   INFO  ----------- MODEL -----------
2025-09-04 09:30:56,695   INFO  cfg.MODEL.NAME: TransFusion
2025-09-04 09:30:56,695   INFO  ----------- VFE -----------
2025-09-04 09:30:56,695   INFO  cfg.MODEL.VFE.NAME: DynamicVoxelVFE
2025-09-04 09:30:56,695   INFO  cfg.MODEL.VFE.WITH_DISTANCE: False
2025-09-04 09:30:56,696   INFO  cfg.MODEL.VFE.USE_ABSLOTE_XYZ: True
2025-09-04 09:30:56,696   INFO  cfg.MODEL.VFE.USE_NORM: True
2025-09-04 09:30:56,696   INFO  cfg.MODEL.VFE.NUM_FILTERS: [64, 64]
2025-09-04 09:30:56,696   INFO  ----------- BACKBONE_3D -----------
2025-09-04 09:30:56,696   INFO  cfg.MODEL.BACKBONE_3D.NAME: FSHNet_nusc
2025-09-04 09:30:56,696   INFO  cfg.MODEL.BACKBONE_3D.FEATURE_DIM: 128
2025-09-04 09:30:56,696   INFO  ----------- SPENCODER -----------
2025-09-04 09:30:56,696   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.DIM: 3
2025-09-04 09:30:56,696   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.NUM_LAYERS: 3
2025-09-04 09:30:56,697   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SHUFFLE: True
2025-09-04 09:30:56,697   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.ORDERS: [['z', 'z-trans'], ['hilbert', 'hilbert-trans'], ['x', 'y']]
2025-09-04 09:30:56,697   INFO  ----------- SMSA -----------
2025-09-04 09:30:56,697   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.EMBED_DIM: 128
2025-09-04 09:30:56,697   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DEPTH: 8
2025-09-04 09:30:56,697   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.NUM_LEVELS: 3
2025-09-04 09:30:56,697   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.WINDOW_SHAPE: [9, 9, 5]
2025-09-04 09:30:56,697   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DROP_PATH: 0.2
2025-09-04 09:30:56,697   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.SPATIAL_ENHANCE: True
2025-09-04 09:30:56,697   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.FUSED_ADD_NORM: True
2025-09-04 09:30:56,698   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.RMS_NORM: True
2025-09-04 09:30:56,698   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.NORM_EPSILON: 1e-05
2025-09-04 09:30:56,698   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.RESIDUAL_IN_FP32: True
2025-09-04 09:30:56,698   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DIFF_COEF: 0.0
2025-09-04 09:30:56,698   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DIFF_KERNEL: [7, 5, 3]
2025-09-04 09:30:56,698   INFO  cfg.MODEL.BACKBONE_3D.AFD_FEATURE_DIM: 128
2025-09-04 09:30:56,698   INFO  cfg.MODEL.BACKBONE_3D.AFD_NUM_LAYERS: 4
2025-09-04 09:30:56,698   INFO  cfg.MODEL.BACKBONE_3D.AFD_NUM_SBB: [2, 1, 1]
2025-09-04 09:30:56,698   INFO  cfg.MODEL.BACKBONE_3D.AFD_DOWN_STRIDE: [1, 2, 2]
2025-09-04 09:30:56,699   INFO  cfg.MODEL.BACKBONE_3D.AFD_DOWN_KERNEL_SIZE: [3, 3, 3]
2025-09-04 09:30:56,699   INFO  cfg.MODEL.BACKBONE_3D.AFD: True
2025-09-04 09:30:56,699   INFO  cfg.MODEL.BACKBONE_3D.FG_THRESHOLD: 0.2
2025-09-04 09:30:56,699   INFO  cfg.MODEL.BACKBONE_3D.FEATMAP_STRIDE: 2
2025-09-04 09:30:56,699   INFO  cfg.MODEL.BACKBONE_3D.DETACH_FEATURE: True
2025-09-04 09:30:56,699   INFO  cfg.MODEL.BACKBONE_3D.GREOUP_POOLING_KERNEL_SIZE: [9, 15, 5, 5]
2025-09-04 09:30:56,699   INFO  cfg.MODEL.BACKBONE_3D.GROUP_CLASS_NAMES: [['car', 'truck', 'construction_vehicle'], ['bus', 'trailer'], ['barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']]
2025-09-04 09:30:56,699   INFO  ----------- DENSE_HEAD -----------
2025-09-04 09:30:56,699   INFO  cfg.MODEL.DENSE_HEAD.CLASS_AGNOSTIC: False
2025-09-04 09:30:56,699   INFO  cfg.MODEL.DENSE_HEAD.NAME: TransFusionHead
2025-09-04 09:30:56,699   INFO  cfg.MODEL.DENSE_HEAD.QUERY_RADIUS: 20
2025-09-04 09:30:56,699   INFO  cfg.MODEL.DENSE_HEAD.QUERY_LOCAL: True
2025-09-04 09:30:56,699   INFO  cfg.MODEL.DENSE_HEAD.USE_BIAS_BEFORE_NORM: True
2025-09-04 09:30:56,699   INFO  cfg.MODEL.DENSE_HEAD.INPUT_FEATURES: 128
2025-09-04 09:30:56,699   INFO  cfg.MODEL.DENSE_HEAD.NUM_PROPOSALS: 600
2025-09-04 09:30:56,699   INFO  cfg.MODEL.DENSE_HEAD.HIDDEN_CHANNEL: 128
2025-09-04 09:30:56,699   INFO  cfg.MODEL.DENSE_HEAD.NUM_CLASSES: 10
2025-09-04 09:30:56,699   INFO  cfg.MODEL.DENSE_HEAD.NUM_HEADS: 8
2025-09-04 09:30:56,699   INFO  cfg.MODEL.DENSE_HEAD.NMS_KERNEL_SIZE: 3
2025-09-04 09:30:56,699   INFO  cfg.MODEL.DENSE_HEAD.FFN_CHANNEL: 256
2025-09-04 09:30:56,699   INFO  cfg.MODEL.DENSE_HEAD.DROPOUT: 0.1
2025-09-04 09:30:56,699   INFO  cfg.MODEL.DENSE_HEAD.BN_MOMENTUM: 0.1
2025-09-04 09:30:56,700   INFO  cfg.MODEL.DENSE_HEAD.ACTIVATION: relu
2025-09-04 09:30:56,700   INFO  cfg.MODEL.DENSE_HEAD.NUM_HM_CONV: 2
2025-09-04 09:30:56,700   INFO  ----------- SEPARATE_HEAD_CFG -----------
2025-09-04 09:30:56,700   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_ORDER: ['center', 'height', 'dim', 'rot', 'vel', 'iou']
2025-09-04 09:30:56,700   INFO  ----------- HEAD_DICT -----------
2025-09-04 09:30:56,700   INFO  ----------- center -----------
2025-09-04 09:30:56,700   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.out_channels: 2
2025-09-04 09:30:56,700   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.num_conv: 2
2025-09-04 09:30:56,700   INFO  ----------- height -----------
2025-09-04 09:30:56,700   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.height.out_channels: 1
2025-09-04 09:30:56,700   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.height.num_conv: 2
2025-09-04 09:30:56,700   INFO  ----------- dim -----------
2025-09-04 09:30:56,700   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.out_channels: 3
2025-09-04 09:30:56,700   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.num_conv: 2
2025-09-04 09:30:56,700   INFO  ----------- rot -----------
2025-09-04 09:30:56,700   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.out_channels: 2
2025-09-04 09:30:56,700   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.num_conv: 2
2025-09-04 09:30:56,700   INFO  ----------- vel -----------
2025-09-04 09:30:56,700   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.out_channels: 2
2025-09-04 09:30:56,700   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.num_conv: 2
2025-09-04 09:30:56,700   INFO  ----------- iou -----------
2025-09-04 09:30:56,701   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.iou.out_channels: 1
2025-09-04 09:30:56,701   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.iou.num_conv: 2
2025-09-04 09:30:56,701   INFO  ----------- TARGET_ASSIGNER_CONFIG -----------
2025-09-04 09:30:56,701   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.FEATURE_MAP_STRIDE: 2
2025-09-04 09:30:56,701   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.DATASET: nuScenes
2025-09-04 09:30:56,701   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.GAUSSIAN_OVERLAP: 0.1
2025-09-04 09:30:56,701   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.MIN_RADIUS: 2
2025-09-04 09:30:56,701   INFO  ----------- HUNGARIAN_ASSIGNER -----------
2025-09-04 09:30:56,701   INFO  ----------- cls_cost -----------
2025-09-04 09:30:56,701   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.gamma: 2.0
2025-09-04 09:30:56,701   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.alpha: 0.25
2025-09-04 09:30:56,701   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.weight: 0.15
2025-09-04 09:30:56,701   INFO  ----------- reg_cost -----------
2025-09-04 09:30:56,701   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.reg_cost.weight: 0.25
2025-09-04 09:30:56,701   INFO  ----------- iou_cost -----------
2025-09-04 09:30:56,701   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.iou_cost.weight: 0.25
2025-09-04 09:30:56,701   INFO  ----------- LOSS_CONFIG -----------
2025-09-04 09:30:56,701   INFO  ----------- LOSS_WEIGHTS -----------
2025-09-04 09:30:56,701   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.cls_weight: 1.0
2025-09-04 09:30:56,701   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.bbox_weight: 0.25
2025-09-04 09:30:56,701   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.hm_weight: 1.0
2025-09-04 09:30:56,702   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.iou_weight: 0.5
2025-09-04 09:30:56,702   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.iou_reg_weight: 0.5
2025-09-04 09:30:56,702   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.code_weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2]
2025-09-04 09:30:56,702   INFO  ----------- LOSS_CLS -----------
2025-09-04 09:30:56,702   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.use_sigmoid: True
2025-09-04 09:30:56,702   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.gamma: 2.0
2025-09-04 09:30:56,702   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.alpha: 0.25
2025-09-04 09:30:56,702   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_IOU: True
2025-09-04 09:30:56,702   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_IOU_REG: True
2025-09-04 09:30:56,702   INFO  ----------- POST_PROCESSING -----------
2025-09-04 09:30:56,702   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.SCORE_THRESH: 0.0
2025-09-04 09:30:56,702   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.POST_CENTER_RANGE: [-61.2, -61.2, -10.0, 61.2, 61.2, 10.0]
2025-09-04 09:30:56,702   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.USE_IOU_TO_RECTIFY_SCORE: True
2025-09-04 09:30:56,702   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.IOU_RECTIFIER: [0.5]
2025-09-04 09:30:56,702   INFO  ----------- NMS_CONFIG -----------
2025-09-04 09:30:56,702   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_TYPE: nms_gpu
2025-09-04 09:30:56,702   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_THRESH: 0.2
2025-09-04 09:30:56,702   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_PRE_MAXSIZE: 1000
2025-09-04 09:30:56,702   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_POST_MAXSIZE: 100
2025-09-04 09:30:56,702   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.SCORE_THRES: 0.0
2025-09-04 09:30:56,702   INFO  ----------- POST_PROCESSING -----------
2025-09-04 09:30:56,703   INFO  cfg.MODEL.POST_PROCESSING.RECALL_THRESH_LIST: [0.3, 0.5, 0.7]
2025-09-04 09:30:56,703   INFO  cfg.MODEL.POST_PROCESSING.SCORE_THRESH: 0.1
2025-09-04 09:30:56,703   INFO  cfg.MODEL.POST_PROCESSING.OUTPUT_RAW_SCORE: False
2025-09-04 09:30:56,703   INFO  cfg.MODEL.POST_PROCESSING.EVAL_METRIC: kitti
2025-09-04 09:30:56,703   INFO  ----------- NMS_CONFIG -----------
2025-09-04 09:30:56,703   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.MULTI_CLASSES_NMS: True
2025-09-04 09:30:56,703   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_TYPE: nms_gpu
2025-09-04 09:30:56,703   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_THRESH: 0.2
2025-09-04 09:30:56,703   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_PRE_MAXSIZE: 1000
2025-09-04 09:30:56,703   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_POST_MAXSIZE: 83
2025-09-04 09:30:56,703   INFO  ----------- OPTIMIZATION -----------
2025-09-04 09:30:56,703   INFO  cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU: 2
2025-09-04 09:30:56,703   INFO  cfg.OPTIMIZATION.NUM_EPOCHS: 36
2025-09-04 09:30:56,703   INFO  cfg.OPTIMIZATION.OPTIMIZER: adam_onecycle
2025-09-04 09:30:56,703   INFO  cfg.OPTIMIZATION.LR: 0.003
2025-09-04 09:30:56,703   INFO  cfg.OPTIMIZATION.WEIGHT_DECAY: 0.03
2025-09-04 09:30:56,703   INFO  cfg.OPTIMIZATION.MOMENTUM: 0.9
2025-09-04 09:30:56,703   INFO  cfg.OPTIMIZATION.MOMS: [0.95, 0.85]
2025-09-04 09:30:56,703   INFO  cfg.OPTIMIZATION.PCT_START: 0.4
2025-09-04 09:30:56,703   INFO  cfg.OPTIMIZATION.DIV_FACTOR: 10
2025-09-04 09:30:56,704   INFO  cfg.OPTIMIZATION.DECAY_STEP_LIST: [35, 45]
2025-09-04 09:30:56,704   INFO  cfg.OPTIMIZATION.LR_DECAY: 0.1
2025-09-04 09:30:56,704   INFO  cfg.OPTIMIZATION.LR_CLIP: 1e-07
2025-09-04 09:30:56,704   INFO  cfg.OPTIMIZATION.LR_WARMUP: False
2025-09-04 09:30:56,704   INFO  cfg.OPTIMIZATION.WARMUP_EPOCH: 1
2025-09-04 09:30:56,704   INFO  cfg.OPTIMIZATION.GRAD_NORM_CLIP: 10
2025-09-04 09:30:56,704   INFO  cfg.OPTIMIZATION.LOSS_SCALE_FP16: 32
2025-09-04 09:30:56,704   INFO  ----------- HOOK -----------
2025-09-04 09:30:56,704   INFO  ----------- DisableAugmentationHook -----------
2025-09-04 09:30:56,704   INFO  cfg.HOOK.DisableAugmentationHook.DISABLE_AUG_LIST: ['gt_sampling']
2025-09-04 09:30:56,704   INFO  cfg.HOOK.DisableAugmentationHook.NUM_LAST_EPOCHS: 5
2025-09-04 09:30:56,704   INFO  cfg.TAG: sparse_former_base
2025-09-04 09:30:56,704   INFO  cfg.EXP_GROUP_PATH: sparse_models
2025-09-04 09:30:56,704   INFO  cfg.OUTPUT_DIR: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_shuflle_.0diff_dense_hm_.3lr_.3wd_copy
2025-09-04 09:30:56,726   INFO  ----------- Create dataloader & network & optimizer -----------
2025-09-04 09:31:01,957   INFO  Database filter by min points car: 339949 => 294532
2025-09-04 09:31:01,968   INFO  Database filter by min points truck: 65262 => 60344
2025-09-04 09:31:01,970   INFO  Database filter by min points construction_vehicle: 11050 => 10589
2025-09-04 09:31:01,971   INFO  Database filter by min points bus: 12286 => 11619
2025-09-04 09:31:01,973   INFO  Database filter by min points trailer: 19202 => 17934
2025-09-04 09:31:01,985   INFO  Database filter by min points barrier: 107507 => 101993
2025-09-04 09:31:01,987   INFO  Database filter by min points motorcycle: 8846 => 8055
2025-09-04 09:31:01,988   INFO  Database filter by min points bicycle: 8185 => 7531
2025-09-04 09:31:02,005   INFO  Database filter by min points pedestrian: 161928 => 148520
2025-09-04 09:31:02,013   INFO  Database filter by min points traffic_cone: 62964 => 55504
2025-09-04 09:31:02,013   INFO  Loading GT database to shared memory
eflops30:23:23 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:23:23 [0] NCCL INFO Bootstrap : Using bond0:10.16.10.80<0>
eflops30:23:23 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops30:23:23 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops30:23:23 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.18.5+cuda11.8
eflops30:29:29 [6] NCCL INFO cudaDriverVersion 12050
eflops30:29:29 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:25:25 [2] NCCL INFO cudaDriverVersion 12050
eflops30:28:28 [5] NCCL INFO cudaDriverVersion 12050
eflops30:30:30 [7] NCCL INFO cudaDriverVersion 12050
eflops30:25:25 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:30:30 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:28:28 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:26:26 [3] NCCL INFO cudaDriverVersion 12050
eflops30:26:26 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:27:27 [4] NCCL INFO cudaDriverVersion 12050
eflops30:24:24 [1] NCCL INFO cudaDriverVersion 12050
eflops30:27:27 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:24:24 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:29:29 [6] NCCL INFO Bootstrap : Using bond0:10.16.10.80<0>
eflops30:30:30 [7] NCCL INFO Bootstrap : Using bond0:10.16.10.80<0>
eflops30:27:27 [4] NCCL INFO Bootstrap : Using bond0:10.16.10.80<0>
eflops30:29:29 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops30:29:29 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops30:30:30 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops30:30:30 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops30:27:27 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops30:27:27 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops30:25:25 [2] NCCL INFO Bootstrap : Using bond0:10.16.10.80<0>
eflops30:25:25 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops30:25:25 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops30:28:28 [5] NCCL INFO Bootstrap : Using bond0:10.16.10.80<0>
eflops30:26:26 [3] NCCL INFO Bootstrap : Using bond0:10.16.10.80<0>
eflops30:24:24 [1] NCCL INFO Bootstrap : Using bond0:10.16.10.80<0>
eflops30:28:28 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops30:28:28 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops30:24:24 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops30:26:26 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops30:24:24 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops30:26:26 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops30:23:99 [0] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops30:23:99 [0] NCCL INFO P2P plugin IBext
eflops30:23:99 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:28:103 [5] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops30:28:103 [5] NCCL INFO P2P plugin IBext
eflops30:28:103 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops30:28:103 [5] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops30:23:99 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops30:23:99 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0

eflops30:28:103 [5] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops30:23:99 [0] NCCL INFO NET/IB : No device found.
eflops30:28:103 [5] NCCL INFO NET/IB : No device found.
eflops30:23:99 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops30:28:103 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops30:23:99 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:28:103 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:26:106 [3] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops30:26:106 [3] NCCL INFO P2P plugin IBext
eflops30:26:106 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:28:103 [5] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.80<0>
eflops30:23:99 [0] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.80<0>
eflops30:28:103 [5] NCCL INFO Using network Socket
eflops30:23:99 [0] NCCL INFO Using network Socket
eflops30:30:102 [7] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops30:30:102 [7] NCCL INFO P2P plugin IBext
eflops30:30:102 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:29:101 [6] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops30:29:101 [6] NCCL INFO P2P plugin IBext
eflops30:29:101 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops30:26:106 [3] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops30:26:106 [3] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops30:26:106 [3] NCCL INFO NET/IB : No device found.
eflops30:26:106 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops30:26:106 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:26:106 [3] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.80<0>
eflops30:26:106 [3] NCCL INFO Using network Socket

eflops30:29:101 [6] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops30:30:102 [7] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops30:29:101 [6] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0

eflops30:30:102 [7] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops30:29:101 [6] NCCL INFO NET/IB : No device found.
eflops30:30:102 [7] NCCL INFO NET/IB : No device found.
eflops30:29:101 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops30:29:101 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:30:102 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops30:30:102 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:27:104 [4] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops30:27:104 [4] NCCL INFO P2P plugin IBext
eflops30:27:104 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:25:100 [2] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops30:25:100 [2] NCCL INFO P2P plugin IBext
eflops30:25:100 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:29:101 [6] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.80<0>
eflops30:29:101 [6] NCCL INFO Using network Socket
eflops30:30:102 [7] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.80<0>
eflops30:30:102 [7] NCCL INFO Using network Socket
eflops30:24:105 [1] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops30:24:105 [1] NCCL INFO P2P plugin IBext
eflops30:24:105 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops30:27:104 [4] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops30:27:104 [4] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops30:27:104 [4] NCCL INFO NET/IB : No device found.
eflops30:27:104 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops30:27:104 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops30:25:100 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops30:25:100 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops30:25:100 [2] NCCL INFO NET/IB : No device found.
eflops30:25:100 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops30:25:100 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:27:104 [4] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.80<0>
eflops30:27:104 [4] NCCL INFO Using network Socket
eflops30:25:100 [2] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.80<0>
eflops30:25:100 [2] NCCL INFO Using network Socket

eflops30:24:105 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops30:24:105 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops30:24:105 [1] NCCL INFO NET/IB : No device found.
eflops30:24:105 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops30:24:105 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops30:24:105 [1] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.80<0>
eflops30:24:105 [1] NCCL INFO Using network Socket
eflops30:30:102 [7] NCCL INFO comm 0x10a86aa0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a4000 commId 0x17100774291777c6 - Init START
eflops30:23:99 [0] NCCL INFO comm 0x10ee7740 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 35000 commId 0x17100774291777c6 - Init START
eflops30:29:101 [6] NCCL INFO comm 0x11843300 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a0000 commId 0x17100774291777c6 - Init START
eflops30:28:103 [5] NCCL INFO comm 0x122b6a50 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 9d000 commId 0x17100774291777c6 - Init START
eflops30:27:104 [4] NCCL INFO comm 0x1228f7c0 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9c000 commId 0x17100774291777c6 - Init START
eflops30:26:106 [3] NCCL INFO comm 0x11eb0a10 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 3d000 commId 0x17100774291777c6 - Init START
eflops30:25:100 [2] NCCL INFO comm 0x116c7370 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 39000 commId 0x17100774291777c6 - Init START
eflops30:24:105 [1] NCCL INFO comm 0x12668730 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 36000 commId 0x17100774291777c6 - Init START
eflops30:25:100 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff
eflops30:29:101 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,00000000,ffffffff,00000000
eflops30:27:104 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,00000000,ffffffff,00000000
eflops30:28:103 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,00000000,ffffffff,00000000
eflops30:30:102 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,00000000,ffffffff,00000000
eflops30:26:106 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff
eflops30:23:99 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
eflops30:24:105 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
eflops30:23:99 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
eflops30:23:99 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
eflops30:23:99 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
eflops30:23:99 [0] NCCL INFO P2P Chunksize set to 131072
eflops30:30:102 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
eflops30:30:102 [7] NCCL INFO P2P Chunksize set to 131072
eflops30:24:105 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
eflops30:29:101 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5
eflops30:28:103 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
eflops30:24:105 [1] NCCL INFO P2P Chunksize set to 131072
eflops30:29:101 [6] NCCL INFO P2P Chunksize set to 131072
eflops30:28:103 [5] NCCL INFO P2P Chunksize set to 131072
eflops30:27:104 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
eflops30:25:100 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
eflops30:26:106 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
eflops30:27:104 [4] NCCL INFO P2P Chunksize set to 131072
eflops30:26:106 [3] NCCL INFO P2P Chunksize set to 131072
eflops30:25:100 [2] NCCL INFO P2P Chunksize set to 131072
eflops30:25:100 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
eflops30:25:100 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
eflops30:26:106 [3] NCCL INFO Channel 00 : 3[3] -> 4[4] via SHM/direct/direct
eflops30:29:101 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC
eflops30:30:102 [7] NCCL INFO Channel 00 : 7[7] -> 0[0] via SHM/direct/direct
eflops30:26:106 [3] NCCL INFO Channel 01 : 3[3] -> 4[4] via SHM/direct/direct
eflops30:23:99 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
eflops30:30:102 [7] NCCL INFO Channel 01 : 7[7] -> 0[0] via SHM/direct/direct
eflops30:29:101 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC
eflops30:24:105 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
eflops30:27:104 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
eflops30:24:105 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
eflops30:28:103 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC
eflops30:23:99 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
eflops30:28:103 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC
eflops30:27:104 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
eflops30:23:99 [0] NCCL INFO Connected all rings
eflops30:25:100 [2] NCCL INFO Connected all rings
eflops30:24:105 [1] NCCL INFO Connected all rings
eflops30:29:101 [6] NCCL INFO Connected all rings
eflops30:27:104 [4] NCCL INFO Connected all rings
eflops30:28:103 [5] NCCL INFO Connected all rings
eflops30:26:106 [3] NCCL INFO Connected all rings
eflops30:24:105 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
eflops30:25:100 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
eflops30:24:105 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
eflops30:27:104 [4] NCCL INFO Channel 00 : 4[4] -> 3[3] via SHM/direct/direct
eflops30:29:101 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC
eflops30:27:104 [4] NCCL INFO Channel 01 : 4[4] -> 3[3] via SHM/direct/direct
eflops30:25:100 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
eflops30:30:102 [7] NCCL INFO Connected all rings
eflops30:30:102 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC
eflops30:28:103 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
eflops30:29:101 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC
eflops30:28:103 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
eflops30:23:99 [0] NCCL INFO Connected all trees
eflops30:30:102 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC
eflops30:23:99 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops30:23:99 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops30:24:105 [1] NCCL INFO Connected all trees
eflops30:24:105 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops30:24:105 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops30:30:102 [7] NCCL INFO Connected all trees
eflops30:30:102 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops30:30:102 [7] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops30:28:103 [5] NCCL INFO Connected all trees
eflops30:28:103 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops30:28:103 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops30:29:101 [6] NCCL INFO Connected all trees
eflops30:29:101 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops30:29:101 [6] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops30:26:106 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
eflops30:26:106 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
eflops30:26:106 [3] NCCL INFO Connected all trees
eflops30:26:106 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops30:26:106 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops30:25:100 [2] NCCL INFO Connected all trees
eflops30:25:100 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops30:25:100 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops30:27:104 [4] NCCL INFO Connected all trees
eflops30:27:104 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops30:27:104 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops30:28:103 [5] NCCL INFO comm 0x122b6a50 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 9d000 commId 0x17100774291777c6 - Init COMPLETE
eflops30:24:105 [1] NCCL INFO comm 0x12668730 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 36000 commId 0x17100774291777c6 - Init COMPLETE
eflops30:26:106 [3] NCCL INFO comm 0x11eb0a10 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 3d000 commId 0x17100774291777c6 - Init COMPLETE
eflops30:23:99 [0] NCCL INFO comm 0x10ee7740 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 35000 commId 0x17100774291777c6 - Init COMPLETE
eflops30:30:102 [7] NCCL INFO comm 0x10a86aa0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a4000 commId 0x17100774291777c6 - Init COMPLETE
eflops30:27:104 [4] NCCL INFO comm 0x1228f7c0 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9c000 commId 0x17100774291777c6 - Init COMPLETE
eflops30:29:101 [6] NCCL INFO comm 0x11843300 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a0000 commId 0x17100774291777c6 - Init COMPLETE
eflops30:25:100 [2] NCCL INFO comm 0x116c7370 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 39000 commId 0x17100774291777c6 - Init COMPLETE
2025-09-04 09:31:19,548   INFO  GT database has been saved to shared memory
2025-09-04 09:31:19,756   INFO  Loading NuScenes dataset
2025-09-04 09:31:21,685   INFO  Total samples for NuScenes dataset: 28130
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]2025-09-04 09:31:22,559   INFO  ----------- Model TransFusion created, param count: 24443458 -----------
2025-09-04 09:31:22,559   INFO  DistributedDataParallel(
  (module): TransFusion(
    (vfe): DynamicVoxelVFE(
      (pfn_layers): ModuleList(
        (0): PFNLayerV2(
          (linear): Linear(in_features=11, out_features=32, bias=False)
          (norm): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
        )
        (1): PFNLayerV2(
          (linear): Linear(in_features=64, out_features=64, bias=False)
          (norm): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
        )
      )
    )
    (backbone_3d): FSHNet_nusc(
      (cpe): SparseSequential(
        (0): SparseSequential(
          (0): SubMConv3d(64, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (2): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
      )
      (down1): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (down2): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (down3): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (stage): SPEncoder(
        (blocks): ModuleList(
          (0): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): ZOrderSerialization()
            )
          )
          (1): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): HilbertSerialization()
            )
          )
          (2): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): FlattenWindowsSerialization()
            )
          )
        )
        (embeddings): ModuleList(
          (0-2): 3 x MSSubConvEmbeddingLearned(
            (stem): ModuleList(
              (0-2): 3 x SparseBasicBlock(
                (conv1): SubMConv3d(3, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
              )
            )
          )
        )
      )
      (app): AttnPillarPool(
        (query_func): SparseMaxPool3d(kernel_size=[10, 1, 1], stride=[10, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], algo=ConvAlgo.MaskImplicitGemm)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (pos_embedding): Embedding(10, 128)
      )
      (cls_conv): SparseSequential(
        (0): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): SubMConv2d(128, 3, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
      )
      (afd_layers): ModuleList(
        (0-3): 4 x SEDLayer(
          (encoder): ModuleList(
            (0): SparseSequential(
              (0): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
              (1): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
            )
            (1-2): 2 x SparseSequential(
              (0): SparseSequential(
                (0): SparseConv2d(128, 128, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
            )
          )
          (decoder): ModuleList(
            (0-1): 2 x SparseSequential(
              (0): SparseInverseConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
              (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (decoder_norm): ModuleList(
            (0-1): 2 x SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (shared_conv): SparseSequential(
        (0): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (map_to_bev_module): None
    (pfe): None
    (backbone_2d): None
    (dense_head): TransFusionHead(
      (loss_cls): SigmoidFocalClassificationLoss()
      (loss_bbox): L1Loss()
      (loss_heatmap): GaussianFocalLoss()
      (shared_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (heatmap_head): Sequential(
        (0): BasicBlock2D(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): Conv2d(128, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (class_encoding): Conv1d(10, 128, kernel_size=(1,), stride=(1,))
      (decoder): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_posembed): PositionEmbeddingLearned(
          (position_embedding_head): Sequential(
            (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
        )
        (cross_posembed): PositionEmbeddingLearned(
          (position_embedding_head): Sequential(
            (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
        )
      )
      (prediction_head): SeparateHead_Transfusion(
        (center): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (height): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
        (dim): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 3, kernel_size=(1,), stride=(1,))
        )
        (rot): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (vel): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (iou): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
        (heatmap): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 10, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (point_head): None
    (roi_head): None
  )
)
2025-09-04 09:31:22,565   INFO  **********************Start training sparse_models/sparse_former_base(default)**********************
epochs:   0%|          | 0/36 [00:00<?, ?it/s]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
2025-09-04 09:31:36,856   INFO  Train:    1/36 (  3%) [   0/1759 (  0%)]  Loss: 1938. (1.94e+03)  LR: 3.000e-04  Grad: 10.0000  max=0.3253(module.dense_head.heatmap_head.1.bias)  min: -0.1004(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1897.6588, loss_cls=21.9540, loss_bbox=9.8572, matched_ious=0.0056, loss_iou=0.2248, loss_iou_reg=0.4853, d_time=1.58(1.58), f_time=11.05(11.05), b_time=12.63(12.63)  Time cost: 00:12/6:14:55 [00:14/224:57:30]  Acc_iter 1           Data time: 1.58(1.58)  Forward time: 11.05(11.05)  Batch time: 12.63(12.63)
