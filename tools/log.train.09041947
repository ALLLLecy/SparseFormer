/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
2025-09-04 19:47:51,389   INFO  **********************Start logging**********************
2025-09-04 19:47:51,389   INFO  CUDA_VISIBLE_DEVICES=ALL
2025-09-04 19:47:51,390   INFO  Training in distributed mode : total_batch_size: 16
2025-09-04 19:47:51,390   INFO  cfg_file         cfgs/sparse_models/sparse_former_base.yaml
2025-09-04 19:47:51,390   INFO  batch_size       2
2025-09-04 19:47:51,390   INFO  epochs           36
2025-09-04 19:47:51,390   INFO  workers          12
2025-09-04 19:47:51,390   INFO  extra_tag        default
2025-09-04 19:47:51,390   INFO  ckpt             None
2025-09-04 19:47:51,390   INFO  pretrained_model None
2025-09-04 19:47:51,390   INFO  launcher         pytorch
2025-09-04 19:47:51,390   INFO  tcp_port         18888
2025-09-04 19:47:51,390   INFO  sync_bn          True
2025-09-04 19:47:51,390   INFO  fix_random_seed  False
2025-09-04 19:47:51,390   INFO  ckpt_save_interval 1
2025-09-04 19:47:51,390   INFO  local_rank       0
2025-09-04 19:47:51,390   INFO  max_ckpt_save_num 30
2025-09-04 19:47:51,390   INFO  merge_all_iters_to_one_epoch False
2025-09-04 19:47:51,390   INFO  set_cfgs         None
2025-09-04 19:47:51,390   INFO  max_waiting_mins 0
2025-09-04 19:47:51,390   INFO  start_epoch      0
2025-09-04 19:47:51,390   INFO  num_epochs_to_eval 0
2025-09-04 19:47:51,390   INFO  save_to_file     False
2025-09-04 19:47:51,390   INFO  use_tqdm_to_record False
2025-09-04 19:47:51,390   INFO  logger_iter_interval 50
2025-09-04 19:47:51,391   INFO  ckpt_save_time_interval 300
2025-09-04 19:47:51,391   INFO  wo_gpu_stat      True
2025-09-04 19:47:51,391   INFO  use_amp          False
2025-09-04 19:47:51,391   INFO  eval_map         False
2025-09-04 19:47:51,391   INFO  dataset          nuscenes
2025-09-04 19:47:51,391   INFO  root_dir         /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2
2025-09-04 19:47:51,391   INFO  output_dir       /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_.0diff_dense_hm_.3lr_.5wd
2025-09-04 19:47:51,391   INFO  cfg.ROOT_DIR: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2
2025-09-04 19:47:51,391   INFO  cfg.LOCAL_RANK: 0
2025-09-04 19:47:51,391   INFO  cfg.CLASS_NAMES: ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']
2025-09-04 19:47:51,391   INFO  ----------- DATA_CONFIG -----------
2025-09-04 19:47:51,391   INFO  cfg.DATA_CONFIG.DATASET: NuScenesDataset
2025-09-04 19:47:51,391   INFO  cfg.DATA_CONFIG.DATA_PATH: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/data/nuscenes
2025-09-04 19:47:51,391   INFO  cfg.DATA_CONFIG.VERSION: v1.0-trainval
2025-09-04 19:47:51,391   INFO  cfg.DATA_CONFIG.MAX_SWEEPS: 10
2025-09-04 19:47:51,391   INFO  cfg.DATA_CONFIG.PRED_VELOCITY: True
2025-09-04 19:47:51,391   INFO  cfg.DATA_CONFIG.SET_NAN_VELOCITY_TO_ZEROS: True
2025-09-04 19:47:51,391   INFO  cfg.DATA_CONFIG.FILTER_MIN_POINTS_IN_GT: 1
2025-09-04 19:47:51,391   INFO  ----------- DATA_SPLIT -----------
2025-09-04 19:47:51,391   INFO  cfg.DATA_CONFIG.DATA_SPLIT.train: train
2025-09-04 19:47:51,391   INFO  cfg.DATA_CONFIG.DATA_SPLIT.test: val
2025-09-04 19:47:51,391   INFO  ----------- INFO_PATH -----------
2025-09-04 19:47:51,391   INFO  cfg.DATA_CONFIG.INFO_PATH.train: ['nuscenes_infos_10sweeps_train.pkl']
2025-09-04 19:47:51,392   INFO  cfg.DATA_CONFIG.INFO_PATH.test: ['nuscenes_infos_10sweeps_val.pkl']
2025-09-04 19:47:51,392   INFO  cfg.DATA_CONFIG.POINT_CLOUD_RANGE: [-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]
2025-09-04 19:47:51,392   INFO  cfg.DATA_CONFIG.BALANCED_RESAMPLING: False
2025-09-04 19:47:51,392   INFO  ----------- DATA_AUGMENTOR -----------
2025-09-04 19:47:51,392   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.DISABLE_AUG_LIST: ['placeholder']
2025-09-04 19:47:51,392   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.AUG_CONFIG_LIST: [{'NAME': 'gt_sampling', 'DB_INFO_PATH': ['nuscenes_dbinfos_10sweeps_withvelo.pkl'], 'USE_SHARED_MEMORY': True, 'DB_DATA_PATH': ['nuscenes_10sweeps_withvelo_lidar.npy'], 'PREPARE': {'filter_by_min_points': ['car:5', 'truck:5', 'construction_vehicle:5', 'bus:5', 'trailer:5', 'barrier:5', 'motorcycle:5', 'bicycle:5', 'pedestrian:5', 'traffic_cone:5']}, 'SAMPLE_GROUPS': ['car:2', 'truck:3', 'construction_vehicle:7', 'bus:4', 'trailer:6', 'barrier:2', 'motorcycle:6', 'bicycle:6', 'pedestrian:2', 'traffic_cone:2'], 'NUM_POINT_FEATURES': 5, 'DATABASE_WITH_FAKELIDAR': False, 'REMOVE_EXTRA_WIDTH': [0.0, 0.0, 0.0], 'LIMIT_WHOLE_SCENE': True}, {'NAME': 'random_world_flip', 'ALONG_AXIS_LIST': ['x', 'y']}, {'NAME': 'random_world_rotation', 'WORLD_ROT_ANGLE': [-0.78539816, 0.78539816]}, {'NAME': 'random_world_scaling', 'WORLD_SCALE_RANGE': [0.9, 1.1]}, {'NAME': 'random_world_translation', 'NOISE_TRANSLATE_STD': [0.5, 0.5, 0.5]}]
2025-09-04 19:47:51,437   INFO  ----------- POINT_FEATURE_ENCODING -----------
2025-09-04 19:47:51,437   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.encoding_type: absolute_coordinates_encoding
2025-09-04 19:47:51,438   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.used_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-09-04 19:47:51,438   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.src_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-09-04 19:47:51,438   INFO  cfg.DATA_CONFIG.DATA_PROCESSOR: [{'NAME': 'mask_points_and_boxes_outside_range', 'REMOVE_OUTSIDE_BOXES': True}, {'NAME': 'shuffle_points', 'SHUFFLE_ENABLED': {'train': True, 'test': True}}, {'NAME': 'transform_points_to_voxels_placeholder', 'VOXEL_SIZE': [0.3, 0.3, 0.2]}]
2025-09-04 19:47:51,438   INFO  cfg.DATA_CONFIG._BASE_CONFIG_: cfgs/dataset_configs/nuscenes_dataset.yaml
2025-09-04 19:47:51,438   INFO  ----------- SAMPLED_INTERVAL -----------
2025-09-04 19:47:51,438   INFO  cfg.DATA_CONFIG.SAMPLED_INTERVAL.train: 1
2025-09-04 19:47:51,438   INFO  cfg.DATA_CONFIG.SAMPLED_INTERVAL.test: 1
2025-09-04 19:47:51,438   INFO  ----------- MODEL -----------
2025-09-04 19:47:51,438   INFO  cfg.MODEL.NAME: TransFusion
2025-09-04 19:47:51,438   INFO  ----------- VFE -----------
2025-09-04 19:47:51,438   INFO  cfg.MODEL.VFE.NAME: DynamicVoxelVFE
2025-09-04 19:47:51,438   INFO  cfg.MODEL.VFE.WITH_DISTANCE: False
2025-09-04 19:47:51,438   INFO  cfg.MODEL.VFE.USE_ABSLOTE_XYZ: True
2025-09-04 19:47:51,438   INFO  cfg.MODEL.VFE.USE_NORM: True
2025-09-04 19:47:51,439   INFO  cfg.MODEL.VFE.NUM_FILTERS: [64, 64]
2025-09-04 19:47:51,439   INFO  ----------- BACKBONE_3D -----------
2025-09-04 19:47:51,439   INFO  cfg.MODEL.BACKBONE_3D.NAME: FSHNet_nusc
2025-09-04 19:47:51,439   INFO  cfg.MODEL.BACKBONE_3D.FEATURE_DIM: 128
2025-09-04 19:47:51,439   INFO  ----------- SPENCODER -----------
2025-09-04 19:47:51,439   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.DIM: 3
2025-09-04 19:47:51,439   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.NUM_LAYERS: 3
2025-09-04 19:47:51,439   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.WINDOW_SHAPE: [9, 9, 5]
2025-09-04 19:47:51,439   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SHUFFLE: False
2025-09-04 19:47:51,439   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.DEPTH: 8
2025-09-04 19:47:51,439   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.ORDERS: [['z', 'z-trans'], ['hilbert', 'hilbert-trans'], ['x', 'y']]
2025-09-04 19:47:51,439   INFO  ----------- SMSA -----------
2025-09-04 19:47:51,439   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.EMBED_DIM: 128
2025-09-04 19:47:51,439   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DEPTH: 8
2025-09-04 19:47:51,439   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.NUM_LEVELS: 3
2025-09-04 19:47:51,440   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.WINDOW_SHAPE: [9, 9, 5]
2025-09-04 19:47:51,440   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DROP_PATH: 0.2
2025-09-04 19:47:51,440   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.SPATIAL_ENHANCE: True
2025-09-04 19:47:51,440   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.FUSED_ADD_NORM: True
2025-09-04 19:47:51,440   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.RMS_NORM: True
2025-09-04 19:47:51,440   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.NORM_EPSILON: 1e-05
2025-09-04 19:47:51,440   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.RESIDUAL_IN_FP32: True
2025-09-04 19:47:51,440   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DIFF_COEF: 0.0
2025-09-04 19:47:51,440   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DIFF_KERNEL: [7, 5, 3]
2025-09-04 19:47:51,440   INFO  cfg.MODEL.BACKBONE_3D.AFD_FEATURE_DIM: 128
2025-09-04 19:47:51,440   INFO  cfg.MODEL.BACKBONE_3D.AFD_NUM_LAYERS: 4
2025-09-04 19:47:51,440   INFO  cfg.MODEL.BACKBONE_3D.AFD_NUM_SBB: [2, 1, 1]
2025-09-04 19:47:51,440   INFO  cfg.MODEL.BACKBONE_3D.AFD_DOWN_STRIDE: [1, 2, 2]
2025-09-04 19:47:51,440   INFO  cfg.MODEL.BACKBONE_3D.AFD_DOWN_KERNEL_SIZE: [3, 3, 3]
2025-09-04 19:47:51,440   INFO  cfg.MODEL.BACKBONE_3D.AFD: True
2025-09-04 19:47:51,441   INFO  cfg.MODEL.BACKBONE_3D.FG_THRESHOLD: 0.2
2025-09-04 19:47:51,441   INFO  cfg.MODEL.BACKBONE_3D.FEATMAP_STRIDE: 2
2025-09-04 19:47:51,441   INFO  cfg.MODEL.BACKBONE_3D.DETACH_FEATURE: True
2025-09-04 19:47:51,441   INFO  cfg.MODEL.BACKBONE_3D.GREOUP_POOLING_KERNEL_SIZE: [9, 15, 5, 5]
2025-09-04 19:47:51,441   INFO  cfg.MODEL.BACKBONE_3D.GROUP_CLASS_NAMES: [['car', 'truck', 'construction_vehicle'], ['bus', 'trailer'], ['barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']]
2025-09-04 19:47:51,441   INFO  ----------- DENSE_HEAD -----------
2025-09-04 19:47:51,441   INFO  cfg.MODEL.DENSE_HEAD.CLASS_AGNOSTIC: False
2025-09-04 19:47:51,441   INFO  cfg.MODEL.DENSE_HEAD.NAME: TransFusionHead
2025-09-04 19:47:51,441   INFO  cfg.MODEL.DENSE_HEAD.QUERY_RADIUS: 20
2025-09-04 19:47:51,441   INFO  cfg.MODEL.DENSE_HEAD.QUERY_LOCAL: True
2025-09-04 19:47:51,441   INFO  cfg.MODEL.DENSE_HEAD.USE_BIAS_BEFORE_NORM: True
2025-09-04 19:47:51,441   INFO  cfg.MODEL.DENSE_HEAD.INPUT_FEATURES: 128
2025-09-04 19:47:51,441   INFO  cfg.MODEL.DENSE_HEAD.NUM_PROPOSALS: 600
2025-09-04 19:47:51,441   INFO  cfg.MODEL.DENSE_HEAD.HIDDEN_CHANNEL: 128
2025-09-04 19:47:51,441   INFO  cfg.MODEL.DENSE_HEAD.NUM_CLASSES: 10
2025-09-04 19:47:51,441   INFO  cfg.MODEL.DENSE_HEAD.NUM_HEADS: 8
2025-09-04 19:47:51,442   INFO  cfg.MODEL.DENSE_HEAD.NMS_KERNEL_SIZE: 3
2025-09-04 19:47:51,442   INFO  cfg.MODEL.DENSE_HEAD.FFN_CHANNEL: 256
2025-09-04 19:47:51,442   INFO  cfg.MODEL.DENSE_HEAD.DROPOUT: 0.1
2025-09-04 19:47:51,442   INFO  cfg.MODEL.DENSE_HEAD.BN_MOMENTUM: 0.1
2025-09-04 19:47:51,442   INFO  cfg.MODEL.DENSE_HEAD.ACTIVATION: relu
2025-09-04 19:47:51,442   INFO  cfg.MODEL.DENSE_HEAD.NUM_HM_CONV: 2
2025-09-04 19:47:51,442   INFO  ----------- SEPARATE_HEAD_CFG -----------
2025-09-04 19:47:51,442   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_ORDER: ['center', 'height', 'dim', 'rot', 'vel', 'iou']
2025-09-04 19:47:51,442   INFO  ----------- HEAD_DICT -----------
2025-09-04 19:47:51,442   INFO  ----------- center -----------
2025-09-04 19:47:51,442   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.out_channels: 2
2025-09-04 19:47:51,442   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.num_conv: 2
2025-09-04 19:47:51,442   INFO  ----------- height -----------
2025-09-04 19:47:51,442   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.height.out_channels: 1
2025-09-04 19:47:51,442   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.height.num_conv: 2
2025-09-04 19:47:51,443   INFO  ----------- dim -----------
2025-09-04 19:47:51,443   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.out_channels: 3
2025-09-04 19:47:51,443   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.num_conv: 2
2025-09-04 19:47:51,443   INFO  ----------- rot -----------
2025-09-04 19:47:51,443   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.out_channels: 2
2025-09-04 19:47:51,443   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.num_conv: 2
2025-09-04 19:47:51,443   INFO  ----------- vel -----------
2025-09-04 19:47:51,443   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.out_channels: 2
2025-09-04 19:47:51,443   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.num_conv: 2
2025-09-04 19:47:51,443   INFO  ----------- iou -----------
2025-09-04 19:47:51,443   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.iou.out_channels: 1
2025-09-04 19:47:51,443   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.iou.num_conv: 2
2025-09-04 19:47:51,443   INFO  ----------- TARGET_ASSIGNER_CONFIG -----------
2025-09-04 19:47:51,443   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.FEATURE_MAP_STRIDE: 2
2025-09-04 19:47:51,443   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.DATASET: nuScenes
2025-09-04 19:47:51,443   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.GAUSSIAN_OVERLAP: 0.1
2025-09-04 19:47:51,444   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.MIN_RADIUS: 2
2025-09-04 19:47:51,444   INFO  ----------- HUNGARIAN_ASSIGNER -----------
2025-09-04 19:47:51,444   INFO  ----------- cls_cost -----------
2025-09-04 19:47:51,444   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.gamma: 2.0
2025-09-04 19:47:51,444   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.alpha: 0.25
2025-09-04 19:47:51,444   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.weight: 0.15
2025-09-04 19:47:51,444   INFO  ----------- reg_cost -----------
2025-09-04 19:47:51,444   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.reg_cost.weight: 0.25
2025-09-04 19:47:51,444   INFO  ----------- iou_cost -----------
2025-09-04 19:47:51,444   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.iou_cost.weight: 0.25
2025-09-04 19:47:51,444   INFO  ----------- LOSS_CONFIG -----------
2025-09-04 19:47:51,444   INFO  ----------- LOSS_WEIGHTS -----------
2025-09-04 19:47:51,444   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.cls_weight: 1.0
2025-09-04 19:47:51,444   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.bbox_weight: 0.25
2025-09-04 19:47:51,444   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.hm_weight: 1.0
2025-09-04 19:47:51,445   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.iou_weight: 0.5
2025-09-04 19:47:51,445   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.iou_reg_weight: 0.5
2025-09-04 19:47:51,445   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.code_weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2]
2025-09-04 19:47:51,445   INFO  ----------- LOSS_CLS -----------
2025-09-04 19:47:51,445   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.use_sigmoid: True
2025-09-04 19:47:51,445   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.gamma: 2.0
2025-09-04 19:47:51,445   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.alpha: 0.25
2025-09-04 19:47:51,445   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_IOU: True
2025-09-04 19:47:51,445   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_IOU_REG: True
2025-09-04 19:47:51,445   INFO  ----------- POST_PROCESSING -----------
2025-09-04 19:47:51,445   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.SCORE_THRESH: 0.0
2025-09-04 19:47:51,445   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.POST_CENTER_RANGE: [-61.2, -61.2, -10.0, 61.2, 61.2, 10.0]
2025-09-04 19:47:51,445   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.USE_IOU_TO_RECTIFY_SCORE: True
2025-09-04 19:47:51,445   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.IOU_RECTIFIER: [0.5]
2025-09-04 19:47:51,445   INFO  ----------- NMS_CONFIG -----------
2025-09-04 19:47:51,446   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_TYPE: nms_gpu
2025-09-04 19:47:51,446   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_THRESH: 0.2
2025-09-04 19:47:51,446   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_PRE_MAXSIZE: 1000
2025-09-04 19:47:51,446   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_POST_MAXSIZE: 100
2025-09-04 19:47:51,446   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.SCORE_THRES: 0.0
2025-09-04 19:47:51,446   INFO  ----------- POST_PROCESSING -----------
2025-09-04 19:47:51,446   INFO  cfg.MODEL.POST_PROCESSING.RECALL_THRESH_LIST: [0.3, 0.5, 0.7]
2025-09-04 19:47:51,446   INFO  cfg.MODEL.POST_PROCESSING.SCORE_THRESH: 0.1
2025-09-04 19:47:51,446   INFO  cfg.MODEL.POST_PROCESSING.OUTPUT_RAW_SCORE: False
2025-09-04 19:47:51,446   INFO  cfg.MODEL.POST_PROCESSING.EVAL_METRIC: kitti
2025-09-04 19:47:51,446   INFO  ----------- NMS_CONFIG -----------
2025-09-04 19:47:51,446   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.MULTI_CLASSES_NMS: True
2025-09-04 19:47:51,446   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_TYPE: nms_gpu
2025-09-04 19:47:51,446   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_THRESH: 0.2
2025-09-04 19:47:51,446   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_PRE_MAXSIZE: 1000
2025-09-04 19:47:51,447   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_POST_MAXSIZE: 83
2025-09-04 19:47:51,447   INFO  ----------- OPTIMIZATION -----------
2025-09-04 19:47:51,447   INFO  cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU: 2
2025-09-04 19:47:51,447   INFO  cfg.OPTIMIZATION.NUM_EPOCHS: 36
2025-09-04 19:47:51,447   INFO  cfg.OPTIMIZATION.OPTIMIZER: adam_onecycle
2025-09-04 19:47:51,447   INFO  cfg.OPTIMIZATION.LR: 0.003
2025-09-04 19:47:51,447   INFO  cfg.OPTIMIZATION.WEIGHT_DECAY: 0.05
2025-09-04 19:47:51,447   INFO  cfg.OPTIMIZATION.MOMENTUM: 0.9
2025-09-04 19:47:51,447   INFO  cfg.OPTIMIZATION.MOMS: [0.95, 0.85]
2025-09-04 19:47:51,447   INFO  cfg.OPTIMIZATION.PCT_START: 0.4
2025-09-04 19:47:51,447   INFO  cfg.OPTIMIZATION.DIV_FACTOR: 10
2025-09-04 19:47:51,447   INFO  cfg.OPTIMIZATION.DECAY_STEP_LIST: [35, 45]
2025-09-04 19:47:51,447   INFO  cfg.OPTIMIZATION.LR_DECAY: 0.1
2025-09-04 19:47:51,447   INFO  cfg.OPTIMIZATION.LR_CLIP: 1e-07
2025-09-04 19:47:51,447   INFO  cfg.OPTIMIZATION.LR_WARMUP: False
2025-09-04 19:47:51,448   INFO  cfg.OPTIMIZATION.WARMUP_EPOCH: 1
2025-09-04 19:47:51,448   INFO  cfg.OPTIMIZATION.GRAD_NORM_CLIP: 10
2025-09-04 19:47:51,448   INFO  cfg.OPTIMIZATION.LOSS_SCALE_FP16: 32
2025-09-04 19:47:51,448   INFO  ----------- HOOK -----------
2025-09-04 19:47:51,448   INFO  ----------- DisableAugmentationHook -----------
2025-09-04 19:47:51,448   INFO  cfg.HOOK.DisableAugmentationHook.DISABLE_AUG_LIST: ['gt_sampling']
2025-09-04 19:47:51,448   INFO  cfg.HOOK.DisableAugmentationHook.NUM_LAST_EPOCHS: 5
2025-09-04 19:47:51,448   INFO  cfg.TAG: sparse_former_base
2025-09-04 19:47:51,448   INFO  cfg.EXP_GROUP_PATH: sparse_models
2025-09-04 19:47:51,448   INFO  cfg.OUTPUT_DIR: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_.0diff_dense_hm_.3lr_.5wd
2025-09-04 19:47:51,462   INFO  ----------- Create dataloader & network & optimizer -----------
2025-09-04 19:47:57,030   INFO  Database filter by min points car: 339949 => 294532
2025-09-04 19:47:57,041   INFO  Database filter by min points truck: 65262 => 60344
2025-09-04 19:47:57,042   INFO  Database filter by min points construction_vehicle: 11050 => 10589
2025-09-04 19:47:57,044   INFO  Database filter by min points bus: 12286 => 11619
2025-09-04 19:47:57,046   INFO  Database filter by min points trailer: 19202 => 17934
2025-09-04 19:47:57,057   INFO  Database filter by min points barrier: 107507 => 101993
2025-09-04 19:47:57,059   INFO  Database filter by min points motorcycle: 8846 => 8055
2025-09-04 19:47:57,060   INFO  Database filter by min points bicycle: 8185 => 7531
2025-09-04 19:47:57,075   INFO  Database filter by min points pedestrian: 161928 => 148520
2025-09-04 19:47:57,083   INFO  Database filter by min points traffic_cone: 62964 => 55504
2025-09-04 19:47:57,084   INFO  Loading GT database to shared memory
eflops103:23:23 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:23:23 [0] NCCL INFO Bootstrap : Using bond0:10.16.9.231<0>
eflops103:23:23 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops103:23:23 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops103:23:23 [0] NCCL INFO cudaDriverVersion 12050
eflops103:25:25 [2] NCCL INFO cudaDriverVersion 12050
eflops103:24:24 [1] NCCL INFO cudaDriverVersion 12050
eflops103:26:26 [3] NCCL INFO cudaDriverVersion 12050
NCCL version 2.18.5+cuda11.8
eflops103:28:28 [5] NCCL INFO cudaDriverVersion 12050
eflops103:28:28 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:24:24 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:25:25 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:26:26 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:29:29 [6] NCCL INFO cudaDriverVersion 12050
eflops103:29:29 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:27:27 [4] NCCL INFO cudaDriverVersion 12050
eflops103:30:30 [7] NCCL INFO cudaDriverVersion 12050
eflops103:27:27 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:30:30 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:24:24 [1] NCCL INFO Bootstrap : Using bond0:10.16.9.231<0>
eflops103:25:25 [2] NCCL INFO Bootstrap : Using bond0:10.16.9.231<0>
eflops103:30:30 [7] NCCL INFO Bootstrap : Using bond0:10.16.9.231<0>
eflops103:24:24 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops103:25:25 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops103:24:24 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops103:25:25 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops103:30:30 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops103:30:30 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops103:28:28 [5] NCCL INFO Bootstrap : Using bond0:10.16.9.231<0>
eflops103:27:27 [4] NCCL INFO Bootstrap : Using bond0:10.16.9.231<0>
eflops103:28:28 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops103:28:28 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops103:27:27 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops103:27:27 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops103:26:26 [3] NCCL INFO Bootstrap : Using bond0:10.16.9.231<0>
eflops103:29:29 [6] NCCL INFO Bootstrap : Using bond0:10.16.9.231<0>
eflops103:26:26 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops103:26:26 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops103:29:29 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops103:29:29 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops103:30:103 [7] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops103:30:103 [7] NCCL INFO P2P plugin IBext
eflops103:30:103 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:27:100 [4] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops103:27:100 [4] NCCL INFO P2P plugin IBext
eflops103:27:100 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:29:106 [6] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops103:29:106 [6] NCCL INFO P2P plugin IBext
eflops103:29:106 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops103:27:100 [4] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops103:30:103 [7] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops103:27:100 [4] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0

eflops103:30:103 [7] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops103:27:100 [4] NCCL INFO NET/IB : No device found.

eflops103:29:106 [6] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed
eflops103:30:103 [7] NCCL INFO NET/IB : No device found.

eflops103:29:106 [6] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops103:29:106 [6] NCCL INFO NET/IB : No device found.
eflops103:27:100 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops103:27:100 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:30:103 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops103:29:106 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops103:30:103 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:29:106 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:24:101 [1] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops103:24:101 [1] NCCL INFO P2P plugin IBext
eflops103:24:101 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:28:102 [5] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops103:28:102 [5] NCCL INFO P2P plugin IBext
eflops103:28:102 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:25:104 [2] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops103:25:104 [2] NCCL INFO P2P plugin IBext
eflops103:25:104 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:26:105 [3] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops103:26:105 [3] NCCL INFO P2P plugin IBext
eflops103:26:105 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:30:103 [7] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.231<0>
eflops103:30:103 [7] NCCL INFO Using network Socket
eflops103:27:100 [4] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.231<0>
eflops103:29:106 [6] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.231<0>
eflops103:29:106 [6] NCCL INFO Using network Socket
eflops103:27:100 [4] NCCL INFO Using network Socket

eflops103:24:101 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops103:24:101 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops103:24:101 [1] NCCL INFO NET/IB : No device found.
eflops103:24:101 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops103:24:101 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops103:28:102 [5] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops103:28:102 [5] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops103:28:102 [5] NCCL INFO NET/IB : No device found.
eflops103:28:102 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops103:28:102 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops103:25:104 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops103:25:104 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops103:25:104 [2] NCCL INFO NET/IB : No device found.
eflops103:25:104 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops103:25:104 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops103:26:105 [3] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops103:26:105 [3] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops103:26:105 [3] NCCL INFO NET/IB : No device found.
eflops103:26:105 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops103:26:105 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:24:101 [1] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.231<0>
eflops103:24:101 [1] NCCL INFO Using network Socket
eflops103:28:102 [5] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.231<0>
eflops103:28:102 [5] NCCL INFO Using network Socket
eflops103:25:104 [2] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.231<0>
eflops103:25:104 [2] NCCL INFO Using network Socket
eflops103:26:105 [3] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.231<0>
eflops103:26:105 [3] NCCL INFO Using network Socket
eflops103:23:99 [0] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops103:23:99 [0] NCCL INFO P2P plugin IBext
eflops103:23:99 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops103:23:99 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops103:23:99 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops103:23:99 [0] NCCL INFO NET/IB : No device found.
eflops103:23:99 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops103:23:99 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops103:23:99 [0] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.231<0>
eflops103:23:99 [0] NCCL INFO Using network Socket
eflops103:23:99 [0] NCCL INFO comm 0x11405e60 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 4f000 commId 0x2225ac6fa87a1ddb - Init START
eflops103:30:103 [7] NCCL INFO comm 0x11c31f20 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a4000 commId 0x2225ac6fa87a1ddb - Init START
eflops103:29:106 [6] NCCL INFO comm 0x114cdf40 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a0000 commId 0x2225ac6fa87a1ddb - Init START
eflops103:24:101 [1] NCCL INFO comm 0x10d78780 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 50000 commId 0x2225ac6fa87a1ddb - Init START
eflops103:26:105 [3] NCCL INFO comm 0x11c03220 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 57000 commId 0x2225ac6fa87a1ddb - Init START
eflops103:27:100 [4] NCCL INFO comm 0x11391750 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9c000 commId 0x2225ac6fa87a1ddb - Init START
eflops103:25:104 [2] NCCL INFO comm 0x11734fb0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 53000 commId 0x2225ac6fa87a1ddb - Init START
eflops103:28:102 [5] NCCL INFO comm 0x111e7dc0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 9d000 commId 0x2225ac6fa87a1ddb - Init START
eflops103:30:103 [7] NCCL INFO Setting affinity for GPU 7 to ffff,fff00000,00ffffff,f0000000
eflops103:27:100 [4] NCCL INFO Setting affinity for GPU 4 to ffff,fff00000,00ffffff,f0000000
eflops103:29:106 [6] NCCL INFO Setting affinity for GPU 6 to ffff,fff00000,00ffffff,f0000000
eflops103:23:99 [0] NCCL INFO Setting affinity for GPU 0 to 0fffff,ff000000,0fffffff
eflops103:26:105 [3] NCCL INFO Setting affinity for GPU 3 to 0fffff,ff000000,0fffffff
eflops103:24:101 [1] NCCL INFO Setting affinity for GPU 1 to 0fffff,ff000000,0fffffff
eflops103:25:104 [2] NCCL INFO Setting affinity for GPU 2 to 0fffff,ff000000,0fffffff
eflops103:28:102 [5] NCCL INFO Setting affinity for GPU 5 to ffff,fff00000,00ffffff,f0000000
eflops103:29:106 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5
eflops103:30:103 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
eflops103:30:103 [7] NCCL INFO P2P Chunksize set to 131072
eflops103:29:106 [6] NCCL INFO P2P Chunksize set to 131072
eflops103:25:104 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
eflops103:28:102 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
eflops103:23:99 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
eflops103:25:104 [2] NCCL INFO P2P Chunksize set to 131072
eflops103:24:101 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
eflops103:23:99 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
eflops103:28:102 [5] NCCL INFO P2P Chunksize set to 131072
eflops103:23:99 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
eflops103:24:101 [1] NCCL INFO P2P Chunksize set to 131072
eflops103:26:105 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
eflops103:23:99 [0] NCCL INFO P2P Chunksize set to 131072
eflops103:27:100 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
eflops103:26:105 [3] NCCL INFO P2P Chunksize set to 131072
eflops103:27:100 [4] NCCL INFO P2P Chunksize set to 131072
eflops103:29:106 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC
eflops103:25:104 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
eflops103:24:101 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
eflops103:28:102 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC
eflops103:30:103 [7] NCCL INFO Channel 00 : 7[7] -> 0[0] via SHM/direct/direct
eflops103:26:105 [3] NCCL INFO Channel 00 : 3[3] -> 4[4] via SHM/direct/direct
eflops103:30:103 [7] NCCL INFO Channel 01 : 7[7] -> 0[0] via SHM/direct/direct
eflops103:24:101 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
eflops103:25:104 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
eflops103:28:102 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC
eflops103:29:106 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC
eflops103:26:105 [3] NCCL INFO Channel 01 : 3[3] -> 4[4] via SHM/direct/direct
eflops103:23:99 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
eflops103:27:100 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
eflops103:23:99 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
eflops103:27:100 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
eflops103:29:106 [6] NCCL INFO Connected all rings
eflops103:25:104 [2] NCCL INFO Connected all rings
eflops103:25:104 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
eflops103:29:106 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC
eflops103:25:104 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
eflops103:29:106 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC
eflops103:23:99 [0] NCCL INFO Connected all rings
eflops103:28:102 [5] NCCL INFO Connected all rings
eflops103:24:101 [1] NCCL INFO Connected all rings
eflops103:27:100 [4] NCCL INFO Connected all rings
eflops103:26:105 [3] NCCL INFO Connected all rings
eflops103:30:103 [7] NCCL INFO Connected all rings
eflops103:30:103 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC
eflops103:28:102 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
eflops103:27:100 [4] NCCL INFO Channel 00 : 4[4] -> 3[3] via SHM/direct/direct
eflops103:30:103 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC
eflops103:28:102 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
eflops103:27:100 [4] NCCL INFO Channel 01 : 4[4] -> 3[3] via SHM/direct/direct
eflops103:24:101 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
eflops103:30:103 [7] NCCL INFO Connected all trees
eflops103:30:103 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops103:30:103 [7] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops103:28:102 [5] NCCL INFO Connected all trees
eflops103:28:102 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops103:28:102 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops103:29:106 [6] NCCL INFO Connected all trees
eflops103:29:106 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops103:29:106 [6] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops103:24:101 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
eflops103:23:99 [0] NCCL INFO Connected all trees
eflops103:23:99 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops103:23:99 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops103:24:101 [1] NCCL INFO Connected all trees
eflops103:24:101 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops103:24:101 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops103:26:105 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
eflops103:26:105 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
eflops103:25:104 [2] NCCL INFO Connected all trees
eflops103:25:104 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops103:25:104 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops103:26:105 [3] NCCL INFO Connected all trees
eflops103:26:105 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops103:26:105 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops103:27:100 [4] NCCL INFO Connected all trees
eflops103:27:100 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops103:27:100 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops103:30:103 [7] NCCL INFO comm 0x11c31f20 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a4000 commId 0x2225ac6fa87a1ddb - Init COMPLETE
eflops103:27:100 [4] NCCL INFO comm 0x11391750 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9c000 commId 0x2225ac6fa87a1ddb - Init COMPLETE
eflops103:28:102 [5] NCCL INFO comm 0x111e7dc0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 9d000 commId 0x2225ac6fa87a1ddb - Init COMPLETE
eflops103:23:99 [0] NCCL INFO comm 0x11405e60 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 4f000 commId 0x2225ac6fa87a1ddb - Init COMPLETE
eflops103:24:101 [1] NCCL INFO comm 0x10d78780 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 50000 commId 0x2225ac6fa87a1ddb - Init COMPLETE
eflops103:26:105 [3] NCCL INFO comm 0x11c03220 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 57000 commId 0x2225ac6fa87a1ddb - Init COMPLETE
eflops103:25:104 [2] NCCL INFO comm 0x11734fb0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 53000 commId 0x2225ac6fa87a1ddb - Init COMPLETE
eflops103:29:106 [6] NCCL INFO comm 0x114cdf40 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a0000 commId 0x2225ac6fa87a1ddb - Init COMPLETE
2025-09-04 19:48:10,835   INFO  GT database has been saved to shared memory
2025-09-04 19:48:11,125   INFO  Loading NuScenes dataset
2025-09-04 19:48:12,901   INFO  Total samples for NuScenes dataset: 28130
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]2025-09-04 19:48:13,357   INFO  ----------- Model TransFusion created, param count: 24443458 -----------
2025-09-04 19:48:13,357   INFO  DistributedDataParallel(
  (module): TransFusion(
    (vfe): DynamicVoxelVFE(
      (pfn_layers): ModuleList(
        (0): PFNLayerV2(
          (linear): Linear(in_features=11, out_features=32, bias=False)
          (norm): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
        )
        (1): PFNLayerV2(
          (linear): Linear(in_features=64, out_features=64, bias=False)
          (norm): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
        )
      )
    )
    (backbone_3d): FSHNet_nusc(
      (cpe): SparseSequential(
        (0): SparseSequential(
          (0): SubMConv3d(64, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (2): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
      )
      (down1): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (down2): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (down3): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (stage): SPEncoder(
        (blocks): ModuleList(
          (0-2): 3 x SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
          )
        )
        (inputs): ModuleList(
          (0): SerializationLayer(
            (serialization): ZOrderSerialization()
          )
          (1): SerializationLayer(
            (serialization): HilbertSerialization()
          )
          (2): SerializationLayer(
            (serialization): FlattenWindowsSerialization()
          )
        )
        (embeddings): ModuleList(
          (0-2): 3 x MSSubConvEmbeddingLearned(
            (stem): ModuleList(
              (0-2): 3 x SparseBasicBlock(
                (conv1): SubMConv3d(3, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
              )
            )
          )
        )
      )
      (app): AttnPillarPool(
        (query_func): SparseMaxPool3d(kernel_size=[10, 1, 1], stride=[10, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], algo=ConvAlgo.MaskImplicitGemm)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (pos_embedding): Embedding(10, 128)
      )
      (cls_conv): SparseSequential(
        (0): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): SubMConv2d(128, 3, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
      )
      (afd_layers): ModuleList(
        (0-3): 4 x SEDLayer(
          (encoder): ModuleList(
            (0): SparseSequential(
              (0): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
              (1): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
            )
            (1-2): 2 x SparseSequential(
              (0): SparseSequential(
                (0): SparseConv2d(128, 128, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
            )
          )
          (decoder): ModuleList(
            (0-1): 2 x SparseSequential(
              (0): SparseInverseConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
              (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (decoder_norm): ModuleList(
            (0-1): 2 x SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (shared_conv): SparseSequential(
        (0): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (map_to_bev_module): None
    (pfe): None
    (backbone_2d): None
    (dense_head): TransFusionHead(
      (loss_cls): SigmoidFocalClassificationLoss()
      (loss_bbox): L1Loss()
      (loss_heatmap): GaussianFocalLoss()
      (shared_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (heatmap_head): Sequential(
        (0): BasicBlock2D(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): Conv2d(128, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (class_encoding): Conv1d(10, 128, kernel_size=(1,), stride=(1,))
      (decoder): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_posembed): PositionEmbeddingLearned(
          (position_embedding_head): Sequential(
            (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
        )
        (cross_posembed): PositionEmbeddingLearned(
          (position_embedding_head): Sequential(
            (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
        )
      )
      (prediction_head): SeparateHead_Transfusion(
        (center): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (height): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
        (dim): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 3, kernel_size=(1,), stride=(1,))
        )
        (rot): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (vel): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (iou): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
        (heatmap): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 10, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (point_head): None
    (roi_head): None
  )
)
2025-09-04 19:48:13,366   INFO  **********************Start training sparse_models/sparse_former_base(default)**********************
epochs:   0%|          | 0/36 [00:00<?, ?it/s]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
2025-09-04 19:48:36,158   INFO  Train:    1/36 (  3%) [   0/1759 (  0%)]  Loss: 1447. (1.45e+03)  LR: 3.000e-04  Grad: 10.0000  max=0.2903(module.dense_head.heatmap_head.1.bias)  min: -0.0804(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1409.3995, loss_cls=20.9680, loss_bbox=9.2674, matched_ious=0.0033, loss_iou=0.4407, loss_iou_reg=0.0000, d_time=2.29(2.29), f_time=18.35(18.35), b_time=20.64(20.64)  Time cost: 00:20/10:00:34 [00:22/360:20:52]  Acc_iter 1           Data time: 2.29(2.29)  Forward time: 18.35(18.35)  Batch time: 20.64(20.64)
2025-09-04 19:49:39,682   INFO  Train:    1/36 (  3%) [  49/1759 (  3%)]  Loss: 17.63 (129.)  LR: 3.000e-04  Grad: 10.0000  max=1.4121(module.dense_head.prediction_head.heatmap.1.bias)  min: -0.3235(module.dense_head.prediction_head.heatmap.0.1.bias)  NaN: False  loss_hm=74.7375, loss_cls=13.4833, loss_bbox=7.9207, matched_ious=0.0113, loss_iou=0.2727, loss_iou_reg=0.3251, d_time=0.00(0.05), f_time=1.23(1.63), b_time=1.23(1.68)  Time cost: 01:24/47:53 [01:26/29:31:53]  Acc_iter 50          Data time: 0.00(0.05)  Forward time: 1.23(1.63)  Batch time: 1.23(1.68)
2025-09-04 19:50:46,176   INFO  Train:    1/36 (  3%) [  99/1759 (  6%)]  Loss: 13.77 (73.3)  LR: 3.001e-04  Grad: 9.1513  max=1.1733(module.dense_head.prediction_head.heatmap.1.bias)  min: -0.2823(module.dense_head.prediction_head.heatmap.0.1.bias)  NaN: False  loss_hm=2.8587, loss_cls=5.5916, loss_bbox=4.7733, matched_ious=0.0552, loss_iou=0.1215, loss_iou_reg=0.4232, d_time=0.00(0.03), f_time=1.26(1.48), b_time=1.26(1.51)  Time cost: 02:30/41:38 [02:32/26:25:56]  Acc_iter 100         Data time: 0.00(0.03)  Forward time: 1.26(1.48)  Batch time: 1.26(1.51)
2025-09-04 19:51:49,156   INFO  Train:    1/36 (  3%) [ 149/1759 (  8%)]  Loss: 11.32 (52.8)  LR: 3.002e-04  Grad: 7.7335  max=0.4736(module.dense_head.prediction_head.heatmap.1.bias)  min: -0.1658(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=2.5665, loss_cls=1.8632, loss_bbox=3.2431, matched_ious=0.1047, loss_iou=0.1162, loss_iou_reg=0.4001, d_time=0.00(0.02), f_time=1.33(1.40), b_time=1.33(1.42)  Time cost: 03:33/38:11 [03:35/24:58:31]  Acc_iter 150         Data time: 0.00(0.02)  Forward time: 1.33(1.40)  Batch time: 1.33(1.42)
2025-09-04 19:52:53,517   INFO  Train:    1/36 (  3%) [ 199/1759 ( 11%)]  Loss: 10.02 (42.2)  LR: 3.004e-04  Grad: 6.9757  max=0.2095(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.1766(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=2.4108, loss_cls=0.9656, loss_bbox=2.8189, matched_ious=0.1199, loss_iou=0.1078, loss_iou_reg=0.3937, d_time=0.00(0.02), f_time=1.15(1.37), b_time=1.16(1.39)  Time cost: 04:37/36:07 [04:40/24:21:35]  Acc_iter 200         Data time: 0.00(0.02)  Forward time: 1.15(1.37)  Batch time: 1.16(1.39)
2025-09-04 19:53:56,379   INFO  Train:    1/36 (  3%) [ 249/1759 ( 14%)]  Loss: 9.436 (35.6)  LR: 3.006e-04  Grad: 6.9935  max=0.2215(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3925(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=2.2729, loss_cls=0.7439, loss_bbox=2.5248, matched_ious=0.1471, loss_iou=0.1106, loss_iou_reg=0.3866, d_time=0.01(0.02), f_time=1.25(1.35), b_time=1.26(1.36)  Time cost: 05:40/34:17 [05:42/23:52:40]  Acc_iter 250         Data time: 0.01(0.02)  Forward time: 1.25(1.35)  Batch time: 1.26(1.36)
2025-09-04 19:55:02,566   INFO  Train:    1/36 (  3%) [ 299/1759 ( 17%)]  Loss: 8.465 (31.1)  LR: 3.009e-04  Grad: 6.9233  max=0.1934(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2227(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=2.1177, loss_cls=0.6321, loss_bbox=2.2860, matched_ious=0.1766, loss_iou=0.1103, loss_iou_reg=0.3702, d_time=0.00(0.02), f_time=1.34(1.34), b_time=1.35(1.36)  Time cost: 06:46/33:00 [06:49/23:44:41]  Acc_iter 300         Data time: 0.00(0.02)  Forward time: 1.34(1.34)  Batch time: 1.35(1.36)
2025-09-04 19:56:06,821   INFO  Train:    1/36 (  3%) [ 349/1759 ( 20%)]  Loss: 8.225 (27.9)  LR: 3.013e-04  Grad: 7.3381  max=0.1988(module.backbone_3d.cls_conv.3.weight)  min: -0.3065(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=2.0377, loss_cls=0.5929, loss_bbox=2.0601, matched_ious=0.2007, loss_iou=0.1109, loss_iou_reg=0.3602, d_time=0.00(0.02), f_time=1.18(1.33), b_time=1.18(1.35)  Time cost: 07:51/31:38 [07:53/23:32:53]  Acc_iter 350         Data time: 0.00(0.02)  Forward time: 1.18(1.33)  Batch time: 1.18(1.35)
2025-09-04 19:57:10,586   INFO  Train:    1/36 (  3%) [ 399/1759 ( 23%)]  Loss: 7.737 (25.4)  LR: 3.017e-04  Grad: 7.4497  max=0.2115(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2298(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.9647, loss_cls=0.5507, loss_bbox=2.0121, matched_ious=0.2129, loss_iou=0.1111, loss_iou_reg=0.3542, d_time=0.00(0.02), f_time=1.18(1.32), b_time=1.18(1.34)  Time cost: 08:54/30:18 [08:57/23:22:28]  Acc_iter 400         Data time: 0.00(0.02)  Forward time: 1.18(1.32)  Batch time: 1.18(1.34)
2025-09-04 19:58:13,523   INFO  Train:    1/36 (  3%) [ 449/1759 ( 26%)]  Loss: 7.383 (23.4)  LR: 3.021e-04  Grad: 8.2550  max=0.3058(module.backbone_3d.cls_conv.3.bias)  min: -0.4001(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.8547, loss_cls=0.5336, loss_bbox=1.8466, matched_ious=0.2370, loss_iou=0.1096, loss_iou_reg=0.3467, d_time=0.00(0.01), f_time=1.27(1.31), b_time=1.27(1.33)  Time cost: 09:57/29:00 [10:00/23:12:13]  Acc_iter 450         Data time: 0.00(0.01)  Forward time: 1.27(1.31)  Batch time: 1.27(1.33)
2025-09-04 19:59:17,046   INFO  Train:    1/36 (  3%) [ 499/1759 ( 28%)]  Loss: 7.637 (21.8)  LR: 3.026e-04  Grad: 8.4643  max=0.2486(module.backbone_3d.cls_conv.3.bias)  min: -0.2467(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.7551, loss_cls=0.5035, loss_bbox=1.7273, matched_ious=0.2625, loss_iou=0.1089, loss_iou_reg=0.3346, d_time=0.00(0.01), f_time=1.32(1.31), b_time=1.32(1.32)  Time cost: 11:01/27:46 [11:03/23:05:01]  Acc_iter 500         Data time: 0.00(0.01)  Forward time: 1.32(1.31)  Batch time: 1.32(1.32)
2025-09-04 20:00:23,299   INFO  Train:    1/36 (  3%) [ 549/1759 ( 31%)]  Loss: 6.436 (20.5)  LR: 3.031e-04  Grad: 8.5658  max=0.2527(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2528(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.7046, loss_cls=0.4807, loss_bbox=1.7363, matched_ious=0.2742, loss_iou=0.1109, loss_iou_reg=0.3286, d_time=0.01(0.01), f_time=1.21(1.31), b_time=1.21(1.32)  Time cost: 12:07/26:40 [12:09/23:04:08]  Acc_iter 550         Data time: 0.01(0.01)  Forward time: 1.21(1.31)  Batch time: 1.21(1.32)
2025-09-04 20:01:26,255   INFO  Train:    1/36 (  3%) [ 599/1759 ( 34%)]  Loss: 6.718 (19.4)  LR: 3.037e-04  Grad: 8.8880  max=0.2410(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2588(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.6646, loss_cls=0.4538, loss_bbox=1.6995, matched_ious=0.2804, loss_iou=0.1096, loss_iou_reg=0.3211, d_time=0.00(0.01), f_time=1.24(1.31), b_time=1.25(1.32)  Time cost: 13:10/25:28 [13:12/22:57:29]  Acc_iter 600         Data time: 0.00(0.01)  Forward time: 1.24(1.31)  Batch time: 1.25(1.32)
2025-09-04 20:02:28,927   INFO  Train:    1/36 (  3%) [ 649/1759 ( 37%)]  Loss: 7.044 (18.5)  LR: 3.044e-04  Grad: 9.1536  max=0.2371(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2646(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.6231, loss_cls=0.4368, loss_bbox=1.7352, matched_ious=0.2922, loss_iou=0.1112, loss_iou_reg=0.3118, d_time=0.00(0.01), f_time=1.26(1.30), b_time=1.27(1.31)  Time cost: 14:13/24:17 [14:15/22:51:13]  Acc_iter 650         Data time: 0.00(0.01)  Forward time: 1.26(1.30)  Batch time: 1.27(1.31)
2025-09-04 20:03:31,989   INFO  Train:    1/36 (  3%) [ 699/1759 ( 40%)]  Loss: 8.575 (17.6)  LR: 3.051e-04  Grad: 9.5641  max=0.4007(module.backbone_3d.cls_conv.3.weight)  min: -0.2526(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.6278, loss_cls=0.4319, loss_bbox=1.7314, matched_ious=0.2986, loss_iou=0.1143, loss_iou_reg=0.3096, d_time=0.00(0.01), f_time=1.26(1.30), b_time=1.27(1.31)  Time cost: 15:16/23:07 [15:18/22:46:17]  Acc_iter 700         Data time: 0.00(0.01)  Forward time: 1.26(1.30)  Batch time: 1.27(1.31)
2025-09-04 20:04:36,060   INFO  Train:    1/36 (  3%) [ 749/1759 ( 43%)]  Loss: 8.576 (16.9)  LR: 3.058e-04  Grad: 9.7467  max=0.8047(module.backbone_3d.cls_conv.3.weight)  min: -0.2454(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.5784, loss_cls=0.4190, loss_bbox=1.6960, matched_ious=0.3087, loss_iou=0.1103, loss_iou_reg=0.3079, d_time=0.00(0.01), f_time=1.24(1.30), b_time=1.25(1.31)  Time cost: 16:20/22:00 [16:22/22:43:17]  Acc_iter 750         Data time: 0.00(0.01)  Forward time: 1.24(1.30)  Batch time: 1.25(1.31)
2025-09-04 20:05:40,214   INFO  Train:    1/36 (  3%) [ 799/1759 ( 45%)]  Loss: 5.967 (16.3)  LR: 3.066e-04  Grad: 9.3759  max=0.2263(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2509(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.5404, loss_cls=0.3963, loss_bbox=1.7445, matched_ious=0.3031, loss_iou=0.1108, loss_iou_reg=0.3091, d_time=0.00(0.01), f_time=1.29(1.30), b_time=1.29(1.31)  Time cost: 17:24/20:53 [17:26/22:40:37]  Acc_iter 800         Data time: 0.00(0.01)  Forward time: 1.29(1.30)  Batch time: 1.29(1.31)
2025-09-04 20:06:43,706   INFO  Train:    1/36 (  3%) [ 849/1759 ( 48%)]  Loss: 7.580 (15.8)  LR: 3.075e-04  Grad: 9.6870  max=0.2250(module.dense_head.heatmap_head.1.weight)  min: -0.2569(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.5394, loss_cls=0.4012, loss_bbox=1.6602, matched_ious=0.3164, loss_iou=0.1139, loss_iou_reg=0.3038, d_time=0.00(0.01), f_time=1.36(1.29), b_time=1.37(1.30)  Time cost: 18:28/19:46 [18:30/22:37:20]  Acc_iter 850         Data time: 0.00(0.01)  Forward time: 1.36(1.29)  Batch time: 1.37(1.30)
2025-09-04 20:07:46,959   INFO  Train:    1/36 (  3%) [ 899/1759 ( 51%)]  Loss: 6.652 (15.3)  LR: 3.084e-04  Grad: 9.3236  max=0.2085(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2501(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.5000, loss_cls=0.3809, loss_bbox=1.6933, matched_ious=0.3154, loss_iou=0.1088, loss_iou_reg=0.3026, d_time=0.00(0.01), f_time=1.25(1.29), b_time=1.25(1.30)  Time cost: 19:31/18:39 [19:33/22:34:01]  Acc_iter 900         Data time: 0.00(0.01)  Forward time: 1.25(1.29)  Batch time: 1.25(1.30)
2025-09-04 20:08:50,677   INFO  Train:    1/36 (  3%) [ 949/1759 ( 54%)]  Loss: 5.993 (14.8)  LR: 3.093e-04  Grad: 9.6739  max=0.2112(module.backbone_3d.cls_conv.3.weight)  min: -0.2497(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.4941, loss_cls=0.3856, loss_bbox=1.6571, matched_ious=0.3249, loss_iou=0.1112, loss_iou_reg=0.2994, d_time=0.00(0.01), f_time=1.21(1.29), b_time=1.21(1.30)  Time cost: 20:35/17:33 [20:37/22:31:27]  Acc_iter 950         Data time: 0.00(0.01)  Forward time: 1.21(1.29)  Batch time: 1.21(1.30)
2025-09-04 20:09:54,355   INFO  Train:    1/36 (  3%) [ 999/1759 ( 57%)]  Loss: 6.829 (14.4)  LR: 3.104e-04  Grad: 9.4202  max=0.2916(module.dense_head.heatmap_head.1.weight)  min: -0.2818(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=1.4975, loss_cls=0.3841, loss_bbox=1.6376, matched_ious=0.3221, loss_iou=0.1065, loss_iou_reg=0.3016, d_time=0.00(0.01), f_time=1.37(1.29), b_time=1.38(1.30)  Time cost: 21:38/16:27 [21:40/22:29:00]  Acc_iter 1000        Data time: 0.00(0.01)  Forward time: 1.37(1.29)  Batch time: 1.38(1.30)
2025-09-04 20:10:58,477   INFO  Train:    1/36 (  3%) [1049/1759 ( 60%)]  Loss: 6.783 (14.0)  LR: 3.114e-04  Grad: 9.4897  max=0.2297(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2521(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.4173, loss_cls=0.3623, loss_bbox=1.6269, matched_ious=0.3264, loss_iou=0.1109, loss_iou_reg=0.2994, d_time=0.00(0.01), f_time=1.26(1.29), b_time=1.27(1.30)  Time cost: 22:42/15:21 [22:45/22:27:07]  Acc_iter 1050        Data time: 0.00(0.01)  Forward time: 1.26(1.29)  Batch time: 1.27(1.30)
2025-09-04 20:12:01,268   INFO  Train:    1/36 (  3%) [1099/1759 ( 62%)]  Loss: 6.502 (13.7)  LR: 3.125e-04  Grad: 9.7439  max=0.2249(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2561(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.4536, loss_cls=0.3635, loss_bbox=1.7046, matched_ious=0.3256, loss_iou=0.1105, loss_iou_reg=0.2969, d_time=0.00(0.01), f_time=1.24(1.29), b_time=1.24(1.30)  Time cost: 23:45/14:15 [23:47/22:24:03]  Acc_iter 1100        Data time: 0.00(0.01)  Forward time: 1.24(1.29)  Batch time: 1.24(1.30)
2025-09-04 20:13:03,895   INFO  Train:    1/36 (  3%) [1149/1759 ( 65%)]  Loss: 6.551 (13.4)  LR: 3.137e-04  Grad: 9.7200  max=0.2615(module.dense_head.heatmap_head.1.weight)  min: -0.2422(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.4141, loss_cls=0.3599, loss_bbox=1.6421, matched_ious=0.3347, loss_iou=0.1087, loss_iou_reg=0.2952, d_time=0.00(0.01), f_time=1.34(1.29), b_time=1.35(1.29)  Time cost: 24:48/13:09 [24:50/22:21:01]  Acc_iter 1150        Data time: 0.00(0.01)  Forward time: 1.34(1.29)  Batch time: 1.35(1.29)
2025-09-04 20:14:08,674   INFO  Train:    1/36 (  3%) [1199/1759 ( 68%)]  Loss: 6.920 (13.1)  LR: 3.149e-04  Grad: 9.8739  max=0.3066(module.vfe.pfn_layers.0.linear.weight)  min: -0.2658(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.4148, loss_cls=0.3560, loss_bbox=1.6531, matched_ious=0.3400, loss_iou=0.1096, loss_iou_reg=0.2895, d_time=0.00(0.01), f_time=1.24(1.29), b_time=1.24(1.29)  Time cost: 25:53/12:04 [25:55/22:20:00]  Acc_iter 1200        Data time: 0.00(0.01)  Forward time: 1.24(1.29)  Batch time: 1.24(1.29)
2025-09-04 20:15:12,769   INFO  Train:    1/36 (  3%) [1249/1759 ( 71%)]  Loss: 6.484 (12.8)  LR: 3.162e-04  Grad: 9.6068  max=0.2207(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2634(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.3640, loss_cls=0.3512, loss_bbox=1.6292, matched_ious=0.3467, loss_iou=0.1076, loss_iou_reg=0.2913, d_time=0.00(0.01), f_time=1.21(1.29), b_time=1.21(1.29)  Time cost: 26:57/10:59 [26:59/22:18:25]  Acc_iter 1250        Data time: 0.00(0.01)  Forward time: 1.21(1.29)  Batch time: 1.21(1.29)
2025-09-04 20:16:17,773   INFO  Train:    1/36 (  3%) [1299/1759 ( 74%)]  Loss: 6.069 (12.6)  LR: 3.175e-04  Grad: 9.7302  max=0.2205(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.3335(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.3737, loss_cls=0.3512, loss_bbox=1.6301, matched_ious=0.3469, loss_iou=0.1097, loss_iou_reg=0.2879, d_time=0.00(0.01), f_time=1.28(1.29), b_time=1.28(1.29)  Time cost: 28:02/09:55 [28:04/22:17:35]  Acc_iter 1300        Data time: 0.00(0.01)  Forward time: 1.28(1.29)  Batch time: 1.28(1.29)
2025-09-04 20:17:20,711   INFO  Train:    1/36 (  3%) [1349/1759 ( 77%)]  Loss: 7.312 (12.3)  LR: 3.189e-04  Grad: 9.7519  max=0.2336(module.backbone_3d.cls_conv.3.bias)  min: -0.2609(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.3677, loss_cls=0.3472, loss_bbox=1.6702, matched_ious=0.3473, loss_iou=0.1114, loss_iou_reg=0.2877, d_time=0.00(0.01), f_time=1.26(1.28), b_time=1.27(1.29)  Time cost: 29:05/08:49 [29:07/22:15:10]  Acc_iter 1350        Data time: 0.00(0.01)  Forward time: 1.26(1.28)  Batch time: 1.27(1.29)
2025-09-04 20:18:25,239   INFO  Train:    1/36 (  3%) [1399/1759 ( 80%)]  Loss: 5.712 (12.1)  LR: 3.203e-04  Grad: 9.7575  max=0.2513(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2911(module.dense_head.heatmap_head.1.weight)  NaN: False  loss_hm=1.3399, loss_cls=0.3411, loss_bbox=1.6171, matched_ious=0.3469, loss_iou=0.1064, loss_iou_reg=0.2894, d_time=0.00(0.01), f_time=1.30(1.28), b_time=1.30(1.29)  Time cost: 30:09/07:45 [30:11/22:14:01]  Acc_iter 1400        Data time: 0.00(0.01)  Forward time: 1.30(1.28)  Batch time: 1.30(1.29)
2025-09-04 20:19:28,387   INFO  Train:    1/36 (  3%) [1449/1759 ( 82%)]  Loss: 6.201 (11.9)  LR: 3.217e-04  Grad: 9.5525  max=0.2202(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2433(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.3414, loss_cls=0.3418, loss_bbox=1.5767, matched_ious=0.3533, loss_iou=0.1086, loss_iou_reg=0.2861, d_time=0.00(0.01), f_time=1.22(1.28), b_time=1.23(1.29)  Time cost: 31:12/06:40 [31:14/22:11:53]  Acc_iter 1450        Data time: 0.00(0.01)  Forward time: 1.22(1.28)  Batch time: 1.23(1.29)
2025-09-04 20:20:32,764   INFO  Train:    1/36 (  3%) [1499/1759 ( 85%)]  Loss: 7.025 (11.7)  LR: 3.233e-04  Grad: 9.8951  max=0.3927(module.vfe.pfn_layers.0.linear.weight)  min: -0.2599(module.dense_head.heatmap_head.1.weight)  NaN: False  loss_hm=1.3371, loss_cls=0.3351, loss_bbox=1.6017, matched_ious=0.3417, loss_iou=0.1090, loss_iou_reg=0.2902, d_time=0.00(0.01), f_time=1.32(1.28), b_time=1.32(1.29)  Time cost: 32:17/05:35 [32:19/22:10:40]  Acc_iter 1500        Data time: 0.00(0.01)  Forward time: 1.32(1.28)  Batch time: 1.32(1.29)
2025-09-04 20:21:35,317   INFO  Train:    1/36 (  3%) [1549/1759 ( 88%)]  Loss: 5.263 (11.6)  LR: 3.248e-04  Grad: 9.5943  max=0.3962(module.vfe.pfn_layers.0.linear.weight)  min: -0.2536(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.3452, loss_cls=0.3376, loss_bbox=1.6974, matched_ious=0.3442, loss_iou=0.1056, loss_iou_reg=0.2898, d_time=0.00(0.01), f_time=1.25(1.28), b_time=1.25(1.29)  Time cost: 33:19/04:30 [33:21/22:08:15]  Acc_iter 1550        Data time: 0.00(0.01)  Forward time: 1.25(1.28)  Batch time: 1.25(1.29)
2025-09-04 20:22:38,613   INFO  Train:    1/36 (  3%) [1599/1759 ( 91%)]  Loss: 5.869 (11.4)  LR: 3.265e-04  Grad: 9.8174  max=0.5972(module.vfe.pfn_layers.0.linear.weight)  min: -0.3964(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.2995, loss_cls=0.3348, loss_bbox=1.5065, matched_ious=0.3584, loss_iou=0.1043, loss_iou_reg=0.2833, d_time=0.00(0.01), f_time=1.29(1.28), b_time=1.30(1.29)  Time cost: 34:22/03:26 [34:25/22:06:24]  Acc_iter 1600        Data time: 0.00(0.01)  Forward time: 1.29(1.28)  Batch time: 1.30(1.29)
2025-09-04 20:23:43,029   INFO  Train:    1/36 (  3%) [1649/1759 ( 94%)]  Loss: 4.996 (11.2)  LR: 3.281e-04  Grad: 9.5857  max=0.3201(module.vfe.pfn_layers.0.linear.weight)  min: -0.2501(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.3356, loss_cls=0.3366, loss_bbox=1.6495, matched_ious=0.3540, loss_iou=0.1074, loss_iou_reg=0.2860, d_time=0.00(0.01), f_time=1.32(1.28), b_time=1.32(1.29)  Time cost: 35:27/02:21 [35:29/22:05:18]  Acc_iter 1650        Data time: 0.00(0.01)  Forward time: 1.32(1.28)  Batch time: 1.32(1.29)
2025-09-04 20:24:46,497   INFO  Train:    1/36 (  3%) [1699/1759 ( 97%)]  Loss: 6.485 (11.1)  LR: 3.299e-04  Grad: 9.8621  max=0.4117(module.backbone_3d.cls_conv.3.weight)  min: -0.2911(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.3249, loss_cls=0.3364, loss_bbox=1.5411, matched_ious=0.3640, loss_iou=0.1059, loss_iou_reg=0.2805, d_time=0.01(0.01), f_time=1.22(1.28), b_time=1.23(1.29)  Time cost: 36:30/01:17 [36:33/22:03:37]  Acc_iter 1700        Data time: 0.01(0.01)  Forward time: 1.22(1.28)  Batch time: 1.23(1.29)
2025-09-04 20:25:49,472   INFO  Train:    1/36 (  3%) [1749/1759 ( 99%)]  Loss: 7.718 (10.9)  LR: 3.316e-04  Grad: 9.8463  max=0.4568(module.vfe.pfn_layers.0.linear.weight)  min: -0.3154(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.3245, loss_cls=0.3334, loss_bbox=1.5807, matched_ious=0.3529, loss_iou=0.1064, loss_iou_reg=0.2855, d_time=0.00(0.01), f_time=1.19(1.28), b_time=1.20(1.29)  Time cost: 37:33/00:12 [37:36/22:01:41]  Acc_iter 1750        Data time: 0.00(0.01)  Forward time: 1.19(1.28)  Batch time: 1.20(1.29)
2025-09-04 20:26:00,278   INFO  Train:    1/36 (  3%) [1758/1759 (100%)]  Loss: 6.831 (10.9)  LR: 3.320e-04  Grad: 9.9509  max=0.2512(module.backbone_3d.cls_conv.3.weight)  min: -0.2552(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.2793, loss_cls=0.3148, loss_bbox=1.5739, matched_ious=0.3688, loss_iou=0.1103, loss_iou_reg=0.2774, d_time=0.00(0.01), f_time=0.75(1.28), b_time=0.75(1.29)  Time cost: 37:44/00:01 [37:46/22:01:02]  Acc_iter 1759        Data time: 0.00(0.01)  Forward time: 0.75(1.28)  Batch time: 0.75(1.29)

                                               [Aepochs:   3%|▎         | 1/36 [37:47<22:02:32, 2267.20s/it]epochs:   3%|▎         | 1/36 [37:47<22:02:32, 2267.21s/it]epochs:   3%|▎         | 1/36 [37:47<22:02:35, 2267.29s/it]epochs:   3%|▎         | 1/36 [37:47<22:02:35, 2267.29s/it]epochs:   3%|▎         | 1/36 [37:47<22:02:35, 2267.30s/it]epochs:   3%|▎         | 1/36 [37:47<22:02:35, 2267.30s/it]epochs:   3%|▎         | 1/36 [37:47<22:02:35, 2267.30s/it]epochs:   3%|▎         | 1/36 [37:47<22:02:46, 2267.60s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 20:26:05,392   INFO  Train:    2/36 (  6%) [   0/1759 (  0%)]  Loss: 5.586 (5.59)  LR: 3.320e-04  Grad: 9.8888  max=0.2503(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4509(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1827, loss_cls=0.3154, loss_bbox=1.4514, matched_ious=0.3698, loss_iou=0.1065, loss_iou_reg=0.2884, d_time=1.29(1.29), f_time=2.68(2.68), b_time=3.97(3.97)  Time cost: 00:03/1:38:51 [37:51/57:39:55]  Acc_iter 1760        Data time: 1.29(1.29)  Forward time: 2.68(2.68)  Batch time: 3.97(3.97)
2025-09-04 20:26:58,038   INFO  Train:    2/36 (  6%) [  40/1759 (  2%)]  Loss: 6.221 (5.97)  LR: 3.335e-04  Grad: 9.8933  max=0.2383(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2472(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2657, loss_cls=0.3203, loss_bbox=1.5114, matched_ious=0.3548, loss_iou=0.1095, loss_iou_reg=0.2870, d_time=0.00(0.04), f_time=1.20(1.34), b_time=1.20(1.38)  Time cost: 00:56/39:08 [38:44/23:21:00]  Acc_iter 1800        Data time: 0.00(0.04)  Forward time: 1.20(1.34)  Batch time: 1.20(1.38)
2025-09-04 20:28:01,480   INFO  Train:    2/36 (  6%) [  90/1759 (  5%)]  Loss: 5.975 (6.00)  LR: 3.353e-04  Grad: 9.8141  max=0.2399(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3748(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2730, loss_cls=0.3296, loss_bbox=1.5445, matched_ious=0.3668, loss_iou=0.1083, loss_iou_reg=0.2814, d_time=0.00(0.02), f_time=1.27(1.30), b_time=1.27(1.32)  Time cost: 01:59/36:30 [39:48/22:25:01]  Acc_iter 1850        Data time: 0.00(0.02)  Forward time: 1.27(1.30)  Batch time: 1.27(1.32)
2025-09-04 20:29:04,703   INFO  Train:    2/36 (  6%) [ 140/1759 (  8%)]  Loss: 6.540 (6.03)  LR: 3.373e-04  Grad: 9.7879  max=0.3238(module.dense_head.prediction_head.height.1.bias)  min: -0.2413(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.2874, loss_cls=0.3224, loss_bbox=1.5304, matched_ious=0.3580, loss_iou=0.1049, loss_iou_reg=0.2851, d_time=0.00(0.01), f_time=1.22(1.29), b_time=1.22(1.30)  Time cost: 03:02/34:57 [40:51/22:06:24]  Acc_iter 1900        Data time: 0.00(0.01)  Forward time: 1.22(1.29)  Batch time: 1.22(1.30)
2025-09-04 20:30:08,206   INFO  Train:    2/36 (  6%) [ 190/1759 ( 11%)]  Loss: 6.065 (5.99)  LR: 3.393e-04  Grad: 9.7001  max=0.3473(module.backbone_3d.cls_conv.3.weight)  min: -0.4031(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2201, loss_cls=0.3204, loss_bbox=1.4900, matched_ious=0.3766, loss_iou=0.1048, loss_iou_reg=0.2764, d_time=0.00(0.01), f_time=1.33(1.28), b_time=1.33(1.29)  Time cost: 04:06/33:42 [41:54/21:58:28]  Acc_iter 1950        Data time: 0.00(0.01)  Forward time: 1.33(1.28)  Batch time: 1.33(1.29)
2025-09-04 20:31:11,969   INFO  Train:    2/36 (  6%) [ 240/1759 ( 14%)]  Loss: 6.448 (5.98)  LR: 3.413e-04  Grad: 9.6155  max=0.2530(module.backbone_3d.cls_conv.3.weight)  min: -0.2787(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2525, loss_cls=0.3228, loss_bbox=1.5273, matched_ious=0.3737, loss_iou=0.1056, loss_iou_reg=0.2781, d_time=0.00(0.01), f_time=1.29(1.28), b_time=1.29(1.29)  Time cost: 05:09/32:33 [42:58/21:54:29]  Acc_iter 2000        Data time: 0.00(0.01)  Forward time: 1.29(1.28)  Batch time: 1.29(1.29)
2025-09-04 20:32:15,943   INFO  Train:    2/36 (  6%) [ 290/1759 ( 16%)]  Loss: 6.875 (5.98)  LR: 3.434e-04  Grad: 9.8044  max=0.2436(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3511(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2333, loss_cls=0.3217, loss_bbox=1.5383, matched_ious=0.3765, loss_iou=0.1069, loss_iou_reg=0.2786, d_time=0.00(0.01), f_time=1.25(1.28), b_time=1.25(1.29)  Time cost: 06:13/31:27 [44:02/21:52:15]  Acc_iter 2050        Data time: 0.00(0.01)  Forward time: 1.25(1.28)  Batch time: 1.25(1.29)
2025-09-04 20:33:19,541   INFO  Train:    2/36 (  6%) [ 340/1759 ( 19%)]  Loss: 6.116 (5.98)  LR: 3.455e-04  Grad: 9.7942  max=0.2582(module.vfe.pfn_layers.0.linear.weight)  min: -0.2408(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2741, loss_cls=0.3201, loss_bbox=1.5342, matched_ious=0.3744, loss_iou=0.1072, loss_iou_reg=0.2769, d_time=0.00(0.01), f_time=1.28(1.28), b_time=1.29(1.28)  Time cost: 07:17/30:20 [45:06/21:49:14]  Acc_iter 2100        Data time: 0.00(0.01)  Forward time: 1.28(1.28)  Batch time: 1.29(1.28)
2025-09-04 20:34:24,445   INFO  Train:    2/36 (  6%) [ 390/1759 ( 22%)]  Loss: 6.123 (5.96)  LR: 3.477e-04  Grad: 9.7045  max=0.2481(module.vfe.pfn_layers.0.linear.weight)  min: -0.2453(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.2424, loss_cls=0.3126, loss_bbox=1.4820, matched_ious=0.3794, loss_iou=0.1058, loss_iou_reg=0.2762, d_time=0.01(0.01), f_time=1.23(1.28), b_time=1.24(1.29)  Time cost: 08:22/29:19 [46:11/21:50:08]  Acc_iter 2150        Data time: 0.01(0.01)  Forward time: 1.23(1.28)  Batch time: 1.24(1.29)
2025-09-04 20:35:27,749   INFO  Train:    2/36 (  6%) [ 440/1759 ( 25%)]  Loss: 5.585 (5.94)  LR: 3.499e-04  Grad: 9.8090  max=0.2469(module.vfe.pfn_layers.0.linear.weight)  min: -0.2266(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1918, loss_cls=0.3151, loss_bbox=1.4123, matched_ious=0.3832, loss_iou=0.1039, loss_iou_reg=0.2779, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.27(1.28)  Time cost: 09:25/28:12 [47:14/21:46:52]  Acc_iter 2200        Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.28)
2025-09-04 20:36:31,301   INFO  Train:    2/36 (  6%) [ 490/1759 ( 28%)]  Loss: 6.519 (5.93)  LR: 3.522e-04  Grad: 9.9267  max=0.2855(module.vfe.pfn_layers.0.linear.weight)  min: -0.2953(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.2353, loss_cls=0.3090, loss_bbox=1.4881, matched_ious=0.3673, loss_iou=0.1052, loss_iou_reg=0.2799, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.20(1.28)  Time cost: 10:29/27:06 [48:17/21:44:35]  Acc_iter 2250        Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.28)
2025-09-04 20:37:36,238   INFO  Train:    2/36 (  6%) [ 540/1759 ( 31%)]  Loss: 6.630 (5.91)  LR: 3.545e-04  Grad: 9.7666  max=0.2999(module.vfe.pfn_layers.0.linear.weight)  min: -0.2509(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1992, loss_cls=0.3127, loss_bbox=1.4917, matched_ious=0.3933, loss_iou=0.1043, loss_iou_reg=0.2698, d_time=0.00(0.01), f_time=1.26(1.28), b_time=1.26(1.28)  Time cost: 11:34/26:04 [49:22/21:45:08]  Acc_iter 2300        Data time: 0.00(0.01)  Forward time: 1.26(1.28)  Batch time: 1.26(1.28)
2025-09-04 20:38:39,565   INFO  Train:    2/36 (  6%) [ 590/1759 ( 34%)]  Loss: 5.683 (5.90)  LR: 3.569e-04  Grad: 9.8790  max=0.7148(module.vfe.pfn_layers.0.linear.weight)  min: -0.2750(module.vfe.pfn_layers.1.linear.weight)  NaN: False  loss_hm=1.2177, loss_cls=0.3107, loss_bbox=1.5015, matched_ious=0.3860, loss_iou=0.1045, loss_iou_reg=0.2734, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.30(1.28)  Time cost: 12:37/24:58 [50:26/21:42:37]  Acc_iter 2350        Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.30(1.28)
2025-09-04 20:39:42,006   INFO  Train:    2/36 (  6%) [ 640/1759 ( 36%)]  Loss: 5.417 (5.89)  LR: 3.593e-04  Grad: 9.7863  max=0.4168(module.vfe.pfn_layers.0.linear.weight)  min: -0.2483(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1991, loss_cls=0.3062, loss_bbox=1.4628, matched_ious=0.3828, loss_iou=0.1073, loss_iou_reg=0.2760, d_time=0.01(0.01), f_time=1.22(1.27), b_time=1.22(1.28)  Time cost: 13:39/23:51 [51:28/21:38:57]  Acc_iter 2400        Data time: 0.01(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.28)
2025-09-04 20:40:45,743   INFO  Train:    2/36 (  6%) [ 690/1759 ( 39%)]  Loss: 5.140 (5.88)  LR: 3.618e-04  Grad: 9.6237  max=0.3000(module.vfe.pfn_layers.0.linear.weight)  min: -0.2375(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1910, loss_cls=0.3099, loss_bbox=1.4500, matched_ious=0.3863, loss_iou=0.1075, loss_iou_reg=0.2721, d_time=0.00(0.01), f_time=1.34(1.27), b_time=1.34(1.28)  Time cost: 14:43/22:47 [52:32/21:37:33]  Acc_iter 2450        Data time: 0.00(0.01)  Forward time: 1.34(1.27)  Batch time: 1.34(1.28)
2025-09-04 20:41:50,041   INFO  Train:    2/36 (  6%) [ 740/1759 ( 42%)]  Loss: 5.301 (5.87)  LR: 3.643e-04  Grad: 9.7263  max=0.4730(module.vfe.pfn_layers.0.linear.weight)  min: -0.3029(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2016, loss_cls=0.3092, loss_bbox=1.4308, matched_ious=0.3837, loss_iou=0.1045, loss_iou_reg=0.2778, d_time=0.00(0.01), f_time=1.36(1.27), b_time=1.37(1.28)  Time cost: 15:48/21:43 [53:36/21:36:58]  Acc_iter 2500        Data time: 0.00(0.01)  Forward time: 1.36(1.27)  Batch time: 1.37(1.28)
2025-09-04 20:42:53,168   INFO  Train:    2/36 (  6%) [ 790/1759 ( 45%)]  Loss: 5.573 (5.86)  LR: 3.669e-04  Grad: 9.7620  max=0.2816(module.vfe.pfn_layers.0.linear.weight)  min: -0.6918(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1711, loss_cls=0.2969, loss_bbox=1.4673, matched_ious=0.3895, loss_iou=0.1090, loss_iou_reg=0.2706, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.28(1.28)  Time cost: 16:51/20:38 [54:39/21:34:49]  Acc_iter 2550        Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.28(1.28)
2025-09-04 20:43:56,862   INFO  Train:    2/36 (  6%) [ 840/1759 ( 48%)]  Loss: 5.729 (5.85)  LR: 3.695e-04  Grad: 9.9801  max=0.4102(module.vfe.pfn_layers.0.linear.weight)  min: -0.4949(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2124, loss_cls=0.3092, loss_bbox=1.4459, matched_ious=0.3814, loss_iou=0.1054, loss_iou_reg=0.2775, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.21(1.28)  Time cost: 17:54/19:34 [55:43/21:33:29]  Acc_iter 2600        Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.28)
2025-09-04 20:44:59,914   INFO  Train:    2/36 (  6%) [ 890/1759 ( 51%)]  Loss: 4.283 (5.84)  LR: 3.722e-04  Grad: 2.7445  max=0.4989(module.vfe.pfn_layers.0.linear.weight)  min: -0.2722(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1829, loss_cls=0.3054, loss_bbox=1.4683, matched_ious=0.3922, loss_iou=0.1044, loss_iou_reg=0.2701, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.31(1.28)  Time cost: 18:57/18:29 [56:46/21:31:27]  Acc_iter 2650        Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.31(1.28)
2025-09-04 20:46:02,770   INFO  Train:    2/36 (  6%) [ 940/1759 ( 53%)]  Loss: 5.320 (5.83)  LR: 3.749e-04  Grad: 2.5255  max=0.2767(module.backbone_3d.cls_conv.3.weight)  min: -0.1976(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=1.1668, loss_cls=0.2997, loss_bbox=1.4212, matched_ious=0.3875, loss_iou=0.1060, loss_iou_reg=0.2713, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.27(1.28)  Time cost: 20:00/17:25 [57:49/21:29:19]  Acc_iter 2700        Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.28)
2025-09-04 20:47:06,447   INFO  Train:    2/36 (  6%) [ 990/1759 ( 56%)]  Loss: 6.250 (5.83)  LR: 3.777e-04  Grad: 2.7439  max=0.3121(module.vfe.pfn_layers.0.linear.weight)  min: -0.2474(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1706, loss_cls=0.2962, loss_bbox=1.5043, matched_ious=0.3882, loss_iou=0.1067, loss_iou_reg=0.2710, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.31(1.28)  Time cost: 21:04/16:21 [58:53/21:28:08]  Acc_iter 2750        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.31(1.28)
2025-09-04 20:48:11,349   INFO  Train:    2/36 (  6%) [1040/1759 ( 59%)]  Loss: 5.461 (5.82)  LR: 3.805e-04  Grad: 3.4352  max=0.8941(module.vfe.pfn_layers.0.linear.weight)  min: -0.2288(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1990, loss_cls=0.3011, loss_bbox=1.4667, matched_ious=0.3876, loss_iou=0.1052, loss_iou_reg=0.2753, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.20(1.28)  Time cost: 22:09/15:18 [59:57/21:28:08]  Acc_iter 2800        Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.28)
2025-09-04 20:49:14,743   INFO  Train:    2/36 (  6%) [1090/1759 ( 62%)]  Loss: 5.262 (5.81)  LR: 3.834e-04  Grad: 3.2712  max=0.8546(module.vfe.pfn_layers.0.linear.weight)  min: -0.2017(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=1.1415, loss_cls=0.2989, loss_bbox=1.4055, matched_ious=0.4049, loss_iou=0.1024, loss_iou_reg=0.2635, d_time=0.00(0.01), f_time=1.18(1.27), b_time=1.19(1.28)  Time cost: 23:12/14:14 [1:01:01/21:26:39]  Acc_iter 2850        Data time: 0.00(0.01)  Forward time: 1.18(1.27)  Batch time: 1.19(1.28)
2025-09-04 20:50:18,490   INFO  Train:    2/36 (  6%) [1140/1759 ( 65%)]  Loss: 5.595 (5.80)  LR: 3.863e-04  Grad: 3.6006  max=0.8852(module.vfe.pfn_layers.0.linear.weight)  min: -0.1523(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1613, loss_cls=0.2959, loss_bbox=1.4421, matched_ious=0.3883, loss_iou=0.1041, loss_iou_reg=0.2714, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.28)  Time cost: 24:16/13:10 [1:02:05/21:25:31]  Acc_iter 2900        Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.28)
2025-09-04 20:51:22,218   INFO  Train:    2/36 (  6%) [1190/1759 ( 68%)]  Loss: 5.366 (5.79)  LR: 3.893e-04  Grad: 3.2001  max=0.4185(module.vfe.pfn_layers.0.linear.weight)  min: -0.4383(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1216, loss_cls=0.2879, loss_bbox=1.3772, matched_ious=0.4009, loss_iou=0.1030, loss_iou_reg=0.2695, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.26(1.28)  Time cost: 25:20/12:06 [1:03:08/21:24:22]  Acc_iter 2950        Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.28)
2025-09-04 20:52:25,169   INFO  Train:    2/36 (  6%) [1240/1759 ( 70%)]  Loss: 5.430 (5.78)  LR: 3.923e-04  Grad: 3.6976  max=0.6483(module.vfe.pfn_layers.0.linear.weight)  min: -0.3038(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=1.1118, loss_cls=0.2886, loss_bbox=1.3232, matched_ious=0.4020, loss_iou=0.1051, loss_iou_reg=0.2683, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.29(1.28)  Time cost: 26:23/11:02 [1:04:11/21:22:36]  Acc_iter 3000        Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.28)
2025-09-04 20:53:28,921   INFO  Train:    2/36 (  6%) [1290/1759 ( 73%)]  Loss: 5.424 (5.77)  LR: 3.954e-04  Grad: 3.9313  max=0.5810(module.vfe.pfn_layers.0.linear.weight)  min: -0.3838(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1347, loss_cls=0.2895, loss_bbox=1.3609, matched_ious=0.3924, loss_iou=0.1064, loss_iou_reg=0.2715, d_time=0.01(0.01), f_time=1.32(1.27), b_time=1.32(1.28)  Time cost: 27:26/09:58 [1:05:15/21:21:31]  Acc_iter 3050        Data time: 0.01(0.01)  Forward time: 1.32(1.27)  Batch time: 1.32(1.28)
2025-09-04 20:54:32,481   INFO  Train:    2/36 (  6%) [1340/1759 ( 76%)]  Loss: 5.501 (5.75)  LR: 3.985e-04  Grad: 3.8705  max=0.1754(module.vfe.pfn_layers.1.linear.weight)  min: -0.5850(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1105, loss_cls=0.2852, loss_bbox=1.3212, matched_ious=0.4073, loss_iou=0.1042, loss_iou_reg=0.2647, d_time=0.01(0.01), f_time=1.37(1.27), b_time=1.38(1.28)  Time cost: 28:30/08:54 [1:06:19/21:20:17]  Acc_iter 3100        Data time: 0.01(0.01)  Forward time: 1.37(1.27)  Batch time: 1.38(1.28)
2025-09-04 20:55:35,425   INFO  Train:    2/36 (  6%) [1390/1759 ( 79%)]  Loss: 5.782 (5.74)  LR: 4.017e-04  Grad: 4.2197  max=0.7721(module.vfe.pfn_layers.0.linear.weight)  min: -0.2378(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1074, loss_cls=0.2811, loss_bbox=1.3883, matched_ious=0.3993, loss_iou=0.1022, loss_iou_reg=0.2703, d_time=0.00(0.01), f_time=1.17(1.27), b_time=1.17(1.28)  Time cost: 29:33/07:50 [1:07:21/21:18:37]  Acc_iter 3150        Data time: 0.00(0.01)  Forward time: 1.17(1.27)  Batch time: 1.17(1.28)
2025-09-04 20:56:39,399   INFO  Train:    2/36 (  6%) [1440/1759 ( 82%)]  Loss: 5.918 (5.74)  LR: 4.049e-04  Grad: 4.5296  max=1.3564(module.vfe.pfn_layers.0.linear.weight)  min: -0.1597(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.1459, loss_cls=0.2863, loss_bbox=1.3818, matched_ious=0.4018, loss_iou=0.1054, loss_iou_reg=0.2657, d_time=0.00(0.01), f_time=1.40(1.27), b_time=1.40(1.28)  Time cost: 30:37/06:46 [1:08:25/21:17:43]  Acc_iter 3200        Data time: 0.00(0.01)  Forward time: 1.40(1.27)  Batch time: 1.40(1.28)
2025-09-04 20:57:42,350   INFO  Train:    2/36 (  6%) [1490/1759 ( 85%)]  Loss: 6.630 (5.72)  LR: 4.081e-04  Grad: 6.0168  max=0.6363(module.vfe.pfn_layers.0.linear.weight)  min: -3.6191(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1132, loss_cls=0.2918, loss_bbox=1.3033, matched_ious=0.4038, loss_iou=0.1024, loss_iou_reg=0.2701, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.27)  Time cost: 31:40/05:42 [1:09:28/21:16:07]  Acc_iter 3250        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.27)
2025-09-04 20:58:47,336   INFO  Train:    2/36 (  6%) [1540/1759 ( 88%)]  Loss: 5.172 (5.71)  LR: 4.114e-04  Grad: 4.9467  max=1.2629(module.vfe.pfn_layers.0.linear.weight)  min: -0.5241(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1306, loss_cls=0.2908, loss_bbox=1.3502, matched_ious=0.4060, loss_iou=0.1061, loss_iou_reg=0.2683, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.32(1.28)  Time cost: 32:45/04:39 [1:10:33/21:15:52]  Acc_iter 3300        Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.32(1.28)
2025-09-04 20:59:50,516   INFO  Train:    2/36 (  6%) [1590/1759 ( 90%)]  Loss: 5.026 (5.70)  LR: 4.148e-04  Grad: 5.7651  max=0.9848(module.vfe.pfn_layers.0.linear.weight)  min: -2.6789(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1002, loss_cls=0.2868, loss_bbox=1.2882, matched_ious=0.4076, loss_iou=0.1065, loss_iou_reg=0.2695, d_time=0.00(0.01), f_time=1.35(1.27), b_time=1.36(1.28)  Time cost: 33:48/03:35 [1:11:37/21:14:27]  Acc_iter 3350        Data time: 0.00(0.01)  Forward time: 1.35(1.27)  Batch time: 1.36(1.28)
2025-09-04 21:00:54,820   INFO  Train:    2/36 (  6%) [1640/1759 ( 93%)]  Loss: 3.867 (5.70)  LR: 4.182e-04  Grad: 5.6017  max=1.4440(module.vfe.pfn_layers.0.linear.weight)  min: -0.4034(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1183, loss_cls=0.2807, loss_bbox=1.3544, matched_ious=0.4016, loss_iou=0.1045, loss_iou_reg=0.2675, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.26(1.28)  Time cost: 34:52/02:31 [1:12:41/21:13:43]  Acc_iter 3400        Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.28)
2025-09-04 21:01:57,963   INFO  Train:    2/36 (  6%) [1690/1759 ( 96%)]  Loss: 5.672 (5.69)  LR: 4.217e-04  Grad: 5.9191  max=1.9325(module.vfe.pfn_layers.0.linear.weight)  min: -2.1276(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1188, loss_cls=0.2781, loss_bbox=1.4206, matched_ious=0.4106, loss_iou=0.1033, loss_iou_reg=0.2663, d_time=0.02(0.01), f_time=1.30(1.27), b_time=1.31(1.28)  Time cost: 35:55/01:27 [1:13:44/21:12:17]  Acc_iter 3450        Data time: 0.02(0.01)  Forward time: 1.30(1.27)  Batch time: 1.31(1.28)
2025-09-04 21:03:01,474   INFO  Train:    2/36 (  6%) [1740/1759 ( 99%)]  Loss: 4.953 (5.68)  LR: 4.251e-04  Grad: 5.4124  max=1.2977(module.vfe.pfn_layers.0.linear.weight)  min: -0.7196(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0799, loss_cls=0.2718, loss_bbox=1.3545, matched_ious=0.4093, loss_iou=0.1007, loss_iou_reg=0.2675, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.28)  Time cost: 36:59/00:24 [1:14:48/21:11:05]  Acc_iter 3500        Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.28)
2025-09-04 21:03:23,186   INFO  Train:    2/36 (  6%) [1758/1759 (100%)]  Loss: 5.071 (5.68)  LR: 4.264e-04  Grad: 6.7515  max=1.3018(module.vfe.pfn_layers.0.linear.weight)  min: -3.1781(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0722, loss_cls=0.2689, loss_bbox=1.2796, matched_ious=0.4235, loss_iou=0.1011, loss_iou_reg=0.2578, d_time=0.00(0.01), f_time=0.67(1.27), b_time=0.68(1.27)  Time cost: 37:21/00:01 [1:15:09/21:10:00]  Acc_iter 3518        Data time: 0.00(0.01)  Forward time: 0.67(1.27)  Batch time: 0.68(1.27)

                                               [Aepochs:   6%|▌         | 2/36 [1:15:10<21:16:40, 2252.94s/it]epochs:   6%|▌         | 2/36 [1:15:10<21:16:40, 2252.95s/it]epochs:   6%|▌         | 2/36 [1:15:10<21:16:40, 2252.96s/it]epochs:   6%|▌         | 2/36 [1:15:10<21:16:40, 2252.96s/it]epochs:   6%|▌         | 2/36 [1:15:10<21:16:40, 2252.96s/it]epochs:   6%|▌         | 2/36 [1:15:10<21:16:40, 2252.96s/it]epochs:   6%|▌         | 2/36 [1:15:10<21:16:40, 2252.97s/it]epochs:   6%|▌         | 2/36 [1:15:10<21:16:44, 2253.07s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 21:03:28,410   INFO  Train:    3/36 (  8%) [   0/1759 (  0%)]  Loss: 4.561 (4.56)  LR: 4.265e-04  Grad: 5.8210  max=1.0143(module.vfe.pfn_layers.0.linear.weight)  min: -1.8458(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8848, loss_cls=0.2630, loss_bbox=0.9514, matched_ious=0.4930, loss_iou=0.0984, loss_iou_reg=0.2319, d_time=1.54(1.54), f_time=2.52(2.52), b_time=4.05(4.05)  Time cost: 00:03/1:44:15 [1:15:14/59:04:41]  Acc_iter 3519        Data time: 1.54(1.54)  Forward time: 2.52(2.52)  Batch time: 4.05(4.05)
2025-09-04 21:04:08,085   INFO  Train:    3/36 (  8%) [  31/1759 (  2%)]  Loss: 5.246 (5.45)  LR: 4.287e-04  Grad: 4.8820  max=1.7459(module.vfe.pfn_layers.0.linear.weight)  min: -0.1781(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.1422, loss_cls=0.2853, loss_bbox=1.3375, matched_ious=0.4039, loss_iou=0.1064, loss_iou_reg=0.2688, d_time=0.00(0.05), f_time=1.24(1.31), b_time=1.25(1.37)  Time cost: 00:43/38:54 [1:15:54/22:25:54]  Acc_iter 3550        Data time: 0.00(0.05)  Forward time: 1.24(1.31)  Batch time: 1.25(1.37)
2025-09-04 21:05:11,386   INFO  Train:    3/36 (  8%) [  81/1759 (  5%)]  Loss: 5.793 (5.42)  LR: 4.323e-04  Grad: 6.2655  max=3.2252(module.vfe.pfn_layers.0.linear.weight)  min: -2.2490(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0778, loss_cls=0.2724, loss_bbox=1.3588, matched_ious=0.4022, loss_iou=0.1051, loss_iou_reg=0.2700, d_time=0.01(0.02), f_time=1.20(1.28), b_time=1.21(1.31)  Time cost: 01:46/36:20 [1:16:57/21:33:12]  Acc_iter 3600        Data time: 0.01(0.02)  Forward time: 1.20(1.28)  Batch time: 1.21(1.31)
2025-09-04 21:06:13,950   INFO  Train:    3/36 (  8%) [ 131/1759 (  7%)]  Loss: 5.186 (5.37)  LR: 4.359e-04  Grad: 5.1545  max=2.8718(module.vfe.pfn_layers.0.linear.weight)  min: -1.1598(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0892, loss_cls=0.2716, loss_bbox=1.3329, matched_ious=0.3975, loss_iou=0.1078, loss_iou_reg=0.2698, d_time=0.00(0.02), f_time=1.18(1.27), b_time=1.18(1.28)  Time cost: 02:49/34:45 [1:18:00/21:14:05]  Acc_iter 3650        Data time: 0.00(0.02)  Forward time: 1.18(1.27)  Batch time: 1.18(1.28)
2025-09-04 21:07:17,760   INFO  Train:    3/36 (  8%) [ 181/1759 ( 10%)]  Loss: 4.857 (5.32)  LR: 4.396e-04  Grad: 6.2387  max=3.1744(module.vfe.pfn_layers.0.linear.weight)  min: -0.0970(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.0404, loss_cls=0.2699, loss_bbox=1.2218, matched_ious=0.4214, loss_iou=0.1039, loss_iou_reg=0.2635, d_time=0.00(0.01), f_time=1.35(1.27), b_time=1.36(1.28)  Time cost: 03:52/33:39 [1:19:04/21:11:42]  Acc_iter 3700        Data time: 0.00(0.01)  Forward time: 1.35(1.27)  Batch time: 1.36(1.28)
2025-09-04 21:08:20,941   INFO  Train:    3/36 (  8%) [ 231/1759 ( 13%)]  Loss: 5.849 (5.32)  LR: 4.433e-04  Grad: 6.5033  max=2.6180(module.vfe.pfn_layers.0.linear.weight)  min: -3.7367(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0715, loss_cls=0.2701, loss_bbox=1.3412, matched_ious=0.4189, loss_iou=0.1007, loss_iou_reg=0.2636, d_time=0.01(0.01), f_time=1.25(1.27), b_time=1.26(1.28)  Time cost: 04:56/32:30 [1:20:07/21:07:11]  Acc_iter 3750        Data time: 0.01(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.28)
2025-09-04 21:09:26,573   INFO  Train:    3/36 (  8%) [ 281/1759 ( 16%)]  Loss: 5.348 (5.31)  LR: 4.471e-04  Grad: 9.4829  max=8.1304(module.vfe.pfn_layers.0.linear.weight)  min: -0.3083(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0727, loss_cls=0.2670, loss_bbox=1.2779, matched_ious=0.4107, loss_iou=0.1068, loss_iou_reg=0.2678, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.24(1.28)  Time cost: 06:01/31:35 [1:21:13/21:12:32]  Acc_iter 3800        Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.28)
2025-09-04 21:10:29,604   INFO  Train:    3/36 (  8%) [ 331/1759 ( 19%)]  Loss: 5.295 (5.30)  LR: 4.509e-04  Grad: 3.7647  max=0.9290(module.vfe.pfn_layers.0.linear.weight)  min: -0.2740(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0787, loss_cls=0.2695, loss_bbox=1.2911, matched_ious=0.4077, loss_iou=0.1025, loss_iou_reg=0.2692, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.19(1.28)  Time cost: 07:04/30:26 [1:22:16/21:08:10]  Acc_iter 3850        Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.19(1.28)
2025-09-04 21:11:33,161   INFO  Train:    3/36 (  8%) [ 381/1759 ( 22%)]  Loss: 5.166 (5.30)  LR: 4.548e-04  Grad: 10.0000  max=8.2041(module.vfe.pfn_layers.0.linear.weight)  min: -0.2684(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=1.0710, loss_cls=0.2682, loss_bbox=1.2765, matched_ious=0.4108, loss_iou=0.1088, loss_iou_reg=0.2683, d_time=0.00(0.01), f_time=1.16(1.27), b_time=1.17(1.28)  Time cost: 08:08/29:21 [1:23:19/21:06:02]  Acc_iter 3900        Data time: 0.00(0.01)  Forward time: 1.16(1.27)  Batch time: 1.17(1.28)
2025-09-04 21:12:37,273   INFO  Train:    3/36 (  8%) [ 431/1759 ( 25%)]  Loss: 5.245 (5.28)  LR: 4.587e-04  Grad: 5.5968  max=4.1304(module.vfe.pfn_layers.0.linear.weight)  min: -1.3658(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0319, loss_cls=0.2618, loss_bbox=1.2356, matched_ious=0.4166, loss_iou=0.1051, loss_iou_reg=0.2634, d_time=0.00(0.01), f_time=1.32(1.27), b_time=1.33(1.28)  Time cost: 09:12/28:18 [1:24:23/21:05:25]  Acc_iter 3950        Data time: 0.00(0.01)  Forward time: 1.32(1.27)  Batch time: 1.33(1.28)
2025-09-04 21:13:41,183   INFO  Train:    3/36 (  8%) [ 481/1759 ( 27%)]  Loss: 6.172 (5.29)  LR: 4.627e-04  Grad: 3.9452  max=1.1443(module.vfe.pfn_layers.0.linear.weight)  min: -2.4778(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0696, loss_cls=0.2680, loss_bbox=1.3238, matched_ious=0.4142, loss_iou=0.1031, loss_iou_reg=0.2638, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.27(1.28)  Time cost: 10:16/27:14 [1:25:27/21:04:18]  Acc_iter 4000        Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.28)
2025-09-04 21:14:43,487   INFO  Train:    3/36 (  8%) [ 531/1759 ( 30%)]  Loss: 5.286 (5.27)  LR: 4.667e-04  Grad: 3.5738  max=1.5314(module.vfe.pfn_layers.0.linear.weight)  min: -1.4627(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0369, loss_cls=0.2593, loss_bbox=1.2419, matched_ious=0.4163, loss_iou=0.1063, loss_iou_reg=0.2657, d_time=0.00(0.01), f_time=1.16(1.27), b_time=1.17(1.28)  Time cost: 11:18/26:06 [1:26:30/21:00:12]  Acc_iter 4050        Data time: 0.00(0.01)  Forward time: 1.16(1.27)  Batch time: 1.17(1.28)
2025-09-04 21:15:46,311   INFO  Train:    3/36 (  8%) [ 581/1759 ( 33%)]  Loss: 5.168 (5.25)  LR: 4.707e-04  Grad: 8.6056  max=3.2778(module.vfe.pfn_layers.0.linear.weight)  min: -7.4013(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9963, loss_cls=0.2561, loss_bbox=1.1817, matched_ious=0.4236, loss_iou=0.1024, loss_iou_reg=0.2624, d_time=0.00(0.01), f_time=1.14(1.27), b_time=1.15(1.27)  Time cost: 12:21/25:00 [1:27:32/20:57:31]  Acc_iter 4100        Data time: 0.00(0.01)  Forward time: 1.14(1.27)  Batch time: 1.15(1.27)
2025-09-04 21:16:50,150   INFO  Train:    3/36 (  8%) [ 631/1759 ( 36%)]  Loss: 5.053 (5.26)  LR: 4.748e-04  Grad: 4.3182  max=1.8350(module.vfe.pfn_layers.0.linear.weight)  min: -1.9481(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0756, loss_cls=0.2668, loss_bbox=1.3051, matched_ious=0.4080, loss_iou=0.1065, loss_iou_reg=0.2702, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.31(1.27)  Time cost: 13:25/23:57 [1:28:36/20:56:40]  Acc_iter 4150        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.31(1.27)
2025-09-04 21:17:53,834   INFO  Train:    3/36 (  8%) [ 681/1759 ( 39%)]  Loss: 6.064 (5.25)  LR: 4.790e-04  Grad: 4.5951  max=3.5525(module.vfe.pfn_layers.0.linear.weight)  min: -0.1181(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0414, loss_cls=0.2612, loss_bbox=1.2627, matched_ious=0.4158, loss_iou=0.1045, loss_iou_reg=0.2659, d_time=0.01(0.01), f_time=1.39(1.27), b_time=1.39(1.27)  Time cost: 14:28/22:53 [1:29:40/20:55:35]  Acc_iter 4200        Data time: 0.01(0.01)  Forward time: 1.39(1.27)  Batch time: 1.39(1.27)
2025-09-04 21:18:58,013   INFO  Train:    3/36 (  8%) [ 731/1759 ( 42%)]  Loss: 4.666 (5.23)  LR: 4.832e-04  Grad: 4.6136  max=2.8406(module.vfe.pfn_layers.0.linear.weight)  min: -1.8908(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9876, loss_cls=0.2496, loss_bbox=1.1989, matched_ious=0.4168, loss_iou=0.1024, loss_iou_reg=0.2657, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.29(1.28)  Time cost: 15:33/21:50 [1:30:44/20:55:09]  Acc_iter 4250        Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.28)
2025-09-04 21:20:03,240   INFO  Train:    3/36 (  8%) [ 781/1759 ( 44%)]  Loss: 4.674 (5.23)  LR: 4.874e-04  Grad: 6.8973  max=1.5431(module.vfe.pfn_layers.0.linear.weight)  min: -4.4305(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0468, loss_cls=0.2543, loss_bbox=1.2560, matched_ious=0.4162, loss_iou=0.1076, loss_iou_reg=0.2651, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.21(1.28)  Time cost: 16:38/20:48 [1:31:49/20:55:57]  Acc_iter 4300        Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.28)
2025-09-04 21:21:05,923   INFO  Train:    3/36 (  8%) [ 831/1759 ( 47%)]  Loss: 5.070 (5.22)  LR: 4.917e-04  Grad: 9.2819  max=8.2427(module.vfe.pfn_layers.0.linear.weight)  min: -1.5086(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0016, loss_cls=0.2486, loss_bbox=1.2161, matched_ious=0.4143, loss_iou=0.1047, loss_iou_reg=0.2689, d_time=0.00(0.01), f_time=1.17(1.27), b_time=1.17(1.28)  Time cost: 17:41/19:43 [1:32:52/20:53:31]  Acc_iter 4350        Data time: 0.00(0.01)  Forward time: 1.17(1.27)  Batch time: 1.17(1.28)
2025-09-04 21:22:10,650   INFO  Train:    3/36 (  8%) [ 881/1759 ( 50%)]  Loss: 5.170 (5.21)  LR: 4.960e-04  Grad: 3.6908  max=2.3805(module.vfe.pfn_layers.0.linear.weight)  min: -0.8390(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0263, loss_cls=0.2526, loss_bbox=1.2413, matched_ious=0.4284, loss_iou=0.1016, loss_iou_reg=0.2612, d_time=0.00(0.01), f_time=1.95(1.27), b_time=1.95(1.28)  Time cost: 18:45/18:40 [1:33:57/20:53:32]  Acc_iter 4400        Data time: 0.00(0.01)  Forward time: 1.95(1.27)  Batch time: 1.95(1.28)
2025-09-04 21:23:14,269   INFO  Train:    3/36 (  8%) [ 931/1759 ( 53%)]  Loss: 5.609 (5.20)  LR: 5.004e-04  Grad: 10.0000  max=9.1547(module.vfe.pfn_layers.0.linear.weight)  min: -1.5599(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0340, loss_cls=0.2617, loss_bbox=1.2075, matched_ious=0.4235, loss_iou=0.1025, loss_iou_reg=0.2647, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.31(1.28)  Time cost: 19:49/17:36 [1:35:00/20:52:16]  Acc_iter 4450        Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.31(1.28)
2025-09-04 21:24:17,457   INFO  Train:    3/36 (  8%) [ 981/1759 ( 56%)]  Loss: 4.787 (5.19)  LR: 5.048e-04  Grad: 3.0006  max=1.2546(module.vfe.pfn_layers.0.linear.weight)  min: -0.8349(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0147, loss_cls=0.2522, loss_bbox=1.2358, matched_ious=0.4294, loss_iou=0.1034, loss_iou_reg=0.2610, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.20(1.28)  Time cost: 20:52/16:32 [1:36:04/20:50:34]  Acc_iter 4500        Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.28)
2025-09-04 21:25:20,995   INFO  Train:    3/36 (  8%) [1031/1759 ( 59%)]  Loss: 4.915 (5.19)  LR: 5.092e-04  Grad: 5.1931  max=4.3187(module.vfe.pfn_layers.0.linear.weight)  min: -0.2820(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0619, loss_cls=0.2550, loss_bbox=1.2822, matched_ious=0.4208, loss_iou=0.1043, loss_iou_reg=0.2638, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.30(1.28)  Time cost: 21:56/15:28 [1:37:07/20:49:17]  Acc_iter 4550        Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.30(1.28)
2025-09-04 21:26:24,688   INFO  Train:    3/36 (  8%) [1081/1759 ( 61%)]  Loss: 4.516 (5.18)  LR: 5.137e-04  Grad: 6.4250  max=5.9579(module.vfe.pfn_layers.0.linear.weight)  min: -0.4890(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0067, loss_cls=0.2493, loss_bbox=1.1979, matched_ious=0.4272, loss_iou=0.1034, loss_iou_reg=0.2600, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.28)  Time cost: 22:59/14:24 [1:38:11/20:48:08]  Acc_iter 4600        Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.28)
2025-09-04 21:27:28,144   INFO  Train:    3/36 (  8%) [1131/1759 ( 64%)]  Loss: 5.195 (5.17)  LR: 5.183e-04  Grad: 6.1824  max=3.3791(module.vfe.pfn_layers.0.linear.weight)  min: -4.5339(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0044, loss_cls=0.2499, loss_bbox=1.1601, matched_ious=0.4378, loss_iou=0.1030, loss_iou_reg=0.2572, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.27(1.28)  Time cost: 24:03/13:20 [1:39:14/20:46:50]  Acc_iter 4650        Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.28)
2025-09-04 21:28:32,485   INFO  Train:    3/36 (  8%) [1181/1759 ( 67%)]  Loss: 4.915 (5.17)  LR: 5.229e-04  Grad: 10.0000  max=9.4959(module.vfe.pfn_layers.0.linear.weight)  min: -0.5216(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0427, loss_cls=0.2553, loss_bbox=1.2224, matched_ious=0.4223, loss_iou=0.1057, loss_iou_reg=0.2650, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.21(1.28)  Time cost: 25:07/12:17 [1:40:19/20:46:15]  Acc_iter 4700        Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.28)
2025-09-04 21:29:35,264   INFO  Train:    3/36 (  8%) [1231/1759 ( 70%)]  Loss: 5.541 (5.17)  LR: 5.275e-04  Grad: 5.8318  max=1.8497(module.vfe.pfn_layers.0.linear.weight)  min: -5.1535(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0127, loss_cls=0.2475, loss_bbox=1.2816, matched_ious=0.4238, loss_iou=0.1036, loss_iou_reg=0.2628, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.24(1.28)  Time cost: 26:10/11:13 [1:41:21/20:44:24]  Acc_iter 4750        Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.28)
2025-09-04 21:30:40,801   INFO  Train:    3/36 (  8%) [1281/1759 ( 73%)]  Loss: 4.866 (5.16)  LR: 5.322e-04  Grad: 8.1196  max=3.7783(module.vfe.pfn_layers.0.linear.weight)  min: -5.8504(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0122, loss_cls=0.2501, loss_bbox=1.2173, matched_ious=0.4303, loss_iou=0.1030, loss_iou_reg=0.2610, d_time=0.00(0.01), f_time=1.39(1.27), b_time=1.40(1.28)  Time cost: 27:15/10:09 [1:42:27/20:44:43]  Acc_iter 4800        Data time: 0.00(0.01)  Forward time: 1.39(1.27)  Batch time: 1.40(1.28)
2025-09-04 21:31:43,891   INFO  Train:    3/36 (  8%) [1331/1759 ( 76%)]  Loss: 5.611 (5.16)  LR: 5.369e-04  Grad: 7.9870  max=5.0940(module.vfe.pfn_layers.0.linear.weight)  min: -3.2447(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0307, loss_cls=0.2552, loss_bbox=1.2372, matched_ious=0.4262, loss_iou=0.1040, loss_iou_reg=0.2597, d_time=0.00(0.01), f_time=1.32(1.27), b_time=1.32(1.28)  Time cost: 28:19/09:05 [1:43:30/20:43:08]  Acc_iter 4850        Data time: 0.00(0.01)  Forward time: 1.32(1.27)  Batch time: 1.32(1.28)
2025-09-04 21:32:48,332   INFO  Train:    3/36 (  8%) [1381/1759 ( 79%)]  Loss: 4.936 (5.16)  LR: 5.416e-04  Grad: 7.5937  max=2.7698(module.vfe.pfn_layers.0.linear.weight)  min: -4.7865(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0079, loss_cls=0.2466, loss_bbox=1.2509, matched_ious=0.4329, loss_iou=0.1051, loss_iou_reg=0.2613, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.21(1.28)  Time cost: 29:23/08:02 [1:44:34/20:42:32]  Acc_iter 4900        Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.28)
2025-09-04 21:33:52,035   INFO  Train:    3/36 (  8%) [1431/1759 ( 81%)]  Loss: 5.700 (5.15)  LR: 5.464e-04  Grad: 7.0228  max=0.2364(module.vfe.pfn_layers.0.linear.weight)  min: -6.6570(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9743, loss_cls=0.2430, loss_bbox=1.1642, matched_ious=0.4391, loss_iou=0.0989, loss_iou_reg=0.2573, d_time=0.01(0.01), f_time=1.15(1.27), b_time=1.15(1.28)  Time cost: 30:27/06:58 [1:45:38/20:41:24]  Acc_iter 4950        Data time: 0.01(0.01)  Forward time: 1.15(1.27)  Batch time: 1.15(1.28)
2025-09-04 21:34:55,603   INFO  Train:    3/36 (  8%) [1481/1759 ( 84%)]  Loss: 4.892 (5.15)  LR: 5.513e-04  Grad: 10.0000  max=4.5093(module.vfe.pfn_layers.0.linear.weight)  min: -8.8416(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0347, loss_cls=0.2517, loss_bbox=1.2522, matched_ious=0.4264, loss_iou=0.1022, loss_iou_reg=0.2603, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.21(1.28)  Time cost: 31:30/05:54 [1:46:42/20:40:11]  Acc_iter 5000        Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.28)
2025-09-04 21:35:58,778   INFO  Train:    3/36 (  8%) [1531/1759 ( 87%)]  Loss: 4.873 (5.14)  LR: 5.562e-04  Grad: 3.9422  max=0.2489(module.backbone_3d.cls_conv.3.weight)  min: -3.2669(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9996, loss_cls=0.2456, loss_bbox=1.2049, matched_ious=0.4314, loss_iou=0.1046, loss_iou_reg=0.2566, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.29(1.28)  Time cost: 32:33/04:50 [1:47:45/20:38:44]  Acc_iter 5050        Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.29(1.28)
2025-09-04 21:37:01,530   INFO  Train:    3/36 (  8%) [1581/1759 ( 90%)]  Loss: 4.943 (5.13)  LR: 5.611e-04  Grad: 4.5026  max=2.2326(module.vfe.pfn_layers.0.linear.weight)  min: -3.5218(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9993, loss_cls=0.2419, loss_bbox=1.1432, matched_ious=0.4396, loss_iou=0.1021, loss_iou_reg=0.2562, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.26(1.28)  Time cost: 33:36/03:46 [1:48:48/20:37:03]  Acc_iter 5100        Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.28)
2025-09-04 21:38:04,848   INFO  Train:    3/36 (  8%) [1631/1759 ( 93%)]  Loss: 4.290 (5.13)  LR: 5.661e-04  Grad: 2.6350  max=0.4967(module.vfe.pfn_layers.0.linear.weight)  min: -1.7156(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9817, loss_cls=0.2376, loss_bbox=1.1861, matched_ious=0.4255, loss_iou=0.1049, loss_iou_reg=0.2622, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.26(1.27)  Time cost: 34:39/02:43 [1:49:51/20:35:44]  Acc_iter 5150        Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.27)
2025-09-04 21:39:08,727   INFO  Train:    3/36 (  8%) [1681/1759 ( 96%)]  Loss: 5.326 (5.12)  LR: 5.711e-04  Grad: 6.6825  max=5.5622(module.vfe.pfn_layers.0.linear.weight)  min: -1.9155(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9670, loss_cls=0.2402, loss_bbox=1.1192, matched_ious=0.4384, loss_iou=0.1009, loss_iou_reg=0.2574, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.28(1.27)  Time cost: 35:43/01:39 [1:50:55/20:34:45]  Acc_iter 5200        Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.28(1.27)
2025-09-04 21:40:11,878   INFO  Train:    3/36 (  8%) [1731/1759 ( 98%)]  Loss: 4.627 (5.11)  LR: 5.761e-04  Grad: 3.2976  max=0.8677(module.vfe.pfn_layers.0.linear.weight)  min: -2.3066(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9885, loss_cls=0.2360, loss_bbox=1.1603, matched_ious=0.4354, loss_iou=0.1015, loss_iou_reg=0.2567, d_time=0.00(0.01), f_time=1.18(1.27), b_time=1.18(1.27)  Time cost: 36:47/00:35 [1:51:58/20:33:22]  Acc_iter 5250        Data time: 0.00(0.01)  Forward time: 1.18(1.27)  Batch time: 1.18(1.27)
2025-09-04 21:40:47,141   INFO  Train:    3/36 (  8%) [1758/1759 (100%)]  Loss: 4.665 (5.11)  LR: 5.789e-04  Grad: 3.4524  max=1.8797(module.vfe.pfn_layers.0.linear.weight)  min: -1.1961(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0303, loss_cls=0.2503, loss_bbox=1.1597, matched_ious=0.4292, loss_iou=0.1048, loss_iou_reg=0.2630, d_time=0.00(0.01), f_time=0.71(1.27), b_time=0.71(1.28)  Time cost: 37:22/00:01 [1:52:33/20:33:16]  Acc_iter 5277        Data time: 0.00(0.01)  Forward time: 0.71(1.27)  Batch time: 0.71(1.28)

                                               [Aepochs:   8%|▊         | 3/36 [1:52:34<20:36:51, 2248.84s/it]epochs:   8%|▊         | 3/36 [1:52:34<20:36:52, 2248.87s/it]epochs:   8%|▊         | 3/36 [1:52:34<20:36:52, 2248.87s/it]epochs:   8%|▊         | 3/36 [1:52:34<20:36:52, 2248.87s/it]epochs:   8%|▊         | 3/36 [1:52:34<20:36:53, 2248.89s/it]epochs:   8%|▊         | 3/36 [1:52:34<20:36:52, 2248.87s/it]epochs:   8%|▊         | 3/36 [1:52:34<20:36:53, 2248.88s/it]epochs:   8%|▊         | 3/36 [1:52:34<20:36:55, 2248.97s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 21:40:52,565   INFO  Train:    4/36 ( 11%) [   0/1759 (  0%)]  Loss: 4.934 (4.93)  LR: 5.790e-04  Grad: 3.1182  max=1.8089(module.vfe.pfn_layers.0.linear.weight)  min: -1.1632(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9012, loss_cls=0.2368, loss_bbox=1.4496, matched_ious=0.4650, loss_iou=0.0994, loss_iou_reg=0.2445, d_time=1.51(1.51), f_time=2.61(2.61), b_time=4.13(4.13)  Time cost: 00:03/1:43:38 [1:52:39/56:59:56]  Acc_iter 5278        Data time: 1.51(1.51)  Forward time: 2.61(2.61)  Batch time: 4.13(4.13)
2025-09-04 21:41:21,658   INFO  Train:    4/36 ( 11%) [  22/1759 (  1%)]  Loss: 4.896 (4.87)  LR: 5.812e-04  Grad: 4.0759  max=3.2740(module.vfe.pfn_layers.0.linear.weight)  min: -0.6211(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0041, loss_cls=0.2436, loss_bbox=1.1558, matched_ious=0.4384, loss_iou=0.1035, loss_iou_reg=0.2575, d_time=0.01(0.07), f_time=1.35(1.37), b_time=1.36(1.44)  Time cost: 00:32/41:04 [1:53:08/22:52:02]  Acc_iter 5300        Data time: 0.01(0.07)  Forward time: 1.35(1.37)  Batch time: 1.36(1.44)
2025-09-04 21:42:25,127   INFO  Train:    4/36 ( 11%) [  72/1759 (  4%)]  Loss: 3.797 (4.92)  LR: 5.864e-04  Grad: 6.2464  max=1.9970(module.vfe.pfn_layers.0.linear.weight)  min: -5.0950(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9750, loss_cls=0.2375, loss_bbox=1.2462, matched_ious=0.4279, loss_iou=0.1037, loss_iou_reg=0.2643, d_time=0.00(0.03), f_time=1.32(1.30), b_time=1.32(1.32)  Time cost: 01:36/37:00 [1:54:11/21:12:01]  Acc_iter 5350        Data time: 0.00(0.03)  Forward time: 1.32(1.30)  Batch time: 1.32(1.32)
2025-09-04 21:43:28,493   INFO  Train:    4/36 ( 11%) [ 122/1759 (  7%)]  Loss: 5.695 (4.90)  LR: 5.915e-04  Grad: 6.9678  max=1.1446(module.vfe.pfn_layers.0.linear.weight)  min: -5.7882(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9822, loss_cls=0.2390, loss_bbox=1.1437, matched_ious=0.4329, loss_iou=0.1004, loss_iou_reg=0.2612, d_time=0.00(0.02), f_time=1.26(1.28), b_time=1.26(1.30)  Time cost: 02:39/35:22 [1:55:15/20:51:37]  Acc_iter 5400        Data time: 0.00(0.02)  Forward time: 1.26(1.28)  Batch time: 1.26(1.30)
2025-09-04 21:44:32,829   INFO  Train:    4/36 ( 11%) [ 172/1759 ( 10%)]  Loss: 4.961 (4.89)  LR: 5.968e-04  Grad: 2.7128  max=0.7360(module.vfe.pfn_layers.0.linear.weight)  min: -1.6149(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9629, loss_cls=0.2355, loss_bbox=1.1384, matched_ious=0.4385, loss_iou=0.1033, loss_iou_reg=0.2588, d_time=0.00(0.01), f_time=1.26(1.28), b_time=1.26(1.30)  Time cost: 03:43/34:13 [1:56:19/20:47:50]  Acc_iter 5450        Data time: 0.00(0.01)  Forward time: 1.26(1.28)  Batch time: 1.26(1.30)
2025-09-04 21:45:36,135   INFO  Train:    4/36 ( 11%) [ 222/1759 ( 13%)]  Loss: 4.858 (4.91)  LR: 6.020e-04  Grad: 8.1856  max=2.7485(module.vfe.pfn_layers.0.linear.weight)  min: -6.4429(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0123, loss_cls=0.2407, loss_bbox=1.2108, matched_ious=0.4361, loss_iou=0.1029, loss_iou_reg=0.2583, d_time=0.01(0.01), f_time=1.24(1.28), b_time=1.25(1.29)  Time cost: 04:47/32:58 [1:57:22/20:40:48]  Acc_iter 5500        Data time: 0.01(0.01)  Forward time: 1.24(1.28)  Batch time: 1.25(1.29)
2025-09-04 21:46:39,765   INFO  Train:    4/36 ( 11%) [ 272/1759 ( 15%)]  Loss: 5.412 (4.92)  LR: 6.073e-04  Grad: 7.0253  max=0.4105(module.vfe.pfn_layers.0.linear.weight)  min: -6.8083(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9961, loss_cls=0.2383, loss_bbox=1.1335, matched_ious=0.4401, loss_iou=0.1022, loss_iou_reg=0.2578, d_time=0.00(0.01), f_time=1.33(1.28), b_time=1.33(1.29)  Time cost: 05:50/31:50 [1:58:26/20:37:06]  Acc_iter 5550        Data time: 0.00(0.01)  Forward time: 1.33(1.28)  Batch time: 1.33(1.29)
2025-09-04 21:47:42,980   INFO  Train:    4/36 ( 11%) [ 322/1759 ( 18%)]  Loss: 4.468 (4.92)  LR: 6.127e-04  Grad: 4.9251  max=0.2247(module.backbone_3d.cls_conv.3.weight)  min: -3.0327(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9889, loss_cls=0.2381, loss_bbox=1.1740, matched_ious=0.4327, loss_iou=0.1040, loss_iou_reg=0.2605, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.22(1.28)  Time cost: 06:53/30:41 [1:59:29/20:32:59]  Acc_iter 5600        Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.22(1.28)
2025-09-04 21:48:46,170   INFO  Train:    4/36 ( 11%) [ 372/1759 ( 21%)]  Loss: 5.476 (4.92)  LR: 6.180e-04  Grad: 4.5925  max=0.2745(module.vfe.pfn_layers.0.linear.weight)  min: -3.6728(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9937, loss_cls=0.2456, loss_bbox=1.1721, matched_ious=0.4345, loss_iou=0.1040, loss_iou_reg=0.2593, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.28)  Time cost: 07:57/29:34 [2:00:32/20:29:37]  Acc_iter 5650        Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.28)
2025-09-04 21:49:50,293   INFO  Train:    4/36 ( 11%) [ 422/1759 ( 24%)]  Loss: 5.238 (4.91)  LR: 6.234e-04  Grad: 7.8405  max=6.1017(module.vfe.pfn_layers.0.linear.weight)  min: -4.4084(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9720, loss_cls=0.2358, loss_bbox=1.1443, matched_ious=0.4394, loss_iou=0.1013, loss_iou_reg=0.2575, d_time=0.01(0.01), f_time=1.24(1.27), b_time=1.25(1.28)  Time cost: 09:01/28:30 [2:01:36/20:28:56]  Acc_iter 5700        Data time: 0.01(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.28)
2025-09-04 21:50:53,980   INFO  Train:    4/36 ( 11%) [ 472/1759 ( 27%)]  Loss: 4.574 (4.89)  LR: 6.289e-04  Grad: 8.7129  max=7.1294(module.vfe.pfn_layers.0.linear.weight)  min: -1.6223(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9397, loss_cls=0.2279, loss_bbox=1.0947, matched_ious=0.4458, loss_iou=0.1025, loss_iou_reg=0.2553, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.26(1.28)  Time cost: 10:04/27:26 [2:02:40/20:27:16]  Acc_iter 5750        Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.28)
2025-09-04 21:52:00,301   INFO  Train:    4/36 ( 11%) [ 522/1759 ( 30%)]  Loss: 4.713 (4.87)  LR: 6.344e-04  Grad: 2.6983  max=1.5048(module.vfe.pfn_layers.0.linear.weight)  min: -0.1790(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9592, loss_cls=0.2320, loss_bbox=1.0986, matched_ious=0.4384, loss_iou=0.1035, loss_iou_reg=0.2587, d_time=0.00(0.01), f_time=1.18(1.28), b_time=1.19(1.28)  Time cost: 11:11/26:27 [2:03:46/20:30:33]  Acc_iter 5800        Data time: 0.00(0.01)  Forward time: 1.18(1.28)  Batch time: 1.19(1.28)
2025-09-04 21:53:03,201   INFO  Train:    4/36 ( 11%) [ 572/1759 ( 33%)]  Loss: 5.591 (4.87)  LR: 6.399e-04  Grad: 4.1620  max=0.3500(module.vfe.pfn_layers.0.linear.weight)  min: -2.8482(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9614, loss_cls=0.2295, loss_bbox=1.1229, matched_ious=0.4385, loss_iou=0.1034, loss_iou_reg=0.2595, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.28(1.28)  Time cost: 12:14/25:20 [2:04:49/20:27:21]  Acc_iter 5850        Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.28(1.28)
2025-09-04 21:54:06,565   INFO  Train:    4/36 ( 11%) [ 622/1759 ( 35%)]  Loss: 4.480 (4.87)  LR: 6.455e-04  Grad: 2.7491  max=0.3997(module.vfe.pfn_layers.0.linear.weight)  min: -1.8687(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9899, loss_cls=0.2422, loss_bbox=1.1254, matched_ious=0.4405, loss_iou=0.1036, loss_iou_reg=0.2570, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.21(1.28)  Time cost: 13:17/24:15 [2:05:53/20:25:13]  Acc_iter 5900        Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.28)
2025-09-04 21:55:10,983   INFO  Train:    4/36 ( 11%) [ 672/1759 ( 38%)]  Loss: 4.164 (4.87)  LR: 6.511e-04  Grad: 3.8663  max=2.0435(module.vfe.pfn_layers.0.linear.weight)  min: -1.9023(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0092, loss_cls=0.2329, loss_bbox=1.1543, matched_ious=0.4391, loss_iou=0.1027, loss_iou_reg=0.2554, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.28)  Time cost: 14:21/23:12 [2:06:57/20:24:44]  Acc_iter 5950        Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.28)
2025-09-04 21:56:15,260   INFO  Train:    4/36 ( 11%) [ 722/1759 ( 41%)]  Loss: 5.536 (4.87)  LR: 6.568e-04  Grad: 6.0452  max=4.1398(module.vfe.pfn_layers.0.linear.weight)  min: -0.1252(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.9664, loss_cls=0.2278, loss_bbox=1.1774, matched_ious=0.4413, loss_iou=0.1010, loss_iou_reg=0.2569, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.31(1.28)  Time cost: 15:26/22:08 [2:08:01/20:23:58]  Acc_iter 6000        Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.31(1.28)
2025-09-04 21:57:18,752   INFO  Train:    4/36 ( 11%) [ 772/1759 ( 44%)]  Loss: 4.306 (4.86)  LR: 6.625e-04  Grad: 7.3025  max=5.9666(module.vfe.pfn_layers.0.linear.weight)  min: -2.5217(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9677, loss_cls=0.2336, loss_bbox=1.0821, matched_ious=0.4484, loss_iou=0.1017, loss_iou_reg=0.2516, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.29(1.28)  Time cost: 16:29/21:03 [2:09:05/20:22:13]  Acc_iter 6050        Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.28)
2025-09-04 21:58:21,938   INFO  Train:    4/36 ( 11%) [ 822/1759 ( 47%)]  Loss: 4.416 (4.85)  LR: 6.682e-04  Grad: 5.1466  max=1.2305(module.vfe.pfn_layers.0.linear.weight)  min: -4.5759(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9462, loss_cls=0.2346, loss_bbox=1.0341, matched_ious=0.4414, loss_iou=0.1020, loss_iou_reg=0.2584, d_time=0.01(0.01), f_time=1.30(1.27), b_time=1.32(1.28)  Time cost: 17:32/19:58 [2:10:08/20:20:10]  Acc_iter 6100        Data time: 0.01(0.01)  Forward time: 1.30(1.27)  Batch time: 1.32(1.28)
2025-09-04 21:59:25,187   INFO  Train:    4/36 ( 11%) [ 872/1759 ( 50%)]  Loss: 4.287 (4.85)  LR: 6.740e-04  Grad: 6.3693  max=1.4727(module.vfe.pfn_layers.0.linear.weight)  min: -5.8771(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9587, loss_cls=0.2253, loss_bbox=1.1636, matched_ious=0.4392, loss_iou=0.1028, loss_iou_reg=0.2592, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.19(1.28)  Time cost: 18:36/18:54 [2:11:11/20:18:20]  Acc_iter 6150        Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.19(1.28)
2025-09-04 22:00:28,861   INFO  Train:    4/36 ( 11%) [ 922/1759 ( 52%)]  Loss: 5.738 (4.85)  LR: 6.798e-04  Grad: 3.8103  max=1.9526(module.vfe.pfn_layers.0.linear.weight)  min: -1.8894(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9714, loss_cls=0.2266, loss_bbox=1.1530, matched_ious=0.4314, loss_iou=0.1023, loss_iou_reg=0.2616, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.20(1.28)  Time cost: 19:39/17:49 [2:12:15/20:17:00]  Acc_iter 6200        Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.28)
2025-09-04 22:01:33,193   INFO  Train:    4/36 ( 11%) [ 972/1759 ( 55%)]  Loss: 4.518 (4.84)  LR: 6.856e-04  Grad: 5.3811  max=4.3652(module.vfe.pfn_layers.0.linear.weight)  min: -0.1368(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.9428, loss_cls=0.2239, loss_bbox=1.1119, matched_ious=0.4409, loss_iou=0.1026, loss_iou_reg=0.2555, d_time=0.00(0.01), f_time=1.18(1.27), b_time=1.18(1.28)  Time cost: 20:44/16:46 [2:13:19/20:16:21]  Acc_iter 6250        Data time: 0.00(0.01)  Forward time: 1.18(1.27)  Batch time: 1.18(1.28)
2025-09-04 22:02:38,226   INFO  Train:    4/36 ( 11%) [1022/1759 ( 58%)]  Loss: 5.309 (4.83)  LR: 6.915e-04  Grad: 4.1104  max=3.1024(module.vfe.pfn_layers.0.linear.weight)  min: -1.6442(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9425, loss_cls=0.2222, loss_bbox=1.1127, matched_ious=0.4411, loss_iou=0.1018, loss_iou_reg=0.2569, d_time=0.00(0.01), f_time=1.40(1.27), b_time=1.40(1.28)  Time cost: 21:49/15:43 [2:14:24/20:16:18]  Acc_iter 6300        Data time: 0.00(0.01)  Forward time: 1.40(1.27)  Batch time: 1.40(1.28)
2025-09-04 22:03:41,264   INFO  Train:    4/36 ( 11%) [1072/1759 ( 61%)]  Loss: 4.277 (4.83)  LR: 6.974e-04  Grad: 2.5255  max=0.3344(module.vfe.pfn_layers.0.linear.weight)  min: -0.8610(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9608, loss_cls=0.2284, loss_bbox=1.0797, matched_ious=0.4520, loss_iou=0.1018, loss_iou_reg=0.2502, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.21(1.28)  Time cost: 22:52/14:38 [2:15:27/20:14:24]  Acc_iter 6350        Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.28)
2025-09-04 22:04:44,744   INFO  Train:    4/36 ( 11%) [1122/1759 ( 64%)]  Loss: 5.020 (4.82)  LR: 7.033e-04  Grad: 2.9760  max=0.4407(module.vfe.pfn_layers.0.linear.weight)  min: -1.9221(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9396, loss_cls=0.2224, loss_bbox=1.0852, matched_ious=0.4533, loss_iou=0.1005, loss_iou_reg=0.2515, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.27(1.28)  Time cost: 23:55/13:34 [2:16:31/20:12:56]  Acc_iter 6400        Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.28)
2025-09-04 22:05:47,886   INFO  Train:    4/36 ( 11%) [1172/1759 ( 67%)]  Loss: 4.453 (4.82)  LR: 7.093e-04  Grad: 6.9950  max=1.3041(module.vfe.pfn_layers.0.linear.weight)  min: -6.2242(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9451, loss_cls=0.2230, loss_bbox=1.1512, matched_ious=0.4413, loss_iou=0.1020, loss_iou_reg=0.2587, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.24(1.28)  Time cost: 24:58/12:30 [2:17:34/20:11:14]  Acc_iter 6450        Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.28)
2025-09-04 22:06:51,416   INFO  Train:    4/36 ( 11%) [1222/1759 ( 69%)]  Loss: 3.887 (4.81)  LR: 7.154e-04  Grad: 3.3500  max=0.8998(module.vfe.pfn_layers.0.linear.weight)  min: -2.6463(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9001, loss_cls=0.2148, loss_bbox=1.0107, matched_ious=0.4519, loss_iou=0.0989, loss_iou_reg=0.2530, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.28(1.28)  Time cost: 26:02/11:26 [2:18:37/20:09:54]  Acc_iter 6500        Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.28(1.28)
2025-09-04 22:07:54,368   INFO  Train:    4/36 ( 11%) [1272/1759 ( 72%)]  Loss: 3.896 (4.80)  LR: 7.214e-04  Grad: 4.5447  max=3.4774(module.vfe.pfn_layers.0.linear.weight)  min: -1.8960(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9492, loss_cls=0.2179, loss_bbox=1.1001, matched_ious=0.4506, loss_iou=0.1039, loss_iou_reg=0.2530, d_time=0.00(0.01), f_time=1.15(1.27), b_time=1.15(1.28)  Time cost: 27:05/10:21 [2:19:40/20:08:09]  Acc_iter 6550        Data time: 0.00(0.01)  Forward time: 1.15(1.27)  Batch time: 1.15(1.28)
2025-09-04 22:08:57,137   INFO  Train:    4/36 ( 11%) [1322/1759 ( 75%)]  Loss: 4.317 (4.80)  LR: 7.275e-04  Grad: 9.1590  max=7.0677(module.vfe.pfn_layers.0.linear.weight)  min: -0.1289(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.9416, loss_cls=0.2192, loss_bbox=1.0961, matched_ious=0.4453, loss_iou=0.1043, loss_iou_reg=0.2588, d_time=0.00(0.01), f_time=1.16(1.27), b_time=1.17(1.28)  Time cost: 28:08/09:17 [2:20:43/20:06:19]  Acc_iter 6600        Data time: 0.00(0.01)  Forward time: 1.16(1.27)  Batch time: 1.17(1.28)
2025-09-04 22:09:59,907   INFO  Train:    4/36 ( 11%) [1372/1759 ( 78%)]  Loss: 4.782 (4.80)  LR: 7.336e-04  Grad: 4.8304  max=3.9738(module.vfe.pfn_layers.0.linear.weight)  min: -0.1533(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.9661, loss_cls=0.2245, loss_bbox=1.1030, matched_ious=0.4397, loss_iou=0.1020, loss_iou_reg=0.2588, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.26(1.28)  Time cost: 29:10/08:13 [2:21:46/20:04:33]  Acc_iter 6650        Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.28)
2025-09-04 22:11:04,012   INFO  Train:    4/36 ( 11%) [1422/1759 ( 81%)]  Loss: 4.887 (4.79)  LR: 7.398e-04  Grad: 4.4158  max=3.3687(module.vfe.pfn_layers.0.linear.weight)  min: -1.8956(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9440, loss_cls=0.2164, loss_bbox=1.0948, matched_ious=0.4547, loss_iou=0.1024, loss_iou_reg=0.2507, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.23(1.28)  Time cost: 30:14/07:09 [2:22:50/20:03:43]  Acc_iter 6700        Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.23(1.28)
2025-09-04 22:12:07,145   INFO  Train:    4/36 ( 11%) [1472/1759 ( 84%)]  Loss: 4.762 (4.79)  LR: 7.460e-04  Grad: 6.5303  max=5.4114(module.vfe.pfn_layers.0.linear.weight)  min: -3.2751(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9670, loss_cls=0.2218, loss_bbox=1.1157, matched_ious=0.4399, loss_iou=0.1032, loss_iou_reg=0.2580, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.29(1.28)  Time cost: 31:18/06:05 [2:23:53/20:02:14]  Acc_iter 6750        Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.29(1.28)
2025-09-04 22:13:11,785   INFO  Train:    4/36 ( 11%) [1522/1759 ( 87%)]  Loss: 5.163 (4.79)  LR: 7.522e-04  Grad: 10.0000  max=2.0583(module.vfe.pfn_layers.0.linear.weight)  min: -9.2074(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9461, loss_cls=0.2202, loss_bbox=1.0896, matched_ious=0.4502, loss_iou=0.1019, loss_iou_reg=0.2528, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.26(1.28)  Time cost: 32:22/05:02 [2:24:58/20:01:44]  Acc_iter 6800        Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.28)
2025-09-04 22:14:15,364   INFO  Train:    4/36 ( 11%) [1572/1759 ( 89%)]  Loss: 5.335 (4.78)  LR: 7.585e-04  Grad: 3.5072  max=2.5537(module.vfe.pfn_layers.0.linear.weight)  min: -1.0460(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9273, loss_cls=0.2172, loss_bbox=1.0949, matched_ious=0.4446, loss_iou=0.1017, loss_iou_reg=0.2554, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.19(1.28)  Time cost: 33:26/03:58 [2:26:01/20:00:32]  Acc_iter 6850        Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.19(1.28)
2025-09-04 22:15:18,708   INFO  Train:    4/36 ( 11%) [1622/1759 ( 92%)]  Loss: 5.065 (4.78)  LR: 7.648e-04  Grad: 3.8476  max=2.2266(module.vfe.pfn_layers.0.linear.weight)  min: -2.1210(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9503, loss_cls=0.2147, loss_bbox=1.1283, matched_ious=0.4447, loss_iou=0.1029, loss_iou_reg=0.2551, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.28(1.28)  Time cost: 34:29/02:54 [2:27:05/19:59:14]  Acc_iter 6900        Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.28(1.28)
2025-09-04 22:16:22,425   INFO  Train:    4/36 ( 11%) [1672/1759 ( 95%)]  Loss: 4.730 (4.78)  LR: 7.711e-04  Grad: 6.1169  max=4.5563(module.vfe.pfn_layers.0.linear.weight)  min: -3.6015(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9592, loss_cls=0.2216, loss_bbox=1.1377, matched_ious=0.4482, loss_iou=0.1021, loss_iou_reg=0.2517, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.28(1.28)  Time cost: 35:33/01:50 [2:28:08/19:58:08]  Acc_iter 6950        Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.28(1.28)
2025-09-04 22:17:25,223   INFO  Train:    4/36 ( 11%) [1722/1759 ( 98%)]  Loss: 5.456 (4.78)  LR: 7.775e-04  Grad: 10.0000  max=6.4967(module.vfe.pfn_layers.0.linear.weight)  min: -7.4444(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9462, loss_cls=0.2194, loss_bbox=1.1355, matched_ious=0.4491, loss_iou=0.1037, loss_iou_reg=0.2538, d_time=0.01(0.01), f_time=1.23(1.27), b_time=1.24(1.27)  Time cost: 36:36/00:47 [2:29:11/19:56:33]  Acc_iter 7000        Data time: 0.01(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.27)
2025-09-04 22:18:09,439   INFO  Train:    4/36 ( 11%) [1758/1759 (100%)]  Loss: 5.116 (4.77)  LR: 7.821e-04  Grad: 7.7641  max=6.2211(module.vfe.pfn_layers.0.linear.weight)  min: -2.6613(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9108, loss_cls=0.2116, loss_bbox=1.0557, matched_ious=0.4505, loss_iou=0.1060, loss_iou_reg=0.2556, d_time=0.00(0.01), f_time=0.68(1.27), b_time=0.68(1.27)  Time cost: 37:20/00:01 [2:29:55/19:54:54]  Acc_iter 7036        Data time: 0.00(0.01)  Forward time: 0.68(1.27)  Batch time: 0.68(1.27)

                                               [Aepochs:  11%|█         | 4/36 [2:29:56<19:57:59, 2246.25s/it]epochs:  11%|█         | 4/36 [2:29:56<19:58:00, 2246.26s/it]epochs:  11%|█         | 4/36 [2:29:56<19:57:59, 2246.25s/it]epochs:  11%|█         | 4/36 [2:29:56<19:58:00, 2246.28s/it]epochs:  11%|█         | 4/36 [2:29:56<19:58:00, 2246.25s/it]epochs:  11%|█         | 4/36 [2:29:56<19:58:00, 2246.26s/it]epochs:  11%|█         | 4/36 [2:29:56<19:58:00, 2246.26s/it]epochs:  11%|█         | 4/36 [2:29:56<19:58:01, 2246.31s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 22:18:14,698   INFO  Train:    5/36 ( 14%) [   0/1759 (  0%)]  Loss: 4.543 (4.54)  LR: 7.823e-04  Grad: 1.9712  max=0.4762(module.vfe.pfn_layers.0.linear.weight)  min: -0.2675(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.7862, loss_cls=0.2084, loss_bbox=1.0645, matched_ious=0.4765, loss_iou=0.1053, loss_iou_reg=0.2335, d_time=1.46(1.46), f_time=2.54(2.54), b_time=4.00(4.00)  Time cost: 00:03/1:38:50 [2:30:01/52:42:51]  Acc_iter 7037        Data time: 1.46(1.46)  Forward time: 2.54(2.54)  Batch time: 4.00(4.00)
2025-09-04 22:18:31,222   INFO  Train:    5/36 ( 14%) [  13/1759 (  1%)]  Loss: 4.818 (4.72)  LR: 7.839e-04  Grad: 3.0381  max=0.3370(module.vfe.pfn_layers.0.linear.weight)  min: -2.2795(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9356, loss_cls=0.2167, loss_bbox=1.1211, matched_ious=0.4414, loss_iou=0.1061, loss_iou_reg=0.2614, d_time=0.00(0.11), f_time=1.20(1.36), b_time=1.20(1.47)  Time cost: 00:19/41:21 [2:30:17/22:12:55]  Acc_iter 7050        Data time: 0.00(0.11)  Forward time: 1.20(1.36)  Batch time: 1.20(1.47)
2025-09-04 22:19:35,514   INFO  Train:    5/36 ( 14%) [  63/1759 (  4%)]  Loss: 4.335 (4.61)  LR: 7.904e-04  Grad: 7.6285  max=4.5251(module.vfe.pfn_layers.0.linear.weight)  min: -4.5439(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9064, loss_cls=0.2061, loss_bbox=1.0716, matched_ious=0.4545, loss_iou=0.1013, loss_iou_reg=0.2516, d_time=0.00(0.03), f_time=1.35(1.30), b_time=1.35(1.33)  Time cost: 01:24/37:10 [2:31:22/20:32:39]  Acc_iter 7100        Data time: 0.00(0.03)  Forward time: 1.35(1.30)  Batch time: 1.35(1.33)
2025-09-04 22:20:38,343   INFO  Train:    5/36 ( 14%) [ 113/1759 (  6%)]  Loss: 3.918 (4.64)  LR: 7.968e-04  Grad: 5.4848  max=0.3163(module.vfe.pfn_layers.0.linear.weight)  min: -4.9148(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9313, loss_cls=0.2136, loss_bbox=1.0958, matched_ious=0.4450, loss_iou=0.1023, loss_iou_reg=0.2558, d_time=0.00(0.02), f_time=1.29(1.28), b_time=1.29(1.30)  Time cost: 02:27/35:22 [2:32:24/20:07:24]  Acc_iter 7150        Data time: 0.00(0.02)  Forward time: 1.29(1.28)  Batch time: 1.29(1.30)
2025-09-04 22:21:42,325   INFO  Train:    5/36 ( 14%) [ 163/1759 (  9%)]  Loss: 4.478 (4.63)  LR: 8.033e-04  Grad: 6.9314  max=6.5732(module.vfe.pfn_layers.0.linear.weight)  min: -0.9125(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9093, loss_cls=0.2122, loss_bbox=1.0340, matched_ious=0.4512, loss_iou=0.1051, loss_iou_reg=0.2580, d_time=0.01(0.02), f_time=1.13(1.27), b_time=1.13(1.29)  Time cost: 03:30/34:13 [2:33:28/20:03:28]  Acc_iter 7200        Data time: 0.01(0.02)  Forward time: 1.13(1.27)  Batch time: 1.13(1.29)
2025-09-04 22:22:45,104   INFO  Train:    5/36 ( 14%) [ 213/1759 ( 12%)]  Loss: 4.976 (4.62)  LR: 8.099e-04  Grad: 7.8741  max=5.8499(module.vfe.pfn_layers.0.linear.weight)  min: -0.2682(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.9158, loss_cls=0.2073, loss_bbox=1.0603, matched_ious=0.4498, loss_iou=0.1028, loss_iou_reg=0.2537, d_time=0.00(0.02), f_time=1.17(1.27), b_time=1.18(1.28)  Time cost: 04:33/32:57 [2:34:31/19:55:38]  Acc_iter 7250        Data time: 0.00(0.02)  Forward time: 1.17(1.27)  Batch time: 1.18(1.28)
2025-09-04 22:23:51,549   INFO  Train:    5/36 ( 14%) [ 263/1759 ( 15%)]  Loss: 5.399 (4.61)  LR: 8.164e-04  Grad: 10.0000  max=7.5912(module.vfe.pfn_layers.0.linear.weight)  min: -6.2674(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9105, loss_cls=0.2137, loss_bbox=1.0334, matched_ious=0.4477, loss_iou=0.1013, loss_iou_reg=0.2562, d_time=0.00(0.02), f_time=1.23(1.27), b_time=1.23(1.29)  Time cost: 05:40/32:07 [2:35:38/20:03:20]  Acc_iter 7300        Data time: 0.00(0.02)  Forward time: 1.23(1.27)  Batch time: 1.23(1.29)
2025-09-04 22:24:54,459   INFO  Train:    5/36 ( 14%) [ 313/1759 ( 18%)]  Loss: 4.642 (4.61)  LR: 8.231e-04  Grad: 4.0983  max=3.3758(module.vfe.pfn_layers.0.linear.weight)  min: -1.3241(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9157, loss_cls=0.2126, loss_bbox=1.0768, matched_ious=0.4591, loss_iou=0.1020, loss_iou_reg=0.2498, d_time=0.00(0.02), f_time=1.26(1.27), b_time=1.26(1.29)  Time cost: 06:43/30:56 [2:36:41/19:57:44]  Acc_iter 7350        Data time: 0.00(0.02)  Forward time: 1.26(1.27)  Batch time: 1.26(1.29)
2025-09-04 22:25:58,237   INFO  Train:    5/36 ( 14%) [ 363/1759 ( 21%)]  Loss: 4.746 (4.60)  LR: 8.297e-04  Grad: 3.4798  max=1.6446(module.vfe.pfn_layers.0.linear.weight)  min: -2.4488(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9085, loss_cls=0.2085, loss_bbox=1.0711, matched_ious=0.4528, loss_iou=0.1024, loss_iou_reg=0.2542, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.28)  Time cost: 07:46/29:50 [2:37:44/19:55:35]  Acc_iter 7400        Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.28)
2025-09-04 22:27:01,217   INFO  Train:    5/36 ( 14%) [ 413/1759 ( 23%)]  Loss: 4.429 (4.60)  LR: 8.363e-04  Grad: 3.8749  max=3.3109(module.vfe.pfn_layers.0.linear.weight)  min: -0.6399(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9331, loss_cls=0.2164, loss_bbox=1.0327, matched_ious=0.4524, loss_iou=0.1012, loss_iou_reg=0.2546, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.20(1.28)  Time cost: 08:49/28:42 [2:38:47/19:51:55]  Acc_iter 7450        Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.28)
2025-09-04 22:28:05,350   INFO  Train:    5/36 ( 14%) [ 463/1759 ( 26%)]  Loss: 4.769 (4.61)  LR: 8.430e-04  Grad: 4.6336  max=0.1691(module.backbone_3d.cls_conv.3.weight)  min: -3.8476(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9292, loss_cls=0.2115, loss_bbox=1.1621, matched_ious=0.4457, loss_iou=0.1029, loss_iou_reg=0.2567, d_time=0.00(0.01), f_time=1.20(1.27), b_time=1.20(1.28)  Time cost: 09:54/27:39 [2:39:51/19:51:08]  Acc_iter 7500        Data time: 0.00(0.01)  Forward time: 1.20(1.27)  Batch time: 1.20(1.28)
2025-09-04 22:29:08,924   INFO  Train:    5/36 ( 14%) [ 513/1759 ( 29%)]  Loss: 5.124 (4.62)  LR: 8.498e-04  Grad: 2.9447  max=0.6641(module.vfe.pfn_layers.0.linear.weight)  min: -1.8698(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9379, loss_cls=0.2108, loss_bbox=1.1433, matched_ious=0.4429, loss_iou=0.1046, loss_iou_reg=0.2595, d_time=0.00(0.01), f_time=1.35(1.27), b_time=1.36(1.28)  Time cost: 10:57/26:34 [2:40:55/19:49:15]  Acc_iter 7550        Data time: 0.00(0.01)  Forward time: 1.35(1.27)  Batch time: 1.36(1.28)
2025-09-04 22:30:11,714   INFO  Train:    5/36 ( 14%) [ 563/1759 ( 32%)]  Loss: 4.996 (4.61)  LR: 8.565e-04  Grad: 5.0263  max=3.0860(module.vfe.pfn_layers.0.linear.weight)  min: -2.1659(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8933, loss_cls=0.2081, loss_bbox=1.0142, matched_ious=0.4543, loss_iou=0.1011, loss_iou_reg=0.2560, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.24(1.28)  Time cost: 12:00/25:27 [2:41:58/19:46:16]  Acc_iter 7600        Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.28)
2025-09-04 22:31:15,897   INFO  Train:    5/36 ( 14%) [ 613/1759 ( 35%)]  Loss: 4.192 (4.61)  LR: 8.633e-04  Grad: 3.7664  max=1.7384(module.vfe.pfn_layers.0.linear.weight)  min: -2.7171(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9147, loss_cls=0.2074, loss_bbox=1.0501, matched_ious=0.4586, loss_iou=0.1016, loss_iou_reg=0.2505, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.24(1.28)  Time cost: 13:04/24:24 [2:43:02/19:45:41]  Acc_iter 7650        Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.28)
2025-09-04 22:32:18,408   INFO  Train:    5/36 ( 14%) [ 663/1759 ( 38%)]  Loss: 5.272 (4.60)  LR: 8.701e-04  Grad: 5.5781  max=0.2381(module.backbone_3d.cls_conv.3.weight)  min: -3.8223(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9135, loss_cls=0.2138, loss_bbox=1.0151, matched_ious=0.4544, loss_iou=0.1066, loss_iou_reg=0.2552, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.27(1.28)  Time cost: 14:07/23:18 [2:44:04/19:42:42]  Acc_iter 7700        Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.28)
2025-09-04 22:33:22,883   INFO  Train:    5/36 ( 14%) [ 713/1759 ( 41%)]  Loss: 4.364 (4.59)  LR: 8.770e-04  Grad: 5.2442  max=4.8269(module.vfe.pfn_layers.0.linear.weight)  min: -0.1858(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.9053, loss_cls=0.2060, loss_bbox=1.0604, matched_ious=0.4504, loss_iou=0.1027, loss_iou_reg=0.2570, d_time=0.00(0.01), f_time=2.12(1.27), b_time=2.12(1.28)  Time cost: 15:11/22:15 [2:45:09/19:42:31]  Acc_iter 7750        Data time: 0.00(0.01)  Forward time: 2.12(1.27)  Batch time: 2.12(1.28)
2025-09-04 22:34:27,924   INFO  Train:    5/36 ( 14%) [ 763/1759 ( 43%)]  Loss: 4.351 (4.59)  LR: 8.839e-04  Grad: 6.4977  max=0.1119(module.dense_head.prediction_head.dim.1.weight)  min: -4.5890(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8986, loss_cls=0.2017, loss_bbox=1.0829, matched_ious=0.4483, loss_iou=0.1008, loss_iou_reg=0.2549, d_time=0.01(0.01), f_time=1.27(1.27), b_time=1.27(1.28)  Time cost: 16:16/21:13 [2:46:14/19:42:55]  Acc_iter 7800        Data time: 0.01(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.28)
2025-09-04 22:35:31,294   INFO  Train:    5/36 ( 14%) [ 813/1759 ( 46%)]  Loss: 4.611 (4.60)  LR: 8.908e-04  Grad: 3.0463  max=1.9336(module.vfe.pfn_layers.0.linear.weight)  min: -0.8095(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9363, loss_cls=0.2141, loss_bbox=1.1033, matched_ious=0.4391, loss_iou=0.1029, loss_iou_reg=0.2592, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.21(1.28)  Time cost: 17:19/20:08 [2:47:17/19:41:14]  Acc_iter 7850        Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.28)
2025-09-04 22:36:34,689   INFO  Train:    5/36 ( 14%) [ 863/1759 ( 49%)]  Loss: 4.567 (4.59)  LR: 8.977e-04  Grad: 5.6498  max=0.1882(module.backbone_3d.cls_conv.3.bias)  min: -3.6886(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8884, loss_cls=0.1998, loss_bbox=1.0510, matched_ious=0.4547, loss_iou=0.1000, loss_iou_reg=0.2524, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.28)  Time cost: 18:23/19:04 [2:48:21/19:39:39]  Acc_iter 7900        Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.28)
2025-09-04 22:37:38,194   INFO  Train:    5/36 ( 14%) [ 913/1759 ( 52%)]  Loss: 4.678 (4.59)  LR: 9.047e-04  Grad: 7.1001  max=0.1402(module.dense_head.prediction_head.dim.1.bias)  min: -6.0384(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9065, loss_cls=0.2032, loss_bbox=1.0882, matched_ious=0.4541, loss_iou=0.1025, loss_iou_reg=0.2527, d_time=0.01(0.01), f_time=1.42(1.27), b_time=1.42(1.28)  Time cost: 19:26/18:00 [2:49:24/19:38:15]  Acc_iter 7950        Data time: 0.01(0.01)  Forward time: 1.42(1.27)  Batch time: 1.42(1.28)
2025-09-04 22:38:41,464   INFO  Train:    5/36 ( 14%) [ 963/1759 ( 55%)]  Loss: 4.136 (4.58)  LR: 9.117e-04  Grad: 4.5619  max=2.5984(module.vfe.pfn_layers.0.linear.weight)  min: -2.7011(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9075, loss_cls=0.2028, loss_bbox=1.0235, matched_ious=0.4515, loss_iou=0.1005, loss_iou_reg=0.2510, d_time=0.00(0.01), f_time=1.18(1.27), b_time=1.19(1.28)  Time cost: 20:30/16:55 [2:50:28/19:36:38]  Acc_iter 8000        Data time: 0.00(0.01)  Forward time: 1.18(1.27)  Batch time: 1.19(1.28)
2025-09-04 22:39:45,148   INFO  Train:    5/36 ( 14%) [1013/1759 ( 58%)]  Loss: 3.717 (4.58)  LR: 9.187e-04  Grad: 2.9446  max=1.3016(module.vfe.pfn_layers.0.linear.weight)  min: -0.4184(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8958, loss_cls=0.2043, loss_bbox=1.0721, matched_ious=0.4471, loss_iou=0.1022, loss_iou_reg=0.2557, d_time=0.00(0.01), f_time=1.33(1.27), b_time=1.33(1.28)  Time cost: 21:33/15:51 [2:51:31/19:35:28]  Acc_iter 8050        Data time: 0.00(0.01)  Forward time: 1.33(1.27)  Batch time: 1.33(1.28)
2025-09-04 22:40:48,919   INFO  Train:    5/36 ( 14%) [1063/1759 ( 60%)]  Loss: 4.202 (4.57)  LR: 9.257e-04  Grad: 4.8634  max=1.4360(module.vfe.pfn_layers.0.linear.weight)  min: -3.6588(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8818, loss_cls=0.1992, loss_bbox=0.9846, matched_ious=0.4616, loss_iou=0.1018, loss_iou_reg=0.2483, d_time=0.00(0.01), f_time=1.18(1.27), b_time=1.19(1.28)  Time cost: 22:37/14:48 [2:52:35/19:34:23]  Acc_iter 8100        Data time: 0.00(0.01)  Forward time: 1.18(1.27)  Batch time: 1.19(1.28)
2025-09-04 22:41:52,026   INFO  Train:    5/36 ( 14%) [1113/1759 ( 63%)]  Loss: 4.070 (4.56)  LR: 9.328e-04  Grad: 7.0947  max=6.5219(module.vfe.pfn_layers.0.linear.weight)  min: -0.3249(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8821, loss_cls=0.2038, loss_bbox=0.9932, matched_ious=0.4661, loss_iou=0.0987, loss_iou_reg=0.2472, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.24(1.28)  Time cost: 23:40/13:43 [2:53:38/19:32:45]  Acc_iter 8150        Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.28)
2025-09-04 22:42:55,200   INFO  Train:    5/36 ( 14%) [1163/1759 ( 66%)]  Loss: 3.981 (4.56)  LR: 9.399e-04  Grad: 3.8854  max=1.4022(module.vfe.pfn_layers.0.linear.weight)  min: -2.0976(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8846, loss_cls=0.2002, loss_bbox=1.0961, matched_ious=0.4463, loss_iou=0.1032, loss_iou_reg=0.2569, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.28(1.28)  Time cost: 24:43/12:39 [2:54:41/19:31:13]  Acc_iter 8200        Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.28(1.28)
2025-09-04 22:43:59,303   INFO  Train:    5/36 ( 14%) [1213/1759 ( 69%)]  Loss: 4.868 (4.56)  LR: 9.471e-04  Grad: 5.9830  max=1.2607(module.vfe.pfn_layers.0.linear.weight)  min: -4.7966(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8692, loss_cls=0.1968, loss_bbox=1.0026, matched_ious=0.4588, loss_iou=0.1005, loss_iou_reg=0.2531, d_time=0.00(0.01), f_time=2.07(1.27), b_time=2.07(1.28)  Time cost: 25:47/11:36 [2:55:45/19:30:26]  Acc_iter 8250        Data time: 0.00(0.01)  Forward time: 2.07(1.27)  Batch time: 2.07(1.28)
2025-09-04 22:45:04,151   INFO  Train:    5/36 ( 14%) [1263/1759 ( 72%)]  Loss: 4.635 (4.55)  LR: 9.542e-04  Grad: 3.2528  max=0.5045(module.vfe.pfn_layers.0.linear.weight)  min: -1.1560(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8729, loss_cls=0.2000, loss_bbox=1.0096, matched_ious=0.4493, loss_iou=0.1013, loss_iou_reg=0.2550, d_time=0.00(0.01), f_time=1.37(1.27), b_time=1.37(1.28)  Time cost: 26:52/10:32 [2:56:50/19:30:10]  Acc_iter 8300        Data time: 0.00(0.01)  Forward time: 1.37(1.27)  Batch time: 1.37(1.28)
2025-09-04 22:46:06,751   INFO  Train:    5/36 ( 14%) [1313/1759 ( 75%)]  Loss: 4.509 (4.55)  LR: 9.614e-04  Grad: 4.7662  max=3.5472(module.vfe.pfn_layers.0.linear.weight)  min: -1.3403(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9044, loss_cls=0.2021, loss_bbox=1.1052, matched_ious=0.4496, loss_iou=0.1028, loss_iou_reg=0.2557, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.24(1.28)  Time cost: 27:55/09:28 [2:57:53/19:28:16]  Acc_iter 8350        Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.28)
2025-09-04 22:47:09,945   INFO  Train:    5/36 ( 14%) [1363/1759 ( 77%)]  Loss: 4.306 (4.55)  LR: 9.686e-04  Grad: 5.8637  max=5.4506(module.vfe.pfn_layers.0.linear.weight)  min: -0.2343(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8813, loss_cls=0.1974, loss_bbox=1.0430, matched_ious=0.4578, loss_iou=0.1018, loss_iou_reg=0.2509, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.32(1.28)  Time cost: 28:58/08:24 [2:58:56/19:26:49]  Acc_iter 8400        Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.32(1.28)
2025-09-04 22:48:13,764   INFO  Train:    5/36 ( 14%) [1413/1759 ( 80%)]  Loss: 3.740 (4.55)  LR: 9.759e-04  Grad: 5.4651  max=1.3391(module.vfe.pfn_layers.0.linear.weight)  min: -4.6725(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8889, loss_cls=0.2021, loss_bbox=1.0575, matched_ious=0.4546, loss_iou=0.1019, loss_iou_reg=0.2536, d_time=0.05(0.01), f_time=1.64(1.27), b_time=1.68(1.28)  Time cost: 30:02/07:21 [3:00:00/19:25:49]  Acc_iter 8450        Data time: 0.05(0.01)  Forward time: 1.64(1.27)  Batch time: 1.68(1.28)
2025-09-04 22:49:16,473   INFO  Train:    5/36 ( 14%) [1463/1759 ( 83%)]  Loss: 4.227 (4.55)  LR: 9.831e-04  Grad: 5.1616  max=3.8653(module.vfe.pfn_layers.0.linear.weight)  min: -2.4604(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8820, loss_cls=0.1990, loss_bbox=0.9967, matched_ious=0.4641, loss_iou=0.0998, loss_iou_reg=0.2506, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.19(1.27)  Time cost: 31:05/06:17 [3:01:03/19:24:07]  Acc_iter 8500        Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.19(1.27)
2025-09-04 22:50:20,538   INFO  Train:    5/36 ( 14%) [1513/1759 ( 86%)]  Loss: 5.454 (4.55)  LR: 9.904e-04  Grad: 4.0240  max=2.1082(module.vfe.pfn_layers.0.linear.weight)  min: -2.2887(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9079, loss_cls=0.2007, loss_bbox=1.0641, matched_ious=0.4471, loss_iou=0.1023, loss_iou_reg=0.2547, d_time=0.01(0.01), f_time=1.23(1.27), b_time=1.24(1.27)  Time cost: 32:09/05:13 [3:02:07/19:23:16]  Acc_iter 8550        Data time: 0.01(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.27)
2025-09-04 22:51:23,703   INFO  Train:    5/36 ( 14%) [1563/1759 ( 89%)]  Loss: 5.064 (4.55)  LR: 9.977e-04  Grad: 4.3781  max=0.5266(module.vfe.pfn_layers.0.linear.weight)  min: -3.8025(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9118, loss_cls=0.2011, loss_bbox=1.0454, matched_ious=0.4451, loss_iou=0.1000, loss_iou_reg=0.2603, d_time=0.00(0.01), f_time=1.16(1.27), b_time=1.16(1.27)  Time cost: 33:12/04:09 [3:03:10/19:21:54]  Acc_iter 8600        Data time: 0.00(0.01)  Forward time: 1.16(1.27)  Batch time: 1.16(1.27)
2025-09-04 22:52:25,555   INFO  Train:    5/36 ( 14%) [1613/1759 ( 92%)]  Loss: 3.529 (4.55)  LR: 1.005e-03  Grad: 4.3104  max=3.1223(module.vfe.pfn_layers.0.linear.weight)  min: -1.5920(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8759, loss_cls=0.1932, loss_bbox=1.0506, matched_ious=0.4578, loss_iou=0.1001, loss_iou_reg=0.2518, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.27(1.27)  Time cost: 34:14/03:05 [3:04:12/19:19:47]  Acc_iter 8650        Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.27)
2025-09-04 22:53:27,954   INFO  Train:    5/36 ( 14%) [1663/1759 ( 95%)]  Loss: 4.188 (4.54)  LR: 1.012e-03  Grad: 3.5016  max=2.5228(module.vfe.pfn_layers.0.linear.weight)  min: -0.7219(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8813, loss_cls=0.1941, loss_bbox=1.0542, matched_ious=0.4539, loss_iou=0.0988, loss_iou_reg=0.2535, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.26(1.27)  Time cost: 35:16/02:02 [3:05:14/19:18:03]  Acc_iter 8700        Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.27)
2025-09-04 22:54:33,268   INFO  Train:    5/36 ( 14%) [1713/1759 ( 97%)]  Loss: 4.429 (4.54)  LR: 1.020e-03  Grad: 1.7897  max=0.1918(module.vfe.pfn_layers.0.linear.weight)  min: -0.7356(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8893, loss_cls=0.1999, loss_bbox=1.0342, matched_ious=0.4525, loss_iou=0.1033, loss_iou_reg=0.2549, d_time=0.00(0.01), f_time=1.33(1.27), b_time=1.33(1.27)  Time cost: 36:21/00:58 [3:06:19/19:17:54]  Acc_iter 8750        Data time: 0.00(0.01)  Forward time: 1.33(1.27)  Batch time: 1.33(1.27)
2025-09-04 22:55:29,391   INFO  Train:    5/36 ( 14%) [1758/1759 (100%)]  Loss: 5.843 (4.54)  LR: 1.027e-03  Grad: 2.8437  max=1.3071(module.vfe.pfn_layers.0.linear.weight)  min: -0.1590(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8818, loss_cls=0.1927, loss_bbox=1.0045, matched_ious=0.4690, loss_iou=0.0998, loss_iou_reg=0.2459, d_time=0.00(0.01), f_time=0.72(1.27), b_time=0.73(1.27)  Time cost: 37:18/00:01 [3:07:15/19:16:21]  Acc_iter 8795        Data time: 0.00(0.01)  Forward time: 0.72(1.27)  Batch time: 0.73(1.27)

                                               [Aepochs:  14%|█▍        | 5/36 [3:07:16<19:19:23, 2243.99s/it]epochs:  14%|█▍        | 5/36 [3:07:16<19:19:23, 2243.99s/it]epochs:  14%|█▍        | 5/36 [3:07:16<19:19:23, 2244.00s/it]epochs:  14%|█▍        | 5/36 [3:07:16<19:19:23, 2244.00s/it]epochs:  14%|█▍        | 5/36 [3:07:16<19:19:23, 2244.00s/it]epochs:  14%|█▍        | 5/36 [3:07:16<19:19:23, 2244.00s/it]epochs:  14%|█▍        | 5/36 [3:07:16<19:19:23, 2244.00s/it]epochs:  14%|█▍        | 5/36 [3:07:16<19:19:24, 2244.01s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 22:55:34,780   INFO  Train:    6/36 ( 17%) [   0/1759 (  0%)]  Loss: 4.310 (4.31)  LR: 1.027e-03  Grad: 4.1569  max=0.3808(module.backbone_3d.cls_conv.3.weight)  min: -2.9763(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9391, loss_cls=0.2020, loss_bbox=0.8860, matched_ious=0.4667, loss_iou=0.0794, loss_iou_reg=0.2506, d_time=1.42(1.42), f_time=2.71(2.71), b_time=4.14(4.14)  Time cost: 00:03/1:51:30 [3:07:21/57:36:58]  Acc_iter 8796        Data time: 1.42(1.42)  Forward time: 2.71(2.71)  Batch time: 4.14(4.14)
2025-09-04 22:55:39,880   INFO  Train:    6/36 ( 17%) [   4/1759 (  0%)]  Loss: 4.332 (4.41)  LR: 1.027e-03  Grad: 4.6399  max=3.9891(module.vfe.pfn_layers.0.linear.weight)  min: -1.1850(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8717, loss_cls=0.1911, loss_bbox=1.0827, matched_ious=0.4430, loss_iou=0.1036, loss_iou_reg=0.2641, d_time=0.00(0.29), f_time=1.17(1.56), b_time=1.18(1.85)  Time cost: 00:08/52:05 [3:07:26/26:58:14]  Acc_iter 8800        Data time: 0.00(0.29)  Forward time: 1.17(1.56)  Batch time: 1.18(1.85)
2025-09-04 22:56:43,536   INFO  Train:    6/36 ( 17%) [  54/1759 (  3%)]  Loss: 5.339 (4.43)  LR: 1.035e-03  Grad: 2.4121  max=1.2460(module.vfe.pfn_layers.0.linear.weight)  min: -0.5125(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8768, loss_cls=0.1947, loss_bbox=1.0214, matched_ious=0.4535, loss_iou=0.1034, loss_iou_reg=0.2541, d_time=0.00(0.03), f_time=1.21(1.29), b_time=1.22(1.33)  Time cost: 01:12/37:29 [3:08:30/19:57:46]  Acc_iter 8850        Data time: 0.00(0.03)  Forward time: 1.21(1.29)  Batch time: 1.22(1.33)
2025-09-04 22:57:46,983   INFO  Train:    6/36 ( 17%) [ 104/1759 (  6%)]  Loss: 4.799 (4.42)  LR: 1.042e-03  Grad: 2.5693  max=0.1350(module.backbone_3d.cls_conv.3.weight)  min: -1.9593(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8841, loss_cls=0.1943, loss_bbox=1.0505, matched_ious=0.4602, loss_iou=0.1019, loss_iou_reg=0.2523, d_time=0.00(0.02), f_time=1.15(1.28), b_time=1.16(1.30)  Time cost: 02:16/35:43 [3:09:33/19:34:57]  Acc_iter 8900        Data time: 0.00(0.02)  Forward time: 1.15(1.28)  Batch time: 1.16(1.30)
2025-09-04 22:58:50,641   INFO  Train:    6/36 ( 17%) [ 154/1759 (  9%)]  Loss: 4.521 (4.40)  LR: 1.050e-03  Grad: 3.4843  max=2.4296(module.vfe.pfn_layers.0.linear.weight)  min: -0.7849(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8576, loss_cls=0.1890, loss_bbox=0.9979, matched_ious=0.4584, loss_iou=0.1011, loss_iou_reg=0.2528, d_time=0.00(0.01), f_time=1.33(1.28), b_time=1.33(1.29)  Time cost: 03:19/34:27 [3:10:37/19:27:23]  Acc_iter 8950        Data time: 0.00(0.01)  Forward time: 1.33(1.28)  Batch time: 1.33(1.29)
2025-09-04 22:59:55,350   INFO  Train:    6/36 ( 17%) [ 204/1759 ( 12%)]  Loss: 4.195 (4.38)  LR: 1.057e-03  Grad: 5.1830  max=3.8945(module.vfe.pfn_layers.0.linear.weight)  min: -1.9579(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8788, loss_cls=0.1940, loss_bbox=0.9853, matched_ious=0.4617, loss_iou=0.1036, loss_iou_reg=0.2519, d_time=0.00(0.01), f_time=1.25(1.28), b_time=1.26(1.29)  Time cost: 04:24/33:25 [3:11:41/19:27:38]  Acc_iter 9000        Data time: 0.00(0.01)  Forward time: 1.25(1.28)  Batch time: 1.26(1.29)
2025-09-04 23:00:59,048   INFO  Train:    6/36 ( 17%) [ 254/1759 ( 14%)]  Loss: 4.209 (4.39)  LR: 1.065e-03  Grad: 2.9237  max=1.4681(module.vfe.pfn_layers.0.linear.weight)  min: -0.2268(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.8584, loss_cls=0.1918, loss_bbox=1.0180, matched_ious=0.4593, loss_iou=0.1007, loss_iou_reg=0.2516, d_time=0.01(0.01), f_time=1.24(1.28), b_time=1.24(1.29)  Time cost: 05:28/32:16 [3:12:45/19:23:48]  Acc_iter 9050        Data time: 0.01(0.01)  Forward time: 1.24(1.28)  Batch time: 1.24(1.29)
2025-09-04 23:02:01,928   INFO  Train:    6/36 ( 17%) [ 304/1759 ( 17%)]  Loss: 4.760 (4.41)  LR: 1.072e-03  Grad: 3.3476  max=0.8048(module.vfe.pfn_layers.0.linear.weight)  min: -2.7102(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9015, loss_cls=0.1939, loss_bbox=1.0687, matched_ious=0.4592, loss_iou=0.1001, loss_iou_reg=0.2489, d_time=0.00(0.01), f_time=1.20(1.27), b_time=1.21(1.28)  Time cost: 06:30/31:05 [3:13:48/19:18:26]  Acc_iter 9100        Data time: 0.00(0.01)  Forward time: 1.20(1.27)  Batch time: 1.21(1.28)
2025-09-04 23:03:04,523   INFO  Train:    6/36 ( 17%) [ 354/1759 ( 20%)]  Loss: 4.603 (4.41)  LR: 1.080e-03  Grad: 4.7881  max=0.1186(module.dense_head.prediction_head.dim.1.weight)  min: -3.4326(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8753, loss_cls=0.1915, loss_bbox=1.0223, matched_ious=0.4663, loss_iou=0.1009, loss_iou_reg=0.2476, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.28(1.28)  Time cost: 07:33/29:55 [3:14:51/19:13:33]  Acc_iter 9150        Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.28(1.28)
2025-09-04 23:04:08,229   INFO  Train:    6/36 ( 17%) [ 404/1759 ( 23%)]  Loss: 3.821 (4.44)  LR: 1.087e-03  Grad: 2.4914  max=1.4263(module.vfe.pfn_layers.0.linear.weight)  min: -0.1695(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.9193, loss_cls=0.1998, loss_bbox=1.0613, matched_ious=0.4481, loss_iou=0.0977, loss_iou_reg=0.2573, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.24(1.28)  Time cost: 08:37/28:50 [3:15:54/19:12:06]  Acc_iter 9200        Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.28)
2025-09-04 23:05:14,778   INFO  Train:    6/36 ( 17%) [ 454/1759 ( 26%)]  Loss: 4.390 (4.43)  LR: 1.095e-03  Grad: 3.5976  max=2.8860(module.vfe.pfn_layers.0.linear.weight)  min: -1.1022(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8731, loss_cls=0.1907, loss_bbox=1.0119, matched_ious=0.4581, loss_iou=0.1018, loss_iou_reg=0.2513, d_time=0.00(0.01), f_time=1.34(1.27), b_time=1.35(1.28)  Time cost: 09:43/27:54 [3:17:01/19:16:22]  Acc_iter 9250        Data time: 0.00(0.01)  Forward time: 1.34(1.27)  Batch time: 1.35(1.28)
2025-09-04 23:06:18,213   INFO  Train:    6/36 ( 17%) [ 504/1759 ( 29%)]  Loss: 3.874 (4.42)  LR: 1.103e-03  Grad: 3.7495  max=1.6232(module.vfe.pfn_layers.0.linear.weight)  min: -2.5838(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8461, loss_cls=0.1894, loss_bbox=0.9732, matched_ious=0.4655, loss_iou=0.1001, loss_iou_reg=0.2499, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.22(1.28)  Time cost: 10:47/26:48 [3:18:04/19:14:01]  Acc_iter 9300        Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.22(1.28)
2025-09-04 23:07:20,724   INFO  Train:    6/36 ( 17%) [ 554/1759 ( 31%)]  Loss: 4.830 (4.43)  LR: 1.110e-03  Grad: 1.7427  max=0.4258(module.vfe.pfn_layers.0.linear.weight)  min: -0.8907(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9037, loss_cls=0.1960, loss_bbox=1.0515, matched_ious=0.4522, loss_iou=0.1017, loss_iou_reg=0.2539, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.30(1.28)  Time cost: 11:49/25:40 [3:19:07/19:10:24]  Acc_iter 9350        Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.30(1.28)
2025-09-04 23:08:23,679   INFO  Train:    6/36 ( 17%) [ 604/1759 ( 34%)]  Loss: 4.486 (4.43)  LR: 1.118e-03  Grad: 2.7706  max=2.0885(module.vfe.pfn_layers.0.linear.weight)  min: -0.1884(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9015, loss_cls=0.1961, loss_bbox=1.0385, matched_ious=0.4515, loss_iou=0.1022, loss_iou_reg=0.2538, d_time=0.00(0.01), f_time=1.17(1.27), b_time=1.18(1.28)  Time cost: 12:52/24:35 [3:20:10/19:07:52]  Acc_iter 9400        Data time: 0.00(0.01)  Forward time: 1.17(1.27)  Batch time: 1.18(1.28)
2025-09-04 23:09:26,836   INFO  Train:    6/36 ( 17%) [ 654/1759 ( 37%)]  Loss: 4.111 (4.43)  LR: 1.126e-03  Grad: 3.8794  max=2.5825(module.vfe.pfn_layers.0.linear.weight)  min: -2.0088(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8940, loss_cls=0.1922, loss_bbox=1.0239, matched_ious=0.4593, loss_iou=0.1014, loss_iou_reg=0.2497, d_time=0.01(0.01), f_time=1.25(1.27), b_time=1.26(1.28)  Time cost: 13:55/23:30 [3:21:13/19:05:51]  Acc_iter 9450        Data time: 0.01(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.28)
2025-09-04 23:10:30,576   INFO  Train:    6/36 ( 17%) [ 704/1759 ( 40%)]  Loss: 4.504 (4.43)  LR: 1.133e-03  Grad: 7.0018  max=1.2717(module.vfe.pfn_layers.0.linear.weight)  min: -6.6332(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8922, loss_cls=0.1985, loss_bbox=1.0111, matched_ious=0.4519, loss_iou=0.1025, loss_iou_reg=0.2539, d_time=0.00(0.01), f_time=1.33(1.27), b_time=1.33(1.28)  Time cost: 14:59/22:26 [3:22:17/19:04:42]  Acc_iter 9500        Data time: 0.00(0.01)  Forward time: 1.33(1.27)  Batch time: 1.33(1.28)
2025-09-04 23:11:34,076   INFO  Train:    6/36 ( 17%) [ 754/1759 ( 43%)]  Loss: 4.709 (4.43)  LR: 1.141e-03  Grad: 2.3852  max=0.6508(module.vfe.pfn_layers.0.linear.weight)  min: -1.4561(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8821, loss_cls=0.1927, loss_bbox=1.0162, matched_ious=0.4607, loss_iou=0.1023, loss_iou_reg=0.2486, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.21(1.28)  Time cost: 16:03/21:22 [3:23:20/19:03:16]  Acc_iter 9550        Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.28)
2025-09-04 23:12:38,069   INFO  Train:    6/36 ( 17%) [ 804/1759 ( 46%)]  Loss: 4.587 (4.42)  LR: 1.149e-03  Grad: 3.0909  max=1.8090(module.vfe.pfn_layers.0.linear.weight)  min: -0.2979(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.8787, loss_cls=0.1919, loss_bbox=0.9234, matched_ious=0.4681, loss_iou=0.0998, loss_iou_reg=0.2486, d_time=0.00(0.01), f_time=1.36(1.27), b_time=1.36(1.28)  Time cost: 17:07/20:18 [3:24:24/19:02:27]  Acc_iter 9600        Data time: 0.00(0.01)  Forward time: 1.36(1.27)  Batch time: 1.36(1.28)
2025-09-04 23:13:40,778   INFO  Train:    6/36 ( 17%) [ 854/1759 ( 49%)]  Loss: 3.788 (4.42)  LR: 1.157e-03  Grad: 2.4016  max=0.2579(module.backbone_3d.cls_conv.3.weight)  min: -1.2952(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8607, loss_cls=0.1821, loss_bbox=1.0226, matched_ious=0.4651, loss_iou=0.1013, loss_iou_reg=0.2476, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.23(1.28)  Time cost: 18:09/19:13 [3:25:27/19:00:15]  Acc_iter 9650        Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.23(1.28)
2025-09-04 23:14:43,614   INFO  Train:    6/36 ( 17%) [ 904/1759 ( 51%)]  Loss: 4.305 (4.41)  LR: 1.165e-03  Grad: 5.2455  max=2.4297(module.vfe.pfn_layers.0.linear.weight)  min: -4.1326(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8858, loss_cls=0.1891, loss_bbox=1.0273, matched_ious=0.4656, loss_iou=0.1012, loss_iou_reg=0.2475, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.24(1.27)  Time cost: 19:12/18:08 [3:26:30/18:58:18]  Acc_iter 9700        Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.27)
2025-09-04 23:15:49,378   INFO  Train:    6/36 ( 17%) [ 954/1759 ( 54%)]  Loss: 4.212 (4.41)  LR: 1.172e-03  Grad: 3.1641  max=1.8344(module.vfe.pfn_layers.0.linear.weight)  min: -0.3243(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.8536, loss_cls=0.1825, loss_bbox=0.9518, matched_ious=0.4583, loss_iou=0.0996, loss_iou_reg=0.2531, d_time=0.01(0.01), f_time=1.27(1.27), b_time=1.28(1.28)  Time cost: 20:18/17:07 [3:27:35/18:59:11]  Acc_iter 9750        Data time: 0.01(0.01)  Forward time: 1.27(1.27)  Batch time: 1.28(1.28)
2025-09-04 23:16:52,541   INFO  Train:    6/36 ( 17%) [1004/1759 ( 57%)]  Loss: 3.885 (4.40)  LR: 1.180e-03  Grad: 2.5977  max=1.9048(module.vfe.pfn_layers.0.linear.weight)  min: -0.3316(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.8651, loss_cls=0.1861, loss_bbox=0.9661, matched_ious=0.4611, loss_iou=0.1006, loss_iou_reg=0.2531, d_time=0.00(0.01), f_time=1.20(1.27), b_time=1.20(1.28)  Time cost: 21:21/16:02 [3:28:39/18:57:34]  Acc_iter 9800        Data time: 0.00(0.01)  Forward time: 1.20(1.27)  Batch time: 1.20(1.28)
2025-09-04 23:17:55,716   INFO  Train:    6/36 ( 17%) [1054/1759 ( 60%)]  Loss: 4.600 (4.39)  LR: 1.188e-03  Grad: 3.9757  max=3.4145(module.vfe.pfn_layers.0.linear.weight)  min: -0.3829(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8391, loss_cls=0.1820, loss_bbox=0.9451, matched_ious=0.4697, loss_iou=0.0982, loss_iou_reg=0.2458, d_time=0.00(0.01), f_time=1.33(1.27), b_time=1.33(1.27)  Time cost: 22:24/14:58 [3:29:42/18:56:00]  Acc_iter 9850        Data time: 0.00(0.01)  Forward time: 1.33(1.27)  Batch time: 1.33(1.27)
2025-09-04 23:18:58,788   INFO  Train:    6/36 ( 17%) [1104/1759 ( 63%)]  Loss: 4.030 (4.39)  LR: 1.196e-03  Grad: 2.5812  max=0.1357(module.dense_head.prediction_head.height.1.bias)  min: -1.5847(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8573, loss_cls=0.1882, loss_bbox=0.9519, matched_ious=0.4683, loss_iou=0.1036, loss_iou_reg=0.2496, d_time=0.01(0.01), f_time=1.27(1.27), b_time=1.28(1.27)  Time cost: 23:27/13:54 [3:30:45/18:54:25]  Acc_iter 9900        Data time: 0.01(0.01)  Forward time: 1.27(1.27)  Batch time: 1.28(1.27)
2025-09-04 23:20:03,043   INFO  Train:    6/36 ( 17%) [1154/1759 ( 66%)]  Loss: 4.571 (4.39)  LR: 1.204e-03  Grad: 2.5295  max=0.1212(module.backbone_3d.cls_conv.3.weight)  min: -1.5294(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8532, loss_cls=0.1864, loss_bbox=0.9750, matched_ious=0.4540, loss_iou=0.1034, loss_iou_reg=0.2545, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.26(1.27)  Time cost: 24:32/12:51 [3:31:49/18:53:47]  Acc_iter 9950        Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.27)
2025-09-04 23:21:06,559   INFO  Train:    6/36 ( 17%) [1204/1759 ( 68%)]  Loss: 4.953 (4.38)  LR: 1.212e-03  Grad: 4.2189  max=1.8112(module.vfe.pfn_layers.0.linear.weight)  min: -2.8496(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8427, loss_cls=0.1834, loss_bbox=0.9564, matched_ious=0.4677, loss_iou=0.1001, loss_iou_reg=0.2483, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.27)  Time cost: 25:35/11:47 [3:32:53/18:52:34]  Acc_iter 10000       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.27)
2025-09-04 23:22:09,890   INFO  Train:    6/36 ( 17%) [1254/1759 ( 71%)]  Loss: 4.705 (4.37)  LR: 1.220e-03  Grad: 3.3031  max=0.3218(module.vfe.pfn_layers.0.linear.weight)  min: -2.1695(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8340, loss_cls=0.1863, loss_bbox=0.9139, matched_ious=0.4702, loss_iou=0.0979, loss_iou_reg=0.2489, d_time=0.00(0.01), f_time=1.16(1.27), b_time=1.17(1.27)  Time cost: 26:38/10:43 [3:33:56/18:51:14]  Acc_iter 10050       Data time: 0.00(0.01)  Forward time: 1.16(1.27)  Batch time: 1.17(1.27)
2025-09-04 23:23:13,391   INFO  Train:    6/36 ( 17%) [1304/1759 ( 74%)]  Loss: 4.892 (4.37)  LR: 1.228e-03  Grad: 3.2710  max=1.8727(module.vfe.pfn_layers.0.linear.weight)  min: -1.0686(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8297, loss_cls=0.1801, loss_bbox=0.9719, matched_ious=0.4694, loss_iou=0.1000, loss_iou_reg=0.2478, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.31(1.27)  Time cost: 27:42/09:39 [3:34:59/18:50:02]  Acc_iter 10100       Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.31(1.27)
2025-09-04 23:24:16,967   INFO  Train:    6/36 ( 17%) [1354/1759 ( 77%)]  Loss: 4.633 (4.37)  LR: 1.236e-03  Grad: 2.9896  max=0.2377(module.dense_head.prediction_head.height.1.weight)  min: -2.1107(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8550, loss_cls=0.1843, loss_bbox=0.9796, matched_ious=0.4693, loss_iou=0.1017, loss_iou_reg=0.2449, d_time=0.00(0.01), f_time=1.38(1.27), b_time=1.38(1.27)  Time cost: 28:45/08:35 [3:36:03/18:48:54]  Acc_iter 10150       Data time: 0.00(0.01)  Forward time: 1.38(1.27)  Batch time: 1.38(1.27)
2025-09-04 23:25:20,022   INFO  Train:    6/36 ( 17%) [1404/1759 ( 80%)]  Loss: 3.834 (4.37)  LR: 1.244e-03  Grad: 2.8168  max=0.5154(module.vfe.pfn_layers.0.linear.weight)  min: -1.5008(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8638, loss_cls=0.1871, loss_bbox=0.9825, matched_ious=0.4576, loss_iou=0.1012, loss_iou_reg=0.2511, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.27(1.27)  Time cost: 29:49/07:32 [3:37:06/18:47:26]  Acc_iter 10200       Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.27)
2025-09-04 23:26:24,818   INFO  Train:    6/36 ( 17%) [1454/1759 ( 83%)]  Loss: 4.889 (4.36)  LR: 1.252e-03  Grad: 3.6657  max=1.6840(module.vfe.pfn_layers.0.linear.weight)  min: -2.1526(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8501, loss_cls=0.1843, loss_bbox=0.9333, matched_ious=0.4651, loss_iou=0.1007, loss_iou_reg=0.2510, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.28(1.27)  Time cost: 30:53/06:28 [3:38:11/18:47:03]  Acc_iter 10250       Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.28(1.27)
2025-09-04 23:27:28,520   INFO  Train:    6/36 ( 17%) [1504/1759 ( 86%)]  Loss: 3.524 (4.37)  LR: 1.260e-03  Grad: 2.5349  max=0.6495(module.vfe.pfn_layers.0.linear.weight)  min: -0.2180(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.8604, loss_cls=0.1833, loss_bbox=1.0048, matched_ious=0.4627, loss_iou=0.1001, loss_iou_reg=0.2506, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.26(1.27)  Time cost: 31:57/05:24 [3:39:15/18:45:59]  Acc_iter 10300       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.27)
2025-09-04 23:28:31,970   INFO  Train:    6/36 ( 17%) [1554/1759 ( 88%)]  Loss: 4.133 (4.36)  LR: 1.268e-03  Grad: 3.5246  max=2.2449(module.vfe.pfn_layers.0.linear.weight)  min: -0.1333(module.dense_head.prediction_head.iou.1.weight)  NaN: False  loss_hm=0.8087, loss_cls=0.1737, loss_bbox=0.9013, matched_ious=0.4734, loss_iou=0.1010, loss_iou_reg=0.2461, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.27)  Time cost: 33:00/04:21 [3:40:18/18:44:47]  Acc_iter 10350       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.27)
2025-09-04 23:29:35,405   INFO  Train:    6/36 ( 17%) [1604/1759 ( 91%)]  Loss: 4.429 (4.36)  LR: 1.276e-03  Grad: 2.8737  max=0.2023(module.backbone_3d.cls_conv.3.weight)  min: -0.7517(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8593, loss_cls=0.1842, loss_bbox=0.9949, matched_ious=0.4541, loss_iou=0.1024, loss_iou_reg=0.2534, d_time=0.00(0.01), f_time=1.20(1.27), b_time=1.21(1.27)  Time cost: 34:04/03:17 [3:41:21/18:43:35]  Acc_iter 10400       Data time: 0.00(0.01)  Forward time: 1.20(1.27)  Batch time: 1.21(1.27)
2025-09-04 23:30:39,130   INFO  Train:    6/36 ( 17%) [1654/1759 ( 94%)]  Loss: 4.435 (4.35)  LR: 1.284e-03  Grad: 3.2899  max=1.1416(module.vfe.pfn_layers.0.linear.weight)  min: -0.1544(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8329, loss_cls=0.1857, loss_bbox=0.9717, matched_ious=0.4573, loss_iou=0.1015, loss_iou_reg=0.2524, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.31(1.27)  Time cost: 35:08/02:13 [3:42:25/18:42:32]  Acc_iter 10450       Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.31(1.27)
2025-09-04 23:31:42,760   INFO  Train:    6/36 ( 17%) [1704/1759 ( 97%)]  Loss: 4.889 (4.35)  LR: 1.292e-03  Grad: 5.0732  max=3.6690(module.vfe.pfn_layers.0.linear.weight)  min: -0.1670(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8518, loss_cls=0.1809, loss_bbox=0.9936, matched_ious=0.4620, loss_iou=0.1016, loss_iou_reg=0.2493, d_time=0.00(0.01), f_time=1.32(1.27), b_time=1.32(1.27)  Time cost: 36:11/01:10 [3:43:29/18:41:27]  Acc_iter 10500       Data time: 0.00(0.01)  Forward time: 1.32(1.27)  Batch time: 1.32(1.27)
2025-09-04 23:32:45,194   INFO  Train:    6/36 ( 17%) [1754/1759 (100%)]  Loss: 4.066 (4.35)  LR: 1.300e-03  Grad: 6.8773  max=2.2653(module.vfe.pfn_layers.0.linear.weight)  min: -5.5046(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8291, loss_cls=0.1769, loss_bbox=0.9952, matched_ious=0.4722, loss_iou=0.0996, loss_iou_reg=0.2445, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.27)  Time cost: 37:14/00:06 [3:44:31/18:39:45]  Acc_iter 10550       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.27)
2025-09-04 23:32:49,919   INFO  Train:    6/36 ( 17%) [1758/1759 (100%)]  Loss: 3.889 (4.35)  LR: 1.300e-03  Grad: 5.7446  max=2.4498(module.vfe.pfn_layers.0.linear.weight)  min: -2.8936(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8655, loss_cls=0.1853, loss_bbox=0.9415, matched_ious=0.4740, loss_iou=0.1039, loss_iou_reg=0.2472, d_time=0.00(0.01), f_time=0.76(1.27), b_time=0.77(1.27)  Time cost: 37:18/00:01 [3:44:36/18:39:29]  Acc_iter 10554       Data time: 0.00(0.01)  Forward time: 0.76(1.27)  Batch time: 0.77(1.27)

                                               [Aepochs:  17%|█▋        | 6/36 [3:44:36<18:41:24, 2242.81s/it]epochs:  17%|█▋        | 6/36 [3:44:36<18:41:24, 2242.81s/it]epochs:  17%|█▋        | 6/36 [3:44:36<18:41:24, 2242.82s/it]epochs:  17%|█▋        | 6/36 [3:44:36<18:41:24, 2242.81s/it]epochs:  17%|█▋        | 6/36 [3:44:36<18:41:24, 2242.80s/it]epochs:  17%|█▋        | 6/36 [3:44:36<18:41:24, 2242.80s/it]epochs:  17%|█▋        | 6/36 [3:44:36<18:41:24, 2242.81s/it]epochs:  17%|█▋        | 6/36 [3:44:37<18:41:24, 2242.83s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 23:32:55,392   INFO  Train:    7/36 ( 19%) [   0/1759 (  0%)]  Loss: 3.635 (3.64)  LR: 1.301e-03  Grad: 3.6383  max=0.1771(module.dense_head.prediction_head.height.1.weight)  min: -1.3471(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7111, loss_cls=0.1923, loss_bbox=0.7806, matched_ious=0.4938, loss_iou=0.1007, loss_iou_reg=0.2418, d_time=1.51(1.51), f_time=2.69(2.69), b_time=4.20(4.20)  Time cost: 00:03/1:44:33 [3:44:41/52:16:56]  Acc_iter 10555       Data time: 1.51(1.51)  Forward time: 2.69(2.69)  Batch time: 4.20(4.20)
2025-09-04 23:33:52,590   INFO  Train:    7/36 ( 19%) [  45/1759 (  3%)]  Loss: 4.131 (4.30)  LR: 1.308e-03  Grad: 3.5910  max=0.5335(module.vfe.pfn_layers.0.linear.weight)  min: -0.4852(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8616, loss_cls=0.1850, loss_bbox=0.9875, matched_ious=0.4631, loss_iou=0.1012, loss_iou_reg=0.2480, d_time=0.00(0.04), f_time=1.30(1.30), b_time=1.31(1.33)  Time cost: 01:00/37:44 [3:45:39/19:20:47]  Acc_iter 10600       Data time: 0.00(0.04)  Forward time: 1.30(1.30)  Batch time: 1.31(1.33)
2025-09-04 23:34:56,035   INFO  Train:    7/36 ( 19%) [  95/1759 (  5%)]  Loss: 4.488 (4.36)  LR: 1.316e-03  Grad: 7.3375  max=4.0987(module.vfe.pfn_layers.0.linear.weight)  min: -4.2016(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8581, loss_cls=0.1811, loss_bbox=1.0694, matched_ious=0.4572, loss_iou=0.1032, loss_iou_reg=0.2509, d_time=0.00(0.02), f_time=1.20(1.28), b_time=1.21(1.30)  Time cost: 02:04/35:52 [3:46:42/18:55:53]  Acc_iter 10650       Data time: 0.00(0.02)  Forward time: 1.20(1.28)  Batch time: 1.21(1.30)
2025-09-04 23:35:59,734   INFO  Train:    7/36 ( 19%) [ 145/1759 (  8%)]  Loss: 3.725 (4.34)  LR: 1.324e-03  Grad: 4.3208  max=1.5731(module.vfe.pfn_layers.0.linear.weight)  min: -0.8915(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8289, loss_cls=0.1782, loss_bbox=0.9960, matched_ious=0.4618, loss_iou=0.1010, loss_iou_reg=0.2516, d_time=0.02(0.02), f_time=1.27(1.28), b_time=1.29(1.29)  Time cost: 03:07/34:37 [3:47:46/18:48:50]  Acc_iter 10700       Data time: 0.02(0.02)  Forward time: 1.27(1.28)  Batch time: 1.29(1.29)
2025-09-04 23:37:06,122   INFO  Train:    7/36 ( 19%) [ 195/1759 ( 11%)]  Loss: 3.538 (4.31)  LR: 1.332e-03  Grad: 4.7886  max=2.7521(module.vfe.pfn_layers.0.linear.weight)  min: -1.0305(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8380, loss_cls=0.1793, loss_bbox=0.9558, matched_ious=0.4686, loss_iou=0.1012, loss_iou_reg=0.2492, d_time=0.00(0.02), f_time=1.28(1.28), b_time=1.29(1.30)  Time cost: 04:14/33:49 [3:48:52/18:56:52]  Acc_iter 10750       Data time: 0.00(0.02)  Forward time: 1.28(1.28)  Batch time: 1.29(1.30)
2025-09-04 23:38:09,567   INFO  Train:    7/36 ( 19%) [ 245/1759 ( 14%)]  Loss: 5.199 (4.31)  LR: 1.340e-03  Grad: 4.2457  max=1.4688(module.vfe.pfn_layers.0.linear.weight)  min: -0.6055(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8442, loss_cls=0.1803, loss_bbox=1.0131, matched_ious=0.4709, loss_iou=0.0969, loss_iou_reg=0.2454, d_time=0.00(0.01), f_time=1.24(1.28), b_time=1.25(1.29)  Time cost: 05:17/32:35 [3:49:56/18:50:42]  Acc_iter 10800       Data time: 0.00(0.01)  Forward time: 1.24(1.28)  Batch time: 1.25(1.29)
2025-09-04 23:39:14,446   INFO  Train:    7/36 ( 19%) [ 295/1759 ( 17%)]  Loss: 4.340 (4.29)  LR: 1.349e-03  Grad: 3.9832  max=1.4139(module.vfe.pfn_layers.0.linear.weight)  min: -0.2325(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8280, loss_cls=0.1813, loss_bbox=0.9184, matched_ious=0.4625, loss_iou=0.1015, loss_iou_reg=0.2517, d_time=0.00(0.02), f_time=1.25(1.28), b_time=1.25(1.29)  Time cost: 06:22/31:32 [3:51:01/18:50:31]  Acc_iter 10850       Data time: 0.00(0.02)  Forward time: 1.25(1.28)  Batch time: 1.25(1.29)
2025-09-04 23:40:18,239   INFO  Train:    7/36 ( 19%) [ 345/1759 ( 20%)]  Loss: 4.284 (4.30)  LR: 1.357e-03  Grad: 4.3906  max=2.6197(module.vfe.pfn_layers.0.linear.weight)  min: -0.2575(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8713, loss_cls=0.1858, loss_bbox=1.0233, matched_ious=0.4568, loss_iou=0.1007, loss_iou_reg=0.2500, d_time=0.00(0.01), f_time=1.31(1.28), b_time=1.31(1.29)  Time cost: 07:26/30:24 [3:52:04/18:47:19]  Acc_iter 10900       Data time: 0.00(0.01)  Forward time: 1.31(1.28)  Batch time: 1.31(1.29)
2025-09-04 23:41:21,343   INFO  Train:    7/36 ( 19%) [ 395/1759 ( 22%)]  Loss: 4.284 (4.29)  LR: 1.365e-03  Grad: 4.3695  max=2.1322(module.vfe.pfn_layers.0.linear.weight)  min: -0.1407(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8457, loss_cls=0.1819, loss_bbox=0.9512, matched_ious=0.4661, loss_iou=0.1010, loss_iou_reg=0.2468, d_time=0.00(0.01), f_time=1.30(1.28), b_time=1.30(1.29)  Time cost: 08:29/29:15 [3:53:07/18:43:08]  Acc_iter 10950       Data time: 0.00(0.01)  Forward time: 1.30(1.28)  Batch time: 1.30(1.29)
2025-09-04 23:42:23,636   INFO  Train:    7/36 ( 19%) [ 445/1759 ( 25%)]  Loss: 4.963 (4.29)  LR: 1.373e-03  Grad: 2.4927  max=1.3840(module.vfe.pfn_layers.0.linear.weight)  min: -0.8195(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8498, loss_cls=0.1804, loss_bbox=0.9828, matched_ious=0.4726, loss_iou=0.0999, loss_iou_reg=0.2447, d_time=0.00(0.01), f_time=1.16(1.27), b_time=1.17(1.28)  Time cost: 09:31/28:04 [3:54:10/18:38:05]  Acc_iter 11000       Data time: 0.00(0.01)  Forward time: 1.16(1.27)  Batch time: 1.17(1.28)
2025-09-04 23:43:27,223   INFO  Train:    7/36 ( 19%) [ 495/1759 ( 28%)]  Loss: 4.328 (4.29)  LR: 1.381e-03  Grad: 3.2195  max=2.6782(module.vfe.pfn_layers.0.linear.weight)  min: -0.9393(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8366, loss_cls=0.1806, loss_bbox=0.9563, matched_ious=0.4671, loss_iou=0.1019, loss_iou_reg=0.2469, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.28)  Time cost: 10:35/26:59 [3:55:13/18:36:05]  Acc_iter 11050       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.28)
2025-09-04 23:44:30,060   INFO  Train:    7/36 ( 19%) [ 545/1759 ( 31%)]  Loss: 4.269 (4.27)  LR: 1.390e-03  Grad: 1.8599  max=0.2821(module.vfe.pfn_layers.0.linear.weight)  min: -0.1007(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8213, loss_cls=0.1773, loss_bbox=0.9139, matched_ious=0.4709, loss_iou=0.1021, loss_iou_reg=0.2485, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.28)  Time cost: 11:38/25:52 [3:56:16/18:33:06]  Acc_iter 11100       Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.28)
2025-09-04 23:45:32,613   INFO  Train:    7/36 ( 19%) [ 595/1759 ( 34%)]  Loss: 4.048 (4.26)  LR: 1.398e-03  Grad: 3.4350  max=2.2586(module.vfe.pfn_layers.0.linear.weight)  min: -1.5350(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8125, loss_cls=0.1775, loss_bbox=0.9407, matched_ious=0.4675, loss_iou=0.0999, loss_iou_reg=0.2487, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.24(1.28)  Time cost: 12:40/24:45 [3:57:19/18:30:00]  Acc_iter 11150       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.28)
2025-09-04 23:46:35,606   INFO  Train:    7/36 ( 19%) [ 645/1759 ( 37%)]  Loss: 4.362 (4.25)  LR: 1.406e-03  Grad: 2.2602  max=0.5177(module.vfe.pfn_layers.0.linear.weight)  min: -0.4004(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8283, loss_cls=0.1762, loss_bbox=0.9285, matched_ious=0.4757, loss_iou=0.0983, loss_iou_reg=0.2446, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.28(1.28)  Time cost: 13:43/23:40 [3:58:22/18:27:49]  Acc_iter 11200       Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.28(1.28)
2025-09-04 23:47:41,734   INFO  Train:    7/36 ( 19%) [ 695/1759 ( 40%)]  Loss: 4.336 (4.25)  LR: 1.414e-03  Grad: 3.2112  max=2.1324(module.vfe.pfn_layers.0.linear.weight)  min: -0.2237(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8261, loss_cls=0.1793, loss_bbox=0.9560, matched_ious=0.4685, loss_iou=0.0999, loss_iou_reg=0.2490, d_time=0.00(0.01), f_time=1.36(1.27), b_time=1.36(1.28)  Time cost: 14:49/22:40 [3:59:28/18:29:43]  Acc_iter 11250       Data time: 0.00(0.01)  Forward time: 1.36(1.27)  Batch time: 1.36(1.28)
2025-09-04 23:48:45,774   INFO  Train:    7/36 ( 19%) [ 745/1759 ( 42%)]  Loss: 4.430 (4.25)  LR: 1.422e-03  Grad: 2.8997  max=1.2729(module.vfe.pfn_layers.0.linear.weight)  min: -0.1330(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.8615, loss_cls=0.1841, loss_bbox=1.0013, matched_ious=0.4676, loss_iou=0.0997, loss_iou_reg=0.2490, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.28)  Time cost: 15:53/21:36 [4:00:32/18:28:47]  Acc_iter 11300       Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.28)
2025-09-04 23:49:49,502   INFO  Train:    7/36 ( 19%) [ 795/1759 ( 45%)]  Loss: 3.862 (4.26)  LR: 1.431e-03  Grad: 7.1405  max=5.4318(module.vfe.pfn_layers.0.linear.weight)  min: -3.3598(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8566, loss_cls=0.1782, loss_bbox=1.0123, matched_ious=0.4659, loss_iou=0.1001, loss_iou_reg=0.2458, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.27(1.28)  Time cost: 16:57/20:32 [4:01:36/18:27:29]  Acc_iter 11350       Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.28)
2025-09-04 23:50:52,225   INFO  Train:    7/36 ( 19%) [ 845/1759 ( 48%)]  Loss: 4.858 (4.26)  LR: 1.439e-03  Grad: 2.7489  max=1.2336(module.vfe.pfn_layers.0.linear.weight)  min: -0.5221(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8457, loss_cls=0.1775, loss_bbox=0.9769, matched_ious=0.4592, loss_iou=0.1029, loss_iou_reg=0.2533, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.27(1.28)  Time cost: 18:00/19:27 [4:02:38/18:25:11]  Acc_iter 11400       Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.28)
2025-09-04 23:51:55,019   INFO  Train:    7/36 ( 19%) [ 895/1759 ( 51%)]  Loss: 5.309 (4.26)  LR: 1.447e-03  Grad: 2.8829  max=0.8636(module.vfe.pfn_layers.0.linear.weight)  min: -1.5093(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8538, loss_cls=0.1788, loss_bbox=0.9830, matched_ious=0.4699, loss_iou=0.0993, loss_iou_reg=0.2469, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.30(1.28)  Time cost: 19:03/18:22 [4:03:41/18:23:06]  Acc_iter 11450       Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.30(1.28)
2025-09-04 23:52:58,048   INFO  Train:    7/36 ( 19%) [ 945/1759 ( 54%)]  Loss: 4.563 (4.26)  LR: 1.456e-03  Grad: 4.1075  max=3.0208(module.vfe.pfn_layers.0.linear.weight)  min: -0.5636(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8346, loss_cls=0.1748, loss_bbox=0.9454, matched_ious=0.4660, loss_iou=0.1011, loss_iou_reg=0.2468, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.28)  Time cost: 20:06/17:17 [4:04:44/18:21:20]  Acc_iter 11500       Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.28)
2025-09-04 23:54:00,551   INFO  Train:    7/36 ( 19%) [ 995/1759 ( 57%)]  Loss: 3.068 (4.26)  LR: 1.464e-03  Grad: 2.3532  max=0.9310(module.vfe.pfn_layers.0.linear.weight)  min: -1.2169(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8312, loss_cls=0.1791, loss_bbox=0.9583, matched_ious=0.4655, loss_iou=0.0991, loss_iou_reg=0.2493, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.27)  Time cost: 21:08/16:13 [4:05:47/18:19:12]  Acc_iter 11550       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.27)
2025-09-04 23:55:03,528   INFO  Train:    7/36 ( 19%) [1045/1759 ( 59%)]  Loss: 4.558 (4.26)  LR: 1.472e-03  Grad: 2.9425  max=1.2009(module.vfe.pfn_layers.0.linear.weight)  min: -1.3988(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8840, loss_cls=0.1847, loss_bbox=0.9639, matched_ious=0.4579, loss_iou=0.1005, loss_iou_reg=0.2521, d_time=0.00(0.01), f_time=1.38(1.27), b_time=1.39(1.27)  Time cost: 22:11/15:09 [4:06:50/18:17:33]  Acc_iter 11600       Data time: 0.00(0.01)  Forward time: 1.38(1.27)  Batch time: 1.39(1.27)
2025-09-04 23:56:06,277   INFO  Train:    7/36 ( 19%) [1095/1759 ( 62%)]  Loss: 4.080 (4.25)  LR: 1.480e-03  Grad: 4.2005  max=0.8859(module.vfe.pfn_layers.0.linear.weight)  min: -3.5371(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8156, loss_cls=0.1776, loss_bbox=0.8818, matched_ious=0.4746, loss_iou=0.0993, loss_iou_reg=0.2448, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.20(1.27)  Time cost: 23:14/14:04 [4:07:52/18:15:46]  Acc_iter 11650       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.20(1.27)
2025-09-04 23:57:09,167   INFO  Train:    7/36 ( 19%) [1145/1759 ( 65%)]  Loss: 4.448 (4.25)  LR: 1.489e-03  Grad: 2.7850  max=1.5687(module.vfe.pfn_layers.0.linear.weight)  min: -0.0976(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.8149, loss_cls=0.1738, loss_bbox=0.9907, matched_ious=0.4603, loss_iou=0.1012, loss_iou_reg=0.2517, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.21(1.27)  Time cost: 24:17/13:00 [4:08:55/18:14:10]  Acc_iter 11700       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.27)
2025-09-04 23:58:15,373   INFO  Train:    7/36 ( 19%) [1195/1759 ( 68%)]  Loss: 3.862 (4.25)  LR: 1.497e-03  Grad: 2.7135  max=1.4455(module.vfe.pfn_layers.0.linear.weight)  min: -0.2364(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8290, loss_cls=0.1745, loss_bbox=0.9657, matched_ious=0.4740, loss_iou=0.1002, loss_iou_reg=0.2452, d_time=0.00(0.01), f_time=1.34(1.27), b_time=1.34(1.27)  Time cost: 25:23/11:58 [4:10:01/18:14:59]  Acc_iter 11750       Data time: 0.00(0.01)  Forward time: 1.34(1.27)  Batch time: 1.34(1.27)
2025-09-04 23:59:18,030   INFO  Train:    7/36 ( 19%) [1245/1759 ( 71%)]  Loss: 4.131 (4.24)  LR: 1.505e-03  Grad: 3.1338  max=0.1775(module.dense_head.prediction_head.height.1.weight)  min: -1.2727(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8339, loss_cls=0.1749, loss_bbox=0.9254, matched_ious=0.4763, loss_iou=0.0999, loss_iou_reg=0.2443, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.26(1.27)  Time cost: 26:26/10:54 [4:11:04/18:13:13]  Acc_iter 11800       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.27)
2025-09-05 00:00:21,077   INFO  Train:    7/36 ( 19%) [1295/1759 ( 74%)]  Loss: 4.013 (4.24)  LR: 1.514e-03  Grad: 3.2058  max=0.1389(module.dense_head.prediction_head.height.1.weight)  min: -1.5461(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8592, loss_cls=0.1803, loss_bbox=0.9600, matched_ious=0.4699, loss_iou=0.0985, loss_iou_reg=0.2468, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.26(1.27)  Time cost: 27:29/09:50 [4:12:07/18:11:45]  Acc_iter 11850       Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.27)
2025-09-05 00:01:24,814   INFO  Train:    7/36 ( 19%) [1345/1759 ( 76%)]  Loss: 4.337 (4.24)  LR: 1.522e-03  Grad: 3.6879  max=1.9044(module.vfe.pfn_layers.0.linear.weight)  min: -0.7614(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8132, loss_cls=0.1740, loss_bbox=0.9618, matched_ious=0.4674, loss_iou=0.0997, loss_iou_reg=0.2470, d_time=0.01(0.01), f_time=1.29(1.27), b_time=1.29(1.27)  Time cost: 28:32/08:46 [4:13:11/18:10:46]  Acc_iter 11900       Data time: 0.01(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.27)
2025-09-05 00:02:27,998   INFO  Train:    7/36 ( 19%) [1395/1759 ( 79%)]  Loss: 4.024 (4.24)  LR: 1.530e-03  Grad: 5.9838  max=4.4657(module.vfe.pfn_layers.0.linear.weight)  min: -0.1602(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.8745, loss_cls=0.1850, loss_bbox=1.0243, matched_ious=0.4673, loss_iou=0.1005, loss_iou_reg=0.2469, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.19(1.27)  Time cost: 29:36/07:43 [4:14:14/18:09:25]  Acc_iter 11950       Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.19(1.27)
2025-09-05 00:03:30,208   INFO  Train:    7/36 ( 19%) [1445/1759 ( 82%)]  Loss: 4.110 (4.25)  LR: 1.539e-03  Grad: 4.0598  max=1.3086(module.vfe.pfn_layers.0.linear.weight)  min: -2.1819(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8513, loss_cls=0.1738, loss_bbox=1.0193, matched_ious=0.4678, loss_iou=0.0988, loss_iou_reg=0.2456, d_time=0.00(0.01), f_time=1.15(1.26), b_time=1.16(1.27)  Time cost: 30:38/06:39 [4:15:16/18:07:32]  Acc_iter 12000       Data time: 0.00(0.01)  Forward time: 1.15(1.26)  Batch time: 1.16(1.27)
2025-09-05 00:04:32,799   INFO  Train:    7/36 ( 19%) [1495/1759 ( 85%)]  Loss: 4.409 (4.25)  LR: 1.547e-03  Grad: 3.1344  max=0.1200(module.backbone_3d.cls_conv.3.weight)  min: -1.1257(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8357, loss_cls=0.1773, loss_bbox=1.0046, matched_ious=0.4704, loss_iou=0.0997, loss_iou_reg=0.2437, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.27(1.27)  Time cost: 31:40/05:35 [4:16:19/18:05:55]  Acc_iter 12050       Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.27(1.27)
2025-09-05 00:05:35,703   INFO  Train:    7/36 ( 19%) [1545/1759 ( 88%)]  Loss: 4.010 (4.25)  LR: 1.555e-03  Grad: 3.2319  max=0.1361(module.dense_head.prediction_head.height.1.weight)  min: -1.3473(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8352, loss_cls=0.1762, loss_bbox=0.9632, matched_ious=0.4629, loss_iou=0.0982, loss_iou_reg=0.2501, d_time=0.00(0.01), f_time=1.31(1.26), b_time=1.31(1.27)  Time cost: 32:43/04:31 [4:17:22/18:04:30]  Acc_iter 12100       Data time: 0.00(0.01)  Forward time: 1.31(1.26)  Batch time: 1.31(1.27)
2025-09-05 00:06:38,304   INFO  Train:    7/36 ( 19%) [1595/1759 ( 91%)]  Loss: 3.788 (4.25)  LR: 1.564e-03  Grad: 4.6694  max=2.4394(module.vfe.pfn_layers.0.linear.weight)  min: -3.1242(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8176, loss_cls=0.1694, loss_bbox=0.9835, matched_ious=0.4699, loss_iou=0.0981, loss_iou_reg=0.2454, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.26(1.27)  Time cost: 33:46/03:28 [4:18:24/18:02:58]  Acc_iter 12150       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.27)
2025-09-05 00:07:41,607   INFO  Train:    7/36 ( 19%) [1645/1759 ( 94%)]  Loss: 4.274 (4.24)  LR: 1.572e-03  Grad: 3.3481  max=0.6163(module.vfe.pfn_layers.0.linear.weight)  min: -1.3255(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8017, loss_cls=0.1724, loss_bbox=0.9047, matched_ious=0.4813, loss_iou=0.0971, loss_iou_reg=0.2411, d_time=0.00(0.01), f_time=1.40(1.26), b_time=1.40(1.27)  Time cost: 34:49/02:24 [4:19:28/18:01:48]  Acc_iter 12200       Data time: 0.00(0.01)  Forward time: 1.40(1.26)  Batch time: 1.40(1.27)
2025-09-05 00:08:49,128   INFO  Train:    7/36 ( 19%) [1695/1759 ( 96%)]  Loss: 4.603 (4.24)  LR: 1.580e-03  Grad: 2.9704  max=0.2908(module.backbone_3d.cls_conv.3.weight)  min: -1.5149(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8281, loss_cls=0.1725, loss_bbox=0.9340, matched_ious=0.4678, loss_iou=0.1010, loss_iou_reg=0.2508, d_time=0.00(0.01), f_time=1.34(1.27), b_time=1.34(1.27)  Time cost: 35:57/01:21 [4:20:35/18:02:47]  Acc_iter 12250       Data time: 0.00(0.01)  Forward time: 1.34(1.27)  Batch time: 1.34(1.27)
2025-09-05 00:09:52,304   INFO  Train:    7/36 ( 19%) [1745/1759 ( 99%)]  Loss: 4.560 (4.24)  LR: 1.589e-03  Grad: 3.6874  max=2.2037(module.vfe.pfn_layers.0.linear.weight)  min: -0.4240(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8138, loss_cls=0.1701, loss_bbox=0.9680, matched_ious=0.4775, loss_iou=0.0983, loss_iou_reg=0.2433, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.30(1.27)  Time cost: 37:00/00:17 [4:21:38/18:01:31]  Acc_iter 12300       Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.30(1.27)
2025-09-05 00:10:07,812   INFO  Train:    7/36 ( 19%) [1758/1759 (100%)]  Loss: 4.697 (4.24)  LR: 1.591e-03  Grad: 3.1622  max=1.4647(module.vfe.pfn_layers.0.linear.weight)  min: -0.8093(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8335, loss_cls=0.1689, loss_bbox=0.9575, matched_ious=0.4745, loss_iou=0.0967, loss_iou_reg=0.2394, d_time=0.01(0.01), f_time=0.73(1.26), b_time=0.75(1.27)  Time cost: 37:15/00:01 [4:21:54/18:00:44]  Acc_iter 12313       Data time: 0.01(0.01)  Forward time: 0.73(1.26)  Batch time: 0.75(1.27)

                                               [Aepochs:  19%|█▉        | 7/36 [4:21:54<18:03:14, 2241.19s/it]epochs:  19%|█▉        | 7/36 [4:21:54<18:03:14, 2241.20s/it]epochs:  19%|█▉        | 7/36 [4:21:54<18:03:14, 2241.20s/it]epochs:  19%|█▉        | 7/36 [4:21:54<18:03:14, 2241.21s/it]epochs:  19%|█▉        | 7/36 [4:21:54<18:03:14, 2241.21s/it]epochs:  19%|█▉        | 7/36 [4:21:54<18:03:15, 2241.22s/it]epochs:  19%|█▉        | 7/36 [4:21:54<18:03:15, 2241.21s/it]epochs:  19%|█▉        | 7/36 [4:21:55<18:03:15, 2241.21s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-05 00:10:13,157   INFO  Train:    8/36 ( 22%) [   0/1759 (  0%)]  Loss: 5.217 (5.22)  LR: 1.591e-03  Grad: 3.9745  max=1.9665(module.vfe.pfn_layers.0.linear.weight)  min: -1.9742(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9923, loss_cls=0.1785, loss_bbox=1.3318, matched_ious=0.4593, loss_iou=0.0977, loss_iou_reg=0.2360, d_time=1.52(1.52), f_time=2.54(2.54), b_time=4.06(4.06)  Time cost: 00:03/1:44:17 [4:21:59/50:24:14]  Acc_iter 12314       Data time: 1.52(1.52)  Forward time: 2.54(2.54)  Batch time: 4.06(4.06)
2025-09-05 00:10:58,906   INFO  Train:    8/36 ( 22%) [  36/1759 (  2%)]  Loss: 4.457 (4.16)  LR: 1.597e-03  Grad: 3.4545  max=1.7890(module.vfe.pfn_layers.0.linear.weight)  min: -0.6854(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7986, loss_cls=0.1686, loss_bbox=0.9173, matched_ious=0.4739, loss_iou=0.1003, loss_iou_reg=0.2460, d_time=0.00(0.05), f_time=1.33(1.30), b_time=1.33(1.35)  Time cost: 00:49/38:16 [4:22:45/18:52:11]  Acc_iter 12350       Data time: 0.00(0.05)  Forward time: 1.33(1.30)  Batch time: 1.33(1.35)
2025-09-05 00:12:02,523   INFO  Train:    8/36 ( 22%) [  86/1759 (  5%)]  Loss: 4.078 (4.14)  LR: 1.606e-03  Grad: 5.7566  max=3.8154(module.vfe.pfn_layers.0.linear.weight)  min: -2.9449(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8048, loss_cls=0.1697, loss_bbox=0.9142, matched_ious=0.4707, loss_iou=0.0995, loss_iou_reg=0.2472, d_time=0.00(0.02), f_time=1.31(1.28), b_time=1.31(1.30)  Time cost: 01:52/36:11 [4:23:49/18:21:39]  Acc_iter 12400       Data time: 0.00(0.02)  Forward time: 1.31(1.28)  Batch time: 1.31(1.30)
2025-09-05 00:13:06,006   INFO  Train:    8/36 ( 22%) [ 136/1759 (  8%)]  Loss: 5.134 (4.17)  LR: 1.614e-03  Grad: 3.9110  max=1.5286(module.vfe.pfn_layers.0.linear.weight)  min: -0.8435(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8310, loss_cls=0.1754, loss_bbox=0.9681, matched_ious=0.4680, loss_iou=0.0974, loss_iou_reg=0.2474, d_time=0.00(0.02), f_time=1.29(1.28), b_time=1.30(1.29)  Time cost: 02:56/34:49 [4:24:52/18:11:48]  Acc_iter 12450       Data time: 0.00(0.02)  Forward time: 1.29(1.28)  Batch time: 1.30(1.29)
2025-09-05 00:14:08,054   INFO  Train:    8/36 ( 22%) [ 186/1759 ( 11%)]  Loss: 4.626 (4.14)  LR: 1.622e-03  Grad: 3.5795  max=0.1425(module.backbone_3d.cls_conv.3.weight)  min: -1.0274(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7995, loss_cls=0.1700, loss_bbox=0.9306, matched_ious=0.4767, loss_iou=0.0994, loss_iou_reg=0.2452, d_time=0.00(0.01), f_time=1.32(1.26), b_time=1.32(1.28)  Time cost: 03:58/33:25 [4:25:54/18:00:09]  Acc_iter 12500       Data time: 0.00(0.01)  Forward time: 1.32(1.26)  Batch time: 1.32(1.28)
2025-09-05 00:15:11,243   INFO  Train:    8/36 ( 22%) [ 236/1759 ( 13%)]  Loss: 4.096 (4.15)  LR: 1.631e-03  Grad: 3.9753  max=1.4367(module.vfe.pfn_layers.0.linear.weight)  min: -0.4329(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8333, loss_cls=0.1728, loss_bbox=0.9571, matched_ious=0.4723, loss_iou=0.1003, loss_iou_reg=0.2464, d_time=0.04(0.01), f_time=1.53(1.26), b_time=1.57(1.27)  Time cost: 05:01/32:18 [4:26:57/17:57:04]  Acc_iter 12550       Data time: 0.04(0.01)  Forward time: 1.53(1.26)  Batch time: 1.57(1.27)
2025-09-05 00:16:14,567   INFO  Train:    8/36 ( 22%) [ 286/1759 ( 16%)]  Loss: 4.427 (4.14)  LR: 1.639e-03  Grad: 4.4605  max=0.0987(module.dense_head.prediction_head.height.1.weight)  min: -3.4379(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7954, loss_cls=0.1652, loss_bbox=0.9051, matched_ious=0.4786, loss_iou=0.1004, loss_iou_reg=0.2447, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 06:04/31:13 [4:28:01/17:55:05]  Acc_iter 12600       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-05 00:17:17,967   INFO  Train:    8/36 ( 22%) [ 336/1759 ( 19%)]  Loss: 4.103 (4.14)  LR: 1.647e-03  Grad: 3.3993  max=0.1801(module.vfe.pfn_layers.0.linear.weight)  min: -2.1419(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8182, loss_cls=0.1725, loss_bbox=0.9477, matched_ious=0.4688, loss_iou=0.1010, loss_iou_reg=0.2479, d_time=0.00(0.01), f_time=1.17(1.26), b_time=1.18(1.27)  Time cost: 07:08/30:08 [4:29:04/17:53:34]  Acc_iter 12650       Data time: 0.00(0.01)  Forward time: 1.17(1.26)  Batch time: 1.18(1.27)
2025-09-05 00:18:21,880   INFO  Train:    8/36 ( 22%) [ 386/1759 ( 22%)]  Loss: 4.171 (4.16)  LR: 1.656e-03  Grad: 1.6848  max=0.3192(module.vfe.pfn_layers.0.linear.weight)  min: -1.0438(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8427, loss_cls=0.1748, loss_bbox=0.9933, matched_ious=0.4720, loss_iou=0.0982, loss_iou_reg=0.2440, d_time=0.00(0.01), f_time=1.31(1.26), b_time=1.32(1.27)  Time cost: 08:12/29:06 [4:30:08/17:53:17]  Acc_iter 12700       Data time: 0.00(0.01)  Forward time: 1.31(1.26)  Batch time: 1.32(1.27)
2025-09-05 00:19:26,469   INFO  Train:    8/36 ( 22%) [ 436/1759 ( 25%)]  Loss: 2.998 (4.17)  LR: 1.664e-03  Grad: 1.9240  max=0.2836(module.dense_head.prediction_head.height.1.bias)  min: -1.0100(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8411, loss_cls=0.1764, loss_bbox=0.9751, matched_ious=0.4658, loss_iou=0.1000, loss_iou_reg=0.2480, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.28)  Time cost: 09:16/28:05 [4:31:13/17:54:07]  Acc_iter 12750       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.28)
2025-09-05 00:20:29,875   INFO  Train:    8/36 ( 22%) [ 486/1759 ( 28%)]  Loss: 4.524 (4.15)  LR: 1.673e-03  Grad: 1.6794  max=1.0780(module.vfe.pfn_layers.0.linear.weight)  min: -0.3485(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7733, loss_cls=0.1632, loss_bbox=0.8950, matched_ious=0.4825, loss_iou=0.0981, loss_iou_reg=0.2422, d_time=0.00(0.01), f_time=1.31(1.26), b_time=1.32(1.27)  Time cost: 10:20/27:01 [4:32:16/17:52:32]  Acc_iter 12800       Data time: 0.00(0.01)  Forward time: 1.31(1.26)  Batch time: 1.32(1.27)
2025-09-05 00:21:32,854   INFO  Train:    8/36 ( 22%) [ 536/1759 ( 30%)]  Loss: 4.362 (4.16)  LR: 1.681e-03  Grad: 3.0397  max=2.5543(module.vfe.pfn_layers.0.linear.weight)  min: -0.0690(module.dense_head.prediction_head.iou.1.bias)  NaN: False  loss_hm=0.8343, loss_cls=0.1744, loss_bbox=0.9214, matched_ious=0.4681, loss_iou=0.0995, loss_iou_reg=0.2486, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.20(1.27)  Time cost: 11:23/25:56 [4:33:19/17:50:21]  Acc_iter 12850       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.27)
2025-09-05 00:22:35,696   INFO  Train:    8/36 ( 22%) [ 586/1759 ( 33%)]  Loss: 4.237 (4.14)  LR: 1.689e-03  Grad: 2.7503  max=1.9692(module.vfe.pfn_layers.0.linear.weight)  min: -1.0923(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7878, loss_cls=0.1651, loss_bbox=0.8975, matched_ious=0.4882, loss_iou=0.0956, loss_iou_reg=0.2378, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.19(1.27)  Time cost: 12:26/24:50 [4:34:22/17:48:11]  Acc_iter 12900       Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.19(1.27)
2025-09-05 00:23:39,572   INFO  Train:    8/36 ( 22%) [ 636/1759 ( 36%)]  Loss: 3.912 (4.13)  LR: 1.698e-03  Grad: 3.7778  max=0.8787(module.vfe.pfn_layers.0.linear.weight)  min: -2.8960(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8041, loss_cls=0.1669, loss_bbox=0.8940, matched_ious=0.4743, loss_iou=0.0999, loss_iou_reg=0.2443, d_time=0.01(0.01), f_time=1.33(1.26), b_time=1.34(1.27)  Time cost: 13:29/23:47 [4:35:26/17:47:34]  Acc_iter 12950       Data time: 0.01(0.01)  Forward time: 1.33(1.26)  Batch time: 1.34(1.27)
2025-09-05 00:24:42,566   INFO  Train:    8/36 ( 22%) [ 686/1759 ( 39%)]  Loss: 3.974 (4.13)  LR: 1.706e-03  Grad: 3.0504  max=1.5509(module.vfe.pfn_layers.0.linear.weight)  min: -0.9187(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7888, loss_cls=0.1690, loss_bbox=0.9552, matched_ious=0.4600, loss_iou=0.1001, loss_iou_reg=0.2513, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.22(1.27)  Time cost: 14:32/22:43 [4:36:29/17:45:47]  Acc_iter 13000       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.27)
2025-09-05 00:25:45,388   INFO  Train:    8/36 ( 22%) [ 736/1759 ( 42%)]  Loss: 5.049 (4.13)  LR: 1.714e-03  Grad: 4.6827  max=2.9834(module.vfe.pfn_layers.0.linear.weight)  min: -2.4090(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8086, loss_cls=0.1671, loss_bbox=0.9396, matched_ious=0.4789, loss_iou=0.0978, loss_iou_reg=0.2421, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.18(1.27)  Time cost: 15:35/21:38 [4:37:31/17:43:55]  Acc_iter 13050       Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.18(1.27)
