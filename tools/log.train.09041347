/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
2025-09-04 13:47:39,893   INFO  **********************Start logging**********************
2025-09-04 13:47:39,893   INFO  CUDA_VISIBLE_DEVICES=ALL
2025-09-04 13:47:39,893   INFO  Training in distributed mode : total_batch_size: 16
2025-09-04 13:47:39,893   INFO  cfg_file         cfgs/sparse_models/sparse_former_base.yaml
2025-09-04 13:47:39,893   INFO  batch_size       2
2025-09-04 13:47:39,893   INFO  epochs           36
2025-09-04 13:47:39,893   INFO  workers          12
2025-09-04 13:47:39,893   INFO  extra_tag        default
2025-09-04 13:47:39,893   INFO  ckpt             None
2025-09-04 13:47:39,893   INFO  pretrained_model None
2025-09-04 13:47:39,893   INFO  launcher         pytorch
2025-09-04 13:47:39,893   INFO  tcp_port         18888
2025-09-04 13:47:39,894   INFO  sync_bn          True
2025-09-04 13:47:39,894   INFO  fix_random_seed  False
2025-09-04 13:47:39,894   INFO  ckpt_save_interval 1
2025-09-04 13:47:39,894   INFO  local_rank       0
2025-09-04 13:47:39,894   INFO  max_ckpt_save_num 30
2025-09-04 13:47:39,894   INFO  merge_all_iters_to_one_epoch False
2025-09-04 13:47:39,894   INFO  set_cfgs         None
2025-09-04 13:47:39,894   INFO  max_waiting_mins 0
2025-09-04 13:47:39,894   INFO  start_epoch      0
2025-09-04 13:47:39,894   INFO  num_epochs_to_eval 0
2025-09-04 13:47:39,894   INFO  save_to_file     False
2025-09-04 13:47:39,894   INFO  use_tqdm_to_record False
2025-09-04 13:47:39,894   INFO  logger_iter_interval 50
2025-09-04 13:47:39,894   INFO  ckpt_save_time_interval 300
2025-09-04 13:47:39,894   INFO  wo_gpu_stat      True
2025-09-04 13:47:39,894   INFO  use_amp          False
2025-09-04 13:47:39,894   INFO  eval_map         False
2025-09-04 13:47:39,894   INFO  dataset          nuscenes
2025-09-04 13:47:39,895   INFO  root_dir         /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2
2025-09-04 13:47:39,895   INFO  output_dir       /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_shuflle_.0diff_dense_hm_.3lr_.3wd
2025-09-04 13:47:39,895   INFO  cfg.ROOT_DIR: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2
2025-09-04 13:47:39,895   INFO  cfg.LOCAL_RANK: 0
2025-09-04 13:47:39,895   INFO  cfg.CLASS_NAMES: ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']
2025-09-04 13:47:39,895   INFO  ----------- DATA_CONFIG -----------
2025-09-04 13:47:39,895   INFO  cfg.DATA_CONFIG.DATASET: NuScenesDataset
2025-09-04 13:47:39,895   INFO  cfg.DATA_CONFIG.DATA_PATH: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/data/nuscenes
2025-09-04 13:47:39,895   INFO  cfg.DATA_CONFIG.VERSION: v1.0-trainval
2025-09-04 13:47:39,895   INFO  cfg.DATA_CONFIG.MAX_SWEEPS: 10
2025-09-04 13:47:39,895   INFO  cfg.DATA_CONFIG.PRED_VELOCITY: True
2025-09-04 13:47:39,895   INFO  cfg.DATA_CONFIG.SET_NAN_VELOCITY_TO_ZEROS: True
2025-09-04 13:47:39,895   INFO  cfg.DATA_CONFIG.FILTER_MIN_POINTS_IN_GT: 1
2025-09-04 13:47:39,895   INFO  ----------- DATA_SPLIT -----------
2025-09-04 13:47:39,895   INFO  cfg.DATA_CONFIG.DATA_SPLIT.train: train
2025-09-04 13:47:39,895   INFO  cfg.DATA_CONFIG.DATA_SPLIT.test: val
2025-09-04 13:47:39,895   INFO  ----------- INFO_PATH -----------
2025-09-04 13:47:39,895   INFO  cfg.DATA_CONFIG.INFO_PATH.train: ['nuscenes_infos_10sweeps_train.pkl']
2025-09-04 13:47:39,895   INFO  cfg.DATA_CONFIG.INFO_PATH.test: ['nuscenes_infos_10sweeps_val.pkl']
2025-09-04 13:47:39,896   INFO  cfg.DATA_CONFIG.POINT_CLOUD_RANGE: [-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]
2025-09-04 13:47:39,896   INFO  cfg.DATA_CONFIG.BALANCED_RESAMPLING: False
2025-09-04 13:47:39,896   INFO  ----------- DATA_AUGMENTOR -----------
2025-09-04 13:47:39,896   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.DISABLE_AUG_LIST: ['placeholder']
2025-09-04 13:47:39,896   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.AUG_CONFIG_LIST: [{'NAME': 'gt_sampling', 'DB_INFO_PATH': ['nuscenes_dbinfos_10sweeps_withvelo.pkl'], 'USE_SHARED_MEMORY': True, 'DB_DATA_PATH': ['nuscenes_10sweeps_withvelo_lidar.npy'], 'PREPARE': {'filter_by_min_points': ['car:5', 'truck:5', 'construction_vehicle:5', 'bus:5', 'trailer:5', 'barrier:5', 'motorcycle:5', 'bicycle:5', 'pedestrian:5', 'traffic_cone:5']}, 'SAMPLE_GROUPS': ['car:2', 'truck:3', 'construction_vehicle:7', 'bus:4', 'trailer:6', 'barrier:2', 'motorcycle:6', 'bicycle:6', 'pedestrian:2', 'traffic_cone:2'], 'NUM_POINT_FEATURES': 5, 'DATABASE_WITH_FAKELIDAR': False, 'REMOVE_EXTRA_WIDTH': [0.0, 0.0, 0.0], 'LIMIT_WHOLE_SCENE': True}, {'NAME': 'random_world_flip', 'ALONG_AXIS_LIST': ['x', 'y']}, {'NAME': 'random_world_rotation', 'WORLD_ROT_ANGLE': [-0.78539816, 0.78539816]}, {'NAME': 'random_world_scaling', 'WORLD_SCALE_RANGE': [0.9, 1.1]}, {'NAME': 'random_world_translation', 'NOISE_TRANSLATE_STD': [0.5, 0.5, 0.5]}]
2025-09-04 13:47:39,922   INFO  ----------- POINT_FEATURE_ENCODING -----------
2025-09-04 13:47:39,922   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.encoding_type: absolute_coordinates_encoding
2025-09-04 13:47:39,922   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.used_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-09-04 13:47:39,922   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.src_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-09-04 13:47:39,922   INFO  cfg.DATA_CONFIG.DATA_PROCESSOR: [{'NAME': 'mask_points_and_boxes_outside_range', 'REMOVE_OUTSIDE_BOXES': True}, {'NAME': 'shuffle_points', 'SHUFFLE_ENABLED': {'train': True, 'test': True}}, {'NAME': 'transform_points_to_voxels_placeholder', 'VOXEL_SIZE': [0.3, 0.3, 0.2]}]
2025-09-04 13:47:39,923   INFO  cfg.DATA_CONFIG._BASE_CONFIG_: cfgs/dataset_configs/nuscenes_dataset.yaml
2025-09-04 13:47:39,923   INFO  ----------- SAMPLED_INTERVAL -----------
2025-09-04 13:47:39,923   INFO  cfg.DATA_CONFIG.SAMPLED_INTERVAL.train: 1
2025-09-04 13:47:39,923   INFO  cfg.DATA_CONFIG.SAMPLED_INTERVAL.test: 1
2025-09-04 13:47:39,923   INFO  ----------- MODEL -----------
2025-09-04 13:47:39,923   INFO  cfg.MODEL.NAME: TransFusion
2025-09-04 13:47:39,923   INFO  ----------- VFE -----------
2025-09-04 13:47:39,923   INFO  cfg.MODEL.VFE.NAME: DynamicVoxelVFE
2025-09-04 13:47:39,923   INFO  cfg.MODEL.VFE.WITH_DISTANCE: False
2025-09-04 13:47:39,924   INFO  cfg.MODEL.VFE.USE_ABSLOTE_XYZ: True
2025-09-04 13:47:39,924   INFO  cfg.MODEL.VFE.USE_NORM: True
2025-09-04 13:47:39,924   INFO  cfg.MODEL.VFE.NUM_FILTERS: [64, 64]
2025-09-04 13:47:39,924   INFO  ----------- BACKBONE_3D -----------
2025-09-04 13:47:39,924   INFO  cfg.MODEL.BACKBONE_3D.NAME: FSHNet_nusc
2025-09-04 13:47:39,924   INFO  cfg.MODEL.BACKBONE_3D.FEATURE_DIM: 128
2025-09-04 13:47:39,924   INFO  ----------- SPENCODER -----------
2025-09-04 13:47:39,924   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.DIM: 3
2025-09-04 13:47:39,924   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.NUM_LAYERS: 3
2025-09-04 13:47:39,924   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.WINDOW_SHAPE: [9, 9, 5]
2025-09-04 13:47:39,925   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SHUFFLE: True
2025-09-04 13:47:39,925   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.DEPTH: 8
2025-09-04 13:47:39,925   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.ORDERS: [['z', 'z-trans'], ['hilbert', 'hilbert-trans'], ['x', 'y']]
2025-09-04 13:47:39,925   INFO  ----------- SMSA -----------
2025-09-04 13:47:39,925   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.EMBED_DIM: 128
2025-09-04 13:47:39,925   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DEPTH: 8
2025-09-04 13:47:39,925   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.NUM_LEVELS: 3
2025-09-04 13:47:39,925   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.WINDOW_SHAPE: [9, 9, 5]
2025-09-04 13:47:39,925   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DROP_PATH: 0.2
2025-09-04 13:47:39,925   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.SPATIAL_ENHANCE: True
2025-09-04 13:47:39,926   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.FUSED_ADD_NORM: True
2025-09-04 13:47:39,926   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.RMS_NORM: True
2025-09-04 13:47:39,926   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.NORM_EPSILON: 1e-05
2025-09-04 13:47:39,926   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.RESIDUAL_IN_FP32: True
2025-09-04 13:47:39,926   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DIFF_COEF: 0.0
2025-09-04 13:47:39,926   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DIFF_KERNEL: [7, 5, 3]
2025-09-04 13:47:39,926   INFO  cfg.MODEL.BACKBONE_3D.AFD_FEATURE_DIM: 128
2025-09-04 13:47:39,926   INFO  cfg.MODEL.BACKBONE_3D.AFD_NUM_LAYERS: 4
2025-09-04 13:47:39,926   INFO  cfg.MODEL.BACKBONE_3D.AFD_NUM_SBB: [2, 1, 1]
2025-09-04 13:47:39,926   INFO  cfg.MODEL.BACKBONE_3D.AFD_DOWN_STRIDE: [1, 2, 2]
2025-09-04 13:47:39,927   INFO  cfg.MODEL.BACKBONE_3D.AFD_DOWN_KERNEL_SIZE: [3, 3, 3]
2025-09-04 13:47:39,927   INFO  cfg.MODEL.BACKBONE_3D.AFD: True
2025-09-04 13:47:39,927   INFO  cfg.MODEL.BACKBONE_3D.FG_THRESHOLD: 0.2
2025-09-04 13:47:39,927   INFO  cfg.MODEL.BACKBONE_3D.FEATMAP_STRIDE: 2
2025-09-04 13:47:39,927   INFO  cfg.MODEL.BACKBONE_3D.DETACH_FEATURE: True
2025-09-04 13:47:39,927   INFO  cfg.MODEL.BACKBONE_3D.GREOUP_POOLING_KERNEL_SIZE: [9, 15, 5, 5]
2025-09-04 13:47:39,927   INFO  cfg.MODEL.BACKBONE_3D.GROUP_CLASS_NAMES: [['car', 'truck', 'construction_vehicle'], ['bus', 'trailer'], ['barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']]
2025-09-04 13:47:39,927   INFO  ----------- DENSE_HEAD -----------
2025-09-04 13:47:39,927   INFO  cfg.MODEL.DENSE_HEAD.CLASS_AGNOSTIC: False
2025-09-04 13:47:39,928   INFO  cfg.MODEL.DENSE_HEAD.NAME: TransFusionHead
2025-09-04 13:47:39,928   INFO  cfg.MODEL.DENSE_HEAD.QUERY_RADIUS: 20
2025-09-04 13:47:39,928   INFO  cfg.MODEL.DENSE_HEAD.QUERY_LOCAL: True
2025-09-04 13:47:39,928   INFO  cfg.MODEL.DENSE_HEAD.USE_BIAS_BEFORE_NORM: True
2025-09-04 13:47:39,928   INFO  cfg.MODEL.DENSE_HEAD.INPUT_FEATURES: 128
2025-09-04 13:47:39,928   INFO  cfg.MODEL.DENSE_HEAD.NUM_PROPOSALS: 600
2025-09-04 13:47:39,928   INFO  cfg.MODEL.DENSE_HEAD.HIDDEN_CHANNEL: 128
2025-09-04 13:47:39,928   INFO  cfg.MODEL.DENSE_HEAD.NUM_CLASSES: 10
2025-09-04 13:47:39,928   INFO  cfg.MODEL.DENSE_HEAD.NUM_HEADS: 8
2025-09-04 13:47:39,928   INFO  cfg.MODEL.DENSE_HEAD.NMS_KERNEL_SIZE: 3
2025-09-04 13:47:39,928   INFO  cfg.MODEL.DENSE_HEAD.FFN_CHANNEL: 256
2025-09-04 13:47:39,929   INFO  cfg.MODEL.DENSE_HEAD.DROPOUT: 0.1
2025-09-04 13:47:39,929   INFO  cfg.MODEL.DENSE_HEAD.BN_MOMENTUM: 0.1
2025-09-04 13:47:39,929   INFO  cfg.MODEL.DENSE_HEAD.ACTIVATION: relu
2025-09-04 13:47:39,929   INFO  cfg.MODEL.DENSE_HEAD.NUM_HM_CONV: 2
2025-09-04 13:47:39,929   INFO  ----------- SEPARATE_HEAD_CFG -----------
2025-09-04 13:47:39,929   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_ORDER: ['center', 'height', 'dim', 'rot', 'vel', 'iou']
2025-09-04 13:47:39,929   INFO  ----------- HEAD_DICT -----------
2025-09-04 13:47:39,929   INFO  ----------- center -----------
2025-09-04 13:47:39,929   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.out_channels: 2
2025-09-04 13:47:39,930   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.num_conv: 2
2025-09-04 13:47:39,930   INFO  ----------- height -----------
2025-09-04 13:47:39,930   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.height.out_channels: 1
2025-09-04 13:47:39,930   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.height.num_conv: 2
2025-09-04 13:47:39,930   INFO  ----------- dim -----------
2025-09-04 13:47:39,930   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.out_channels: 3
2025-09-04 13:47:39,930   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.num_conv: 2
2025-09-04 13:47:39,930   INFO  ----------- rot -----------
2025-09-04 13:47:39,930   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.out_channels: 2
2025-09-04 13:47:39,930   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.num_conv: 2
2025-09-04 13:47:39,931   INFO  ----------- vel -----------
2025-09-04 13:47:39,931   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.out_channels: 2
2025-09-04 13:47:39,931   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.num_conv: 2
2025-09-04 13:47:39,931   INFO  ----------- iou -----------
2025-09-04 13:47:39,931   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.iou.out_channels: 1
2025-09-04 13:47:39,931   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.iou.num_conv: 2
2025-09-04 13:47:39,931   INFO  ----------- TARGET_ASSIGNER_CONFIG -----------
2025-09-04 13:47:39,931   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.FEATURE_MAP_STRIDE: 2
2025-09-04 13:47:39,931   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.DATASET: nuScenes
2025-09-04 13:47:39,931   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.GAUSSIAN_OVERLAP: 0.1
2025-09-04 13:47:39,932   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.MIN_RADIUS: 2
2025-09-04 13:47:39,932   INFO  ----------- HUNGARIAN_ASSIGNER -----------
2025-09-04 13:47:39,932   INFO  ----------- cls_cost -----------
2025-09-04 13:47:39,932   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.gamma: 2.0
2025-09-04 13:47:39,932   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.alpha: 0.25
2025-09-04 13:47:39,932   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.weight: 0.15
2025-09-04 13:47:39,932   INFO  ----------- reg_cost -----------
2025-09-04 13:47:39,932   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.reg_cost.weight: 0.25
2025-09-04 13:47:39,932   INFO  ----------- iou_cost -----------
2025-09-04 13:47:39,932   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.iou_cost.weight: 0.25
2025-09-04 13:47:39,933   INFO  ----------- LOSS_CONFIG -----------
2025-09-04 13:47:39,933   INFO  ----------- LOSS_WEIGHTS -----------
2025-09-04 13:47:39,933   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.cls_weight: 1.0
2025-09-04 13:47:39,933   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.bbox_weight: 0.25
2025-09-04 13:47:39,933   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.hm_weight: 1.0
2025-09-04 13:47:39,933   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.iou_weight: 0.5
2025-09-04 13:47:39,933   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.iou_reg_weight: 0.5
2025-09-04 13:47:39,933   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.code_weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2]
2025-09-04 13:47:39,933   INFO  ----------- LOSS_CLS -----------
2025-09-04 13:47:39,934   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.use_sigmoid: True
2025-09-04 13:47:39,934   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.gamma: 2.0
2025-09-04 13:47:39,934   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.alpha: 0.25
2025-09-04 13:47:39,934   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_IOU: True
2025-09-04 13:47:39,934   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_IOU_REG: True
2025-09-04 13:47:39,934   INFO  ----------- POST_PROCESSING -----------
2025-09-04 13:47:39,934   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.SCORE_THRESH: 0.0
2025-09-04 13:47:39,934   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.POST_CENTER_RANGE: [-61.2, -61.2, -10.0, 61.2, 61.2, 10.0]
2025-09-04 13:47:39,934   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.USE_IOU_TO_RECTIFY_SCORE: True
2025-09-04 13:47:39,934   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.IOU_RECTIFIER: [0.5]
2025-09-04 13:47:39,935   INFO  ----------- NMS_CONFIG -----------
2025-09-04 13:47:39,935   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_TYPE: nms_gpu
2025-09-04 13:47:39,935   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_THRESH: 0.2
2025-09-04 13:47:39,935   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_PRE_MAXSIZE: 1000
2025-09-04 13:47:39,935   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_POST_MAXSIZE: 100
2025-09-04 13:47:39,935   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.SCORE_THRES: 0.0
2025-09-04 13:47:39,935   INFO  ----------- POST_PROCESSING -----------
2025-09-04 13:47:39,935   INFO  cfg.MODEL.POST_PROCESSING.RECALL_THRESH_LIST: [0.3, 0.5, 0.7]
2025-09-04 13:47:39,935   INFO  cfg.MODEL.POST_PROCESSING.SCORE_THRESH: 0.1
2025-09-04 13:47:39,936   INFO  cfg.MODEL.POST_PROCESSING.OUTPUT_RAW_SCORE: False
2025-09-04 13:47:39,936   INFO  cfg.MODEL.POST_PROCESSING.EVAL_METRIC: kitti
2025-09-04 13:47:39,936   INFO  ----------- NMS_CONFIG -----------
2025-09-04 13:47:39,936   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.MULTI_CLASSES_NMS: True
2025-09-04 13:47:39,936   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_TYPE: nms_gpu
2025-09-04 13:47:39,936   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_THRESH: 0.2
2025-09-04 13:47:39,936   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_PRE_MAXSIZE: 1000
2025-09-04 13:47:39,936   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_POST_MAXSIZE: 83
2025-09-04 13:47:39,936   INFO  ----------- OPTIMIZATION -----------
2025-09-04 13:47:39,936   INFO  cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU: 2
2025-09-04 13:47:39,937   INFO  cfg.OPTIMIZATION.NUM_EPOCHS: 36
2025-09-04 13:47:39,937   INFO  cfg.OPTIMIZATION.OPTIMIZER: adam_onecycle
2025-09-04 13:47:39,937   INFO  cfg.OPTIMIZATION.LR: 0.003
2025-09-04 13:47:39,937   INFO  cfg.OPTIMIZATION.WEIGHT_DECAY: 0.03
2025-09-04 13:47:39,937   INFO  cfg.OPTIMIZATION.MOMENTUM: 0.9
2025-09-04 13:47:39,937   INFO  cfg.OPTIMIZATION.MOMS: [0.95, 0.85]
2025-09-04 13:47:39,937   INFO  cfg.OPTIMIZATION.PCT_START: 0.4
2025-09-04 13:47:39,937   INFO  cfg.OPTIMIZATION.DIV_FACTOR: 10
2025-09-04 13:47:39,937   INFO  cfg.OPTIMIZATION.DECAY_STEP_LIST: [35, 45]
2025-09-04 13:47:39,937   INFO  cfg.OPTIMIZATION.LR_DECAY: 0.1
2025-09-04 13:47:39,938   INFO  cfg.OPTIMIZATION.LR_CLIP: 1e-07
2025-09-04 13:47:39,938   INFO  cfg.OPTIMIZATION.LR_WARMUP: False
2025-09-04 13:47:39,938   INFO  cfg.OPTIMIZATION.WARMUP_EPOCH: 1
2025-09-04 13:47:39,938   INFO  cfg.OPTIMIZATION.GRAD_NORM_CLIP: 10
2025-09-04 13:47:39,938   INFO  cfg.OPTIMIZATION.LOSS_SCALE_FP16: 32
2025-09-04 13:47:39,938   INFO  ----------- HOOK -----------
2025-09-04 13:47:39,938   INFO  ----------- DisableAugmentationHook -----------
2025-09-04 13:47:39,938   INFO  cfg.HOOK.DisableAugmentationHook.DISABLE_AUG_LIST: ['gt_sampling']
2025-09-04 13:47:39,938   INFO  cfg.HOOK.DisableAugmentationHook.NUM_LAST_EPOCHS: 5
2025-09-04 13:47:39,938   INFO  cfg.TAG: sparse_former_base
2025-09-04 13:47:39,939   INFO  cfg.EXP_GROUP_PATH: sparse_models
2025-09-04 13:47:39,939   INFO  cfg.OUTPUT_DIR: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_shuflle_.0diff_dense_hm_.3lr_.3wd
2025-09-04 13:47:39,950   INFO  ----------- Create dataloader & network & optimizer -----------
2025-09-04 13:47:45,287   INFO  Database filter by min points car: 339949 => 294532
2025-09-04 13:47:45,298   INFO  Database filter by min points truck: 65262 => 60344
2025-09-04 13:47:45,299   INFO  Database filter by min points construction_vehicle: 11050 => 10589
2025-09-04 13:47:45,301   INFO  Database filter by min points bus: 12286 => 11619
2025-09-04 13:47:45,303   INFO  Database filter by min points trailer: 19202 => 17934
2025-09-04 13:47:45,316   INFO  Database filter by min points barrier: 107507 => 101993
2025-09-04 13:47:45,318   INFO  Database filter by min points motorcycle: 8846 => 8055
2025-09-04 13:47:45,319   INFO  Database filter by min points bicycle: 8185 => 7531
2025-09-04 13:47:45,336   INFO  Database filter by min points pedestrian: 161928 => 148520
2025-09-04 13:47:45,345   INFO  Database filter by min points traffic_cone: 62964 => 55504
2025-09-04 13:47:45,345   INFO  Loading GT database to shared memory
eflops102:23:23 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:23:23 [0] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:23:23 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:23:23 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:23:23 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.18.5+cuda11.8
eflops102:30:30 [7] NCCL INFO cudaDriverVersion 12050
eflops102:26:26 [3] NCCL INFO cudaDriverVersion 12050
eflops102:24:24 [1] NCCL INFO cudaDriverVersion 12050
eflops102:30:30 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:28:28 [5] NCCL INFO cudaDriverVersion 12050
eflops102:27:27 [4] NCCL INFO cudaDriverVersion 12050
eflops102:25:25 [2] NCCL INFO cudaDriverVersion 12050
eflops102:26:26 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:24:24 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:28:28 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:27:27 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:25:25 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:30:30 [7] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:26:26 [3] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:24:24 [1] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:30:30 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:30:30 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:26:26 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:26:26 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:24:24 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:24:24 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:29:29 [6] NCCL INFO cudaDriverVersion 12050
eflops102:29:29 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:27:27 [4] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:28:28 [5] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:25:25 [2] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:27:27 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:27:27 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:29:29 [6] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:29:29 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:29:29 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:28:28 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:28:28 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:25:25 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:25:25 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:23:99 [0] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:23:99 [0] NCCL INFO P2P plugin IBext
eflops102:23:99 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:23:99 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:23:99 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:23:99 [0] NCCL INFO NET/IB : No device found.
eflops102:23:99 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:23:99 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:23:99 [0] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:23:99 [0] NCCL INFO Using network Socket
eflops102:30:100 [7] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:30:100 [7] NCCL INFO P2P plugin IBext
eflops102:30:100 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:30:100 [7] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:30:100 [7] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:30:100 [7] NCCL INFO NET/IB : No device found.
eflops102:30:100 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:30:100 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:30:100 [7] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:30:100 [7] NCCL INFO Using network Socket
eflops102:24:102 [1] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:24:102 [1] NCCL INFO P2P plugin IBext
eflops102:24:102 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:24:102 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:24:102 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:24:102 [1] NCCL INFO NET/IB : No device found.
eflops102:24:102 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:24:102 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:24:102 [1] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:24:102 [1] NCCL INFO Using network Socket
eflops102:27:103 [4] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:27:103 [4] NCCL INFO P2P plugin IBext
eflops102:27:103 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:26:101 [3] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:26:101 [3] NCCL INFO P2P plugin IBext
eflops102:26:101 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:27:103 [4] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:27:103 [4] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:27:103 [4] NCCL INFO NET/IB : No device found.
eflops102:27:103 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:27:103 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:26:101 [3] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:26:101 [3] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:26:101 [3] NCCL INFO NET/IB : No device found.
eflops102:26:101 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:26:101 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:27:103 [4] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:27:103 [4] NCCL INFO Using network Socket
eflops102:26:101 [3] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:26:101 [3] NCCL INFO Using network Socket
eflops102:29:104 [6] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:29:104 [6] NCCL INFO P2P plugin IBext
eflops102:29:104 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:28:105 [5] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:28:105 [5] NCCL INFO P2P plugin IBext
eflops102:28:105 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:29:104 [6] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:29:104 [6] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:29:104 [6] NCCL INFO NET/IB : No device found.
eflops102:29:104 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:29:104 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:29:104 [6] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:29:104 [6] NCCL INFO Using network Socket

eflops102:28:105 [5] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:28:105 [5] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:28:105 [5] NCCL INFO NET/IB : No device found.
eflops102:28:105 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:28:105 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:28:105 [5] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:28:105 [5] NCCL INFO Using network Socket
eflops102:25:106 [2] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:25:106 [2] NCCL INFO P2P plugin IBext
eflops102:25:106 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:25:106 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:25:106 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:25:106 [2] NCCL INFO NET/IB : No device found.
eflops102:25:106 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:25:106 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:25:106 [2] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:25:106 [2] NCCL INFO Using network Socket
eflops102:30:100 [7] NCCL INFO comm 0x10c24f30 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a4000 commId 0x65267956a5cdf9a8 - Init START
eflops102:29:104 [6] NCCL INFO comm 0x11171c20 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a0000 commId 0x65267956a5cdf9a8 - Init START
eflops102:28:105 [5] NCCL INFO comm 0x10ac0020 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 9d000 commId 0x65267956a5cdf9a8 - Init START
eflops102:27:103 [4] NCCL INFO comm 0x1104b450 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9c000 commId 0x65267956a5cdf9a8 - Init START
eflops102:26:101 [3] NCCL INFO comm 0x11cab560 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 57000 commId 0x65267956a5cdf9a8 - Init START
eflops102:23:99 [0] NCCL INFO comm 0x11cafd40 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 4f000 commId 0x65267956a5cdf9a8 - Init START
eflops102:25:106 [2] NCCL INFO comm 0x10cafa30 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 53000 commId 0x65267956a5cdf9a8 - Init START
eflops102:24:102 [1] NCCL INFO comm 0x11258eb0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 50000 commId 0x65267956a5cdf9a8 - Init START
eflops102:30:100 [7] NCCL INFO Setting affinity for GPU 7 to ffff,fff00000,00ffffff,f0000000
eflops102:29:104 [6] NCCL INFO Setting affinity for GPU 6 to ffff,fff00000,00ffffff,f0000000
eflops102:27:103 [4] NCCL INFO Setting affinity for GPU 4 to ffff,fff00000,00ffffff,f0000000
eflops102:25:106 [2] NCCL INFO Setting affinity for GPU 2 to 0fffff,ff000000,0fffffff
eflops102:23:99 [0] NCCL INFO Setting affinity for GPU 0 to 0fffff,ff000000,0fffffff
eflops102:28:105 [5] NCCL INFO Setting affinity for GPU 5 to ffff,fff00000,00ffffff,f0000000
eflops102:24:102 [1] NCCL INFO Setting affinity for GPU 1 to 0fffff,ff000000,0fffffff
eflops102:26:101 [3] NCCL INFO Setting affinity for GPU 3 to 0fffff,ff000000,0fffffff
eflops102:29:104 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5
eflops102:28:105 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
eflops102:29:104 [6] NCCL INFO P2P Chunksize set to 131072
eflops102:28:105 [5] NCCL INFO P2P Chunksize set to 131072
eflops102:27:103 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
eflops102:26:101 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
eflops102:27:103 [4] NCCL INFO P2P Chunksize set to 131072
eflops102:25:106 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
eflops102:26:101 [3] NCCL INFO P2P Chunksize set to 131072
eflops102:25:106 [2] NCCL INFO P2P Chunksize set to 131072
eflops102:23:99 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
eflops102:30:100 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
eflops102:23:99 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
eflops102:30:100 [7] NCCL INFO P2P Chunksize set to 131072
eflops102:23:99 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
eflops102:24:102 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
eflops102:23:99 [0] NCCL INFO P2P Chunksize set to 131072
eflops102:24:102 [1] NCCL INFO P2P Chunksize set to 131072
eflops102:25:106 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
eflops102:29:104 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC
eflops102:28:105 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC
eflops102:24:102 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
eflops102:25:106 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
eflops102:30:100 [7] NCCL INFO Channel 00 : 7[7] -> 0[0] via SHM/direct/direct
eflops102:29:104 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC
eflops102:26:101 [3] NCCL INFO Channel 00 : 3[3] -> 4[4] via SHM/direct/direct
eflops102:28:105 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC
eflops102:24:102 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
eflops102:30:100 [7] NCCL INFO Channel 01 : 7[7] -> 0[0] via SHM/direct/direct
eflops102:26:101 [3] NCCL INFO Channel 01 : 3[3] -> 4[4] via SHM/direct/direct
eflops102:27:103 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
eflops102:23:99 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
eflops102:27:103 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
eflops102:23:99 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
eflops102:29:104 [6] NCCL INFO Connected all rings
eflops102:25:106 [2] NCCL INFO Connected all rings
eflops102:29:104 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC
eflops102:29:104 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC
eflops102:28:105 [5] NCCL INFO Connected all rings
eflops102:25:106 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
eflops102:28:105 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
eflops102:24:102 [1] NCCL INFO Connected all rings
eflops102:28:105 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
eflops102:23:99 [0] NCCL INFO Connected all rings
eflops102:30:100 [7] NCCL INFO Connected all rings
eflops102:30:100 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC
eflops102:24:102 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
eflops102:26:101 [3] NCCL INFO Connected all rings
eflops102:27:103 [4] NCCL INFO Connected all rings
eflops102:25:106 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
eflops102:30:100 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC
eflops102:24:102 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
eflops102:30:100 [7] NCCL INFO Connected all trees
eflops102:30:100 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:30:100 [7] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:27:103 [4] NCCL INFO Channel 00 : 4[4] -> 3[3] via SHM/direct/direct
eflops102:23:99 [0] NCCL INFO Connected all trees
eflops102:23:99 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:23:99 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:29:104 [6] NCCL INFO Connected all trees
eflops102:29:104 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:29:104 [6] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:27:103 [4] NCCL INFO Channel 01 : 4[4] -> 3[3] via SHM/direct/direct
eflops102:24:102 [1] NCCL INFO Connected all trees
eflops102:24:102 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:24:102 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:28:105 [5] NCCL INFO Connected all trees
eflops102:28:105 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:28:105 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:26:101 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
eflops102:26:101 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
eflops102:25:106 [2] NCCL INFO Connected all trees
eflops102:25:106 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:25:106 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:26:101 [3] NCCL INFO Connected all trees
eflops102:26:101 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:26:101 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:27:103 [4] NCCL INFO Connected all trees
eflops102:27:103 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:27:103 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:30:100 [7] NCCL INFO comm 0x10c24f30 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a4000 commId 0x65267956a5cdf9a8 - Init COMPLETE
eflops102:27:103 [4] NCCL INFO comm 0x1104b450 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9c000 commId 0x65267956a5cdf9a8 - Init COMPLETE
eflops102:24:102 [1] NCCL INFO comm 0x11258eb0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 50000 commId 0x65267956a5cdf9a8 - Init COMPLETE
eflops102:28:105 [5] NCCL INFO comm 0x10ac0020 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 9d000 commId 0x65267956a5cdf9a8 - Init COMPLETE
eflops102:26:101 [3] NCCL INFO comm 0x11cab560 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 57000 commId 0x65267956a5cdf9a8 - Init COMPLETE
eflops102:23:99 [0] NCCL INFO comm 0x11cafd40 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 4f000 commId 0x65267956a5cdf9a8 - Init COMPLETE
eflops102:25:106 [2] NCCL INFO comm 0x10cafa30 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 53000 commId 0x65267956a5cdf9a8 - Init COMPLETE
eflops102:29:104 [6] NCCL INFO comm 0x11171c20 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a0000 commId 0x65267956a5cdf9a8 - Init COMPLETE
2025-09-04 13:47:58,600   INFO  GT database has been saved to shared memory
2025-09-04 13:47:58,764   INFO  Loading NuScenes dataset
2025-09-04 13:48:00,649   INFO  Total samples for NuScenes dataset: 28130
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2025-09-04 13:48:01,158   INFO  ----------- Model TransFusion created, param count: 24443458 -----------
epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]2025-09-04 13:48:01,159   INFO  DistributedDataParallel(
  (module): TransFusion(
    (vfe): DynamicVoxelVFE(
      (pfn_layers): ModuleList(
        (0): PFNLayerV2(
          (linear): Linear(in_features=11, out_features=32, bias=False)
          (norm): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
        )
        (1): PFNLayerV2(
          (linear): Linear(in_features=64, out_features=64, bias=False)
          (norm): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
        )
      )
    )
    (backbone_3d): FSHNet_nusc(
      (cpe): SparseSequential(
        (0): SparseSequential(
          (0): SubMConv3d(64, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (2): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
      )
      (down1): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (down2): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (down3): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (stage): SPEncoder(
        (blocks): ModuleList(
          (0-2): 3 x SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
          )
        )
        (inputs): ModuleList(
          (0): SerializationLayer(
            (serialization): ZOrderSerialization()
          )
          (1): SerializationLayer(
            (serialization): HilbertSerialization()
          )
          (2): SerializationLayer(
            (serialization): FlattenWindowsSerialization()
          )
        )
        (embeddings): ModuleList(
          (0-2): 3 x MSSubConvEmbeddingLearned(
            (stem): ModuleList(
              (0-2): 3 x SparseBasicBlock(
                (conv1): SubMConv3d(3, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
              )
            )
          )
        )
      )
      (app): AttnPillarPool(
        (query_func): SparseMaxPool3d(kernel_size=[10, 1, 1], stride=[10, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], algo=ConvAlgo.MaskImplicitGemm)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (pos_embedding): Embedding(10, 128)
      )
      (cls_conv): SparseSequential(
        (0): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): SubMConv2d(128, 3, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
      )
      (afd_layers): ModuleList(
        (0-3): 4 x SEDLayer(
          (encoder): ModuleList(
            (0): SparseSequential(
              (0): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
              (1): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
            )
            (1-2): 2 x SparseSequential(
              (0): SparseSequential(
                (0): SparseConv2d(128, 128, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
            )
          )
          (decoder): ModuleList(
            (0-1): 2 x SparseSequential(
              (0): SparseInverseConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
              (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (decoder_norm): ModuleList(
            (0-1): 2 x SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (shared_conv): SparseSequential(
        (0): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (map_to_bev_module): None
    (pfe): None
    (backbone_2d): None
    (dense_head): TransFusionHead(
      (loss_cls): SigmoidFocalClassificationLoss()
      (loss_bbox): L1Loss()
      (loss_heatmap): GaussianFocalLoss()
      (shared_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (heatmap_head): Sequential(
        (0): BasicBlock2D(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): Conv2d(128, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (class_encoding): Conv1d(10, 128, kernel_size=(1,), stride=(1,))
      (decoder): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_posembed): PositionEmbeddingLearned(
          (position_embedding_head): Sequential(
            (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
        )
        (cross_posembed): PositionEmbeddingLearned(
          (position_embedding_head): Sequential(
            (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
        )
      )
      (prediction_head): SeparateHead_Transfusion(
        (center): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (height): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
        (dim): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 3, kernel_size=(1,), stride=(1,))
        )
        (rot): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (vel): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (iou): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
        (heatmap): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 10, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (point_head): None
    (roi_head): None
  )
)
epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]2025-09-04 13:48:01,164   INFO  **********************Start training sparse_models/sparse_former_base(default)**********************
epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
2025-09-04 13:48:16,695   INFO  Train:    1/36 (  3%) [   0/1759 (  0%)]  Loss: 1408. (1.41e+03)  LR: 3.000e-04  Grad: 10.0000  max=0.3397(module.dense_head.heatmap_head.1.bias)  min: -0.1353(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1372.8212, loss_cls=19.8499, loss_bbox=7.7654, matched_ious=0.0173, loss_iou=0.4519, loss_iou_reg=0.4175, d_time=1.80(1.80), f_time=11.38(11.38), b_time=13.18(13.18)  Time cost: 00:13/6:49:27 [00:15/245:40:30]  Acc_iter 1           Data time: 1.80(1.80)  Forward time: 11.38(11.38)  Batch time: 13.18(13.18)
2025-09-04 13:49:24,534   INFO  Train:    1/36 (  3%) [  49/1759 (  3%)]  Loss: 17.06 (124.)  LR: 3.000e-04  Grad: 10.0000  max=1.4223(module.dense_head.prediction_head.heatmap.1.bias)  min: -0.2666(module.dense_head.prediction_head.heatmap.0.1.bias)  NaN: False  loss_hm=71.1533, loss_cls=11.8554, loss_bbox=8.6312, matched_ious=0.0134, loss_iou=0.3551, loss_iou_reg=0.3178, d_time=0.00(0.09), f_time=1.25(1.53), b_time=1.25(1.62)  Time cost: 01:21/46:37 [01:23/28:45:24]  Acc_iter 50          Data time: 0.00(0.09)  Forward time: 1.25(1.53)  Batch time: 1.25(1.62)
2025-09-04 13:50:27,964   INFO  Train:    1/36 (  3%) [  99/1759 (  6%)]  Loss: 13.63 (69.8)  LR: 3.001e-04  Grad: 8.4559  max=0.8656(module.dense_head.prediction_head.heatmap.1.bias)  min: -0.1898(module.dense_head.prediction_head.heatmap.0.1.bias)  NaN: False  loss_hm=2.7505, loss_cls=4.5719, loss_bbox=4.1668, matched_ious=0.0660, loss_iou=0.1419, loss_iou_reg=0.4172, d_time=0.00(0.05), f_time=1.30(1.40), b_time=1.31(1.44)  Time cost: 02:25/40:10 [02:26/25:30:26]  Acc_iter 100         Data time: 0.00(0.05)  Forward time: 1.30(1.40)  Batch time: 1.31(1.44)
2025-09-04 13:51:31,222   INFO  Train:    1/36 (  3%) [ 149/1759 (  8%)]  Loss: 11.54 (50.3)  LR: 3.002e-04  Grad: 7.0564  max=0.3997(module.dense_head.prediction_head.heatmap.1.bias)  min: -0.2332(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=2.5237, loss_cls=1.5060, loss_bbox=3.1588, matched_ious=0.1038, loss_iou=0.1081, loss_iou_reg=0.4047, d_time=0.00(0.03), f_time=1.40(1.35), b_time=1.40(1.38)  Time cost: 03:28/37:17 [03:29/24:23:30]  Acc_iter 150         Data time: 0.00(0.03)  Forward time: 1.40(1.35)  Batch time: 1.40(1.38)
2025-09-04 13:52:34,807   INFO  Train:    1/36 (  3%) [ 199/1759 ( 11%)]  Loss: 9.405 (40.2)  LR: 3.004e-04  Grad: 7.1787  max=0.2072(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2238(module.dense_head.heatmap_head.1.weight)  NaN: False  loss_hm=2.3774, loss_cls=0.8831, loss_bbox=2.7755, matched_ious=0.1221, loss_iou=0.1082, loss_iou_reg=0.3969, d_time=0.00(0.03), f_time=1.20(1.33), b_time=1.21(1.36)  Time cost: 04:32/35:22 [04:33/23:51:14]  Acc_iter 200         Data time: 0.00(0.03)  Forward time: 1.20(1.33)  Batch time: 1.21(1.36)
2025-09-04 13:53:38,751   INFO  Train:    1/36 (  3%) [ 249/1759 ( 14%)]  Loss: 8.986 (34.1)  LR: 3.006e-04  Grad: 6.2131  max=0.2087(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.1958(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=2.2662, loss_cls=0.6987, loss_bbox=2.5633, matched_ious=0.1427, loss_iou=0.1081, loss_iou_reg=0.3868, d_time=0.00(0.02), f_time=1.31(1.32), b_time=1.31(1.34)  Time cost: 05:36/33:49 [05:37/23:32:58]  Acc_iter 250         Data time: 0.00(0.02)  Forward time: 1.31(1.32)  Batch time: 1.31(1.34)
2025-09-04 13:54:46,073   INFO  Train:    1/36 (  3%) [ 299/1759 ( 17%)]  Loss: 8.981 (29.9)  LR: 3.009e-04  Grad: 6.3694  max=0.1881(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2024(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=2.1493, loss_cls=0.6103, loss_bbox=2.3042, matched_ious=0.1725, loss_iou=0.1117, loss_iou_reg=0.3737, d_time=0.00(0.02), f_time=1.43(1.32), b_time=1.43(1.34)  Time cost: 06:43/32:42 [06:44/23:32:16]  Acc_iter 300         Data time: 0.00(0.02)  Forward time: 1.43(1.32)  Batch time: 1.43(1.34)
2025-09-04 13:55:50,523   INFO  Train:    1/36 (  3%) [ 349/1759 ( 20%)]  Loss: 7.613 (26.8)  LR: 3.013e-04  Grad: 6.8688  max=0.1924(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3224(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=2.0298, loss_cls=0.5735, loss_bbox=2.1465, matched_ious=0.2003, loss_iou=0.1103, loss_iou_reg=0.3584, d_time=0.00(0.02), f_time=1.16(1.32), b_time=1.17(1.33)  Time cost: 07:47/31:24 [07:49/23:22:49]  Acc_iter 350         Data time: 0.00(0.02)  Forward time: 1.16(1.32)  Batch time: 1.17(1.33)
2025-09-04 13:56:54,954   INFO  Train:    1/36 (  3%) [ 399/1759 ( 23%)]  Loss: 8.153 (24.5)  LR: 3.017e-04  Grad: 7.0414  max=0.2143(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.1790(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.9667, loss_cls=0.5405, loss_bbox=2.0640, matched_ious=0.2146, loss_iou=0.1076, loss_iou_reg=0.3533, d_time=0.01(0.02), f_time=1.28(1.31), b_time=1.28(1.33)  Time cost: 08:52/30:09 [08:53/23:15:25]  Acc_iter 400         Data time: 0.01(0.02)  Forward time: 1.28(1.31)  Batch time: 1.28(1.33)
2025-09-04 13:57:58,063   INFO  Train:    1/36 (  3%) [ 449/1759 ( 26%)]  Loss: 7.693 (22.6)  LR: 3.021e-04  Grad: 7.8244  max=0.2628(module.dense_head.heatmap_head.1.weight)  min: -0.1883(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.8757, loss_cls=0.5221, loss_bbox=1.9584, matched_ious=0.2358, loss_iou=0.1096, loss_iou_reg=0.3444, d_time=0.01(0.01), f_time=1.29(1.31), b_time=1.29(1.32)  Time cost: 09:55/28:53 [09:56/23:06:20]  Acc_iter 450         Data time: 0.01(0.01)  Forward time: 1.29(1.31)  Batch time: 1.29(1.32)
2025-09-04 13:59:01,819   INFO  Train:    1/36 (  3%) [ 499/1759 ( 28%)]  Loss: 8.275 (21.1)  LR: 3.026e-04  Grad: 8.1788  max=0.3293(module.backbone_3d.cls_conv.3.weight)  min: -0.1990(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.7883, loss_cls=0.4942, loss_bbox=1.8535, matched_ious=0.2527, loss_iou=0.1108, loss_iou_reg=0.3356, d_time=0.00(0.01), f_time=1.36(1.30), b_time=1.36(1.32)  Time cost: 10:59/27:40 [11:00/23:00:14]  Acc_iter 500         Data time: 0.00(0.01)  Forward time: 1.36(1.30)  Batch time: 1.36(1.32)
2025-09-04 14:00:05,070   INFO  Train:    1/36 (  3%) [ 549/1759 ( 31%)]  Loss: 7.280 (19.9)  LR: 3.031e-04  Grad: 8.3007  max=0.2199(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2588(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.7365, loss_cls=0.4773, loss_bbox=1.7983, matched_ious=0.2698, loss_iou=0.1122, loss_iou_reg=0.3258, d_time=0.01(0.01), f_time=1.19(1.30), b_time=1.19(1.31)  Time cost: 12:02/26:29 [12:03/22:54:05]  Acc_iter 550         Data time: 0.01(0.01)  Forward time: 1.19(1.30)  Batch time: 1.19(1.31)
2025-09-04 14:01:08,175   INFO  Train:    1/36 (  3%) [ 599/1759 ( 34%)]  Loss: 6.987 (18.8)  LR: 3.037e-04  Grad: 8.5544  max=0.2260(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2233(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.6832, loss_cls=0.4584, loss_bbox=1.7230, matched_ious=0.2795, loss_iou=0.1127, loss_iou_reg=0.3219, d_time=0.00(0.01), f_time=1.33(1.30), b_time=1.33(1.31)  Time cost: 13:05/25:18 [13:06/22:48:32]  Acc_iter 600         Data time: 0.00(0.01)  Forward time: 1.33(1.30)  Batch time: 1.33(1.31)
2025-09-04 14:02:11,524   INFO  Train:    1/36 (  3%) [ 649/1759 ( 37%)]  Loss: 7.592 (17.9)  LR: 3.044e-04  Grad: 8.7280  max=0.2126(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2248(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.6091, loss_cls=0.4344, loss_bbox=1.7253, matched_ious=0.2939, loss_iou=0.1122, loss_iou_reg=0.3132, d_time=0.00(0.01), f_time=1.29(1.29), b_time=1.29(1.30)  Time cost: 14:08/24:09 [14:10/22:44:03]  Acc_iter 650         Data time: 0.00(0.01)  Forward time: 1.29(1.29)  Batch time: 1.29(1.30)
2025-09-04 14:03:15,055   INFO  Train:    1/36 (  3%) [ 699/1759 ( 40%)]  Loss: 8.107 (17.1)  LR: 3.051e-04  Grad: 9.2291  max=0.2011(module.dense_head.heatmap_head.1.weight)  min: -0.2180(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.6253, loss_cls=0.4309, loss_bbox=1.7145, matched_ious=0.2932, loss_iou=0.1113, loss_iou_reg=0.3161, d_time=0.00(0.01), f_time=1.30(1.29), b_time=1.30(1.30)  Time cost: 15:12/23:01 [15:13/22:40:20]  Acc_iter 700         Data time: 0.00(0.01)  Forward time: 1.30(1.29)  Batch time: 1.30(1.30)
2025-09-04 14:04:19,024   INFO  Train:    1/36 (  3%) [ 749/1759 ( 43%)]  Loss: 7.969 (16.5)  LR: 3.058e-04  Grad: 9.4073  max=0.2159(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2427(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.5896, loss_cls=0.4188, loss_bbox=1.6571, matched_ious=0.3036, loss_iou=0.1136, loss_iou_reg=0.3095, d_time=0.00(0.01), f_time=1.25(1.29), b_time=1.25(1.30)  Time cost: 16:16/21:54 [16:17/22:37:35]  Acc_iter 750         Data time: 0.00(0.01)  Forward time: 1.25(1.29)  Batch time: 1.25(1.30)
2025-09-04 14:05:24,532   INFO  Train:    1/36 (  3%) [ 799/1759 ( 45%)]  Loss: 6.515 (15.9)  LR: 3.066e-04  Grad: 9.7143  max=0.2268(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.3582(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.5333, loss_cls=0.4005, loss_bbox=1.6721, matched_ious=0.3129, loss_iou=0.1089, loss_iou_reg=0.3069, d_time=0.00(0.01), f_time=1.28(1.29), b_time=1.29(1.30)  Time cost: 17:21/20:50 [17:23/22:37:03]  Acc_iter 800         Data time: 0.00(0.01)  Forward time: 1.28(1.29)  Batch time: 1.29(1.30)
2025-09-04 14:06:28,185   INFO  Train:    1/36 (  3%) [ 849/1759 ( 48%)]  Loss: 6.678 (15.3)  LR: 3.075e-04  Grad: 9.3896  max=0.2046(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2300(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.5470, loss_cls=0.3977, loss_bbox=1.6987, matched_ious=0.3120, loss_iou=0.1119, loss_iou_reg=0.3044, d_time=0.00(0.01), f_time=1.31(1.29), b_time=1.32(1.30)  Time cost: 18:25/19:43 [18:26/22:34:11]  Acc_iter 850         Data time: 0.00(0.01)  Forward time: 1.31(1.29)  Batch time: 1.32(1.30)
2025-09-04 14:07:31,559   INFO  Train:    1/36 (  3%) [ 899/1759 ( 51%)]  Loss: 6.286 (14.9)  LR: 3.084e-04  Grad: 9.4616  max=0.2193(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2348(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.4890, loss_cls=0.3822, loss_bbox=1.6504, matched_ious=0.3166, loss_iou=0.1088, loss_iou_reg=0.3050, d_time=0.00(0.01), f_time=1.21(1.29), b_time=1.22(1.30)  Time cost: 19:28/18:36 [19:30/22:31:11]  Acc_iter 900         Data time: 0.00(0.01)  Forward time: 1.21(1.29)  Batch time: 1.22(1.30)
2025-09-04 14:08:34,964   INFO  Train:    1/36 (  3%) [ 949/1759 ( 54%)]  Loss: 6.852 (14.4)  LR: 3.093e-04  Grad: 9.5334  max=0.2202(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2389(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.4942, loss_cls=0.3859, loss_bbox=1.6910, matched_ious=0.3214, loss_iou=0.1134, loss_iou_reg=0.2990, d_time=0.01(0.01), f_time=1.25(1.29), b_time=1.27(1.30)  Time cost: 20:32/17:30 [20:33/22:28:26]  Acc_iter 950         Data time: 0.01(0.01)  Forward time: 1.25(1.29)  Batch time: 1.27(1.30)
2025-09-04 14:09:39,265   INFO  Train:    1/36 (  3%) [ 999/1759 ( 57%)]  Loss: 7.421 (14.0)  LR: 3.104e-04  Grad: 9.7240  max=0.2152(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2377(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.4818, loss_cls=0.3787, loss_bbox=1.6427, matched_ious=0.3253, loss_iou=0.1096, loss_iou_reg=0.2983, d_time=0.00(0.01), f_time=1.39(1.29), b_time=1.40(1.30)  Time cost: 21:36/16:25 [21:38/22:26:46]  Acc_iter 1000        Data time: 0.00(0.01)  Forward time: 1.39(1.29)  Batch time: 1.40(1.30)
2025-09-04 14:10:43,620   INFO  Train:    1/36 (  3%) [1049/1759 ( 60%)]  Loss: 6.862 (13.7)  LR: 3.114e-04  Grad: 9.4247  max=0.2229(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2895(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.4326, loss_cls=0.3624, loss_bbox=1.7058, matched_ious=0.3265, loss_iou=0.1107, loss_iou_reg=0.3003, d_time=0.01(0.01), f_time=1.29(1.29), b_time=1.30(1.30)  Time cost: 22:40/15:20 [22:42/22:25:13]  Acc_iter 1050        Data time: 0.01(0.01)  Forward time: 1.29(1.29)  Batch time: 1.30(1.30)
2025-09-04 14:11:47,371   INFO  Train:    1/36 (  3%) [1099/1759 ( 62%)]  Loss: 6.476 (13.4)  LR: 3.125e-04  Grad: 9.5693  max=0.2255(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2346(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.4786, loss_cls=0.3591, loss_bbox=1.8050, matched_ious=0.3224, loss_iou=0.1091, loss_iou_reg=0.2956, d_time=0.00(0.01), f_time=1.23(1.29), b_time=1.24(1.29)  Time cost: 23:44/14:14 [23:46/22:23:09]  Acc_iter 1100        Data time: 0.00(0.01)  Forward time: 1.23(1.29)  Batch time: 1.24(1.29)
2025-09-04 14:12:51,011   INFO  Train:    1/36 (  3%) [1149/1759 ( 65%)]  Loss: 6.628 (13.1)  LR: 3.137e-04  Grad: 9.8626  max=0.2627(module.backbone_3d.cls_conv.3.bias)  min: -0.2879(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.3891, loss_cls=0.3622, loss_bbox=1.5835, matched_ious=0.3399, loss_iou=0.1094, loss_iou_reg=0.2925, d_time=0.00(0.01), f_time=1.31(1.28), b_time=1.31(1.29)  Time cost: 24:48/13:09 [24:49/22:21:04]  Acc_iter 1150        Data time: 0.00(0.01)  Forward time: 1.31(1.28)  Batch time: 1.31(1.29)
2025-09-04 14:13:55,820   INFO  Train:    1/36 (  3%) [1199/1759 ( 68%)]  Loss: 6.928 (12.8)  LR: 3.149e-04  Grad: 9.8254  max=0.2831(module.backbone_3d.cls_conv.3.bias)  min: -0.2367(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.4310, loss_cls=0.3559, loss_bbox=1.6441, matched_ious=0.3370, loss_iou=0.1081, loss_iou_reg=0.2917, d_time=0.00(0.01), f_time=1.25(1.28), b_time=1.26(1.29)  Time cost: 25:53/12:04 [25:54/22:20:04]  Acc_iter 1200        Data time: 0.00(0.01)  Forward time: 1.25(1.28)  Batch time: 1.26(1.29)
2025-09-04 14:14:59,683   INFO  Train:    1/36 (  3%) [1249/1759 ( 71%)]  Loss: 6.585 (12.5)  LR: 3.162e-04  Grad: 9.8336  max=0.2170(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2324(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.3556, loss_cls=0.3464, loss_bbox=1.6159, matched_ious=0.3493, loss_iou=0.1095, loss_iou_reg=0.2883, d_time=0.00(0.01), f_time=1.24(1.28), b_time=1.24(1.29)  Time cost: 26:56/10:59 [26:58/22:18:18]  Acc_iter 1250        Data time: 0.00(0.01)  Forward time: 1.24(1.28)  Batch time: 1.24(1.29)
2025-09-04 14:16:06,198   INFO  Train:    1/36 (  3%) [1299/1759 ( 74%)]  Loss: 6.862 (12.3)  LR: 3.175e-04  Grad: 9.9620  max=0.2479(module.backbone_3d.cls_conv.3.weight)  min: -0.3077(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=1.3787, loss_cls=0.3464, loss_bbox=1.6735, matched_ious=0.3444, loss_iou=0.1091, loss_iou_reg=0.2908, d_time=0.00(0.01), f_time=1.25(1.29), b_time=1.26(1.29)  Time cost: 28:03/09:55 [28:04/22:18:40]  Acc_iter 1300        Data time: 0.00(0.01)  Forward time: 1.25(1.29)  Batch time: 1.26(1.29)
2025-09-04 14:17:08,932   INFO  Train:    1/36 (  3%) [1349/1759 ( 77%)]  Loss: 6.128 (12.1)  LR: 3.189e-04  Grad: 9.8866  max=0.2187(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2369(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.3639, loss_cls=0.3439, loss_bbox=1.6236, matched_ious=0.3431, loss_iou=0.1102, loss_iou_reg=0.2905, d_time=0.00(0.01), f_time=1.29(1.28), b_time=1.29(1.29)  Time cost: 29:06/08:50 [29:07/22:16:03]  Acc_iter 1350        Data time: 0.00(0.01)  Forward time: 1.29(1.28)  Batch time: 1.29(1.29)
2025-09-04 14:18:13,210   INFO  Train:    1/36 (  3%) [1399/1759 ( 80%)]  Loss: 5.055 (11.9)  LR: 3.203e-04  Grad: 9.9152  max=0.3188(module.backbone_3d.cls_conv.3.weight)  min: -0.2282(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.3412, loss_cls=0.3393, loss_bbox=1.6480, matched_ious=0.3430, loss_iou=0.1068, loss_iou_reg=0.2920, d_time=0.00(0.01), f_time=1.31(1.28), b_time=1.31(1.29)  Time cost: 30:10/07:45 [30:11/22:14:41]  Acc_iter 1400        Data time: 0.00(0.01)  Forward time: 1.31(1.28)  Batch time: 1.31(1.29)
2025-09-04 14:19:16,789   INFO  Train:    1/36 (  3%) [1449/1759 ( 82%)]  Loss: 6.001 (11.7)  LR: 3.217e-04  Grad: 9.4397  max=0.2150(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2143(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.3497, loss_cls=0.3472, loss_bbox=1.5757, matched_ious=0.3454, loss_iou=0.1067, loss_iou_reg=0.2912, d_time=0.00(0.01), f_time=1.22(1.28), b_time=1.22(1.29)  Time cost: 31:14/06:40 [31:15/22:12:50]  Acc_iter 1450        Data time: 0.00(0.01)  Forward time: 1.22(1.28)  Batch time: 1.22(1.29)
2025-09-04 14:20:21,344   INFO  Train:    1/36 (  3%) [1499/1759 ( 85%)]  Loss: 6.463 (11.5)  LR: 3.233e-04  Grad: 9.8959  max=0.2858(module.backbone_3d.cls_conv.3.weight)  min: -0.2155(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.3294, loss_cls=0.3356, loss_bbox=1.6106, matched_ious=0.3525, loss_iou=0.1068, loss_iou_reg=0.2863, d_time=0.01(0.01), f_time=1.32(1.28), b_time=1.33(1.29)  Time cost: 32:18/05:36 [32:20/22:11:43]  Acc_iter 1500        Data time: 0.01(0.01)  Forward time: 1.32(1.28)  Batch time: 1.33(1.29)
2025-09-04 14:21:24,049   INFO  Train:    1/36 (  3%) [1549/1759 ( 88%)]  Loss: 5.198 (11.3)  LR: 3.248e-04  Grad: 9.7319  max=0.2808(module.backbone_3d.cls_conv.3.weight)  min: -0.2230(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.3565, loss_cls=0.3446, loss_bbox=1.6490, matched_ious=0.3406, loss_iou=0.1075, loss_iou_reg=0.2912, d_time=0.00(0.01), f_time=1.25(1.28), b_time=1.25(1.29)  Time cost: 33:21/04:31 [33:22/22:09:22]  Acc_iter 1550        Data time: 0.00(0.01)  Forward time: 1.25(1.28)  Batch time: 1.25(1.29)
2025-09-04 14:22:27,545   INFO  Train:    1/36 (  3%) [1599/1759 ( 91%)]  Loss: 6.094 (11.2)  LR: 3.265e-04  Grad: 9.8138  max=0.2299(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3574(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.2850, loss_cls=0.3359, loss_bbox=1.5137, matched_ious=0.3617, loss_iou=0.1056, loss_iou_reg=0.2813, d_time=0.00(0.01), f_time=1.27(1.28), b_time=1.27(1.29)  Time cost: 34:24/03:26 [34:26/22:07:36]  Acc_iter 1600        Data time: 0.00(0.01)  Forward time: 1.27(1.28)  Batch time: 1.27(1.29)
2025-09-04 14:23:32,651   INFO  Train:    1/36 (  3%) [1649/1759 ( 94%)]  Loss: 4.663 (11.0)  LR: 3.281e-04  Grad: 9.5460  max=0.3520(module.backbone_3d.cls_conv.3.weight)  min: -0.2272(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2928, loss_cls=0.3382, loss_bbox=1.5535, matched_ious=0.3598, loss_iou=0.1061, loss_iou_reg=0.2855, d_time=0.00(0.01), f_time=1.24(1.28), b_time=1.25(1.29)  Time cost: 35:29/02:21 [35:31/22:06:53]  Acc_iter 1650        Data time: 0.00(0.01)  Forward time: 1.24(1.28)  Batch time: 1.25(1.29)
2025-09-04 14:24:35,905   INFO  Train:    1/36 (  3%) [1699/1759 ( 97%)]  Loss: 6.319 (10.9)  LR: 3.299e-04  Grad: 9.8434  max=0.2750(module.backbone_3d.cls_conv.3.weight)  min: -0.2930(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2942, loss_cls=0.3403, loss_bbox=1.5428, matched_ious=0.3641, loss_iou=0.1074, loss_iou_reg=0.2817, d_time=0.00(0.01), f_time=1.27(1.28), b_time=1.27(1.29)  Time cost: 36:33/01:17 [36:34/22:05:02]  Acc_iter 1700        Data time: 0.00(0.01)  Forward time: 1.27(1.28)  Batch time: 1.27(1.29)
2025-09-04 14:25:38,564   INFO  Train:    1/36 (  3%) [1749/1759 ( 99%)]  Loss: 7.494 (10.8)  LR: 3.316e-04  Grad: 9.4987  max=0.4405(module.vfe.pfn_layers.0.linear.weight)  min: -0.2113(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.3271, loss_cls=0.3361, loss_bbox=1.6463, matched_ious=0.3561, loss_iou=0.1086, loss_iou_reg=0.2830, d_time=0.00(0.01), f_time=1.21(1.28), b_time=1.21(1.29)  Time cost: 37:35/00:12 [37:37/22:02:53]  Acc_iter 1750        Data time: 0.00(0.01)  Forward time: 1.21(1.28)  Batch time: 1.21(1.29)
2025-09-04 14:25:49,282   INFO  Train:    1/36 (  3%) [1758/1759 (100%)]  Loss: 6.837 (10.7)  LR: 3.320e-04  Grad: 10.0000  max=0.6600(module.backbone_3d.cls_conv.3.weight)  min: -0.5646(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.3212, loss_cls=0.3302, loss_bbox=1.6696, matched_ious=0.3541, loss_iou=0.1132, loss_iou_reg=0.2864, d_time=0.00(0.01), f_time=0.75(1.28), b_time=0.76(1.29)  Time cost: 37:46/00:01 [37:48/22:02:10]  Acc_iter 1759        Data time: 0.00(0.01)  Forward time: 0.75(1.28)  Batch time: 0.76(1.29)

                                               [Aepochs:   3%|▎         | 1/36 [37:48<22:03:13, 2268.39s/it]epochs:   3%|▎         | 1/36 [37:48<22:03:16, 2268.46s/it]epochs:   3%|▎         | 1/36 [37:48<22:03:15, 2268.46s/it]epochs:   3%|▎         | 1/36 [37:48<22:03:16, 2268.46s/it]epochs:   3%|▎         | 1/36 [37:48<22:03:16, 2268.47s/it]epochs:   3%|▎         | 1/36 [37:48<22:03:16, 2268.47s/it]epochs:   3%|▎         | 1/36 [37:48<22:03:16, 2268.47s/it]epochs:   3%|▎         | 1/36 [37:48<22:03:27, 2268.78s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 14:25:54,433   INFO  Train:    2/36 (  6%) [   0/1759 (  0%)]  Loss: 5.854 (5.85)  LR: 3.320e-04  Grad: 9.5095  max=0.2194(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2685(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=1.2339, loss_cls=0.3294, loss_bbox=1.4202, matched_ious=0.3442, loss_iou=0.1050, loss_iou_reg=0.3040, d_time=1.16(1.16), f_time=2.90(2.90), b_time=4.06(4.06)  Time cost: 00:03/1:49:02 [37:53/63:36:35]  Acc_iter 1760        Data time: 1.16(1.16)  Forward time: 2.90(2.90)  Batch time: 4.06(4.06)
2025-09-04 14:26:48,574   INFO  Train:    2/36 (  6%) [  40/1759 (  2%)]  Loss: 6.000 (6.10)  LR: 3.335e-04  Grad: 9.6215  max=0.2196(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2235(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=1.2610, loss_cls=0.3214, loss_bbox=1.6298, matched_ious=0.3641, loss_iou=0.1071, loss_iou_reg=0.2774, d_time=0.00(0.06), f_time=1.27(1.36), b_time=1.27(1.42)  Time cost: 00:57/40:25 [38:47/24:07:05]  Acc_iter 1800        Data time: 0.00(0.06)  Forward time: 1.27(1.36)  Batch time: 1.27(1.42)
2025-09-04 14:27:52,024   INFO  Train:    2/36 (  6%) [  90/1759 (  5%)]  Loss: 6.050 (6.09)  LR: 3.353e-04  Grad: 9.7604  max=0.2260(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2312(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2752, loss_cls=0.3313, loss_bbox=1.5137, matched_ious=0.3655, loss_iou=0.1076, loss_iou_reg=0.2833, d_time=0.00(0.03), f_time=1.34(1.31), b_time=1.35(1.34)  Time cost: 02:01/37:04 [39:50/22:45:51]  Acc_iter 1850        Data time: 0.00(0.03)  Forward time: 1.34(1.31)  Batch time: 1.35(1.34)
2025-09-04 14:28:55,196   INFO  Train:    2/36 (  6%) [ 140/1759 (  8%)]  Loss: 6.237 (6.10)  LR: 3.373e-04  Grad: 9.8164  max=0.2251(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3424(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2692, loss_cls=0.3297, loss_bbox=1.5620, matched_ious=0.3562, loss_iou=0.1063, loss_iou_reg=0.2867, d_time=0.00(0.02), f_time=1.26(1.29), b_time=1.26(1.31)  Time cost: 03:04/35:18 [40:53/22:19:27]  Acc_iter 1900        Data time: 0.00(0.02)  Forward time: 1.26(1.29)  Batch time: 1.26(1.31)
2025-09-04 14:29:58,254   INFO  Train:    2/36 (  6%) [ 190/1759 ( 11%)]  Loss: 5.920 (6.07)  LR: 3.393e-04  Grad: 9.8554  max=0.4099(module.backbone_3d.cls_conv.3.weight)  min: -0.2196(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2245, loss_cls=0.3240, loss_bbox=1.5157, matched_ious=0.3680, loss_iou=0.1059, loss_iou_reg=0.2801, d_time=0.00(0.02), f_time=1.29(1.28), b_time=1.30(1.30)  Time cost: 04:07/33:53 [41:57/22:05:42]  Acc_iter 1950        Data time: 0.00(0.02)  Forward time: 1.29(1.28)  Batch time: 1.30(1.30)
2025-09-04 14:31:02,151   INFO  Train:    2/36 (  6%) [ 240/1759 ( 14%)]  Loss: 5.637 (6.05)  LR: 3.413e-04  Grad: 9.8641  max=0.4801(module.vfe.pfn_layers.0.linear.weight)  min: -0.2314(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2562, loss_cls=0.3257, loss_bbox=1.4831, matched_ious=0.3717, loss_iou=0.1079, loss_iou_reg=0.2800, d_time=0.00(0.02), f_time=1.29(1.28), b_time=1.30(1.29)  Time cost: 05:11/32:42 [43:00/22:00:48]  Acc_iter 2000        Data time: 0.00(0.02)  Forward time: 1.29(1.28)  Batch time: 1.30(1.29)
2025-09-04 14:32:06,402   INFO  Train:    2/36 (  6%) [ 290/1759 ( 16%)]  Loss: 6.132 (6.03)  LR: 3.434e-04  Grad: 9.8174  max=0.5040(module.vfe.pfn_layers.0.linear.weight)  min: -0.2376(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2258, loss_cls=0.3255, loss_bbox=1.5072, matched_ious=0.3787, loss_iou=0.1038, loss_iou_reg=0.2773, d_time=0.00(0.01), f_time=1.26(1.28), b_time=1.26(1.29)  Time cost: 06:15/31:36 [44:05/21:58:27]  Acc_iter 2050        Data time: 0.00(0.01)  Forward time: 1.26(1.28)  Batch time: 1.26(1.29)
2025-09-04 14:33:09,593   INFO  Train:    2/36 (  6%) [ 340/1759 ( 19%)]  Loss: 6.269 (6.03)  LR: 3.455e-04  Grad: 9.6372  max=0.2246(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2354(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2833, loss_cls=0.3265, loss_bbox=1.5497, matched_ious=0.3759, loss_iou=0.1055, loss_iou_reg=0.2789, d_time=0.00(0.01), f_time=1.23(1.28), b_time=1.23(1.29)  Time cost: 07:18/30:26 [45:08/21:53:18]  Acc_iter 2100        Data time: 0.00(0.01)  Forward time: 1.23(1.28)  Batch time: 1.23(1.29)
2025-09-04 14:34:13,893   INFO  Train:    2/36 (  6%) [ 390/1759 ( 22%)]  Loss: 7.098 (6.02)  LR: 3.477e-04  Grad: 9.9678  max=0.3269(module.dense_head.prediction_head.height.1.bias)  min: -0.5643(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2424, loss_cls=0.3172, loss_bbox=1.5111, matched_ious=0.3760, loss_iou=0.1071, loss_iou_reg=0.2791, d_time=0.00(0.01), f_time=1.30(1.28), b_time=1.30(1.29)  Time cost: 08:23/29:21 [46:12/21:52:06]  Acc_iter 2150        Data time: 0.00(0.01)  Forward time: 1.30(1.28)  Batch time: 1.30(1.29)
2025-09-04 14:35:17,306   INFO  Train:    2/36 (  6%) [ 440/1759 ( 25%)]  Loss: 6.660 (6.00)  LR: 3.499e-04  Grad: 9.8822  max=0.3481(module.backbone_3d.cls_conv.3.weight)  min: -0.2401(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2052, loss_cls=0.3120, loss_bbox=1.5069, matched_ious=0.3748, loss_iou=0.1092, loss_iou_reg=0.2800, d_time=0.00(0.01), f_time=1.27(1.28), b_time=1.27(1.29)  Time cost: 09:26/28:14 [47:16/21:48:52]  Acc_iter 2200        Data time: 0.00(0.01)  Forward time: 1.27(1.28)  Batch time: 1.27(1.29)
2025-09-04 14:36:20,742   INFO  Train:    2/36 (  6%) [ 490/1759 ( 28%)]  Loss: 7.322 (5.99)  LR: 3.522e-04  Grad: 9.8978  max=0.2353(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2288(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2393, loss_cls=0.3121, loss_bbox=1.4961, matched_ious=0.3642, loss_iou=0.1092, loss_iou_reg=0.2821, d_time=0.00(0.01), f_time=1.16(1.27), b_time=1.16(1.28)  Time cost: 10:30/27:08 [48:19/21:46:08]  Acc_iter 2250        Data time: 0.00(0.01)  Forward time: 1.16(1.27)  Batch time: 1.16(1.28)
2025-09-04 14:37:27,501   INFO  Train:    2/36 (  6%) [ 540/1759 ( 31%)]  Loss: 6.466 (5.98)  LR: 3.545e-04  Grad: 9.8729  max=0.2456(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4369(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2232, loss_cls=0.3162, loss_bbox=1.4695, matched_ious=0.3862, loss_iou=0.1072, loss_iou_reg=0.2750, d_time=0.01(0.01), f_time=1.23(1.28), b_time=1.24(1.29)  Time cost: 11:36/26:10 [49:26/21:49:57]  Acc_iter 2300        Data time: 0.01(0.01)  Forward time: 1.23(1.28)  Batch time: 1.24(1.29)
2025-09-04 14:38:31,006   INFO  Train:    2/36 (  6%) [ 590/1759 ( 34%)]  Loss: 5.408 (5.97)  LR: 3.569e-04  Grad: 9.8395  max=0.4272(module.backbone_3d.cls_conv.3.weight)  min: -0.2111(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2280, loss_cls=0.3173, loss_bbox=1.4693, matched_ious=0.3899, loss_iou=0.1056, loss_iou_reg=0.2738, d_time=0.00(0.01), f_time=1.36(1.28), b_time=1.37(1.29)  Time cost: 12:40/25:03 [50:29/21:47:21]  Acc_iter 2350        Data time: 0.00(0.01)  Forward time: 1.36(1.28)  Batch time: 1.37(1.29)
2025-09-04 14:39:33,913   INFO  Train:    2/36 (  6%) [ 640/1759 ( 36%)]  Loss: 5.628 (5.96)  LR: 3.593e-04  Grad: 9.8652  max=0.2363(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.7260(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2135, loss_cls=0.3139, loss_bbox=1.4280, matched_ious=0.3848, loss_iou=0.1076, loss_iou_reg=0.2740, d_time=0.00(0.01), f_time=1.24(1.28), b_time=1.24(1.28)  Time cost: 13:43/23:57 [51:32/21:44:02]  Acc_iter 2400        Data time: 0.00(0.01)  Forward time: 1.24(1.28)  Batch time: 1.24(1.28)
2025-09-04 14:40:37,926   INFO  Train:    2/36 (  6%) [ 690/1759 ( 39%)]  Loss: 5.235 (5.96)  LR: 3.618e-04  Grad: 9.4575  max=0.2421(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2294(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2194, loss_cls=0.3152, loss_bbox=1.4723, matched_ious=0.3832, loss_iou=0.1061, loss_iou_reg=0.2757, d_time=0.00(0.01), f_time=1.28(1.28), b_time=1.29(1.28)  Time cost: 14:47/22:52 [52:36/21:42:40]  Acc_iter 2450        Data time: 0.00(0.01)  Forward time: 1.28(1.28)  Batch time: 1.29(1.28)
2025-09-04 14:41:42,126   INFO  Train:    2/36 (  6%) [ 740/1759 ( 42%)]  Loss: 6.301 (5.95)  LR: 3.643e-04  Grad: 9.6480  max=0.2793(module.dense_head.prediction_head.height.1.bias)  min: -0.3042(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2155, loss_cls=0.3110, loss_bbox=1.4592, matched_ious=0.3773, loss_iou=0.1041, loss_iou_reg=0.2802, d_time=0.01(0.01), f_time=1.32(1.28), b_time=1.32(1.28)  Time cost: 15:51/21:48 [53:40/21:41:36]  Acc_iter 2500        Data time: 0.01(0.01)  Forward time: 1.32(1.28)  Batch time: 1.32(1.28)
2025-09-04 14:42:45,856   INFO  Train:    2/36 (  6%) [ 790/1759 ( 45%)]  Loss: 5.218 (5.93)  LR: 3.669e-04  Grad: 9.8494  max=0.6173(module.vfe.pfn_layers.0.linear.weight)  min: -0.2921(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=1.1965, loss_cls=0.3025, loss_bbox=1.4334, matched_ious=0.3920, loss_iou=0.1064, loss_iou_reg=0.2682, d_time=0.00(0.01), f_time=1.32(1.28), b_time=1.32(1.28)  Time cost: 16:55/20:43 [54:44/21:39:56]  Acc_iter 2550        Data time: 0.00(0.01)  Forward time: 1.32(1.28)  Batch time: 1.32(1.28)
2025-09-04 14:43:50,239   INFO  Train:    2/36 (  6%) [ 840/1759 ( 48%)]  Loss: 6.270 (5.93)  LR: 3.695e-04  Grad: 9.8009  max=0.2481(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6651(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2055, loss_cls=0.3098, loss_bbox=1.4760, matched_ious=0.3876, loss_iou=0.1032, loss_iou_reg=0.2733, d_time=0.00(0.01), f_time=1.21(1.28), b_time=1.21(1.28)  Time cost: 17:59/19:39 [55:49/21:39:07]  Acc_iter 2600        Data time: 0.00(0.01)  Forward time: 1.21(1.28)  Batch time: 1.21(1.28)
2025-09-04 14:44:53,309   INFO  Train:    2/36 (  6%) [ 890/1759 ( 51%)]  Loss: 4.839 (5.91)  LR: 3.722e-04  Grad: 9.6875  max=0.4801(module.vfe.pfn_layers.0.linear.weight)  min: -0.2358(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1862, loss_cls=0.3082, loss_bbox=1.3920, matched_ious=0.4020, loss_iou=0.1026, loss_iou_reg=0.2669, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.29(1.28)  Time cost: 19:02/18:34 [56:52/21:36:48]  Acc_iter 2650        Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.29(1.28)
2025-09-04 14:45:57,022   INFO  Train:    2/36 (  6%) [ 940/1759 ( 53%)]  Loss: 5.609 (5.91)  LR: 3.749e-04  Grad: 9.5712  max=0.3604(module.vfe.pfn_layers.0.linear.weight)  min: -0.2279(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1952, loss_cls=0.3047, loss_bbox=1.4573, matched_ious=0.3908, loss_iou=0.1055, loss_iou_reg=0.2732, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.29(1.28)  Time cost: 20:06/17:29 [57:55/21:35:17]  Acc_iter 2700        Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.28)
2025-09-04 14:47:00,506   INFO  Train:    2/36 (  6%) [ 990/1759 ( 56%)]  Loss: 5.886 (5.90)  LR: 3.777e-04  Grad: 9.6248  max=0.5595(module.vfe.pfn_layers.0.linear.weight)  min: -0.2284(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1521, loss_cls=0.2986, loss_bbox=1.4389, matched_ious=0.3887, loss_iou=0.1070, loss_iou_reg=0.2715, d_time=0.00(0.01), f_time=1.36(1.27), b_time=1.36(1.28)  Time cost: 21:09/16:25 [58:59/21:33:36]  Acc_iter 2750        Data time: 0.00(0.01)  Forward time: 1.36(1.27)  Batch time: 1.36(1.28)
2025-09-04 14:48:07,842   INFO  Train:    2/36 (  6%) [1040/1759 ( 59%)]  Loss: 5.954 (5.89)  LR: 3.805e-04  Grad: 9.9195  max=0.6503(module.vfe.pfn_layers.0.linear.weight)  min: -1.0985(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1960, loss_cls=0.3082, loss_bbox=1.4909, matched_ious=0.3906, loss_iou=0.1051, loss_iou_reg=0.2718, d_time=0.00(0.01), f_time=1.18(1.28), b_time=1.18(1.28)  Time cost: 22:17/15:23 [1:00:06/21:35:42]  Acc_iter 2800        Data time: 0.00(0.01)  Forward time: 1.18(1.28)  Batch time: 1.18(1.28)
2025-09-04 14:49:11,575   INFO  Train:    2/36 (  6%) [1090/1759 ( 62%)]  Loss: 5.491 (5.88)  LR: 3.834e-04  Grad: 9.8126  max=0.4245(module.vfe.pfn_layers.0.linear.weight)  min: -0.2647(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.1532, loss_cls=0.3060, loss_bbox=1.3578, matched_ious=0.4049, loss_iou=0.1032, loss_iou_reg=0.2650, d_time=0.00(0.01), f_time=1.19(1.28), b_time=1.19(1.28)  Time cost: 23:20/14:19 [1:01:10/21:34:10]  Acc_iter 2850        Data time: 0.00(0.01)  Forward time: 1.19(1.28)  Batch time: 1.19(1.28)
2025-09-04 14:50:15,381   INFO  Train:    2/36 (  6%) [1140/1759 ( 65%)]  Loss: 6.566 (5.87)  LR: 3.863e-04  Grad: 9.8179  max=0.6108(module.vfe.pfn_layers.0.linear.weight)  min: -0.4197(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1830, loss_cls=0.2988, loss_bbox=1.4809, matched_ious=0.3883, loss_iou=0.1047, loss_iou_reg=0.2738, d_time=0.00(0.01), f_time=1.22(1.28), b_time=1.22(1.28)  Time cost: 24:24/13:14 [1:02:14/21:32:45]  Acc_iter 2900        Data time: 0.00(0.01)  Forward time: 1.22(1.28)  Batch time: 1.22(1.28)
2025-09-04 14:51:19,822   INFO  Train:    2/36 (  6%) [1190/1759 ( 68%)]  Loss: 5.563 (5.86)  LR: 3.893e-04  Grad: 9.7373  max=0.6698(module.vfe.pfn_layers.0.linear.weight)  min: -0.6907(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1393, loss_cls=0.2915, loss_bbox=1.4229, matched_ious=0.3932, loss_iou=0.1031, loss_iou_reg=0.2723, d_time=0.00(0.01), f_time=1.26(1.28), b_time=1.26(1.28)  Time cost: 25:29/12:10 [1:03:18/21:31:54]  Acc_iter 2950        Data time: 0.00(0.01)  Forward time: 1.26(1.28)  Batch time: 1.26(1.28)
2025-09-04 14:52:23,299   INFO  Train:    2/36 (  6%) [1240/1759 ( 70%)]  Loss: 5.506 (5.85)  LR: 3.923e-04  Grad: 9.7699  max=0.2623(module.dense_head.prediction_head.height.1.weight)  min: -0.5569(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1087, loss_cls=0.2917, loss_bbox=1.3526, matched_ious=0.4092, loss_iou=0.1065, loss_iou_reg=0.2655, d_time=0.00(0.01), f_time=1.33(1.28), b_time=1.34(1.28)  Time cost: 26:32/11:06 [1:04:22/21:30:15]  Acc_iter 3000        Data time: 0.00(0.01)  Forward time: 1.33(1.28)  Batch time: 1.34(1.28)
2025-09-04 14:53:27,203   INFO  Train:    2/36 (  6%) [1290/1759 ( 73%)]  Loss: 5.842 (5.83)  LR: 3.954e-04  Grad: 9.5182  max=0.5313(module.vfe.pfn_layers.0.linear.weight)  min: -0.2243(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.1442, loss_cls=0.2946, loss_bbox=1.3327, matched_ious=0.3976, loss_iou=0.1046, loss_iou_reg=0.2679, d_time=0.00(0.01), f_time=1.29(1.28), b_time=1.30(1.28)  Time cost: 27:36/10:01 [1:05:25/21:28:59]  Acc_iter 3050        Data time: 0.00(0.01)  Forward time: 1.29(1.28)  Batch time: 1.30(1.28)
2025-09-04 14:54:30,907   INFO  Train:    2/36 (  6%) [1340/1759 ( 76%)]  Loss: 5.861 (5.82)  LR: 3.985e-04  Grad: 9.5739  max=0.9726(module.vfe.pfn_layers.0.linear.weight)  min: -0.9642(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1094, loss_cls=0.2892, loss_bbox=1.3698, matched_ious=0.4093, loss_iou=0.1072, loss_iou_reg=0.2662, d_time=0.00(0.01), f_time=1.32(1.28), b_time=1.33(1.28)  Time cost: 28:40/08:57 [1:06:29/21:27:34]  Acc_iter 3100        Data time: 0.00(0.01)  Forward time: 1.32(1.28)  Batch time: 1.33(1.28)
2025-09-04 14:55:34,259   INFO  Train:    2/36 (  6%) [1390/1759 ( 79%)]  Loss: 5.490 (5.81)  LR: 4.017e-04  Grad: 9.0631  max=1.1921(module.vfe.pfn_layers.0.linear.weight)  min: -0.2327(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0991, loss_cls=0.2891, loss_bbox=1.2875, matched_ious=0.4036, loss_iou=0.1056, loss_iou_reg=0.2731, d_time=0.00(0.01), f_time=1.22(1.28), b_time=1.22(1.28)  Time cost: 29:43/07:53 [1:07:33/21:25:56]  Acc_iter 3150        Data time: 0.00(0.01)  Forward time: 1.22(1.28)  Batch time: 1.22(1.28)
2025-09-04 14:56:38,142   INFO  Train:    2/36 (  6%) [1440/1759 ( 82%)]  Loss: 6.449 (5.79)  LR: 4.049e-04  Grad: 9.2232  max=1.0462(module.vfe.pfn_layers.0.linear.weight)  min: -0.2288(module.vfe.pfn_layers.1.linear.weight)  NaN: False  loss_hm=1.1153, loss_cls=0.2894, loss_bbox=1.3102, matched_ious=0.4126, loss_iou=0.1028, loss_iou_reg=0.2639, d_time=0.02(0.01), f_time=1.35(1.28), b_time=1.36(1.28)  Time cost: 30:47/06:48 [1:08:36/21:24:43]  Acc_iter 3200        Data time: 0.02(0.01)  Forward time: 1.35(1.28)  Batch time: 1.36(1.28)
2025-09-04 14:57:42,291   INFO  Train:    2/36 (  6%) [1490/1759 ( 85%)]  Loss: 5.581 (5.78)  LR: 4.081e-04  Grad: 9.2416  max=0.2736(module.vfe.pfn_layers.0.linear.weight)  min: -0.2161(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.1006, loss_cls=0.2895, loss_bbox=1.3205, matched_ious=0.4083, loss_iou=0.1026, loss_iou_reg=0.2669, d_time=0.00(0.01), f_time=1.34(1.28), b_time=1.35(1.28)  Time cost: 31:51/05:44 [1:09:41/21:23:40]  Acc_iter 3250        Data time: 0.00(0.01)  Forward time: 1.34(1.28)  Batch time: 1.35(1.28)
2025-09-04 14:58:49,106   INFO  Train:    2/36 (  6%) [1540/1759 ( 88%)]  Loss: 5.726 (5.77)  LR: 4.114e-04  Grad: 9.3104  max=0.3911(module.vfe.pfn_layers.0.linear.weight)  min: -0.2241(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1506, loss_cls=0.2911, loss_bbox=1.4262, matched_ious=0.4032, loss_iou=0.1036, loss_iou_reg=0.2672, d_time=0.00(0.01), f_time=1.37(1.28), b_time=1.38(1.28)  Time cost: 32:58/04:41 [1:10:47/21:24:22]  Acc_iter 3300        Data time: 0.00(0.01)  Forward time: 1.37(1.28)  Batch time: 1.38(1.28)
2025-09-04 14:59:52,590   INFO  Train:    2/36 (  6%) [1590/1759 ( 90%)]  Loss: 5.105 (5.76)  LR: 4.148e-04  Grad: 9.9718  max=1.3572(module.vfe.pfn_layers.0.linear.weight)  min: -2.5904(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1259, loss_cls=0.2895, loss_bbox=1.3159, matched_ious=0.4091, loss_iou=0.1060, loss_iou_reg=0.2681, d_time=0.00(0.01), f_time=1.37(1.28), b_time=1.37(1.28)  Time cost: 34:01/03:36 [1:11:51/21:22:51]  Acc_iter 3350        Data time: 0.00(0.01)  Forward time: 1.37(1.28)  Batch time: 1.37(1.28)
2025-09-04 15:00:57,055   INFO  Train:    2/36 (  6%) [1640/1759 ( 93%)]  Loss: 3.941 (5.75)  LR: 4.182e-04  Grad: 9.7009  max=0.2820(module.backbone_3d.cls_conv.3.weight)  min: -1.6101(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1231, loss_cls=0.2857, loss_bbox=1.3443, matched_ious=0.4004, loss_iou=0.1059, loss_iou_reg=0.2697, d_time=0.00(0.01), f_time=1.25(1.28), b_time=1.25(1.28)  Time cost: 35:06/02:32 [1:12:55/21:21:58]  Acc_iter 3400        Data time: 0.00(0.01)  Forward time: 1.25(1.28)  Batch time: 1.25(1.28)
2025-09-04 15:01:59,727   INFO  Train:    2/36 (  6%) [1690/1759 ( 96%)]  Loss: 5.262 (5.75)  LR: 4.217e-04  Grad: 9.5029  max=0.8275(module.vfe.pfn_layers.0.linear.weight)  min: -0.3064(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=1.1215, loss_cls=0.2840, loss_bbox=1.4114, matched_ious=0.4037, loss_iou=0.1042, loss_iou_reg=0.2689, d_time=0.00(0.01), f_time=1.21(1.28), b_time=1.22(1.28)  Time cost: 36:09/01:28 [1:13:58/21:20:00]  Acc_iter 3450        Data time: 0.00(0.01)  Forward time: 1.21(1.28)  Batch time: 1.22(1.28)
2025-09-04 15:03:04,095   INFO  Train:    2/36 (  6%) [1740/1759 ( 99%)]  Loss: 4.960 (5.74)  LR: 4.251e-04  Grad: 7.1443  max=1.2734(module.vfe.pfn_layers.0.linear.weight)  min: -1.3820(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0889, loss_cls=0.2747, loss_bbox=1.3009, matched_ious=0.4048, loss_iou=0.1030, loss_iou_reg=0.2721, d_time=0.00(0.01), f_time=1.29(1.28), b_time=1.29(1.28)  Time cost: 37:13/00:24 [1:15:02/21:19:04]  Acc_iter 3500        Data time: 0.00(0.01)  Forward time: 1.29(1.28)  Batch time: 1.29(1.28)
2025-09-04 15:03:25,699   INFO  Train:    2/36 (  6%) [1758/1759 (100%)]  Loss: 4.740 (5.73)  LR: 4.264e-04  Grad: 8.1703  max=3.6473(module.vfe.pfn_layers.0.linear.weight)  min: -0.8108(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0410, loss_cls=0.2718, loss_bbox=1.2052, matched_ious=0.4370, loss_iou=0.1026, loss_iou_reg=0.2536, d_time=0.00(0.01), f_time=0.69(1.28), b_time=0.69(1.28)  Time cost: 37:34/00:01 [1:15:24/21:17:50]  Acc_iter 3518        Data time: 0.00(0.01)  Forward time: 0.69(1.28)  Batch time: 0.69(1.28)

                                               [Aepochs:   6%|▌         | 2/36 [1:15:24<21:21:27, 2261.39s/it]epochs:   6%|▌         | 2/36 [1:15:24<21:21:26, 2261.38s/it]epochs:   6%|▌         | 2/36 [1:15:24<21:21:27, 2261.38s/it]epochs:   6%|▌         | 2/36 [1:15:24<21:21:28, 2261.43s/it]epochs:   6%|▌         | 2/36 [1:15:24<21:21:28, 2261.44s/it]epochs:   6%|▌         | 2/36 [1:15:24<21:21:28, 2261.44s/it]epochs:   6%|▌         | 2/36 [1:15:24<21:21:28, 2261.44s/it]epochs:   6%|▌         | 2/36 [1:15:25<21:21:33, 2261.58s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 15:03:31,086   INFO  Train:    3/36 (  8%) [   0/1759 (  0%)]  Loss: 4.945 (4.95)  LR: 4.265e-04  Grad: 6.7239  max=0.5504(module.vfe.pfn_layers.0.linear.weight)  min: -0.1626(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.0012, loss_cls=0.2480, loss_bbox=1.0974, matched_ious=0.4532, loss_iou=0.1086, loss_iou_reg=0.2444, d_time=1.35(1.35), f_time=2.84(2.84), b_time=4.19(4.19)  Time cost: 00:03/1:49:26 [1:15:29/62:00:54]  Acc_iter 3519        Data time: 1.35(1.35)  Forward time: 2.84(2.84)  Batch time: 4.19(4.19)
2025-09-04 15:04:10,903   INFO  Train:    3/36 (  8%) [  31/1759 (  2%)]  Loss: 4.686 (5.41)  LR: 4.287e-04  Grad: 7.1720  max=0.2332(module.vfe.pfn_layers.0.linear.weight)  min: -1.4825(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0970, loss_cls=0.2871, loss_bbox=1.3290, matched_ious=0.4017, loss_iou=0.1039, loss_iou_reg=0.2719, d_time=0.01(0.05), f_time=1.20(1.33), b_time=1.21(1.38)  Time cost: 00:43/39:11 [1:16:09/22:35:44]  Acc_iter 3550        Data time: 0.01(0.05)  Forward time: 1.20(1.33)  Batch time: 1.21(1.38)
2025-09-04 15:05:14,448   INFO  Train:    3/36 (  8%) [  81/1759 (  5%)]  Loss: 6.241 (5.38)  LR: 4.323e-04  Grad: 8.0896  max=0.1997(module.vfe.pfn_layers.0.linear.weight)  min: -2.4595(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0733, loss_cls=0.2765, loss_bbox=1.3103, matched_ious=0.4146, loss_iou=0.1025, loss_iou_reg=0.2647, d_time=0.00(0.02), f_time=1.21(1.29), b_time=1.21(1.31)  Time cost: 01:47/36:31 [1:17:13/21:40:03]  Acc_iter 3600        Data time: 0.00(0.02)  Forward time: 1.21(1.29)  Batch time: 1.21(1.31)
2025-09-04 15:06:17,363   INFO  Train:    3/36 (  8%) [ 131/1759 (  7%)]  Loss: 4.927 (5.41)  LR: 4.359e-04  Grad: 8.0251  max=1.3401(module.vfe.pfn_layers.0.linear.weight)  min: -2.3434(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0987, loss_cls=0.2731, loss_bbox=1.3638, matched_ious=0.4094, loss_iou=0.1056, loss_iou_reg=0.2667, d_time=0.00(0.02), f_time=1.17(1.28), b_time=1.18(1.29)  Time cost: 02:50/34:56 [1:18:16/21:20:58]  Acc_iter 3650        Data time: 0.00(0.02)  Forward time: 1.17(1.28)  Batch time: 1.18(1.29)
2025-09-04 15:07:21,040   INFO  Train:    3/36 (  8%) [ 181/1759 ( 10%)]  Loss: 5.238 (5.38)  LR: 4.396e-04  Grad: 7.8505  max=0.6173(module.vfe.pfn_layers.0.linear.weight)  min: -2.1408(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0586, loss_cls=0.2660, loss_bbox=1.3397, matched_ious=0.4182, loss_iou=0.1027, loss_iou_reg=0.2622, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.29)  Time cost: 03:53/33:46 [1:19:19/21:15:58]  Acc_iter 3700        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.29)
2025-09-04 15:08:25,000   INFO  Train:    3/36 (  8%) [ 231/1759 ( 13%)]  Loss: 5.146 (5.37)  LR: 4.433e-04  Grad: 9.0569  max=4.7946(module.vfe.pfn_layers.0.linear.weight)  min: -0.2520(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0829, loss_cls=0.2759, loss_bbox=1.3016, matched_ious=0.4207, loss_iou=0.1027, loss_iou_reg=0.2626, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.28)  Time cost: 04:57/32:40 [1:20:23/21:13:52]  Acc_iter 3750        Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.28)
2025-09-04 15:09:32,652   INFO  Train:    3/36 (  8%) [ 281/1759 ( 16%)]  Loss: 4.870 (5.35)  LR: 4.471e-04  Grad: 8.2694  max=1.6027(module.vfe.pfn_layers.0.linear.weight)  min: -2.1333(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0750, loss_cls=0.2725, loss_bbox=1.2695, matched_ious=0.4118, loss_iou=0.1048, loss_iou_reg=0.2676, d_time=0.00(0.01), f_time=1.25(1.28), b_time=1.25(1.30)  Time cost: 06:05/31:54 [1:21:31/21:25:07]  Acc_iter 3800        Data time: 0.00(0.01)  Forward time: 1.25(1.28)  Batch time: 1.25(1.30)
2025-09-04 15:10:36,480   INFO  Train:    3/36 (  8%) [ 331/1759 ( 19%)]  Loss: 5.138 (5.33)  LR: 4.509e-04  Grad: 8.8716  max=2.9178(module.vfe.pfn_layers.0.linear.weight)  min: -0.8653(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0503, loss_cls=0.2710, loss_bbox=1.1969, matched_ious=0.4164, loss_iou=0.1029, loss_iou_reg=0.2682, d_time=0.00(0.01), f_time=1.19(1.28), b_time=1.19(1.29)  Time cost: 07:09/30:45 [1:22:35/21:21:14]  Acc_iter 3850        Data time: 0.00(0.01)  Forward time: 1.19(1.28)  Batch time: 1.19(1.29)
2025-09-04 15:11:39,925   INFO  Train:    3/36 (  8%) [ 381/1759 ( 22%)]  Loss: 4.791 (5.32)  LR: 4.548e-04  Grad: 8.8240  max=2.1878(module.vfe.pfn_layers.0.linear.weight)  min: -3.3752(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0682, loss_cls=0.2741, loss_bbox=1.2807, matched_ious=0.4122, loss_iou=0.1042, loss_iou_reg=0.2682, d_time=0.00(0.01), f_time=1.19(1.28), b_time=1.20(1.29)  Time cost: 08:12/29:36 [1:23:38/21:17:05]  Acc_iter 3900        Data time: 0.00(0.01)  Forward time: 1.19(1.28)  Batch time: 1.20(1.29)
2025-09-04 15:12:44,044   INFO  Train:    3/36 (  8%) [ 431/1759 ( 25%)]  Loss: 4.811 (5.31)  LR: 4.587e-04  Grad: 7.1831  max=1.7140(module.vfe.pfn_layers.0.linear.weight)  min: -0.1533(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.0148, loss_cls=0.2632, loss_bbox=1.2762, matched_ious=0.4233, loss_iou=0.1042, loss_iou_reg=0.2629, d_time=0.00(0.01), f_time=1.26(1.28), b_time=1.27(1.29)  Time cost: 09:16/28:31 [1:24:42/21:15:12]  Acc_iter 3950        Data time: 0.00(0.01)  Forward time: 1.26(1.28)  Batch time: 1.27(1.29)
2025-09-04 15:13:47,737   INFO  Train:    3/36 (  8%) [ 481/1759 ( 27%)]  Loss: 5.656 (5.30)  LR: 4.627e-04  Grad: 7.0788  max=2.3536(module.vfe.pfn_layers.0.linear.weight)  min: -1.7712(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0573, loss_cls=0.2680, loss_bbox=1.2731, matched_ious=0.4143, loss_iou=0.1022, loss_iou_reg=0.2659, d_time=0.00(0.01), f_time=1.31(1.28), b_time=1.31(1.29)  Time cost: 10:20/27:24 [1:25:46/21:12:37]  Acc_iter 4000        Data time: 0.00(0.01)  Forward time: 1.31(1.28)  Batch time: 1.31(1.29)
2025-09-04 15:14:50,630   INFO  Train:    3/36 (  8%) [ 531/1759 ( 30%)]  Loss: 5.080 (5.29)  LR: 4.667e-04  Grad: 7.5845  max=1.7893(module.vfe.pfn_layers.0.linear.weight)  min: -2.6935(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0405, loss_cls=0.2638, loss_bbox=1.2533, matched_ious=0.4172, loss_iou=0.1035, loss_iou_reg=0.2678, d_time=0.00(0.01), f_time=1.23(1.28), b_time=1.23(1.29)  Time cost: 11:23/26:17 [1:26:49/21:08:50]  Acc_iter 4050        Data time: 0.00(0.01)  Forward time: 1.23(1.28)  Batch time: 1.23(1.29)
2025-09-04 15:15:53,626   INFO  Train:    3/36 (  8%) [ 581/1759 ( 33%)]  Loss: 5.376 (5.29)  LR: 4.707e-04  Grad: 7.9270  max=4.2172(module.vfe.pfn_layers.0.linear.weight)  min: -0.7446(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0424, loss_cls=0.2626, loss_bbox=1.2499, matched_ious=0.4134, loss_iou=0.1060, loss_iou_reg=0.2665, d_time=0.00(0.01), f_time=1.17(1.27), b_time=1.17(1.28)  Time cost: 12:26/25:10 [1:27:52/21:05:41]  Acc_iter 4100        Data time: 0.00(0.01)  Forward time: 1.17(1.27)  Batch time: 1.17(1.28)
2025-09-04 15:16:57,391   INFO  Train:    3/36 (  8%) [ 631/1759 ( 36%)]  Loss: 4.931 (5.28)  LR: 4.748e-04  Grad: 6.6569  max=0.4924(module.vfe.pfn_layers.0.linear.weight)  min: -2.5267(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0684, loss_cls=0.2660, loss_bbox=1.2754, matched_ious=0.4177, loss_iou=0.1069, loss_iou_reg=0.2650, d_time=0.00(0.01), f_time=1.32(1.27), b_time=1.32(1.28)  Time cost: 13:30/24:05 [1:28:56/21:04:04]  Acc_iter 4150        Data time: 0.00(0.01)  Forward time: 1.32(1.27)  Batch time: 1.32(1.28)
2025-09-04 15:18:01,229   INFO  Train:    3/36 (  8%) [ 681/1759 ( 39%)]  Loss: 5.120 (5.27)  LR: 4.790e-04  Grad: 6.7630  max=1.5937(module.vfe.pfn_layers.0.linear.weight)  min: -2.4560(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0281, loss_cls=0.2590, loss_bbox=1.2215, matched_ious=0.4155, loss_iou=0.1038, loss_iou_reg=0.2651, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.32(1.28)  Time cost: 14:33/23:01 [1:29:59/21:02:39]  Acc_iter 4200        Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.32(1.28)
2025-09-04 15:19:05,228   INFO  Train:    3/36 (  8%) [ 731/1759 ( 42%)]  Loss: 5.950 (5.27)  LR: 4.832e-04  Grad: 5.9362  max=1.6206(module.vfe.pfn_layers.0.linear.weight)  min: -0.3108(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0016, loss_cls=0.2568, loss_bbox=1.3040, matched_ious=0.4141, loss_iou=0.1046, loss_iou_reg=0.2658, d_time=0.01(0.01), f_time=1.24(1.27), b_time=1.24(1.28)  Time cost: 15:37/21:57 [1:31:03/21:01:29]  Acc_iter 4250        Data time: 0.01(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.28)
2025-09-04 15:20:12,217   INFO  Train:    3/36 (  8%) [ 781/1759 ( 44%)]  Loss: 5.336 (5.27)  LR: 4.874e-04  Grad: 5.2783  max=1.2832(module.vfe.pfn_layers.0.linear.weight)  min: -0.2711(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0765, loss_cls=0.2630, loss_bbox=1.2384, matched_ious=0.4187, loss_iou=0.1013, loss_iou_reg=0.2656, d_time=0.00(0.01), f_time=1.34(1.28), b_time=1.35(1.29)  Time cost: 16:44/20:56 [1:32:10/21:04:06]  Acc_iter 4300        Data time: 0.00(0.01)  Forward time: 1.34(1.28)  Batch time: 1.35(1.29)
2025-09-04 15:21:15,330   INFO  Train:    3/36 (  8%) [ 831/1759 ( 47%)]  Loss: 5.456 (5.26)  LR: 4.917e-04  Grad: 6.8815  max=3.4397(module.vfe.pfn_layers.0.linear.weight)  min: -1.8477(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0118, loss_cls=0.2569, loss_bbox=1.2697, matched_ious=0.4252, loss_iou=0.1038, loss_iou_reg=0.2611, d_time=0.00(0.01), f_time=1.20(1.28), b_time=1.21(1.28)  Time cost: 17:47/19:51 [1:33:14/21:01:41]  Acc_iter 4350        Data time: 0.00(0.01)  Forward time: 1.20(1.28)  Batch time: 1.21(1.28)
2025-09-04 15:22:19,165   INFO  Train:    3/36 (  8%) [ 881/1759 ( 50%)]  Loss: 5.362 (5.25)  LR: 4.960e-04  Grad: 6.2421  max=1.0535(module.vfe.pfn_layers.0.linear.weight)  min: -2.3521(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0402, loss_cls=0.2599, loss_bbox=1.2873, matched_ious=0.4221, loss_iou=0.1014, loss_iou_reg=0.2622, d_time=0.00(0.01), f_time=1.25(1.28), b_time=1.25(1.28)  Time cost: 18:51/18:46 [1:34:17/21:00:14]  Acc_iter 4400        Data time: 0.00(0.01)  Forward time: 1.25(1.28)  Batch time: 1.25(1.28)
2025-09-04 15:23:23,031   INFO  Train:    3/36 (  8%) [ 931/1759 ( 53%)]  Loss: 5.355 (5.25)  LR: 5.004e-04  Grad: 6.0250  max=0.6501(module.vfe.pfn_layers.0.linear.weight)  min: -2.7976(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0418, loss_cls=0.2628, loss_bbox=1.2003, matched_ious=0.4291, loss_iou=0.1042, loss_iou_reg=0.2618, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.31(1.28)  Time cost: 19:55/17:42 [1:35:21/20:58:51]  Acc_iter 4450        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.31(1.28)
2025-09-04 15:24:25,869   INFO  Train:    3/36 (  8%) [ 981/1759 ( 56%)]  Loss: 5.615 (5.24)  LR: 5.048e-04  Grad: 9.3287  max=2.4991(module.vfe.pfn_layers.0.linear.weight)  min: -6.2521(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0364, loss_cls=0.2565, loss_bbox=1.2155, matched_ious=0.4306, loss_iou=0.1038, loss_iou_reg=0.2607, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.21(1.28)  Time cost: 20:58/16:37 [1:36:24/20:56:29]  Acc_iter 4500        Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.28)
2025-09-04 15:25:28,942   INFO  Train:    3/36 (  8%) [1031/1759 ( 59%)]  Loss: 5.023 (5.23)  LR: 5.092e-04  Grad: 5.5256  max=1.2188(module.vfe.pfn_layers.0.linear.weight)  min: -2.2487(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0227, loss_cls=0.2530, loss_bbox=1.2207, matched_ious=0.4245, loss_iou=0.1043, loss_iou_reg=0.2619, d_time=0.02(0.01), f_time=1.28(1.27), b_time=1.30(1.28)  Time cost: 22:01/15:32 [1:37:27/20:54:27]  Acc_iter 4550        Data time: 0.02(0.01)  Forward time: 1.28(1.27)  Batch time: 1.30(1.28)
2025-09-04 15:26:32,520   INFO  Train:    3/36 (  8%) [1081/1759 ( 61%)]  Loss: 5.082 (5.23)  LR: 5.137e-04  Grad: 6.7325  max=3.7167(module.vfe.pfn_layers.0.linear.weight)  min: -0.2604(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.0286, loss_cls=0.2507, loss_bbox=1.2603, matched_ious=0.4281, loss_iou=0.1014, loss_iou_reg=0.2634, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.31(1.28)  Time cost: 23:05/14:27 [1:38:31/20:52:58]  Acc_iter 4600        Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.31(1.28)
2025-09-04 15:27:36,112   INFO  Train:    3/36 (  8%) [1131/1759 ( 64%)]  Loss: 4.803 (5.22)  LR: 5.183e-04  Grad: 5.1750  max=0.4912(module.vfe.pfn_layers.0.linear.weight)  min: -0.8766(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0128, loss_cls=0.2490, loss_bbox=1.2413, matched_ious=0.4294, loss_iou=0.1040, loss_iou_reg=0.2584, d_time=0.00(0.01), f_time=1.34(1.27), b_time=1.34(1.28)  Time cost: 24:08/13:23 [1:39:34/20:51:33]  Acc_iter 4650        Data time: 0.00(0.01)  Forward time: 1.34(1.27)  Batch time: 1.34(1.28)
2025-09-04 15:28:40,637   INFO  Train:    3/36 (  8%) [1181/1759 ( 67%)]  Loss: 4.943 (5.22)  LR: 5.229e-04  Grad: 5.4640  max=1.7702(module.vfe.pfn_layers.0.linear.weight)  min: -1.7493(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0281, loss_cls=0.2575, loss_bbox=1.2380, matched_ious=0.4287, loss_iou=0.1050, loss_iou_reg=0.2601, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.30(1.28)  Time cost: 25:13/12:19 [1:40:39/20:50:56]  Acc_iter 4700        Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.30(1.28)
2025-09-04 15:29:44,198   INFO  Train:    3/36 (  8%) [1231/1759 ( 70%)]  Loss: 5.326 (5.22)  LR: 5.275e-04  Grad: 5.1755  max=1.6430(module.vfe.pfn_layers.0.linear.weight)  min: -0.8537(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0041, loss_cls=0.2469, loss_bbox=1.2363, matched_ious=0.4351, loss_iou=0.1026, loss_iou_reg=0.2573, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.27(1.28)  Time cost: 26:16/11:15 [1:41:42/20:49:30]  Acc_iter 4750        Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.28)
2025-09-04 15:30:51,486   INFO  Train:    3/36 (  8%) [1281/1759 ( 73%)]  Loss: 5.034 (5.21)  LR: 5.322e-04  Grad: 4.9096  max=1.3772(module.vfe.pfn_layers.0.linear.weight)  min: -0.7475(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0328, loss_cls=0.2502, loss_bbox=1.2850, matched_ious=0.4283, loss_iou=0.1008, loss_iou_reg=0.2577, d_time=0.00(0.01), f_time=1.35(1.28), b_time=1.35(1.28)  Time cost: 27:24/10:13 [1:42:50/20:50:56]  Acc_iter 4800        Data time: 0.00(0.01)  Forward time: 1.35(1.28)  Batch time: 1.35(1.28)
2025-09-04 15:31:54,688   INFO  Train:    3/36 (  8%) [1331/1759 ( 76%)]  Loss: 5.456 (5.22)  LR: 5.369e-04  Grad: 7.8544  max=4.3613(module.vfe.pfn_layers.0.linear.weight)  min: -3.7903(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0351, loss_cls=0.2488, loss_bbox=1.2743, matched_ious=0.4187, loss_iou=0.1036, loss_iou_reg=0.2644, d_time=0.01(0.01), f_time=1.35(1.27), b_time=1.36(1.28)  Time cost: 28:27/09:08 [1:43:53/20:49:12]  Acc_iter 4850        Data time: 0.01(0.01)  Forward time: 1.35(1.27)  Batch time: 1.36(1.28)
2025-09-04 15:32:59,223   INFO  Train:    3/36 (  8%) [1381/1759 ( 79%)]  Loss: 5.242 (5.22)  LR: 5.416e-04  Grad: 4.9898  max=2.3081(module.vfe.pfn_layers.0.linear.weight)  min: -0.9893(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0405, loss_cls=0.2508, loss_bbox=1.2882, matched_ious=0.4314, loss_iou=0.1020, loss_iou_reg=0.2576, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.19(1.28)  Time cost: 29:31/08:04 [1:44:57/20:48:25]  Acc_iter 4900        Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.19(1.28)
2025-09-04 15:34:02,242   INFO  Train:    3/36 (  8%) [1431/1759 ( 81%)]  Loss: 4.618 (5.21)  LR: 5.464e-04  Grad: 8.7221  max=0.3620(module.vfe.pfn_layers.0.linear.weight)  min: -5.9267(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9749, loss_cls=0.2433, loss_bbox=1.1748, matched_ious=0.4287, loss_iou=0.1013, loss_iou_reg=0.2607, d_time=0.01(0.01), f_time=1.22(1.27), b_time=1.23(1.28)  Time cost: 30:34/07:00 [1:46:01/20:46:38]  Acc_iter 4950        Data time: 0.01(0.01)  Forward time: 1.22(1.27)  Batch time: 1.23(1.28)
2025-09-04 15:35:05,639   INFO  Train:    3/36 (  8%) [1481/1759 ( 84%)]  Loss: 4.885 (5.20)  LR: 5.513e-04  Grad: 2.8876  max=1.2146(module.vfe.pfn_layers.0.linear.weight)  min: -0.4122(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0030, loss_cls=0.2495, loss_bbox=1.1946, matched_ious=0.4324, loss_iou=0.1039, loss_iou_reg=0.2584, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.22(1.28)  Time cost: 31:38/05:56 [1:47:04/20:45:08]  Acc_iter 5000        Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.22(1.28)
2025-09-04 15:36:09,418   INFO  Train:    3/36 (  8%) [1531/1759 ( 87%)]  Loss: 4.549 (5.19)  LR: 5.562e-04  Grad: 4.2314  max=2.1916(module.vfe.pfn_layers.0.linear.weight)  min: -0.3915(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0134, loss_cls=0.2503, loss_bbox=1.1625, matched_ious=0.4349, loss_iou=0.1038, loss_iou_reg=0.2595, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.28)  Time cost: 32:42/04:52 [1:48:08/20:43:54]  Acc_iter 5050        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.28)
2025-09-04 15:37:12,642   INFO  Train:    3/36 (  8%) [1581/1759 ( 90%)]  Loss: 4.977 (5.18)  LR: 5.611e-04  Grad: 4.3340  max=2.5430(module.vfe.pfn_layers.0.linear.weight)  min: -1.1708(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9964, loss_cls=0.2450, loss_bbox=1.1812, matched_ious=0.4363, loss_iou=0.1010, loss_iou_reg=0.2546, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.28(1.28)  Time cost: 33:45/03:47 [1:49:11/20:42:20]  Acc_iter 5100        Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.28(1.28)
2025-09-04 15:38:16,111   INFO  Train:    3/36 (  8%) [1631/1759 ( 93%)]  Loss: 4.499 (5.18)  LR: 5.661e-04  Grad: 2.5255  max=0.4017(module.vfe.pfn_layers.0.linear.weight)  min: -0.6710(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9911, loss_cls=0.2446, loss_bbox=1.1879, matched_ious=0.4318, loss_iou=0.1042, loss_iou_reg=0.2596, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.28)  Time cost: 34:48/02:43 [1:50:14/20:40:56]  Acc_iter 5150        Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.28)
2025-09-04 15:39:19,911   INFO  Train:    3/36 (  8%) [1681/1759 ( 96%)]  Loss: 5.065 (5.17)  LR: 5.711e-04  Grad: 3.1006  max=1.4565(module.vfe.pfn_layers.0.linear.weight)  min: -0.2077(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9867, loss_cls=0.2434, loss_bbox=1.1609, matched_ious=0.4390, loss_iou=0.1018, loss_iou_reg=0.2570, d_time=0.00(0.01), f_time=1.33(1.27), b_time=1.33(1.28)  Time cost: 35:52/01:39 [1:51:18/20:39:46]  Acc_iter 5200        Data time: 0.00(0.01)  Forward time: 1.33(1.27)  Batch time: 1.33(1.28)
2025-09-04 15:40:23,291   INFO  Train:    3/36 (  8%) [1731/1759 ( 98%)]  Loss: 4.910 (5.16)  LR: 5.761e-04  Grad: 4.1129  max=3.0022(module.vfe.pfn_layers.0.linear.weight)  min: -0.4054(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9818, loss_cls=0.2395, loss_bbox=1.1306, matched_ious=0.4348, loss_iou=0.1019, loss_iou_reg=0.2596, d_time=0.00(0.01), f_time=1.20(1.27), b_time=1.20(1.28)  Time cost: 36:55/00:35 [1:52:22/20:38:21]  Acc_iter 5250        Data time: 0.00(0.01)  Forward time: 1.20(1.27)  Batch time: 1.20(1.28)
2025-09-04 15:41:00,176   INFO  Train:    3/36 (  8%) [1758/1759 (100%)]  Loss: 4.818 (5.16)  LR: 5.789e-04  Grad: 3.1010  max=1.1356(module.vfe.pfn_layers.0.linear.weight)  min: -0.4943(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0032, loss_cls=0.2454, loss_bbox=1.1611, matched_ious=0.4291, loss_iou=0.1023, loss_iou_reg=0.2651, d_time=0.00(0.01), f_time=0.71(1.27), b_time=0.72(1.28)  Time cost: 37:32/00:01 [1:52:58/20:39:04]  Acc_iter 5277        Data time: 0.00(0.01)  Forward time: 0.71(1.27)  Batch time: 0.72(1.28)

                                               [Aepochs:   8%|▊         | 3/36 [1:52:59<20:42:01, 2258.23s/it]epochs:   8%|▊         | 3/36 [1:52:59<20:42:01, 2258.23s/it]epochs:   8%|▊         | 3/36 [1:52:59<20:42:02, 2258.26s/it]epochs:   8%|▊         | 3/36 [1:52:59<20:42:02, 2258.25s/it]epochs:   8%|▊         | 3/36 [1:52:59<20:42:02, 2258.25s/it]epochs:   8%|▊         | 3/36 [1:52:59<20:42:02, 2258.25s/it]epochs:   8%|▊         | 3/36 [1:52:59<20:42:02, 2258.26s/it]epochs:   8%|▊         | 3/36 [1:52:59<20:42:04, 2258.32s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 15:41:05,592   INFO  Train:    4/36 ( 11%) [   0/1759 (  0%)]  Loss: 4.909 (4.91)  LR: 5.790e-04  Grad: 5.1089  max=0.1057(module.vfe.pfn_layers.0.linear.weight)  min: -3.3202(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9893, loss_cls=0.2533, loss_bbox=1.2572, matched_ious=0.4384, loss_iou=0.1034, loss_iou_reg=0.2498, d_time=1.53(1.53), f_time=2.66(2.66), b_time=4.19(4.19)  Time cost: 00:03/1:52:09 [1:53:04/61:41:15]  Acc_iter 5278        Data time: 1.53(1.53)  Forward time: 2.66(2.66)  Batch time: 4.19(4.19)
2025-09-04 15:41:33,601   INFO  Train:    4/36 ( 11%) [  22/1759 (  1%)]  Loss: 5.151 (4.80)  LR: 5.812e-04  Grad: 4.8913  max=1.1913(module.vfe.pfn_layers.0.linear.weight)  min: -2.4126(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9968, loss_cls=0.2476, loss_bbox=1.0791, matched_ious=0.4443, loss_iou=0.1030, loss_iou_reg=0.2593, d_time=0.00(0.07), f_time=1.39(1.33), b_time=1.39(1.40)  Time cost: 00:31/40:04 [1:53:32/22:18:41]  Acc_iter 5300        Data time: 0.00(0.07)  Forward time: 1.39(1.33)  Batch time: 1.39(1.40)
2025-09-04 15:42:38,184   INFO  Train:    4/36 ( 11%) [  72/1759 (  4%)]  Loss: 4.792 (4.94)  LR: 5.864e-04  Grad: 3.8162  max=0.1405(module.vfe.pfn_layers.0.linear.weight)  min: -1.8440(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9788, loss_cls=0.2381, loss_bbox=1.2421, matched_ious=0.4184, loss_iou=0.1024, loss_iou_reg=0.2660, d_time=0.00(0.04), f_time=1.36(1.29), b_time=1.36(1.33)  Time cost: 01:36/37:08 [1:54:36/21:16:16]  Acc_iter 5350        Data time: 0.00(0.04)  Forward time: 1.36(1.29)  Batch time: 1.36(1.33)
2025-09-04 15:43:41,560   INFO  Train:    4/36 ( 11%) [ 122/1759 (  7%)]  Loss: 6.481 (4.97)  LR: 5.915e-04  Grad: 3.9424  max=2.7295(module.vfe.pfn_layers.0.linear.weight)  min: -0.2889(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0090, loss_cls=0.2439, loss_bbox=1.1746, matched_ious=0.4352, loss_iou=0.1022, loss_iou_reg=0.2596, d_time=0.01(0.02), f_time=1.27(1.28), b_time=1.28(1.30)  Time cost: 02:39/35:26 [1:55:40/20:54:14]  Acc_iter 5400        Data time: 0.01(0.02)  Forward time: 1.27(1.28)  Batch time: 1.28(1.30)
2025-09-04 15:44:45,691   INFO  Train:    4/36 ( 11%) [ 172/1759 ( 10%)]  Loss: 5.249 (4.97)  LR: 5.968e-04  Grad: 2.8501  max=1.1379(module.vfe.pfn_layers.0.linear.weight)  min: -0.2520(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9823, loss_cls=0.2398, loss_bbox=1.1785, matched_ious=0.4369, loss_iou=0.1029, loss_iou_reg=0.2586, d_time=0.01(0.02), f_time=1.25(1.28), b_time=1.25(1.30)  Time cost: 03:43/34:14 [1:56:44/20:48:31]  Acc_iter 5450        Data time: 0.01(0.02)  Forward time: 1.25(1.28)  Batch time: 1.25(1.30)
2025-09-04 15:45:49,426   INFO  Train:    4/36 ( 11%) [ 222/1759 ( 13%)]  Loss: 4.674 (4.98)  LR: 6.020e-04  Grad: 4.3365  max=1.2645(module.vfe.pfn_layers.0.linear.weight)  min: -2.9388(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0169, loss_cls=0.2471, loss_bbox=1.1687, matched_ious=0.4399, loss_iou=0.1032, loss_iou_reg=0.2560, d_time=0.02(0.02), f_time=1.32(1.28), b_time=1.34(1.29)  Time cost: 04:47/33:02 [1:57:48/20:43:12]  Acc_iter 5500        Data time: 0.02(0.02)  Forward time: 1.32(1.28)  Batch time: 1.34(1.29)
2025-09-04 15:46:53,245   INFO  Train:    4/36 ( 11%) [ 272/1759 ( 15%)]  Loss: 4.961 (4.98)  LR: 6.073e-04  Grad: 7.6561  max=0.2821(module.vfe.pfn_layers.0.linear.weight)  min: -5.3626(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9813, loss_cls=0.2386, loss_bbox=1.2129, matched_ious=0.4387, loss_iou=0.1019, loss_iou_reg=0.2570, d_time=0.00(0.01), f_time=1.34(1.27), b_time=1.34(1.29)  Time cost: 05:51/31:54 [1:58:52/20:39:44]  Acc_iter 5550        Data time: 0.00(0.01)  Forward time: 1.34(1.27)  Batch time: 1.34(1.29)
2025-09-04 15:47:56,734   INFO  Train:    4/36 ( 11%) [ 322/1759 ( 18%)]  Loss: 5.042 (4.99)  LR: 6.127e-04  Grad: 3.2287  max=0.7051(module.vfe.pfn_layers.0.linear.weight)  min: -1.5401(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9945, loss_cls=0.2372, loss_bbox=1.2207, matched_ious=0.4270, loss_iou=0.1039, loss_iou_reg=0.2631, d_time=0.00(0.01), f_time=1.20(1.27), b_time=1.20(1.29)  Time cost: 06:54/30:46 [1:59:55/20:36:01]  Acc_iter 5600        Data time: 0.00(0.01)  Forward time: 1.20(1.27)  Batch time: 1.20(1.29)
2025-09-04 15:48:59,982   INFO  Train:    4/36 ( 11%) [ 372/1759 ( 21%)]  Loss: 5.032 (4.98)  LR: 6.180e-04  Grad: 4.8530  max=0.1845(module.vfe.pfn_layers.0.linear.weight)  min: -2.6241(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0061, loss_cls=0.2402, loss_bbox=1.1739, matched_ious=0.4347, loss_iou=0.1030, loss_iou_reg=0.2595, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.24(1.28)  Time cost: 07:58/29:38 [2:00:58/20:32:24]  Acc_iter 5650        Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.28)
2025-09-04 15:50:04,033   INFO  Train:    4/36 ( 11%) [ 422/1759 ( 24%)]  Loss: 5.028 (4.97)  LR: 6.234e-04  Grad: 6.8990  max=0.6013(module.vfe.pfn_layers.0.linear.weight)  min: -5.3925(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9733, loss_cls=0.2362, loss_bbox=1.2053, matched_ious=0.4342, loss_iou=0.1031, loss_iou_reg=0.2591, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.28)  Time cost: 09:02/28:33 [2:02:02/20:31:13]  Acc_iter 5700        Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.28)
2025-09-04 15:51:07,103   INFO  Train:    4/36 ( 11%) [ 472/1759 ( 27%)]  Loss: 5.303 (4.96)  LR: 6.289e-04  Grad: 4.5063  max=2.9965(module.vfe.pfn_layers.0.linear.weight)  min: -1.1570(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9586, loss_cls=0.2319, loss_bbox=1.1530, matched_ious=0.4371, loss_iou=0.1033, loss_iou_reg=0.2597, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.26(1.28)  Time cost: 10:05/27:27 [2:03:05/20:28:03]  Acc_iter 5750        Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.28)
2025-09-04 15:52:13,792   INFO  Train:    4/36 ( 11%) [ 522/1759 ( 30%)]  Loss: 4.621 (4.95)  LR: 6.344e-04  Grad: 5.4265  max=3.6222(module.vfe.pfn_layers.0.linear.weight)  min: -1.1707(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9732, loss_cls=0.2330, loss_bbox=1.1064, matched_ious=0.4461, loss_iou=0.1023, loss_iou_reg=0.2529, d_time=0.00(0.01), f_time=1.18(1.27), b_time=1.18(1.29)  Time cost: 11:12/26:29 [2:04:12/20:31:56]  Acc_iter 5800        Data time: 0.00(0.01)  Forward time: 1.18(1.27)  Batch time: 1.18(1.29)
2025-09-04 15:53:17,009   INFO  Train:    4/36 ( 11%) [ 572/1759 ( 33%)]  Loss: 5.679 (4.94)  LR: 6.399e-04  Grad: 5.9593  max=1.1392(module.vfe.pfn_layers.0.linear.weight)  min: -3.1776(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9614, loss_cls=0.2280, loss_bbox=1.1407, matched_ious=0.4344, loss_iou=0.1007, loss_iou_reg=0.2606, d_time=0.01(0.01), f_time=1.24(1.27), b_time=1.25(1.28)  Time cost: 12:15/25:23 [2:05:15/20:29:09]  Acc_iter 5850        Data time: 0.01(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.28)
2025-09-04 15:54:20,538   INFO  Train:    4/36 ( 11%) [ 622/1759 ( 35%)]  Loss: 4.619 (4.94)  LR: 6.455e-04  Grad: 8.9712  max=4.6045(module.vfe.pfn_layers.0.linear.weight)  min: -5.3397(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0011, loss_cls=0.2343, loss_bbox=1.1717, matched_ious=0.4315, loss_iou=0.1029, loss_iou_reg=0.2587, d_time=0.01(0.01), f_time=1.23(1.27), b_time=1.24(1.28)  Time cost: 13:18/24:17 [2:06:19/20:27:07]  Acc_iter 5900        Data time: 0.01(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.28)
2025-09-04 15:55:25,001   INFO  Train:    4/36 ( 11%) [ 672/1759 ( 38%)]  Loss: 4.643 (4.94)  LR: 6.511e-04  Grad: 3.4288  max=0.4101(module.backbone_3d.cls_conv.3.weight)  min: -1.7631(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9798, loss_cls=0.2284, loss_bbox=1.1226, matched_ious=0.4389, loss_iou=0.1044, loss_iou_reg=0.2570, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.28)  Time cost: 14:23/23:14 [2:07:23/20:26:33]  Acc_iter 5950        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.28)
2025-09-04 15:56:29,124   INFO  Train:    4/36 ( 11%) [ 722/1759 ( 41%)]  Loss: 4.450 (4.93)  LR: 6.568e-04  Grad: 2.7636  max=0.7125(module.vfe.pfn_layers.0.linear.weight)  min: -0.8375(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9486, loss_cls=0.2261, loss_bbox=1.1912, matched_ious=0.4352, loss_iou=0.1043, loss_iou_reg=0.2593, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.28)  Time cost: 15:27/22:10 [2:08:27/20:25:28]  Acc_iter 6000        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.28)
2025-09-04 15:57:32,803   INFO  Train:    4/36 ( 11%) [ 772/1759 ( 44%)]  Loss: 4.139 (4.94)  LR: 6.625e-04  Grad: 4.8404  max=2.5258(module.vfe.pfn_layers.0.linear.weight)  min: -0.4734(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9969, loss_cls=0.2359, loss_bbox=1.2195, matched_ious=0.4353, loss_iou=0.1030, loss_iou_reg=0.2584, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.21(1.28)  Time cost: 16:31/21:05 [2:09:31/20:23:50]  Acc_iter 6050        Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.28)
2025-09-04 15:58:36,602   INFO  Train:    4/36 ( 11%) [ 822/1759 ( 47%)]  Loss: 4.533 (4.93)  LR: 6.682e-04  Grad: 5.6330  max=1.8816(module.vfe.pfn_layers.0.linear.weight)  min: -3.1595(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9618, loss_cls=0.2316, loss_bbox=1.1075, matched_ious=0.4386, loss_iou=0.1021, loss_iou_reg=0.2581, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.31(1.28)  Time cost: 17:34/20:00 [2:10:35/20:22:25]  Acc_iter 6100        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.31(1.28)
2025-09-04 15:59:39,621   INFO  Train:    4/36 ( 11%) [ 872/1759 ( 50%)]  Loss: 4.198 (4.92)  LR: 6.740e-04  Grad: 4.4773  max=3.4675(module.vfe.pfn_layers.0.linear.weight)  min: -0.9591(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9769, loss_cls=0.2246, loss_bbox=1.1399, matched_ious=0.4385, loss_iou=0.1005, loss_iou_reg=0.2591, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.31(1.28)  Time cost: 18:37/18:55 [2:11:38/20:20:11]  Acc_iter 6150        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.31(1.28)
2025-09-04 16:00:43,894   INFO  Train:    4/36 ( 11%) [ 922/1759 ( 52%)]  Loss: 4.464 (4.92)  LR: 6.798e-04  Grad: 2.7345  max=1.6774(module.vfe.pfn_layers.0.linear.weight)  min: -0.3038(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9502, loss_cls=0.2220, loss_bbox=1.1493, matched_ious=0.4360, loss_iou=0.1002, loss_iou_reg=0.2605, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.28)  Time cost: 19:42/17:51 [2:12:42/20:19:22]  Acc_iter 6200        Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.28)
2025-09-04 16:01:48,251   INFO  Train:    4/36 ( 11%) [ 972/1759 ( 55%)]  Loss: 4.764 (4.92)  LR: 6.856e-04  Grad: 3.0682  max=1.1136(module.vfe.pfn_layers.0.linear.weight)  min: -0.1989(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9687, loss_cls=0.2267, loss_bbox=1.1594, matched_ious=0.4385, loss_iou=0.1012, loss_iou_reg=0.2568, d_time=0.00(0.01), f_time=1.16(1.27), b_time=1.16(1.28)  Time cost: 20:46/16:48 [2:13:47/20:18:37]  Acc_iter 6250        Data time: 0.00(0.01)  Forward time: 1.16(1.27)  Batch time: 1.16(1.28)
2025-09-04 16:02:54,647   INFO  Train:    4/36 ( 11%) [1022/1759 ( 58%)]  Loss: 4.952 (4.91)  LR: 6.915e-04  Grad: 4.6248  max=2.1025(module.vfe.pfn_layers.0.linear.weight)  min: -0.1518(module.dense_head.prediction_head.dim.1.bias)  NaN: False  loss_hm=0.9584, loss_cls=0.2220, loss_bbox=1.1054, matched_ious=0.4363, loss_iou=0.1020, loss_iou_reg=0.2604, d_time=0.00(0.01), f_time=1.35(1.28), b_time=1.35(1.28)  Time cost: 21:52/15:45 [2:14:53/20:19:44]  Acc_iter 6300        Data time: 0.00(0.01)  Forward time: 1.35(1.28)  Batch time: 1.35(1.28)
2025-09-04 16:03:57,261   INFO  Train:    4/36 ( 11%) [1072/1759 ( 61%)]  Loss: 4.344 (4.90)  LR: 6.974e-04  Grad: 3.9031  max=2.1395(module.vfe.pfn_layers.0.linear.weight)  min: -1.0694(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9560, loss_cls=0.2233, loss_bbox=1.0825, matched_ious=0.4510, loss_iou=0.1015, loss_iou_reg=0.2537, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.20(1.28)  Time cost: 22:55/14:40 [2:15:56/20:17:17]  Acc_iter 6350        Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.28)
2025-09-04 16:05:00,459   INFO  Train:    4/36 ( 11%) [1122/1759 ( 64%)]  Loss: 5.483 (4.90)  LR: 7.033e-04  Grad: 4.4823  max=0.7583(module.vfe.pfn_layers.0.linear.weight)  min: -2.5053(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9767, loss_cls=0.2277, loss_bbox=1.1180, matched_ious=0.4453, loss_iou=0.1016, loss_iou_reg=0.2551, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.28)  Time cost: 23:58/13:36 [2:16:59/20:15:27]  Acc_iter 6400        Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.28)
2025-09-04 16:06:03,827   INFO  Train:    4/36 ( 11%) [1172/1759 ( 67%)]  Loss: 4.774 (4.89)  LR: 7.093e-04  Grad: 8.5600  max=5.7820(module.vfe.pfn_layers.0.linear.weight)  min: -0.1350(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9435, loss_cls=0.2209, loss_bbox=1.1583, matched_ious=0.4379, loss_iou=0.1017, loss_iou_reg=0.2586, d_time=0.05(0.01), f_time=1.48(1.27), b_time=1.53(1.28)  Time cost: 25:02/12:31 [2:18:02/20:13:50]  Acc_iter 6450        Data time: 0.05(0.01)  Forward time: 1.48(1.27)  Batch time: 1.53(1.28)
2025-09-04 16:07:07,430   INFO  Train:    4/36 ( 11%) [1222/1759 ( 69%)]  Loss: 3.996 (4.89)  LR: 7.154e-04  Grad: 5.5081  max=3.4185(module.vfe.pfn_layers.0.linear.weight)  min: -0.2790(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.9382, loss_cls=0.2154, loss_bbox=1.0952, matched_ious=0.4398, loss_iou=0.1034, loss_iou_reg=0.2580, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.28)  Time cost: 26:05/11:27 [2:19:06/20:12:26]  Acc_iter 6500        Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.28)
2025-09-04 16:08:11,094   INFO  Train:    4/36 ( 11%) [1272/1759 ( 72%)]  Loss: 4.308 (4.88)  LR: 7.214e-04  Grad: 3.4874  max=1.5392(module.vfe.pfn_layers.0.linear.weight)  min: -1.5830(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9324, loss_cls=0.2209, loss_bbox=1.0748, matched_ious=0.4459, loss_iou=0.1027, loss_iou_reg=0.2563, d_time=0.02(0.01), f_time=1.16(1.27), b_time=1.18(1.28)  Time cost: 27:09/10:23 [2:20:09/20:11:07]  Acc_iter 6550        Data time: 0.02(0.01)  Forward time: 1.16(1.27)  Batch time: 1.18(1.28)
2025-09-04 16:09:14,006   INFO  Train:    4/36 ( 11%) [1322/1759 ( 75%)]  Loss: 4.356 (4.87)  LR: 7.275e-04  Grad: 3.9922  max=2.4034(module.vfe.pfn_layers.0.linear.weight)  min: -1.5672(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9248, loss_cls=0.2151, loss_bbox=1.1029, matched_ious=0.4385, loss_iou=0.1017, loss_iou_reg=0.2623, d_time=0.00(0.01), f_time=1.16(1.27), b_time=1.17(1.28)  Time cost: 28:12/09:18 [2:21:12/20:09:16]  Acc_iter 6600        Data time: 0.00(0.01)  Forward time: 1.16(1.27)  Batch time: 1.17(1.28)
2025-09-04 16:10:16,927   INFO  Train:    4/36 ( 11%) [1372/1759 ( 78%)]  Loss: 4.214 (4.87)  LR: 7.336e-04  Grad: 5.9608  max=4.2197(module.vfe.pfn_layers.0.linear.weight)  min: -1.3585(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9339, loss_cls=0.2212, loss_bbox=1.1015, matched_ious=0.4400, loss_iou=0.1018, loss_iou_reg=0.2592, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.26(1.28)  Time cost: 29:15/08:14 [2:22:15/20:07:30]  Acc_iter 6650        Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.28)
2025-09-04 16:11:20,603   INFO  Train:    4/36 ( 11%) [1422/1759 ( 81%)]  Loss: 4.020 (4.86)  LR: 7.398e-04  Grad: 3.7112  max=0.2884(module.vfe.pfn_layers.0.linear.weight)  min: -2.6454(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9461, loss_cls=0.2189, loss_bbox=1.0471, matched_ious=0.4501, loss_iou=0.1036, loss_iou_reg=0.2536, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.22(1.28)  Time cost: 30:18/07:10 [2:23:19/20:06:16]  Acc_iter 6700        Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.22(1.28)
2025-09-04 16:12:23,527   INFO  Train:    4/36 ( 11%) [1472/1759 ( 84%)]  Loss: 4.836 (4.85)  LR: 7.460e-04  Grad: 3.9562  max=2.7285(module.vfe.pfn_layers.0.linear.weight)  min: -1.2015(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9457, loss_cls=0.2139, loss_bbox=1.1189, matched_ious=0.4461, loss_iou=0.1005, loss_iou_reg=0.2535, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.28(1.28)  Time cost: 31:21/06:06 [2:24:22/20:04:34]  Acc_iter 6750        Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.28(1.28)
2025-09-04 16:13:28,746   INFO  Train:    4/36 ( 11%) [1522/1759 ( 87%)]  Loss: 4.929 (4.85)  LR: 7.522e-04  Grad: 2.5738  max=0.9463(module.vfe.pfn_layers.0.linear.weight)  min: -0.1248(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9483, loss_cls=0.2153, loss_bbox=1.1357, matched_ious=0.4480, loss_iou=0.1001, loss_iou_reg=0.2551, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.26(1.28)  Time cost: 32:26/05:02 [2:25:27/20:04:20]  Acc_iter 6800        Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.28)
2025-09-04 16:14:32,236   INFO  Train:    4/36 ( 11%) [1572/1759 ( 89%)]  Loss: 5.746 (4.84)  LR: 7.585e-04  Grad: 9.1055  max=0.1199(module.dense_head.prediction_head.height.1.weight)  min: -5.9703(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9356, loss_cls=0.2153, loss_bbox=1.0656, matched_ious=0.4483, loss_iou=0.1013, loss_iou_reg=0.2571, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.20(1.28)  Time cost: 33:30/03:59 [2:26:31/20:03:01]  Acc_iter 6850        Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.28)
2025-09-04 16:15:35,926   INFO  Train:    4/36 ( 11%) [1622/1759 ( 92%)]  Loss: 5.562 (4.84)  LR: 7.648e-04  Grad: 4.6106  max=0.2282(module.backbone_3d.cls_conv.3.weight)  min: -3.4171(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9501, loss_cls=0.2170, loss_bbox=1.1491, matched_ious=0.4422, loss_iou=0.1018, loss_iou_reg=0.2571, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.31(1.28)  Time cost: 34:34/02:55 [2:27:34/20:01:50]  Acc_iter 6900        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.31(1.28)
2025-09-04 16:16:39,690   INFO  Train:    4/36 ( 11%) [1672/1759 ( 95%)]  Loss: 5.170 (4.84)  LR: 7.711e-04  Grad: 4.1778  max=1.7820(module.vfe.pfn_layers.0.linear.weight)  min: -1.8128(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9308, loss_cls=0.2131, loss_bbox=1.1180, matched_ious=0.4522, loss_iou=0.1000, loss_iou_reg=0.2496, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.28)  Time cost: 35:37/01:51 [2:28:38/20:00:41]  Acc_iter 6950        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.28)
2025-09-04 16:17:42,160   INFO  Train:    4/36 ( 11%) [1722/1759 ( 98%)]  Loss: 5.860 (4.84)  LR: 7.775e-04  Grad: 5.7809  max=4.5227(module.vfe.pfn_layers.0.linear.weight)  min: -0.2527(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9603, loss_cls=0.2140, loss_bbox=1.1379, matched_ious=0.4498, loss_iou=0.1002, loss_iou_reg=0.2527, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.28)  Time cost: 36:40/00:47 [2:29:40/19:58:51]  Acc_iter 7000        Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.28)
2025-09-04 16:18:26,684   INFO  Train:    4/36 ( 11%) [1758/1759 (100%)]  Loss: 4.558 (4.83)  LR: 7.821e-04  Grad: 7.6902  max=1.5051(module.vfe.pfn_layers.0.linear.weight)  min: -5.5425(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9078, loss_cls=0.2055, loss_bbox=1.0612, matched_ious=0.4499, loss_iou=0.1003, loss_iou_reg=0.2543, d_time=0.00(0.01), f_time=1.05(1.27), b_time=1.05(1.28)  Time cost: 37:24/00:01 [2:30:25/19:57:18]  Acc_iter 7036        Data time: 0.00(0.01)  Forward time: 1.05(1.27)  Batch time: 1.05(1.28)

                                               [Aepochs:  11%|█         | 4/36 [2:30:25<20:01:56, 2253.63s/it]epochs:  11%|█         | 4/36 [2:30:25<20:01:56, 2253.63s/it]epochs:  11%|█         | 4/36 [2:30:25<20:01:55, 2253.62s/it]epochs:  11%|█         | 4/36 [2:30:25<20:01:56, 2253.63s/it]epochs:  11%|█         | 4/36 [2:30:25<20:01:55, 2253.62s/it]epochs:  11%|█         | 4/36 [2:30:25<20:01:55, 2253.62s/it]epochs:  11%|█         | 4/36 [2:30:25<20:01:55, 2253.62s/it]epochs:  11%|█         | 4/36 [2:30:26<20:01:55, 2253.61s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 16:18:32,141   INFO  Train:    5/36 ( 14%) [   0/1759 (  0%)]  Loss: 4.572 (4.57)  LR: 7.823e-04  Grad: 3.7000  max=0.3731(module.vfe.pfn_layers.0.linear.weight)  min: -2.3515(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8273, loss_cls=0.1888, loss_bbox=1.1120, matched_ious=0.5052, loss_iou=0.1111, loss_iou_reg=0.2167, d_time=1.61(1.61), f_time=2.62(2.62), b_time=4.23(4.23)  Time cost: 00:04/1:58:45 [2:30:30/63:20:09]  Acc_iter 7037        Data time: 1.61(1.61)  Forward time: 2.62(2.62)  Batch time: 4.23(4.23)
2025-09-04 16:18:48,515   INFO  Train:    5/36 ( 14%) [  13/1759 (  1%)]  Loss: 4.587 (4.81)  LR: 7.839e-04  Grad: 4.9247  max=3.0780(module.vfe.pfn_layers.0.linear.weight)  min: -2.1159(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9282, loss_cls=0.2135, loss_bbox=1.1459, matched_ious=0.4304, loss_iou=0.1006, loss_iou_reg=0.2631, d_time=0.00(0.12), f_time=1.19(1.35), b_time=1.19(1.47)  Time cost: 00:20/42:27 [2:30:47/22:48:17]  Acc_iter 7050        Data time: 0.00(0.12)  Forward time: 1.19(1.35)  Batch time: 1.19(1.47)
2025-09-04 16:19:53,301   INFO  Train:    5/36 ( 14%) [  63/1759 (  4%)]  Loss: 5.060 (4.69)  LR: 7.904e-04  Grad: 3.2234  max=1.9728(module.vfe.pfn_layers.0.linear.weight)  min: -0.4820(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9194, loss_cls=0.2099, loss_bbox=1.0880, matched_ious=0.4462, loss_iou=0.1022, loss_iou_reg=0.2554, d_time=0.00(0.03), f_time=1.36(1.30), b_time=1.37(1.33)  Time cost: 01:25/37:38 [2:31:52/20:47:38]  Acc_iter 7100        Data time: 0.00(0.03)  Forward time: 1.36(1.30)  Batch time: 1.37(1.33)
2025-09-04 16:20:56,932   INFO  Train:    5/36 ( 14%) [ 113/1759 (  6%)]  Loss: 4.244 (4.70)  LR: 7.968e-04  Grad: 6.1147  max=3.3612(module.vfe.pfn_layers.0.linear.weight)  min: -0.0950(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.9190, loss_cls=0.2126, loss_bbox=1.0994, matched_ious=0.4436, loss_iou=0.1021, loss_iou_reg=0.2585, d_time=0.00(0.03), f_time=1.22(1.28), b_time=1.22(1.31)  Time cost: 02:28/35:49 [2:32:55/20:22:24]  Acc_iter 7150        Data time: 0.00(0.03)  Forward time: 1.22(1.28)  Batch time: 1.22(1.31)
2025-09-04 16:21:59,950   INFO  Train:    5/36 ( 14%) [ 163/1759 (  9%)]  Loss: 4.429 (4.66)  LR: 8.033e-04  Grad: 3.9535  max=0.9866(module.vfe.pfn_layers.0.linear.weight)  min: -2.0776(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9020, loss_cls=0.2058, loss_bbox=1.0370, matched_ious=0.4501, loss_iou=0.1013, loss_iou_reg=0.2569, d_time=0.00(0.02), f_time=1.10(1.27), b_time=1.10(1.29)  Time cost: 03:31/34:21 [2:33:58/20:08:23]  Acc_iter 7200        Data time: 0.00(0.02)  Forward time: 1.10(1.27)  Batch time: 1.10(1.29)
2025-09-04 16:23:03,489   INFO  Train:    5/36 ( 14%) [ 213/1759 ( 12%)]  Loss: 5.003 (4.66)  LR: 8.099e-04  Grad: 6.4547  max=4.4311(module.vfe.pfn_layers.0.linear.weight)  min: -2.7239(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9255, loss_cls=0.2099, loss_bbox=1.0777, matched_ious=0.4498, loss_iou=0.1021, loss_iou_reg=0.2535, d_time=0.71(0.02), f_time=1.21(1.27), b_time=1.92(1.29)  Time cost: 04:35/33:09 [2:35:02/20:02:43]  Acc_iter 7250        Data time: 0.71(0.02)  Forward time: 1.21(1.27)  Batch time: 1.92(1.29)
2025-09-04 16:24:09,142   INFO  Train:    5/36 ( 14%) [ 263/1759 ( 15%)]  Loss: 5.111 (4.67)  LR: 8.164e-04  Grad: 8.6509  max=6.0870(module.vfe.pfn_layers.0.linear.weight)  min: -0.2138(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.9357, loss_cls=0.2129, loss_bbox=1.1563, matched_ious=0.4425, loss_iou=0.1008, loss_iou_reg=0.2569, d_time=0.01(0.02), f_time=1.32(1.28), b_time=1.32(1.29)  Time cost: 05:41/32:12 [2:36:07/20:06:16]  Acc_iter 7300        Data time: 0.01(0.02)  Forward time: 1.32(1.28)  Batch time: 1.32(1.29)
2025-09-04 16:25:12,706   INFO  Train:    5/36 ( 14%) [ 313/1759 ( 18%)]  Loss: 4.643 (4.67)  LR: 8.231e-04  Grad: 5.1200  max=3.7765(module.vfe.pfn_layers.0.linear.weight)  min: -1.4415(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9290, loss_cls=0.2033, loss_bbox=1.1070, matched_ious=0.4454, loss_iou=0.1010, loss_iou_reg=0.2553, d_time=0.02(0.01), f_time=1.26(1.27), b_time=1.27(1.29)  Time cost: 06:44/31:03 [2:37:11/20:02:08]  Acc_iter 7350        Data time: 0.02(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.29)
2025-09-04 16:26:16,440   INFO  Train:    5/36 ( 14%) [ 363/1759 ( 21%)]  Loss: 5.719 (4.66)  LR: 8.297e-04  Grad: 5.7057  max=1.3905(module.vfe.pfn_layers.0.linear.weight)  min: -3.9880(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9020, loss_cls=0.2052, loss_bbox=1.0682, matched_ious=0.4514, loss_iou=0.1016, loss_iou_reg=0.2552, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.29)  Time cost: 07:48/29:56 [2:38:15/19:59:17]  Acc_iter 7400        Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.29)
2025-09-04 16:27:19,777   INFO  Train:    5/36 ( 14%) [ 413/1759 ( 23%)]  Loss: 5.014 (4.65)  LR: 8.363e-04  Grad: 4.6588  max=0.2451(module.backbone_3d.cls_conv.3.weight)  min: -2.5995(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9384, loss_cls=0.2119, loss_bbox=1.0685, matched_ious=0.4458, loss_iou=0.1008, loss_iou_reg=0.2567, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.20(1.28)  Time cost: 08:51/28:48 [2:39:18/19:55:58]  Acc_iter 7450        Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.28)
2025-09-04 16:28:24,177   INFO  Train:    5/36 ( 14%) [ 463/1759 ( 26%)]  Loss: 4.014 (4.65)  LR: 8.430e-04  Grad: 7.4906  max=0.1218(module.backbone_3d.cls_conv.3.weight)  min: -4.5465(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9205, loss_cls=0.2082, loss_bbox=1.1207, matched_ious=0.4436, loss_iou=0.1029, loss_iou_reg=0.2587, d_time=0.01(0.01), f_time=1.19(1.27), b_time=1.19(1.29)  Time cost: 09:56/27:44 [2:40:22/19:55:16]  Acc_iter 7500        Data time: 0.01(0.01)  Forward time: 1.19(1.27)  Batch time: 1.19(1.29)
2025-09-04 16:29:28,223   INFO  Train:    5/36 ( 14%) [ 513/1759 ( 29%)]  Loss: 5.615 (4.66)  LR: 8.498e-04  Grad: 3.3784  max=0.9915(module.vfe.pfn_layers.0.linear.weight)  min: -1.6631(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9337, loss_cls=0.2088, loss_bbox=1.1073, matched_ious=0.4544, loss_iou=0.1001, loss_iou_reg=0.2506, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.29(1.28)  Time cost: 11:00/26:40 [2:41:26/19:53:52]  Acc_iter 7550        Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.28)
2025-09-04 16:30:31,116   INFO  Train:    5/36 ( 14%) [ 563/1759 ( 32%)]  Loss: 5.220 (4.65)  LR: 8.565e-04  Grad: 5.4198  max=2.3637(module.vfe.pfn_layers.0.linear.weight)  min: -2.9797(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9078, loss_cls=0.2060, loss_bbox=1.0758, matched_ious=0.4486, loss_iou=0.1000, loss_iou_reg=0.2561, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.24(1.28)  Time cost: 12:03/25:33 [2:42:29/19:50:37]  Acc_iter 7600        Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.28)
2025-09-04 16:31:34,499   INFO  Train:    5/36 ( 14%) [ 613/1759 ( 35%)]  Loss: 4.180 (4.64)  LR: 8.633e-04  Grad: 3.0122  max=0.1324(module.dense_head.prediction_head.height.1.weight)  min: -1.8251(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9008, loss_cls=0.2051, loss_bbox=0.9830, matched_ious=0.4594, loss_iou=0.0998, loss_iou_reg=0.2501, d_time=0.01(0.01), f_time=1.27(1.27), b_time=1.27(1.28)  Time cost: 13:06/24:27 [2:43:33/19:48:28]  Acc_iter 7650        Data time: 0.01(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.28)
2025-09-04 16:32:37,261   INFO  Train:    5/36 ( 14%) [ 663/1759 ( 38%)]  Loss: 5.305 (4.63)  LR: 8.701e-04  Grad: 7.6978  max=5.2488(module.vfe.pfn_layers.0.linear.weight)  min: -2.4602(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9117, loss_cls=0.2068, loss_bbox=1.0235, matched_ious=0.4546, loss_iou=0.1028, loss_iou_reg=0.2537, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.28)  Time cost: 14:09/23:21 [2:44:36/19:45:37]  Acc_iter 7700        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.28)
2025-09-04 16:33:42,100   INFO  Train:    5/36 ( 14%) [ 713/1759 ( 41%)]  Loss: 4.806 (4.63)  LR: 8.770e-04  Grad: 8.9217  max=4.3160(module.vfe.pfn_layers.0.linear.weight)  min: -6.0645(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9112, loss_cls=0.2037, loss_bbox=1.0929, matched_ious=0.4509, loss_iou=0.0978, loss_iou_reg=0.2521, d_time=0.01(0.01), f_time=1.35(1.27), b_time=1.35(1.28)  Time cost: 15:14/22:19 [2:45:40/19:45:42]  Acc_iter 7750        Data time: 0.01(0.01)  Forward time: 1.35(1.27)  Batch time: 1.35(1.28)
2025-09-04 16:34:47,811   INFO  Train:    5/36 ( 14%) [ 763/1759 ( 43%)]  Loss: 4.522 (4.63)  LR: 8.839e-04  Grad: 6.0962  max=5.0363(module.vfe.pfn_layers.0.linear.weight)  min: -0.2961(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9161, loss_cls=0.2053, loss_bbox=1.0792, matched_ious=0.4439, loss_iou=0.1010, loss_iou_reg=0.2549, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.29(1.28)  Time cost: 16:19/21:17 [2:46:46/19:46:42]  Acc_iter 7800        Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.29(1.28)
2025-09-04 16:35:51,012   INFO  Train:    5/36 ( 14%) [ 813/1759 ( 46%)]  Loss: 4.946 (4.63)  LR: 8.908e-04  Grad: 5.0013  max=3.5231(module.vfe.pfn_layers.0.linear.weight)  min: -1.8083(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9113, loss_cls=0.2066, loss_bbox=1.1077, matched_ious=0.4492, loss_iou=0.1015, loss_iou_reg=0.2550, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.23(1.28)  Time cost: 17:22/20:12 [2:47:49/19:44:36]  Acc_iter 7850        Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.23(1.28)
2025-09-04 16:36:54,023   INFO  Train:    5/36 ( 14%) [ 863/1759 ( 49%)]  Loss: 4.947 (4.62)  LR: 8.977e-04  Grad: 3.9017  max=3.1491(module.vfe.pfn_layers.0.linear.weight)  min: -0.4731(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8830, loss_cls=0.2016, loss_bbox=1.0365, matched_ious=0.4593, loss_iou=0.0988, loss_iou_reg=0.2507, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.23(1.28)  Time cost: 18:25/19:06 [2:48:52/19:42:24]  Acc_iter 7900        Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.23(1.28)
2025-09-04 16:37:57,354   INFO  Train:    5/36 ( 14%) [ 913/1759 ( 52%)]  Loss: 4.723 (4.61)  LR: 9.047e-04  Grad: 2.7940  max=0.7466(module.vfe.pfn_layers.0.linear.weight)  min: -1.7996(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8914, loss_cls=0.1998, loss_bbox=1.0768, matched_ious=0.4521, loss_iou=0.1014, loss_iou_reg=0.2545, d_time=0.00(0.01), f_time=1.32(1.27), b_time=1.33(1.28)  Time cost: 19:29/18:02 [2:49:56/19:40:40]  Acc_iter 7950        Data time: 0.00(0.01)  Forward time: 1.32(1.27)  Batch time: 1.33(1.28)
2025-09-04 16:39:00,753   INFO  Train:    5/36 ( 14%) [ 963/1759 ( 55%)]  Loss: 4.469 (4.61)  LR: 9.117e-04  Grad: 8.8943  max=2.9324(module.vfe.pfn_layers.0.linear.weight)  min: -6.4529(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8966, loss_cls=0.2014, loss_bbox=1.0716, matched_ious=0.4521, loss_iou=0.1039, loss_iou_reg=0.2528, d_time=0.00(0.01), f_time=1.20(1.27), b_time=1.20(1.28)  Time cost: 20:32/16:57 [2:50:59/19:39:03]  Acc_iter 8000        Data time: 0.00(0.01)  Forward time: 1.20(1.27)  Batch time: 1.20(1.28)
2025-09-04 16:40:04,330   INFO  Train:    5/36 ( 14%) [1013/1759 ( 58%)]  Loss: 4.485 (4.62)  LR: 9.187e-04  Grad: 10.0000  max=4.3678(module.vfe.pfn_layers.0.linear.weight)  min: -5.8767(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9178, loss_cls=0.1998, loss_bbox=1.1552, matched_ious=0.4380, loss_iou=0.1054, loss_iou_reg=0.2629, d_time=0.01(0.01), f_time=1.26(1.27), b_time=1.27(1.28)  Time cost: 21:36/15:53 [2:52:03/19:37:40]  Acc_iter 8050        Data time: 0.01(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.28)
2025-09-04 16:41:08,417   INFO  Train:    5/36 ( 14%) [1063/1759 ( 60%)]  Loss: 4.638 (4.61)  LR: 9.257e-04  Grad: 3.8426  max=2.0475(module.vfe.pfn_layers.0.linear.weight)  min: -1.6889(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8927, loss_cls=0.1982, loss_bbox=1.0308, matched_ious=0.4606, loss_iou=0.1007, loss_iou_reg=0.2468, d_time=0.00(0.01), f_time=1.13(1.27), b_time=1.13(1.28)  Time cost: 22:40/14:49 [2:53:07/19:36:45]  Acc_iter 8100        Data time: 0.00(0.01)  Forward time: 1.13(1.27)  Batch time: 1.13(1.28)
2025-09-04 16:42:11,953   INFO  Train:    5/36 ( 14%) [1113/1759 ( 63%)]  Loss: 4.216 (4.61)  LR: 9.328e-04  Grad: 4.4181  max=1.7820(module.vfe.pfn_layers.0.linear.weight)  min: -3.2153(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9198, loss_cls=0.2024, loss_bbox=1.0299, matched_ious=0.4614, loss_iou=0.0986, loss_iou_reg=0.2475, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.24(1.28)  Time cost: 23:43/13:45 [2:54:10/19:35:22]  Acc_iter 8150        Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.28)
2025-09-04 16:43:15,440   INFO  Train:    5/36 ( 14%) [1163/1759 ( 66%)]  Loss: 4.261 (4.61)  LR: 9.399e-04  Grad: 4.4118  max=2.2735(module.vfe.pfn_layers.0.linear.weight)  min: -0.2062(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.8735, loss_cls=0.1952, loss_bbox=1.0703, matched_ious=0.4436, loss_iou=0.1032, loss_iou_reg=0.2587, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.30(1.28)  Time cost: 24:47/12:41 [2:55:14/19:33:58]  Acc_iter 8200        Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.30(1.28)
2025-09-04 16:44:21,281   INFO  Train:    5/36 ( 14%) [1213/1759 ( 69%)]  Loss: 5.225 (4.60)  LR: 9.471e-04  Grad: 4.7486  max=2.6163(module.vfe.pfn_layers.0.linear.weight)  min: -1.8922(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9134, loss_cls=0.1999, loss_bbox=1.0729, matched_ious=0.4534, loss_iou=0.0974, loss_iou_reg=0.2523, d_time=0.01(0.01), f_time=2.10(1.27), b_time=2.11(1.28)  Time cost: 25:53/11:38 [2:56:20/19:34:22]  Acc_iter 8250        Data time: 0.01(0.01)  Forward time: 2.10(1.27)  Batch time: 2.11(1.28)
2025-09-04 16:45:24,526   INFO  Train:    5/36 ( 14%) [1263/1759 ( 72%)]  Loss: 4.458 (4.60)  LR: 9.542e-04  Grad: 4.1546  max=1.7425(module.vfe.pfn_layers.0.linear.weight)  min: -1.8492(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8664, loss_cls=0.1939, loss_bbox=0.9936, matched_ious=0.4500, loss_iou=0.1006, loss_iou_reg=0.2551, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.26(1.28)  Time cost: 26:56/10:34 [2:57:23/19:32:46]  Acc_iter 8300        Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.28)
2025-09-04 16:46:27,298   INFO  Train:    5/36 ( 14%) [1313/1759 ( 75%)]  Loss: 4.150 (4.59)  LR: 9.614e-04  Grad: 3.4833  max=2.2323(module.vfe.pfn_layers.0.linear.weight)  min: -1.5808(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8612, loss_cls=0.1944, loss_bbox=0.9860, matched_ious=0.4611, loss_iou=0.1007, loss_iou_reg=0.2522, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.28)  Time cost: 27:59/09:29 [2:58:26/19:30:54]  Acc_iter 8350        Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.28)
2025-09-04 16:47:31,089   INFO  Train:    5/36 ( 14%) [1363/1759 ( 77%)]  Loss: 4.732 (4.59)  LR: 9.686e-04  Grad: 6.3934  max=4.3973(module.vfe.pfn_layers.0.linear.weight)  min: -1.4611(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8704, loss_cls=0.1942, loss_bbox=1.0600, matched_ious=0.4531, loss_iou=0.1016, loss_iou_reg=0.2518, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.31(1.28)  Time cost: 29:02/08:26 [2:59:29/19:29:46]  Acc_iter 8400        Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.31(1.28)
2025-09-04 16:48:35,194   INFO  Train:    5/36 ( 14%) [1413/1759 ( 80%)]  Loss: 3.790 (4.58)  LR: 9.759e-04  Grad: 4.8378  max=3.6830(module.vfe.pfn_layers.0.linear.weight)  min: -1.1081(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8811, loss_cls=0.1972, loss_bbox=1.0563, matched_ious=0.4549, loss_iou=0.1015, loss_iou_reg=0.2557, d_time=0.00(0.01), f_time=1.36(1.27), b_time=1.37(1.28)  Time cost: 30:07/07:22 [3:00:33/19:28:50]  Acc_iter 8450        Data time: 0.00(0.01)  Forward time: 1.36(1.27)  Batch time: 1.37(1.28)
2025-09-04 16:49:38,136   INFO  Train:    5/36 ( 14%) [1463/1759 ( 83%)]  Loss: 3.800 (4.58)  LR: 9.831e-04  Grad: 3.7300  max=1.9556(module.vfe.pfn_layers.0.linear.weight)  min: -0.3554(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8919, loss_cls=0.1970, loss_bbox=1.0735, matched_ious=0.4555, loss_iou=0.0985, loss_iou_reg=0.2529, d_time=0.00(0.01), f_time=1.17(1.27), b_time=1.17(1.28)  Time cost: 31:10/06:18 [3:01:36/19:27:10]  Acc_iter 8500        Data time: 0.00(0.01)  Forward time: 1.17(1.27)  Batch time: 1.17(1.28)
2025-09-04 16:50:42,081   INFO  Train:    5/36 ( 14%) [1513/1759 ( 86%)]  Loss: 4.464 (4.58)  LR: 9.904e-04  Grad: 6.2306  max=4.7973(module.vfe.pfn_layers.0.linear.weight)  min: -1.8140(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8980, loss_cls=0.1963, loss_bbox=1.0829, matched_ious=0.4521, loss_iou=0.0997, loss_iou_reg=0.2523, d_time=0.01(0.01), f_time=1.23(1.27), b_time=1.25(1.28)  Time cost: 32:13/05:14 [3:02:40/19:26:09]  Acc_iter 8550        Data time: 0.01(0.01)  Forward time: 1.23(1.27)  Batch time: 1.25(1.28)
2025-09-04 16:51:45,461   INFO  Train:    5/36 ( 14%) [1563/1759 ( 89%)]  Loss: 4.818 (4.58)  LR: 9.977e-04  Grad: 5.9428  max=4.0765(module.vfe.pfn_layers.0.linear.weight)  min: -2.9182(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9098, loss_cls=0.1983, loss_bbox=1.0023, matched_ious=0.4541, loss_iou=0.1008, loss_iou_reg=0.2555, d_time=0.01(0.01), f_time=1.19(1.27), b_time=1.21(1.28)  Time cost: 33:17/04:10 [3:03:44/19:24:48]  Acc_iter 8600        Data time: 0.01(0.01)  Forward time: 1.19(1.27)  Batch time: 1.21(1.28)
2025-09-04 16:52:47,720   INFO  Train:    5/36 ( 14%) [1613/1759 ( 92%)]  Loss: 4.650 (4.57)  LR: 1.005e-03  Grad: 5.0703  max=3.7124(module.vfe.pfn_layers.0.linear.weight)  min: -0.9387(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8914, loss_cls=0.1967, loss_bbox=1.0384, matched_ious=0.4552, loss_iou=0.1008, loss_iou_reg=0.2519, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.31(1.28)  Time cost: 34:19/03:06 [3:04:46/19:22:50]  Acc_iter 8650        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.31(1.28)
2025-09-04 16:53:50,531   INFO  Train:    5/36 ( 14%) [1663/1759 ( 95%)]  Loss: 4.230 (4.57)  LR: 1.012e-03  Grad: 2.8391  max=1.6500(module.vfe.pfn_layers.0.linear.weight)  min: -1.0894(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8658, loss_cls=0.1938, loss_bbox=1.0005, matched_ious=0.4592, loss_iou=0.1017, loss_iou_reg=0.2498, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.28(1.28)  Time cost: 35:22/02:02 [3:05:49/19:21:14]  Acc_iter 8700        Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.28(1.28)
2025-09-04 16:54:55,912   INFO  Train:    5/36 ( 14%) [1713/1759 ( 97%)]  Loss: 4.339 (4.57)  LR: 1.020e-03  Grad: 5.3854  max=4.3337(module.vfe.pfn_layers.0.linear.weight)  min: -1.0363(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8963, loss_cls=0.1965, loss_bbox=1.0338, matched_ious=0.4597, loss_iou=0.1016, loss_iou_reg=0.2514, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.29(1.28)  Time cost: 36:27/00:58 [3:06:54/19:21:01]  Acc_iter 8750        Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.28)
2025-09-04 16:55:52,418   INFO  Train:    5/36 ( 14%) [1758/1759 (100%)]  Loss: 5.489 (4.57)  LR: 1.027e-03  Grad: 4.4988  max=2.6846(module.vfe.pfn_layers.0.linear.weight)  min: -2.2204(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8930, loss_cls=0.1936, loss_bbox=1.0550, matched_ious=0.4623, loss_iou=0.1015, loss_iou_reg=0.2458, d_time=0.00(0.01), f_time=0.71(1.27), b_time=0.72(1.28)  Time cost: 37:24/00:01 [3:07:51/19:19:35]  Acc_iter 8795        Data time: 0.00(0.01)  Forward time: 0.71(1.27)  Batch time: 0.72(1.28)

                                               [Aepochs:  14%|█▍        | 5/36 [3:07:51<19:22:53, 2250.76s/it]epochs:  14%|█▍        | 5/36 [3:07:51<19:22:53, 2250.76s/it]epochs:  14%|█▍        | 5/36 [3:07:51<19:22:54, 2250.79s/it]epochs:  14%|█▍        | 5/36 [3:07:51<19:22:54, 2250.79s/it]epochs:  14%|█▍        | 5/36 [3:07:51<19:22:54, 2250.79s/it]epochs:  14%|█▍        | 5/36 [3:07:51<19:22:54, 2250.79s/it]epochs:  14%|█▍        | 5/36 [3:07:51<19:22:54, 2250.79s/it]epochs:  14%|█▍        | 5/36 [3:07:51<19:22:54, 2250.81s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 16:55:57,994   INFO  Train:    6/36 ( 17%) [   0/1759 (  0%)]  Loss: 4.248 (4.25)  LR: 1.027e-03  Grad: 3.6972  max=1.9586(module.vfe.pfn_layers.0.linear.weight)  min: -0.1159(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.8892, loss_cls=0.1895, loss_bbox=0.9869, matched_ious=0.4743, loss_iou=0.1034, loss_iou_reg=0.2494, d_time=1.46(1.46), f_time=2.82(2.82), b_time=4.28(4.28)  Time cost: 00:04/1:58:11 [3:07:56/61:03:59]  Acc_iter 8796        Data time: 1.46(1.46)  Forward time: 2.82(2.82)  Batch time: 4.28(4.28)
2025-09-04 16:56:03,162   INFO  Train:    6/36 ( 17%) [   4/1759 (  0%)]  Loss: 3.951 (4.29)  LR: 1.027e-03  Grad: 4.5638  max=3.2177(module.vfe.pfn_layers.0.linear.weight)  min: -1.9117(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8637, loss_cls=0.2095, loss_bbox=0.9174, matched_ious=0.4606, loss_iou=0.0961, loss_iou_reg=0.2597, d_time=0.01(0.30), f_time=1.26(1.59), b_time=1.27(1.89)  Time cost: 00:09/53:48 [3:08:01/27:51:57]  Acc_iter 8800        Data time: 0.01(0.30)  Forward time: 1.26(1.59)  Batch time: 1.27(1.89)
2025-09-04 16:57:07,411   INFO  Train:    6/36 ( 17%) [  54/1759 (  3%)]  Loss: 5.588 (4.49)  LR: 1.035e-03  Grad: 4.5023  max=1.8112(module.vfe.pfn_layers.0.linear.weight)  min: -2.7858(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8956, loss_cls=0.1975, loss_bbox=1.0248, matched_ious=0.4553, loss_iou=0.0999, loss_iou_reg=0.2509, d_time=0.00(0.03), f_time=1.22(1.31), b_time=1.22(1.34)  Time cost: 01:13/37:56 [3:09:06/20:12:27]  Acc_iter 8850        Data time: 0.00(0.03)  Forward time: 1.22(1.31)  Batch time: 1.22(1.34)
2025-09-04 16:58:10,847   INFO  Train:    6/36 ( 17%) [ 104/1759 (  6%)]  Loss: 5.143 (4.46)  LR: 1.042e-03  Grad: 5.1807  max=3.7713(module.vfe.pfn_layers.0.linear.weight)  min: -0.1422(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.8782, loss_cls=0.1912, loss_bbox=1.0084, matched_ious=0.4607, loss_iou=0.1026, loss_iou_reg=0.2511, d_time=0.00(0.02), f_time=1.19(1.29), b_time=1.19(1.31)  Time cost: 02:16/35:57 [3:10:09/19:42:32]  Acc_iter 8900        Data time: 0.00(0.02)  Forward time: 1.19(1.29)  Batch time: 1.19(1.31)
2025-09-04 16:59:14,718   INFO  Train:    6/36 ( 17%) [ 154/1759 (  9%)]  Loss: 4.843 (4.46)  LR: 1.050e-03  Grad: 6.2721  max=0.6379(module.vfe.pfn_layers.0.linear.weight)  min: -4.4963(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8752, loss_cls=0.1907, loss_bbox=1.0214, matched_ious=0.4526, loss_iou=0.1014, loss_iou_reg=0.2549, d_time=0.00(0.01), f_time=1.30(1.28), b_time=1.31(1.30)  Time cost: 03:20/34:38 [3:11:13/19:33:46]  Acc_iter 8950        Data time: 0.00(0.01)  Forward time: 1.30(1.28)  Batch time: 1.31(1.30)
2025-09-04 17:00:19,169   INFO  Train:    6/36 ( 17%) [ 204/1759 ( 12%)]  Loss: 4.631 (4.44)  LR: 1.057e-03  Grad: 8.7288  max=7.5411(module.vfe.pfn_layers.0.linear.weight)  min: -0.4821(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8746, loss_cls=0.1923, loss_bbox=1.0258, matched_ious=0.4628, loss_iou=0.1014, loss_iou_reg=0.2496, d_time=0.00(0.02), f_time=1.24(1.28), b_time=1.25(1.29)  Time cost: 04:25/33:31 [3:12:17/19:31:19]  Acc_iter 9000        Data time: 0.00(0.02)  Forward time: 1.24(1.28)  Batch time: 1.25(1.29)
2025-09-04 17:01:22,828   INFO  Train:    6/36 ( 17%) [ 254/1759 ( 14%)]  Loss: 4.065 (4.42)  LR: 1.065e-03  Grad: 7.2471  max=4.6328(module.vfe.pfn_layers.0.linear.weight)  min: -2.7292(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8530, loss_cls=0.1895, loss_bbox=0.9972, matched_ious=0.4649, loss_iou=0.1001, loss_iou_reg=0.2509, d_time=0.00(0.01), f_time=1.25(1.28), b_time=1.26(1.29)  Time cost: 05:28/32:20 [3:13:21/19:26:36]  Acc_iter 9050        Data time: 0.00(0.01)  Forward time: 1.25(1.28)  Batch time: 1.26(1.29)
2025-09-04 17:02:25,983   INFO  Train:    6/36 ( 17%) [ 304/1759 ( 17%)]  Loss: 4.005 (4.43)  LR: 1.072e-03  Grad: 3.0671  max=1.5086(module.vfe.pfn_layers.0.linear.weight)  min: -1.2936(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8848, loss_cls=0.1932, loss_bbox=1.0271, matched_ious=0.4590, loss_iou=0.1026, loss_iou_reg=0.2514, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.19(1.29)  Time cost: 06:32/31:10 [3:14:24/19:21:36]  Acc_iter 9100        Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.19(1.29)
2025-09-04 17:03:28,049   INFO  Train:    6/36 ( 17%) [ 354/1759 ( 20%)]  Loss: 4.906 (4.43)  LR: 1.080e-03  Grad: 4.0300  max=2.9849(module.vfe.pfn_layers.0.linear.weight)  min: -1.4351(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8535, loss_cls=0.1873, loss_bbox=1.0367, matched_ious=0.4621, loss_iou=0.0997, loss_iou_reg=0.2516, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.26(1.28)  Time cost: 07:34/29:57 [3:15:26/19:14:55]  Acc_iter 9150        Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.28)
2025-09-04 17:04:31,525   INFO  Train:    6/36 ( 17%) [ 404/1759 ( 23%)]  Loss: 3.893 (4.44)  LR: 1.087e-03  Grad: 2.6236  max=1.2389(module.vfe.pfn_layers.0.linear.weight)  min: -0.7363(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9052, loss_cls=0.1986, loss_bbox=1.0603, matched_ious=0.4524, loss_iou=0.1003, loss_iou_reg=0.2541, d_time=0.00(0.01), f_time=1.20(1.27), b_time=1.21(1.28)  Time cost: 08:37/28:51 [3:16:30/19:12:48]  Acc_iter 9200        Data time: 0.00(0.01)  Forward time: 1.20(1.27)  Batch time: 1.21(1.28)
2025-09-04 17:05:37,671   INFO  Train:    6/36 ( 17%) [ 454/1759 ( 26%)]  Loss: 4.282 (4.44)  LR: 1.095e-03  Grad: 4.9062  max=3.2946(module.vfe.pfn_layers.0.linear.weight)  min: -0.8346(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8598, loss_cls=0.1891, loss_bbox=1.0242, matched_ious=0.4597, loss_iou=0.0998, loss_iou_reg=0.2523, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.31(1.28)  Time cost: 09:43/27:54 [3:17:36/19:16:11]  Acc_iter 9250        Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.31(1.28)
2025-09-04 17:06:41,091   INFO  Train:    6/36 ( 17%) [ 504/1759 ( 29%)]  Loss: 4.686 (4.43)  LR: 1.103e-03  Grad: 6.1361  max=4.9996(module.vfe.pfn_layers.0.linear.weight)  min: -1.4306(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8626, loss_cls=0.1912, loss_bbox=1.0050, matched_ious=0.4581, loss_iou=0.1000, loss_iou_reg=0.2533, d_time=0.00(0.01), f_time=1.18(1.27), b_time=1.18(1.28)  Time cost: 10:47/26:48 [3:18:39/19:13:49]  Acc_iter 9300        Data time: 0.00(0.01)  Forward time: 1.18(1.27)  Batch time: 1.18(1.28)
2025-09-04 17:07:43,577   INFO  Train:    6/36 ( 17%) [ 554/1759 ( 31%)]  Loss: 4.195 (4.44)  LR: 1.110e-03  Grad: 5.2948  max=3.1829(module.vfe.pfn_layers.0.linear.weight)  min: -2.6965(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8930, loss_cls=0.1912, loss_bbox=1.0724, matched_ious=0.4630, loss_iou=0.1005, loss_iou_reg=0.2477, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.28)  Time cost: 11:49/25:40 [3:19:42/19:10:11]  Acc_iter 9350        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.28)
2025-09-04 17:08:47,242   INFO  Train:    6/36 ( 17%) [ 604/1759 ( 34%)]  Loss: 4.650 (4.43)  LR: 1.118e-03  Grad: 5.7499  max=3.7205(module.vfe.pfn_layers.0.linear.weight)  min: -1.4023(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8707, loss_cls=0.1916, loss_bbox=0.9682, matched_ious=0.4624, loss_iou=0.0995, loss_iou_reg=0.2518, d_time=0.00(0.01), f_time=1.18(1.27), b_time=1.18(1.28)  Time cost: 12:53/24:36 [3:20:46/19:08:44]  Acc_iter 9400        Data time: 0.00(0.01)  Forward time: 1.18(1.27)  Batch time: 1.18(1.28)
2025-09-04 17:09:50,509   INFO  Train:    6/36 ( 17%) [ 654/1759 ( 37%)]  Loss: 4.383 (4.44)  LR: 1.126e-03  Grad: 2.1985  max=0.8195(module.vfe.pfn_layers.0.linear.weight)  min: -1.1734(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9010, loss_cls=0.1909, loss_bbox=1.0849, matched_ious=0.4626, loss_iou=0.1009, loss_iou_reg=0.2446, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.28)  Time cost: 13:56/23:31 [3:21:49/19:06:47]  Acc_iter 9450        Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.28)
2025-09-04 17:10:54,424   INFO  Train:    6/36 ( 17%) [ 704/1759 ( 40%)]  Loss: 4.013 (4.44)  LR: 1.133e-03  Grad: 4.1462  max=3.3534(module.vfe.pfn_layers.0.linear.weight)  min: -0.8108(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9029, loss_cls=0.1958, loss_bbox=1.0583, matched_ious=0.4518, loss_iou=0.1010, loss_iou_reg=0.2546, d_time=0.01(0.01), f_time=1.29(1.27), b_time=1.29(1.28)  Time cost: 15:00/22:27 [3:22:53/19:05:48]  Acc_iter 9500        Data time: 0.01(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.28)
2025-09-04 17:11:58,020   INFO  Train:    6/36 ( 17%) [ 754/1759 ( 43%)]  Loss: 4.629 (4.44)  LR: 1.141e-03  Grad: 4.1184  max=3.1447(module.vfe.pfn_layers.0.linear.weight)  min: -1.6693(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8796, loss_cls=0.1914, loss_bbox=1.0002, matched_ious=0.4544, loss_iou=0.1052, loss_iou_reg=0.2536, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.28)  Time cost: 16:04/21:23 [3:23:56/19:04:25]  Acc_iter 9550        Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.28)
2025-09-04 17:13:01,903   INFO  Train:    6/36 ( 17%) [ 804/1759 ( 46%)]  Loss: 4.301 (4.43)  LR: 1.149e-03  Grad: 3.9210  max=2.1562(module.vfe.pfn_layers.0.linear.weight)  min: -2.4899(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8592, loss_cls=0.1888, loss_bbox=0.9679, matched_ious=0.4681, loss_iou=0.0992, loss_iou_reg=0.2480, d_time=0.01(0.01), f_time=1.45(1.27), b_time=1.46(1.28)  Time cost: 17:07/20:19 [3:25:00/19:03:23]  Acc_iter 9600        Data time: 0.01(0.01)  Forward time: 1.45(1.27)  Batch time: 1.46(1.28)
2025-09-04 17:14:04,672   INFO  Train:    6/36 ( 17%) [ 854/1759 ( 49%)]  Loss: 3.785 (4.43)  LR: 1.157e-03  Grad: 4.0262  max=3.1415(module.vfe.pfn_layers.0.linear.weight)  min: -0.1079(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.8522, loss_cls=0.1853, loss_bbox=0.9863, matched_ious=0.4648, loss_iou=0.1030, loss_iou_reg=0.2499, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.28(1.28)  Time cost: 18:10/19:14 [3:26:03/19:01:12]  Acc_iter 9650        Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.28(1.28)
2025-09-04 17:15:06,725   INFO  Train:    6/36 ( 17%) [ 904/1759 ( 51%)]  Loss: 4.100 (4.42)  LR: 1.165e-03  Grad: 3.1163  max=1.9343(module.vfe.pfn_layers.0.linear.weight)  min: -0.5972(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8633, loss_cls=0.1866, loss_bbox=0.9792, matched_ious=0.4695, loss_iou=0.0998, loss_iou_reg=0.2452, d_time=0.01(0.01), f_time=1.17(1.26), b_time=1.18(1.27)  Time cost: 19:12/18:09 [3:27:05/18:58:25]  Acc_iter 9700        Data time: 0.01(0.01)  Forward time: 1.17(1.26)  Batch time: 1.18(1.27)
2025-09-04 17:16:13,423   INFO  Train:    6/36 ( 17%) [ 954/1759 ( 54%)]  Loss: 4.019 (4.41)  LR: 1.172e-03  Grad: 3.1075  max=2.1650(module.vfe.pfn_layers.0.linear.weight)  min: -0.4375(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8535, loss_cls=0.1842, loss_bbox=0.9821, matched_ious=0.4676, loss_iou=0.0975, loss_iou_reg=0.2473, d_time=0.00(0.01), f_time=1.41(1.27), b_time=1.42(1.28)  Time cost: 20:19/17:07 [3:28:12/19:00:11]  Acc_iter 9750        Data time: 0.00(0.01)  Forward time: 1.41(1.27)  Batch time: 1.42(1.28)
2025-09-04 17:17:16,969   INFO  Train:    6/36 ( 17%) [1004/1759 ( 57%)]  Loss: 3.733 (4.40)  LR: 1.180e-03  Grad: 5.5474  max=2.8142(module.vfe.pfn_layers.0.linear.weight)  min: -3.8298(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8457, loss_cls=0.1872, loss_bbox=0.9447, matched_ious=0.4642, loss_iou=0.0996, loss_iou_reg=0.2494, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.29(1.28)  Time cost: 21:23/16:03 [3:29:15/18:58:51]  Acc_iter 9800        Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.28)
2025-09-04 17:18:20,657   INFO  Train:    6/36 ( 17%) [1054/1759 ( 60%)]  Loss: 4.191 (4.40)  LR: 1.188e-03  Grad: 3.3358  max=2.4723(module.vfe.pfn_layers.0.linear.weight)  min: -0.8779(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8285, loss_cls=0.1848, loss_bbox=0.9351, matched_ious=0.4725, loss_iou=0.0993, loss_iou_reg=0.2444, d_time=0.00(0.01), f_time=1.32(1.27), b_time=1.32(1.28)  Time cost: 22:26/14:59 [3:30:19/18:57:40]  Acc_iter 9850        Data time: 0.00(0.01)  Forward time: 1.32(1.27)  Batch time: 1.32(1.28)
2025-09-04 17:19:23,809   INFO  Train:    6/36 ( 17%) [1104/1759 ( 63%)]  Loss: 4.751 (4.39)  LR: 1.196e-03  Grad: 4.2435  max=1.0297(module.vfe.pfn_layers.0.linear.weight)  min: -3.5386(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8564, loss_cls=0.1879, loss_bbox=0.9779, matched_ious=0.4701, loss_iou=0.0987, loss_iou_reg=0.2483, d_time=0.01(0.01), f_time=1.28(1.27), b_time=1.29(1.28)  Time cost: 23:29/13:55 [3:31:22/18:56:03]  Acc_iter 9900        Data time: 0.01(0.01)  Forward time: 1.28(1.27)  Batch time: 1.29(1.28)
2025-09-04 17:20:28,054   INFO  Train:    6/36 ( 17%) [1154/1759 ( 66%)]  Loss: 4.190 (4.39)  LR: 1.204e-03  Grad: 3.2626  max=0.6177(module.vfe.pfn_layers.0.linear.weight)  min: -2.0124(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8527, loss_cls=0.1883, loss_bbox=0.9971, matched_ious=0.4585, loss_iou=0.1036, loss_iou_reg=0.2524, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.29(1.28)  Time cost: 24:34/12:52 [3:32:26/18:55:20]  Acc_iter 9950        Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.29(1.28)
2025-09-04 17:21:31,435   INFO  Train:    6/36 ( 17%) [1204/1759 ( 68%)]  Loss: 5.496 (4.39)  LR: 1.212e-03  Grad: 3.2787  max=0.5763(module.vfe.pfn_layers.0.linear.weight)  min: -1.8880(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8554, loss_cls=0.1848, loss_bbox=1.0137, matched_ious=0.4668, loss_iou=0.1003, loss_iou_reg=0.2462, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.28)  Time cost: 25:37/11:48 [3:33:30/18:53:57]  Acc_iter 10000       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.28)
2025-09-04 17:22:34,570   INFO  Train:    6/36 ( 17%) [1254/1759 ( 71%)]  Loss: 4.083 (4.38)  LR: 1.220e-03  Grad: 6.1416  max=5.2322(module.vfe.pfn_layers.0.linear.weight)  min: -1.2498(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8443, loss_cls=0.1836, loss_bbox=0.9712, matched_ious=0.4648, loss_iou=0.0993, loss_iou_reg=0.2493, d_time=0.00(0.01), f_time=1.12(1.27), b_time=1.13(1.28)  Time cost: 26:40/10:44 [3:34:33/18:52:25]  Acc_iter 10050       Data time: 0.00(0.01)  Forward time: 1.12(1.27)  Batch time: 1.13(1.28)
2025-09-04 17:23:37,992   INFO  Train:    6/36 ( 17%) [1304/1759 ( 74%)]  Loss: 4.790 (4.38)  LR: 1.228e-03  Grad: 4.3055  max=2.5384(module.vfe.pfn_layers.0.linear.weight)  min: -2.4014(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8186, loss_cls=0.1752, loss_bbox=0.9556, matched_ious=0.4693, loss_iou=0.0995, loss_iou_reg=0.2481, d_time=0.00(0.01), f_time=1.33(1.27), b_time=1.33(1.28)  Time cost: 27:44/09:40 [3:35:36/18:51:08]  Acc_iter 10100       Data time: 0.00(0.01)  Forward time: 1.33(1.27)  Batch time: 1.33(1.28)
2025-09-04 17:24:41,501   INFO  Train:    6/36 ( 17%) [1354/1759 ( 77%)]  Loss: 4.435 (4.38)  LR: 1.236e-03  Grad: 3.8580  max=2.1744(module.vfe.pfn_layers.0.linear.weight)  min: -2.4489(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8553, loss_cls=0.1785, loss_bbox=1.0292, matched_ious=0.4705, loss_iou=0.0990, loss_iou_reg=0.2457, d_time=0.00(0.01), f_time=1.33(1.27), b_time=1.33(1.28)  Time cost: 28:47/08:36 [3:36:40/18:49:54]  Acc_iter 10150       Data time: 0.00(0.01)  Forward time: 1.33(1.27)  Batch time: 1.33(1.28)
2025-09-04 17:25:44,432   INFO  Train:    6/36 ( 17%) [1404/1759 ( 80%)]  Loss: 4.895 (4.37)  LR: 1.244e-03  Grad: 3.4820  max=2.4461(module.vfe.pfn_layers.0.linear.weight)  min: -0.3241(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8296, loss_cls=0.1825, loss_bbox=0.9588, matched_ious=0.4763, loss_iou=0.0981, loss_iou_reg=0.2443, d_time=0.00(0.01), f_time=1.34(1.27), b_time=1.35(1.27)  Time cost: 29:50/07:32 [3:37:43/18:48:20]  Acc_iter 10200       Data time: 0.00(0.01)  Forward time: 1.34(1.27)  Batch time: 1.35(1.27)
2025-09-04 17:26:49,916   INFO  Train:    6/36 ( 17%) [1454/1759 ( 83%)]  Loss: 4.672 (4.37)  LR: 1.252e-03  Grad: 4.8984  max=4.1739(module.vfe.pfn_layers.0.linear.weight)  min: -0.1110(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.8398, loss_cls=0.1824, loss_bbox=0.9262, matched_ious=0.4641, loss_iou=0.1017, loss_iou_reg=0.2505, d_time=0.03(0.01), f_time=1.26(1.27), b_time=1.29(1.28)  Time cost: 30:55/06:29 [3:38:48/18:48:20]  Acc_iter 10250       Data time: 0.03(0.01)  Forward time: 1.26(1.27)  Batch time: 1.29(1.28)
2025-09-04 17:27:53,408   INFO  Train:    6/36 ( 17%) [1504/1759 ( 86%)]  Loss: 3.545 (4.37)  LR: 1.260e-03  Grad: 5.7076  max=1.8403(module.vfe.pfn_layers.0.linear.weight)  min: -4.4215(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8642, loss_cls=0.1905, loss_bbox=0.9946, matched_ious=0.4658, loss_iou=0.1002, loss_iou_reg=0.2513, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.24(1.28)  Time cost: 31:59/05:25 [3:39:52/18:47:06]  Acc_iter 10300       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.28)
2025-09-04 17:28:56,089   INFO  Train:    6/36 ( 17%) [1554/1759 ( 88%)]  Loss: 3.845 (4.37)  LR: 1.268e-03  Grad: 3.8944  max=2.2292(module.vfe.pfn_layers.0.linear.weight)  min: -1.7474(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8441, loss_cls=0.1813, loss_bbox=0.9236, matched_ious=0.4702, loss_iou=0.0992, loss_iou_reg=0.2456, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.27(1.27)  Time cost: 33:02/04:21 [3:40:54/18:45:26]  Acc_iter 10350       Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.27)
2025-09-04 17:29:59,649   INFO  Train:    6/36 ( 17%) [1604/1759 ( 91%)]  Loss: 4.411 (4.37)  LR: 1.276e-03  Grad: 3.4684  max=1.6527(module.vfe.pfn_layers.0.linear.weight)  min: -0.3068(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.8604, loss_cls=0.1842, loss_bbox=1.0164, matched_ious=0.4646, loss_iou=0.1007, loss_iou_reg=0.2500, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.27)  Time cost: 34:05/03:17 [3:41:58/18:44:16]  Acc_iter 10400       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.27)
2025-09-04 17:31:03,118   INFO  Train:    6/36 ( 17%) [1654/1759 ( 94%)]  Loss: 4.690 (4.36)  LR: 1.284e-03  Grad: 3.3669  max=2.0660(module.vfe.pfn_layers.0.linear.weight)  min: -1.1168(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8522, loss_cls=0.1853, loss_bbox=0.9533, matched_ious=0.4696, loss_iou=0.0972, loss_iou_reg=0.2460, d_time=0.02(0.01), f_time=1.29(1.27), b_time=1.31(1.27)  Time cost: 35:09/02:13 [3:43:01/18:43:04]  Acc_iter 10450       Data time: 0.02(0.01)  Forward time: 1.29(1.27)  Batch time: 1.31(1.27)
2025-09-04 17:32:06,725   INFO  Train:    6/36 ( 17%) [1704/1759 ( 97%)]  Loss: 3.819 (4.36)  LR: 1.292e-03  Grad: 2.8475  max=0.9549(module.vfe.pfn_layers.0.linear.weight)  min: -0.1939(module.dense_head.prediction_head.dim.1.bias)  NaN: False  loss_hm=0.8463, loss_cls=0.1856, loss_bbox=0.9883, matched_ious=0.4642, loss_iou=0.1014, loss_iou_reg=0.2491, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.31(1.27)  Time cost: 36:12/01:10 [3:44:05/18:41:57]  Acc_iter 10500       Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.31(1.27)
2025-09-04 17:33:08,618   INFO  Train:    6/36 ( 17%) [1754/1759 (100%)]  Loss: 4.880 (4.36)  LR: 1.300e-03  Grad: 3.5422  max=1.3567(module.vfe.pfn_layers.0.linear.weight)  min: -1.6708(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8178, loss_cls=0.1782, loss_bbox=0.9140, matched_ious=0.4807, loss_iou=0.0984, loss_iou_reg=0.2425, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.23(1.27)  Time cost: 37:14/00:06 [3:45:07/18:39:58]  Acc_iter 10550       Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.23(1.27)
2025-09-04 17:33:13,364   INFO  Train:    6/36 ( 17%) [1758/1759 (100%)]  Loss: 3.464 (4.36)  LR: 1.300e-03  Grad: 5.8753  max=3.8258(module.vfe.pfn_layers.0.linear.weight)  min: -0.1562(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7153, loss_cls=0.1683, loss_bbox=0.8018, matched_ious=0.4670, loss_iou=0.1012, loss_iou_reg=0.2570, d_time=0.00(0.01), f_time=0.77(1.27), b_time=0.78(1.27)  Time cost: 37:19/00:01 [3:45:12/18:39:43]  Acc_iter 10554       Data time: 0.00(0.01)  Forward time: 0.77(1.27)  Batch time: 0.78(1.27)

                                               [Aepochs:  17%|█▋        | 6/36 [3:45:12<18:43:42, 2247.41s/it]epochs:  17%|█▋        | 6/36 [3:45:12<18:43:43, 2247.46s/it]epochs:  17%|█▋        | 6/36 [3:45:12<18:43:43, 2247.46s/it]epochs:  17%|█▋        | 6/36 [3:45:12<18:43:43, 2247.45s/it]epochs:  17%|█▋        | 6/36 [3:45:12<18:43:43, 2247.45s/it]epochs:  17%|█▋        | 6/36 [3:45:12<18:43:43, 2247.45s/it]epochs:  17%|█▋        | 6/36 [3:45:12<18:43:43, 2247.46s/it]epochs:  17%|█▋        | 6/36 [3:45:12<18:43:44, 2247.47s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 17:33:18,804   INFO  Train:    7/36 ( 19%) [   0/1759 (  0%)]  Loss: 4.004 (4.00)  LR: 1.301e-03  Grad: 3.2219  max=1.1878(module.vfe.pfn_layers.0.linear.weight)  min: -2.1640(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7806, loss_cls=0.1881, loss_bbox=1.0002, matched_ious=0.4838, loss_iou=0.0853, loss_iou_reg=0.2384, d_time=1.68(1.68), f_time=2.43(2.43), b_time=4.11(4.11)  Time cost: 00:03/1:50:28 [3:45:17/55:14:07]  Acc_iter 10555       Data time: 1.68(1.68)  Forward time: 2.43(2.43)  Batch time: 4.11(4.11)
2025-09-04 17:34:15,915   INFO  Train:    7/36 ( 19%) [  45/1759 (  3%)]  Loss: 4.863 (4.22)  LR: 1.308e-03  Grad: 5.9338  max=4.6948(module.vfe.pfn_layers.0.linear.weight)  min: -0.1666(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8365, loss_cls=0.1802, loss_bbox=0.9664, matched_ious=0.4685, loss_iou=0.1000, loss_iou_reg=0.2478, d_time=0.01(0.04), f_time=1.37(1.29), b_time=1.37(1.33)  Time cost: 01:00/37:48 [3:46:14/19:22:59]  Acc_iter 10600       Data time: 0.01(0.04)  Forward time: 1.37(1.29)  Batch time: 1.37(1.33)
2025-09-04 17:35:19,999   INFO  Train:    7/36 ( 19%) [  95/1759 (  5%)]  Loss: 4.264 (4.27)  LR: 1.316e-03  Grad: 3.7917  max=2.3627(module.vfe.pfn_layers.0.linear.weight)  min: -0.9046(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8511, loss_cls=0.1798, loss_bbox=1.0230, matched_ious=0.4623, loss_iou=0.0981, loss_iou_reg=0.2489, d_time=0.00(0.02), f_time=1.23(1.28), b_time=1.23(1.31)  Time cost: 02:04/36:06 [3:47:18/19:02:46]  Acc_iter 10650       Data time: 0.00(0.02)  Forward time: 1.23(1.28)  Batch time: 1.23(1.31)
2025-09-04 17:36:24,153   INFO  Train:    7/36 ( 19%) [ 145/1759 (  8%)]  Loss: 4.179 (4.25)  LR: 1.324e-03  Grad: 2.0451  max=0.1894(module.backbone_3d.cls_conv.3.weight)  min: -0.8304(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8131, loss_cls=0.1795, loss_bbox=0.9516, matched_ious=0.4687, loss_iou=0.0987, loss_iou_reg=0.2476, d_time=0.00(0.02), f_time=1.31(1.28), b_time=1.31(1.30)  Time cost: 03:09/34:50 [3:48:22/18:56:06]  Acc_iter 10700       Data time: 0.00(0.02)  Forward time: 1.31(1.28)  Batch time: 1.31(1.30)
2025-09-04 17:37:31,150   INFO  Train:    7/36 ( 19%) [ 195/1759 ( 11%)]  Loss: 3.521 (4.23)  LR: 1.332e-03  Grad: 2.4779  max=1.3138(module.vfe.pfn_layers.0.linear.weight)  min: -0.2692(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8231, loss_cls=0.1803, loss_bbox=0.9323, matched_ious=0.4736, loss_iou=0.1006, loss_iou_reg=0.2457, d_time=0.00(0.02), f_time=1.24(1.29), b_time=1.25(1.31)  Time cost: 04:16/34:03 [3:49:29/19:05:00]  Acc_iter 10750       Data time: 0.00(0.02)  Forward time: 1.24(1.29)  Batch time: 1.25(1.31)
2025-09-04 17:38:35,489   INFO  Train:    7/36 ( 19%) [ 245/1759 ( 14%)]  Loss: 3.559 (4.22)  LR: 1.340e-03  Grad: 3.8551  max=2.5139(module.vfe.pfn_layers.0.linear.weight)  min: -2.1734(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8138, loss_cls=0.1758, loss_bbox=0.9552, matched_ious=0.4699, loss_iou=0.0992, loss_iou_reg=0.2476, d_time=0.01(0.01), f_time=1.27(1.29), b_time=1.28(1.30)  Time cost: 05:20/32:52 [3:50:34/19:00:22]  Acc_iter 10800       Data time: 0.01(0.01)  Forward time: 1.27(1.29)  Batch time: 1.28(1.30)
2025-09-04 17:39:40,045   INFO  Train:    7/36 ( 19%) [ 295/1759 ( 17%)]  Loss: 4.158 (4.21)  LR: 1.349e-03  Grad: 2.9788  max=2.0907(module.vfe.pfn_layers.0.linear.weight)  min: -1.1832(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7915, loss_cls=0.1698, loss_bbox=0.9697, matched_ious=0.4757, loss_iou=0.0982, loss_iou_reg=0.2452, d_time=0.00(0.01), f_time=1.21(1.29), b_time=1.22(1.30)  Time cost: 06:25/31:44 [3:51:38/18:57:34]  Acc_iter 10850       Data time: 0.00(0.01)  Forward time: 1.21(1.29)  Batch time: 1.22(1.30)
2025-09-04 17:40:43,655   INFO  Train:    7/36 ( 19%) [ 345/1759 ( 20%)]  Loss: 5.068 (4.23)  LR: 1.357e-03  Grad: 9.0216  max=8.5463(module.vfe.pfn_layers.0.linear.weight)  min: -0.3241(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8718, loss_cls=0.1849, loss_bbox=0.9938, matched_ious=0.4598, loss_iou=0.1013, loss_iou_reg=0.2517, d_time=0.01(0.01), f_time=1.25(1.28), b_time=1.26(1.30)  Time cost: 07:28/30:33 [3:52:42/18:52:53]  Acc_iter 10900       Data time: 0.01(0.01)  Forward time: 1.25(1.28)  Batch time: 1.26(1.30)
2025-09-04 17:41:46,820   INFO  Train:    7/36 ( 19%) [ 395/1759 ( 22%)]  Loss: 3.330 (4.23)  LR: 1.365e-03  Grad: 7.0498  max=6.0897(module.vfe.pfn_layers.0.linear.weight)  min: -1.2982(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8342, loss_cls=0.1774, loss_bbox=0.9266, matched_ious=0.4724, loss_iou=0.0999, loss_iou_reg=0.2472, d_time=0.00(0.01), f_time=1.29(1.28), b_time=1.29(1.29)  Time cost: 08:31/29:22 [3:53:45/18:48:08]  Acc_iter 10950       Data time: 0.00(0.01)  Forward time: 1.29(1.28)  Batch time: 1.29(1.29)
2025-09-04 17:42:49,464   INFO  Train:    7/36 ( 19%) [ 445/1759 ( 25%)]  Loss: 4.849 (4.23)  LR: 1.373e-03  Grad: 4.7933  max=1.9942(module.vfe.pfn_layers.0.linear.weight)  min: -3.7217(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8472, loss_cls=0.1824, loss_bbox=0.9538, matched_ious=0.4819, loss_iou=0.0983, loss_iou_reg=0.2424, d_time=0.01(0.01), f_time=1.24(1.28), b_time=1.24(1.29)  Time cost: 09:34/28:12 [3:54:48/18:43:12]  Acc_iter 11000       Data time: 0.01(0.01)  Forward time: 1.24(1.28)  Batch time: 1.24(1.29)
2025-09-04 17:43:53,649   INFO  Train:    7/36 ( 19%) [ 495/1759 ( 28%)]  Loss: 5.031 (4.24)  LR: 1.381e-03  Grad: 3.5112  max=1.4543(module.vfe.pfn_layers.0.linear.weight)  min: -2.1864(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8385, loss_cls=0.1794, loss_bbox=0.9458, matched_ious=0.4624, loss_iou=0.1035, loss_iou_reg=0.2494, d_time=0.00(0.01), f_time=1.26(1.28), b_time=1.26(1.29)  Time cost: 10:38/27:07 [3:55:52/18:41:45]  Acc_iter 11050       Data time: 0.00(0.01)  Forward time: 1.26(1.28)  Batch time: 1.26(1.29)
2025-09-04 17:44:56,264   INFO  Train:    7/36 ( 19%) [ 545/1759 ( 31%)]  Loss: 4.115 (4.23)  LR: 1.390e-03  Grad: 5.6463  max=1.5982(module.vfe.pfn_layers.0.linear.weight)  min: -4.9704(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8307, loss_cls=0.1790, loss_bbox=0.9067, matched_ious=0.4745, loss_iou=0.1012, loss_iou_reg=0.2466, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.29(1.28)  Time cost: 11:41/25:59 [3:56:55/18:37:52]  Acc_iter 11100       Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.29(1.28)
2025-09-04 17:45:59,050   INFO  Train:    7/36 ( 19%) [ 595/1759 ( 34%)]  Loss: 4.219 (4.22)  LR: 1.398e-03  Grad: 4.7094  max=3.4637(module.vfe.pfn_layers.0.linear.weight)  min: -2.0034(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8040, loss_cls=0.1779, loss_bbox=0.9033, matched_ious=0.4734, loss_iou=0.0978, loss_iou_reg=0.2454, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.28)  Time cost: 12:44/24:52 [3:57:57/18:34:43]  Acc_iter 11150       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.28)
2025-09-04 17:47:02,202   INFO  Train:    7/36 ( 19%) [ 645/1759 ( 37%)]  Loss: 4.244 (4.21)  LR: 1.406e-03  Grad: 4.2866  max=1.8694(module.vfe.pfn_layers.0.linear.weight)  min: -2.8962(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8101, loss_cls=0.1723, loss_bbox=0.9409, matched_ious=0.4739, loss_iou=0.0999, loss_iou_reg=0.2448, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.28(1.28)  Time cost: 13:47/23:46 [3:59:00/18:32:23]  Acc_iter 11200       Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.28(1.28)
2025-09-04 17:48:08,230   INFO  Train:    7/36 ( 19%) [ 695/1759 ( 40%)]  Loss: 4.623 (4.22)  LR: 1.414e-03  Grad: 3.1797  max=1.1486(module.vfe.pfn_layers.0.linear.weight)  min: -1.5807(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8387, loss_cls=0.1788, loss_bbox=0.9851, matched_ious=0.4658, loss_iou=0.0987, loss_iou_reg=0.2495, d_time=0.01(0.01), f_time=1.32(1.27), b_time=1.33(1.28)  Time cost: 14:53/22:45 [4:00:06/18:33:49]  Acc_iter 11250       Data time: 0.01(0.01)  Forward time: 1.32(1.27)  Batch time: 1.33(1.28)
2025-09-04 17:49:12,231   INFO  Train:    7/36 ( 19%) [ 745/1759 ( 42%)]  Loss: 3.854 (4.23)  LR: 1.422e-03  Grad: 3.6285  max=0.8865(module.vfe.pfn_layers.0.linear.weight)  min: -2.9908(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8734, loss_cls=0.1871, loss_bbox=1.0058, matched_ious=0.4553, loss_iou=0.1023, loss_iou_reg=0.2542, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.28)  Time cost: 15:57/21:41 [4:01:10/18:32:33]  Acc_iter 11300       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.28)
2025-09-04 17:50:16,009   INFO  Train:    7/36 ( 19%) [ 795/1759 ( 45%)]  Loss: 3.782 (4.24)  LR: 1.431e-03  Grad: 3.9640  max=2.2801(module.vfe.pfn_layers.0.linear.weight)  min: -2.2428(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8476, loss_cls=0.1782, loss_bbox=0.9668, matched_ious=0.4657, loss_iou=0.1018, loss_iou_reg=0.2474, d_time=0.01(0.01), f_time=1.31(1.27), b_time=1.32(1.28)  Time cost: 17:00/20:36 [4:02:14/18:31:04]  Acc_iter 11350       Data time: 0.01(0.01)  Forward time: 1.31(1.27)  Batch time: 1.32(1.28)
2025-09-04 17:51:18,763   INFO  Train:    7/36 ( 19%) [ 845/1759 ( 48%)]  Loss: 4.463 (4.24)  LR: 1.439e-03  Grad: 2.9387  max=1.9314(module.vfe.pfn_layers.0.linear.weight)  min: -0.3532(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8301, loss_cls=0.1776, loss_bbox=0.9630, matched_ious=0.4580, loss_iou=0.1017, loss_iou_reg=0.2534, d_time=0.00(0.01), f_time=1.33(1.27), b_time=1.33(1.28)  Time cost: 18:03/19:30 [4:03:17/18:28:35]  Acc_iter 11400       Data time: 0.00(0.01)  Forward time: 1.33(1.27)  Batch time: 1.33(1.28)
2025-09-04 17:52:21,925   INFO  Train:    7/36 ( 19%) [ 895/1759 ( 51%)]  Loss: 4.771 (4.25)  LR: 1.447e-03  Grad: 4.4585  max=3.4404(module.vfe.pfn_layers.0.linear.weight)  min: -0.0776(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.8616, loss_cls=0.1820, loss_bbox=0.9917, matched_ious=0.4624, loss_iou=0.1012, loss_iou_reg=0.2523, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.27(1.28)  Time cost: 19:06/18:25 [4:04:20/18:26:40]  Acc_iter 11450       Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.28)
2025-09-04 17:53:24,844   INFO  Train:    7/36 ( 19%) [ 945/1759 ( 54%)]  Loss: 5.039 (4.25)  LR: 1.456e-03  Grad: 10.0000  max=0.6278(module.vfe.pfn_layers.0.linear.weight)  min: -9.6735(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8334, loss_cls=0.1774, loss_bbox=0.9846, matched_ious=0.4684, loss_iou=0.1020, loss_iou_reg=0.2474, d_time=0.01(0.01), f_time=1.26(1.27), b_time=1.26(1.28)  Time cost: 20:09/17:20 [4:05:23/18:24:37]  Acc_iter 11500       Data time: 0.01(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.28)
2025-09-04 17:54:27,289   INFO  Train:    7/36 ( 19%) [ 995/1759 ( 57%)]  Loss: 3.274 (4.25)  LR: 1.464e-03  Grad: 3.9354  max=2.6729(module.vfe.pfn_layers.0.linear.weight)  min: -0.6401(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8364, loss_cls=0.1790, loss_bbox=0.9618, matched_ious=0.4643, loss_iou=0.1005, loss_iou_reg=0.2499, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.27(1.28)  Time cost: 21:12/16:15 [4:06:26/18:22:15]  Acc_iter 11550       Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.28)
2025-09-04 17:55:29,938   INFO  Train:    7/36 ( 19%) [1045/1759 ( 59%)]  Loss: 4.697 (4.25)  LR: 1.472e-03  Grad: 4.3139  max=0.4138(module.vfe.pfn_layers.0.linear.weight)  min: -2.9055(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8609, loss_cls=0.1826, loss_bbox=0.9755, matched_ious=0.4707, loss_iou=0.0986, loss_iou_reg=0.2462, d_time=0.01(0.01), f_time=1.34(1.27), b_time=1.35(1.28)  Time cost: 22:14/15:11 [4:07:28/18:20:11]  Acc_iter 11600       Data time: 0.01(0.01)  Forward time: 1.34(1.27)  Batch time: 1.35(1.28)
2025-09-04 17:56:32,839   INFO  Train:    7/36 ( 19%) [1095/1759 ( 62%)]  Loss: 3.960 (4.25)  LR: 1.480e-03  Grad: 6.0252  max=0.2099(module.backbone_3d.cls_conv.3.weight)  min: -3.7348(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8389, loss_cls=0.1748, loss_bbox=0.9528, matched_ious=0.4709, loss_iou=0.1024, loss_iou_reg=0.2465, d_time=0.00(0.01), f_time=1.20(1.27), b_time=1.20(1.28)  Time cost: 23:17/14:06 [4:08:31/18:18:24]  Acc_iter 11650       Data time: 0.00(0.01)  Forward time: 1.20(1.27)  Batch time: 1.20(1.28)
2025-09-04 17:57:36,111   INFO  Train:    7/36 ( 19%) [1145/1759 ( 65%)]  Loss: 4.323 (4.24)  LR: 1.489e-03  Grad: 4.0920  max=1.6023(module.vfe.pfn_layers.0.linear.weight)  min: -2.4730(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8002, loss_cls=0.1681, loss_bbox=0.9460, matched_ious=0.4691, loss_iou=0.0993, loss_iou_reg=0.2491, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.28(1.28)  Time cost: 24:21/13:02 [4:09:34/18:16:58]  Acc_iter 11700       Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.28(1.28)
2025-09-04 17:58:42,235   INFO  Train:    7/36 ( 19%) [1195/1759 ( 68%)]  Loss: 4.401 (4.24)  LR: 1.497e-03  Grad: 2.1614  max=0.7454(module.vfe.pfn_layers.0.linear.weight)  min: -0.4143(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8129, loss_cls=0.1679, loss_bbox=0.9367, matched_ious=0.4762, loss_iou=0.1001, loss_iou_reg=0.2457, d_time=0.00(0.01), f_time=1.39(1.27), b_time=1.40(1.28)  Time cost: 25:27/12:00 [4:10:41/18:17:37]  Acc_iter 11750       Data time: 0.00(0.01)  Forward time: 1.39(1.27)  Batch time: 1.40(1.28)
2025-09-04 17:59:45,057   INFO  Train:    7/36 ( 19%) [1245/1759 ( 71%)]  Loss: 3.883 (4.24)  LR: 1.505e-03  Grad: 3.1037  max=1.2627(module.vfe.pfn_layers.0.linear.weight)  min: -1.4969(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8175, loss_cls=0.1714, loss_bbox=0.9359, matched_ious=0.4690, loss_iou=0.0991, loss_iou_reg=0.2486, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.28)  Time cost: 26:30/10:55 [4:11:43/18:15:51]  Acc_iter 11800       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.28)
2025-09-04 18:00:48,144   INFO  Train:    7/36 ( 19%) [1295/1759 ( 74%)]  Loss: 3.841 (4.23)  LR: 1.514e-03  Grad: 2.3624  max=1.1148(module.vfe.pfn_layers.0.linear.weight)  min: -0.4558(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8313, loss_cls=0.1750, loss_bbox=0.9163, matched_ious=0.4813, loss_iou=0.0976, loss_iou_reg=0.2422, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.23(1.28)  Time cost: 27:33/09:51 [4:12:46/18:14:18]  Acc_iter 11850       Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.23(1.28)
2025-09-04 18:01:51,693   INFO  Train:    7/36 ( 19%) [1345/1759 ( 76%)]  Loss: 4.334 (4.23)  LR: 1.522e-03  Grad: 2.4438  max=1.4251(module.vfe.pfn_layers.0.linear.weight)  min: -0.4735(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8094, loss_cls=0.1698, loss_bbox=0.8801, matched_ious=0.4774, loss_iou=0.0977, loss_iou_reg=0.2439, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.26(1.28)  Time cost: 28:36/08:48 [4:13:50/18:13:06]  Acc_iter 11900       Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.28)
2025-09-04 18:02:55,335   INFO  Train:    7/36 ( 19%) [1395/1759 ( 79%)]  Loss: 3.645 (4.23)  LR: 1.530e-03  Grad: 4.6177  max=4.2105(module.vfe.pfn_layers.0.linear.weight)  min: -0.1370(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.8631, loss_cls=0.1829, loss_bbox=1.0172, matched_ious=0.4732, loss_iou=0.0989, loss_iou_reg=0.2441, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.28)  Time cost: 29:40/07:44 [4:14:54/18:11:57]  Acc_iter 11950       Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.28)
2025-09-04 18:03:57,985   INFO  Train:    7/36 ( 19%) [1445/1759 ( 82%)]  Loss: 3.583 (4.23)  LR: 1.539e-03  Grad: 3.1257  max=2.5613(module.vfe.pfn_layers.0.linear.weight)  min: -1.1361(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8314, loss_cls=0.1744, loss_bbox=0.9879, matched_ious=0.4770, loss_iou=0.0983, loss_iou_reg=0.2419, d_time=0.00(0.01), f_time=1.14(1.27), b_time=1.14(1.27)  Time cost: 30:42/06:40 [4:15:56/18:10:14]  Acc_iter 12000       Data time: 0.00(0.01)  Forward time: 1.14(1.27)  Batch time: 1.14(1.27)
2025-09-04 18:05:00,724   INFO  Train:    7/36 ( 19%) [1495/1759 ( 85%)]  Loss: 3.899 (4.23)  LR: 1.547e-03  Grad: 2.0756  max=0.2127(module.backbone_3d.cls_conv.3.weight)  min: -1.2671(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8234, loss_cls=0.1757, loss_bbox=0.9468, matched_ious=0.4724, loss_iou=0.0988, loss_iou_reg=0.2441, d_time=0.00(0.01), f_time=1.36(1.27), b_time=1.36(1.27)  Time cost: 31:45/05:36 [4:16:59/18:08:36]  Acc_iter 12050       Data time: 0.00(0.01)  Forward time: 1.36(1.27)  Batch time: 1.36(1.27)
2025-09-04 18:06:03,410   INFO  Train:    7/36 ( 19%) [1545/1759 ( 88%)]  Loss: 4.412 (4.24)  LR: 1.555e-03  Grad: 3.0698  max=2.1994(module.vfe.pfn_layers.0.linear.weight)  min: -0.1576(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.8410, loss_cls=0.1758, loss_bbox=1.0104, matched_ious=0.4645, loss_iou=0.0988, loss_iou_reg=0.2489, d_time=0.00(0.01), f_time=1.33(1.27), b_time=1.33(1.27)  Time cost: 32:48/04:32 [4:18:02/18:06:59]  Acc_iter 12100       Data time: 0.00(0.01)  Forward time: 1.33(1.27)  Batch time: 1.33(1.27)
2025-09-04 18:07:06,832   INFO  Train:    7/36 ( 19%) [1595/1759 ( 91%)]  Loss: 3.885 (4.23)  LR: 1.564e-03  Grad: 2.3535  max=0.7621(module.vfe.pfn_layers.0.linear.weight)  min: -0.7502(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8248, loss_cls=0.1690, loss_bbox=0.9608, matched_ious=0.4748, loss_iou=0.0972, loss_iou_reg=0.2431, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.24(1.27)  Time cost: 33:51/03:28 [4:19:05/18:05:48]  Acc_iter 12150       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.27)
2025-09-04 18:08:10,102   INFO  Train:    7/36 ( 19%) [1645/1759 ( 94%)]  Loss: 4.385 (4.23)  LR: 1.572e-03  Grad: 3.3569  max=1.7829(module.vfe.pfn_layers.0.linear.weight)  min: -1.6168(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7946, loss_cls=0.1632, loss_bbox=0.9321, matched_ious=0.4820, loss_iou=0.0968, loss_iou_reg=0.2418, d_time=0.00(0.01), f_time=1.33(1.27), b_time=1.33(1.27)  Time cost: 34:55/02:25 [4:20:08/18:04:33]  Acc_iter 12200       Data time: 0.00(0.01)  Forward time: 1.33(1.27)  Batch time: 1.33(1.27)
2025-09-04 18:09:18,051   INFO  Train:    7/36 ( 19%) [1695/1759 ( 96%)]  Loss: 3.572 (4.23)  LR: 1.580e-03  Grad: 4.8048  max=4.2581(module.vfe.pfn_layers.0.linear.weight)  min: -0.2080(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8037, loss_cls=0.1664, loss_bbox=0.9442, matched_ious=0.4694, loss_iou=0.0998, loss_iou_reg=0.2474, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.29(1.28)  Time cost: 36:03/01:21 [4:21:16/18:05:39]  Acc_iter 12250       Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.28)
2025-09-04 18:10:21,368   INFO  Train:    7/36 ( 19%) [1745/1759 ( 99%)]  Loss: 4.052 (4.23)  LR: 1.589e-03  Grad: 6.7109  max=5.6052(module.vfe.pfn_layers.0.linear.weight)  min: -2.4576(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8045, loss_cls=0.1675, loss_bbox=0.9307, matched_ious=0.4756, loss_iou=0.0983, loss_iou_reg=0.2448, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.27(1.28)  Time cost: 37:06/00:17 [4:22:20/18:04:22]  Acc_iter 12300       Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.28)
2025-09-04 18:10:36,828   INFO  Train:    7/36 ( 19%) [1758/1759 (100%)]  Loss: 3.513 (4.23)  LR: 1.591e-03  Grad: 3.6182  max=0.1954(module.vfe.pfn_layers.0.linear.weight)  min: -2.1274(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8589, loss_cls=0.1798, loss_bbox=1.0508, matched_ious=0.4737, loss_iou=0.0968, loss_iou_reg=0.2384, d_time=0.00(0.01), f_time=0.70(1.27), b_time=0.71(1.27)  Time cost: 37:21/00:01 [4:22:35/18:03:32]  Acc_iter 12313       Data time: 0.00(0.01)  Forward time: 0.70(1.27)  Batch time: 0.71(1.27)

                                               [Aepochs:  19%|█▉        | 7/36 [4:22:36<18:05:37, 2246.13s/it]epochs:  19%|█▉        | 7/36 [4:22:36<18:05:37, 2246.11s/it]epochs:  19%|█▉        | 7/36 [4:22:36<18:05:37, 2246.12s/it]epochs:  19%|█▉        | 7/36 [4:22:36<18:05:37, 2246.13s/it]epochs:  19%|█▉        | 7/36 [4:22:36<18:05:37, 2246.13s/it]epochs:  19%|█▉        | 7/36 [4:22:36<18:05:37, 2246.13s/it]epochs:  19%|█▉        | 7/36 [4:22:36<18:05:37, 2246.13s/it]epochs:  19%|█▉        | 7/36 [4:22:36<18:05:37, 2246.13s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 18:10:42,288   INFO  Train:    8/36 ( 22%) [   0/1759 (  0%)]  Loss: 5.059 (5.06)  LR: 1.591e-03  Grad: 5.4319  max=3.1547(module.vfe.pfn_layers.0.linear.weight)  min: -2.8792(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9522, loss_cls=0.1789, loss_bbox=1.4227, matched_ious=0.4455, loss_iou=0.0982, loss_iou_reg=0.2393, d_time=1.60(1.60), f_time=2.59(2.59), b_time=4.19(4.19)  Time cost: 00:03/1:55:02 [4:22:41/55:36:22]  Acc_iter 12314       Data time: 1.60(1.60)  Forward time: 2.59(2.59)  Batch time: 4.19(4.19)
2025-09-04 18:11:28,126   INFO  Train:    8/36 ( 22%) [  36/1759 (  2%)]  Loss: 3.805 (4.23)  LR: 1.597e-03  Grad: 2.9952  max=1.7694(module.vfe.pfn_layers.0.linear.weight)  min: -0.4414(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8036, loss_cls=0.1659, loss_bbox=0.9402, matched_ious=0.4691, loss_iou=0.1001, loss_iou_reg=0.2494, d_time=0.00(0.05), f_time=1.23(1.30), b_time=1.24(1.35)  Time cost: 00:49/38:37 [4:23:26/19:02:38]  Acc_iter 12350       Data time: 0.00(0.05)  Forward time: 1.23(1.30)  Batch time: 1.24(1.35)
2025-09-04 18:12:31,730   INFO  Train:    8/36 ( 22%) [  86/1759 (  5%)]  Loss: 4.120 (4.15)  LR: 1.606e-03  Grad: 4.7257  max=1.1653(module.vfe.pfn_layers.0.linear.weight)  min: -3.8752(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7908, loss_cls=0.1661, loss_bbox=0.9032, matched_ious=0.4797, loss_iou=0.0985, loss_iou_reg=0.2432, d_time=0.01(0.02), f_time=1.23(1.28), b_time=1.24(1.31)  Time cost: 01:53/36:20 [4:24:30/18:25:58]  Acc_iter 12400       Data time: 0.01(0.02)  Forward time: 1.23(1.28)  Batch time: 1.24(1.31)
2025-09-04 18:13:34,751   INFO  Train:    8/36 ( 22%) [ 136/1759 (  8%)]  Loss: 4.765 (4.15)  LR: 1.614e-03  Grad: 3.0987  max=1.0372(module.vfe.pfn_layers.0.linear.weight)  min: -1.7793(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8207, loss_cls=0.1727, loss_bbox=0.9115, matched_ious=0.4761, loss_iou=0.1000, loss_iou_reg=0.2447, d_time=0.00(0.02), f_time=1.26(1.27), b_time=1.27(1.29)  Time cost: 02:56/34:49 [4:25:33/18:11:41]  Acc_iter 12450       Data time: 0.00(0.02)  Forward time: 1.26(1.27)  Batch time: 1.27(1.29)
2025-09-04 18:14:36,944   INFO  Train:    8/36 ( 22%) [ 186/1759 ( 11%)]  Loss: 4.911 (4.14)  LR: 1.622e-03  Grad: 2.5313  max=0.8151(module.vfe.pfn_layers.0.linear.weight)  min: -0.4246(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8035, loss_cls=0.1679, loss_bbox=0.9744, matched_ious=0.4714, loss_iou=0.0976, loss_iou_reg=0.2459, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.23(1.28)  Time cost: 03:58/33:26 [4:26:35/18:00:42]  Acc_iter 12500       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.28)
2025-09-04 18:15:40,903   INFO  Train:    8/36 ( 22%) [ 236/1759 ( 13%)]  Loss: 4.259 (4.14)  LR: 1.631e-03  Grad: 3.6254  max=0.4953(module.vfe.pfn_layers.0.linear.weight)  min: -1.9870(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8153, loss_cls=0.1715, loss_bbox=0.9346, matched_ious=0.4762, loss_iou=0.1003, loss_iou_reg=0.2437, d_time=0.03(0.01), f_time=1.49(1.27), b_time=1.52(1.28)  Time cost: 05:02/32:24 [4:27:39/18:00:15]  Acc_iter 12550       Data time: 0.03(0.01)  Forward time: 1.49(1.27)  Batch time: 1.52(1.28)
2025-09-04 18:16:44,425   INFO  Train:    8/36 ( 22%) [ 286/1759 ( 16%)]  Loss: 4.876 (4.14)  LR: 1.639e-03  Grad: 2.8600  max=0.7936(module.vfe.pfn_layers.0.linear.weight)  min: -0.6028(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7934, loss_cls=0.1687, loss_bbox=0.8969, matched_ious=0.4770, loss_iou=0.1001, loss_iou_reg=0.2442, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.28)  Time cost: 06:06/31:18 [4:28:43/17:58:18]  Acc_iter 12600       Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.28)
2025-09-04 18:17:47,461   INFO  Train:    8/36 ( 22%) [ 336/1759 ( 19%)]  Loss: 4.110 (4.13)  LR: 1.647e-03  Grad: 3.3082  max=0.3925(module.dense_head.prediction_head.height.1.bias)  min: -0.7235(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7899, loss_cls=0.1692, loss_bbox=0.9355, matched_ious=0.4720, loss_iou=0.1004, loss_iou_reg=0.2450, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.22(1.27)  Time cost: 07:09/30:11 [4:29:46/17:55:23]  Acc_iter 12650       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.27)
2025-09-04 18:18:51,369   INFO  Train:    8/36 ( 22%) [ 386/1759 ( 22%)]  Loss: 3.879 (4.15)  LR: 1.656e-03  Grad: 4.2288  max=2.2771(module.vfe.pfn_layers.0.linear.weight)  min: -1.7001(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8218, loss_cls=0.1751, loss_bbox=0.9784, matched_ious=0.4745, loss_iou=0.0996, loss_iou_reg=0.2454, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.26(1.27)  Time cost: 08:13/29:09 [4:30:50/17:54:52]  Acc_iter 12700       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.27)
2025-09-04 18:19:56,859   INFO  Train:    8/36 ( 22%) [ 436/1759 ( 25%)]  Loss: 3.045 (4.16)  LR: 1.664e-03  Grad: 5.6492  max=3.9917(module.vfe.pfn_layers.0.linear.weight)  min: -0.7754(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8249, loss_cls=0.1727, loss_bbox=0.9437, matched_ious=0.4760, loss_iou=0.0989, loss_iou_reg=0.2447, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.28)  Time cost: 09:18/28:10 [4:31:55/17:57:16]  Acc_iter 12750       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.28)
2025-09-04 18:20:59,909   INFO  Train:    8/36 ( 22%) [ 486/1759 ( 28%)]  Loss: 4.357 (4.14)  LR: 1.673e-03  Grad: 3.0956  max=1.5598(module.vfe.pfn_layers.0.linear.weight)  min: -0.7550(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7669, loss_cls=0.1638, loss_bbox=0.8667, matched_ious=0.4786, loss_iou=0.0972, loss_iou_reg=0.2445, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.31(1.28)  Time cost: 10:21/27:04 [4:32:58/17:54:43]  Acc_iter 12800       Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.31(1.28)
2025-09-04 18:22:03,192   INFO  Train:    8/36 ( 22%) [ 536/1759 ( 30%)]  Loss: 3.911 (4.15)  LR: 1.681e-03  Grad: 3.4993  max=1.5953(module.vfe.pfn_layers.0.linear.weight)  min: -1.7629(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7964, loss_cls=0.1681, loss_bbox=0.9119, matched_ious=0.4749, loss_iou=0.0978, loss_iou_reg=0.2451, d_time=0.00(0.01), f_time=1.20(1.27), b_time=1.20(1.28)  Time cost: 11:24/25:59 [4:34:01/17:52:50]  Acc_iter 12850       Data time: 0.00(0.01)  Forward time: 1.20(1.27)  Batch time: 1.20(1.28)
2025-09-04 18:23:06,489   INFO  Train:    8/36 ( 22%) [ 586/1759 ( 33%)]  Loss: 4.524 (4.14)  LR: 1.689e-03  Grad: 4.9062  max=3.2652(module.vfe.pfn_layers.0.linear.weight)  min: -2.2181(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7902, loss_cls=0.1665, loss_bbox=0.9171, matched_ious=0.4870, loss_iou=0.0981, loss_iou_reg=0.2390, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.21(1.27)  Time cost: 12:28/24:54 [4:35:05/17:51:06]  Acc_iter 12900       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.27)
2025-09-04 18:24:09,977   INFO  Train:    8/36 ( 22%) [ 636/1759 ( 36%)]  Loss: 3.909 (4.14)  LR: 1.698e-03  Grad: 3.3948  max=1.3984(module.vfe.pfn_layers.0.linear.weight)  min: -0.1355(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.8030, loss_cls=0.1684, loss_bbox=0.9276, matched_ious=0.4766, loss_iou=0.1006, loss_iou_reg=0.2436, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 13:31/23:50 [4:36:08/17:49:43]  Acc_iter 12950       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-04 18:25:12,964   INFO  Train:    8/36 ( 22%) [ 686/1759 ( 39%)]  Loss: 3.672 (4.14)  LR: 1.706e-03  Grad: 3.3695  max=0.6627(module.vfe.pfn_layers.0.linear.weight)  min: -1.0188(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7758, loss_cls=0.1619, loss_bbox=0.9153, matched_ious=0.4664, loss_iou=0.0998, loss_iou_reg=0.2482, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.20(1.27)  Time cost: 14:34/22:46 [4:37:11/17:47:47]  Acc_iter 13000       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.20(1.27)
2025-09-04 18:26:16,013   INFO  Train:    8/36 ( 22%) [ 736/1759 ( 42%)]  Loss: 3.899 (4.13)  LR: 1.714e-03  Grad: 4.3823  max=0.1855(module.vfe.pfn_layers.0.linear.weight)  min: -3.0775(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7736, loss_cls=0.1637, loss_bbox=0.8782, matched_ious=0.4884, loss_iou=0.0957, loss_iou_reg=0.2388, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.22(1.27)  Time cost: 15:37/21:41 [4:38:14/17:46:02]  Acc_iter 13050       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.27)
2025-09-04 18:27:19,549   INFO  Train:    8/36 ( 22%) [ 786/1759 ( 45%)]  Loss: 4.269 (4.12)  LR: 1.723e-03  Grad: 4.1691  max=3.2798(module.vfe.pfn_layers.0.linear.weight)  min: -0.2259(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7989, loss_cls=0.1684, loss_bbox=0.9222, matched_ious=0.4714, loss_iou=0.1002, loss_iou_reg=0.2483, d_time=0.00(0.01), f_time=1.32(1.26), b_time=1.32(1.27)  Time cost: 16:41/20:37 [4:39:18/17:44:54]  Acc_iter 13100       Data time: 0.00(0.01)  Forward time: 1.32(1.26)  Batch time: 1.32(1.27)
2025-09-04 18:28:23,889   INFO  Train:    8/36 ( 22%) [ 836/1759 ( 48%)]  Loss: 3.893 (4.12)  LR: 1.731e-03  Grad: 3.5020  max=2.2734(module.vfe.pfn_layers.0.linear.weight)  min: -0.7501(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7946, loss_cls=0.1673, loss_bbox=0.8879, matched_ious=0.4838, loss_iou=0.0992, loss_iou_reg=0.2409, d_time=0.01(0.01), f_time=1.29(1.26), b_time=1.30(1.27)  Time cost: 17:45/19:35 [4:40:22/17:44:34]  Acc_iter 13150       Data time: 0.01(0.01)  Forward time: 1.29(1.26)  Batch time: 1.30(1.27)
2025-09-04 18:29:27,029   INFO  Train:    8/36 ( 22%) [ 886/1759 ( 50%)]  Loss: 4.285 (4.11)  LR: 1.739e-03  Grad: 4.5098  max=2.8877(module.vfe.pfn_layers.0.linear.weight)  min: -2.2532(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7618, loss_cls=0.1648, loss_bbox=0.8851, matched_ious=0.4873, loss_iou=0.0979, loss_iou_reg=0.2410, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.27(1.27)  Time cost: 18:48/18:30 [4:41:25/17:43:01]  Acc_iter 13200       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.27)
2025-09-04 18:30:33,249   INFO  Train:    8/36 ( 22%) [ 936/1759 ( 53%)]  Loss: 4.130 (4.11)  LR: 1.748e-03  Grad: 3.4376  max=1.2916(module.vfe.pfn_layers.0.linear.weight)  min: -1.7188(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7975, loss_cls=0.1675, loss_bbox=0.9518, matched_ious=0.4811, loss_iou=0.0972, loss_iou_reg=0.2417, d_time=0.00(0.01), f_time=1.15(1.27), b_time=1.15(1.28)  Time cost: 19:54/17:29 [4:42:32/17:44:16]  Acc_iter 13250       Data time: 0.00(0.01)  Forward time: 1.15(1.27)  Batch time: 1.15(1.28)
2025-09-04 18:31:36,517   INFO  Train:    8/36 ( 22%) [ 986/1759 ( 56%)]  Loss: 3.897 (4.12)  LR: 1.756e-03  Grad: 4.0198  max=2.4178(module.vfe.pfn_layers.0.linear.weight)  min: -1.5415(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8311, loss_cls=0.1700, loss_bbox=0.9587, matched_ious=0.4770, loss_iou=0.0977, loss_iou_reg=0.2422, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.21(1.27)  Time cost: 20:58/16:25 [4:43:35/17:42:48]  Acc_iter 13300       Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.27)
2025-09-04 18:32:38,964   INFO  Train:    8/36 ( 22%) [1036/1759 ( 59%)]  Loss: 4.362 (4.11)  LR: 1.764e-03  Grad: 3.1599  max=0.6598(module.vfe.pfn_layers.0.linear.weight)  min: -0.6608(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8069, loss_cls=0.1675, loss_bbox=0.8851, matched_ious=0.4843, loss_iou=0.0979, loss_iou_reg=0.2411, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.27(1.27)  Time cost: 22:00/15:20 [4:44:37/17:40:42]  Acc_iter 13350       Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.27)
2025-09-04 18:33:42,932   INFO  Train:    8/36 ( 22%) [1086/1759 ( 62%)]  Loss: 3.998 (4.12)  LR: 1.773e-03  Grad: 3.5809  max=0.4619(module.vfe.pfn_layers.0.linear.weight)  min: -1.3596(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8107, loss_cls=0.1662, loss_bbox=1.0432, matched_ious=0.4670, loss_iou=0.0985, loss_iou_reg=0.2470, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.29(1.27)  Time cost: 23:04/14:17 [4:45:41/17:39:52]  Acc_iter 13400       Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.27)
2025-09-04 18:34:46,201   INFO  Train:    8/36 ( 22%) [1136/1759 ( 65%)]  Loss: 3.727 (4.12)  LR: 1.781e-03  Grad: 3.6507  max=1.6013(module.vfe.pfn_layers.0.linear.weight)  min: -0.3125(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7867, loss_cls=0.1673, loss_bbox=0.9000, matched_ious=0.4850, loss_iou=0.1003, loss_iou_reg=0.2401, d_time=0.00(0.01), f_time=1.17(1.27), b_time=1.18(1.27)  Time cost: 24:07/13:13 [4:46:44/17:38:30]  Acc_iter 13450       Data time: 0.00(0.01)  Forward time: 1.17(1.27)  Batch time: 1.18(1.27)
2025-09-04 18:35:49,619   INFO  Train:    8/36 ( 22%) [1186/1759 ( 67%)]  Loss: 3.826 (4.12)  LR: 1.789e-03  Grad: 3.2785  max=1.7574(module.vfe.pfn_layers.0.linear.weight)  min: -0.1724(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.8051, loss_cls=0.1642, loss_bbox=0.8963, matched_ious=0.4881, loss_iou=0.0990, loss_iou_reg=0.2379, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.27(1.27)  Time cost: 25:11/12:09 [4:47:48/17:37:15]  Acc_iter 13500       Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.27)
2025-09-04 18:36:52,745   INFO  Train:    8/36 ( 22%) [1236/1759 ( 70%)]  Loss: 4.181 (4.12)  LR: 1.798e-03  Grad: 2.5153  max=0.3775(module.vfe.pfn_layers.0.linear.weight)  min: -0.1200(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8152, loss_cls=0.1687, loss_bbox=0.9398, matched_ious=0.4813, loss_iou=0.0987, loss_iou_reg=0.2395, d_time=0.00(0.01), f_time=1.33(1.27), b_time=1.34(1.27)  Time cost: 26:14/11:05 [4:48:51/17:35:50]  Acc_iter 13550       Data time: 0.00(0.01)  Forward time: 1.33(1.27)  Batch time: 1.34(1.27)
2025-09-04 18:37:56,845   INFO  Train:    8/36 ( 22%) [1286/1759 ( 73%)]  Loss: 3.754 (4.11)  LR: 1.806e-03  Grad: 2.2232  max=0.3049(module.backbone_3d.cls_conv.3.weight)  min: -0.9594(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7499, loss_cls=0.1627, loss_bbox=0.8170, matched_ious=0.4884, loss_iou=0.0969, loss_iou_reg=0.2428, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.29(1.27)  Time cost: 27:18/10:02 [4:49:55/17:35:04]  Acc_iter 13600       Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.27)
2025-09-04 18:39:00,682   INFO  Train:    8/36 ( 22%) [1336/1759 ( 76%)]  Loss: 4.242 (4.11)  LR: 1.814e-03  Grad: 2.1831  max=0.6500(module.vfe.pfn_layers.0.linear.weight)  min: -1.0071(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8035, loss_cls=0.1686, loss_bbox=0.9019, matched_ious=0.4836, loss_iou=0.0964, loss_iou_reg=0.2385, d_time=0.00(0.01), f_time=1.32(1.27), b_time=1.33(1.27)  Time cost: 28:22/08:58 [4:50:59/17:34:08]  Acc_iter 13650       Data time: 0.00(0.01)  Forward time: 1.32(1.27)  Batch time: 1.33(1.27)
2025-09-04 18:40:04,340   INFO  Train:    8/36 ( 22%) [1386/1759 ( 79%)]  Loss: 4.010 (4.11)  LR: 1.823e-03  Grad: 8.2335  max=5.6132(module.vfe.pfn_layers.0.linear.weight)  min: -5.2709(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7866, loss_cls=0.1652, loss_bbox=0.9030, matched_ious=0.4852, loss_iou=0.0966, loss_iou_reg=0.2377, d_time=0.01(0.01), f_time=1.25(1.27), b_time=1.26(1.27)  Time cost: 29:25/07:54 [4:52:03/17:33:04]  Acc_iter 13700       Data time: 0.01(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.27)
2025-09-04 18:41:08,907   INFO  Train:    8/36 ( 22%) [1436/1759 ( 82%)]  Loss: 4.253 (4.10)  LR: 1.831e-03  Grad: 2.5142  max=0.3840(module.vfe.pfn_layers.0.linear.weight)  min: -0.5483(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7751, loss_cls=0.1616, loss_bbox=0.8794, matched_ious=0.4820, loss_iou=0.0971, loss_iou_reg=0.2403, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.27)  Time cost: 30:30/06:51 [4:53:07/17:32:31]  Acc_iter 13750       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.27)
2025-09-04 18:42:11,344   INFO  Train:    8/36 ( 22%) [1486/1759 ( 84%)]  Loss: 3.648 (4.10)  LR: 1.839e-03  Grad: 3.2197  max=1.8683(module.vfe.pfn_layers.0.linear.weight)  min: -0.3082(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8010, loss_cls=0.1664, loss_bbox=0.9213, matched_ious=0.4751, loss_iou=0.0979, loss_iou_reg=0.2432, d_time=0.00(0.01), f_time=1.18(1.27), b_time=1.18(1.27)  Time cost: 31:32/05:47 [4:54:10/17:30:46]  Acc_iter 13800       Data time: 0.00(0.01)  Forward time: 1.18(1.27)  Batch time: 1.18(1.27)
2025-09-04 18:43:14,259   INFO  Train:    8/36 ( 22%) [1536/1759 ( 87%)]  Loss: 4.500 (4.10)  LR: 1.848e-03  Grad: 3.1796  max=0.3093(module.vfe.pfn_layers.0.linear.weight)  min: -1.5312(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7795, loss_cls=0.1643, loss_bbox=0.8648, matched_ious=0.4769, loss_iou=0.0958, loss_iou_reg=0.2457, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.27)  Time cost: 32:35/04:43 [4:55:13/17:29:18]  Acc_iter 13850       Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.27)
2025-09-04 18:44:17,403   INFO  Train:    8/36 ( 22%) [1586/1759 ( 90%)]  Loss: 4.038 (4.10)  LR: 1.856e-03  Grad: 3.2245  max=0.1910(module.dense_head.prediction_head.height.1.bias)  min: -0.8162(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7893, loss_cls=0.1661, loss_bbox=0.9014, matched_ious=0.4858, loss_iou=0.0973, loss_iou_reg=0.2395, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.21(1.27)  Time cost: 33:39/03:40 [4:56:16/17:28:00]  Acc_iter 13900       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.27)
2025-09-04 18:45:20,430   INFO  Train:    8/36 ( 22%) [1636/1759 ( 93%)]  Loss: 3.165 (4.09)  LR: 1.864e-03  Grad: 2.4862  max=0.8315(module.vfe.pfn_layers.0.linear.weight)  min: -0.6991(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7728, loss_cls=0.1577, loss_bbox=0.9059, matched_ious=0.4851, loss_iou=0.0971, loss_iou_reg=0.2393, d_time=0.00(0.01), f_time=1.39(1.26), b_time=1.39(1.27)  Time cost: 34:42/02:36 [4:57:19/17:26:39]  Acc_iter 13950       Data time: 0.00(0.01)  Forward time: 1.39(1.26)  Batch time: 1.39(1.27)
2025-09-04 18:46:23,613   INFO  Train:    8/36 ( 22%) [1686/1759 ( 96%)]  Loss: 3.401 (4.09)  LR: 1.872e-03  Grad: 3.8613  max=0.9327(module.vfe.pfn_layers.0.linear.weight)  min: -2.8617(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7962, loss_cls=0.1606, loss_bbox=0.9034, matched_ious=0.4824, loss_iou=0.0988, loss_iou_reg=0.2390, d_time=0.02(0.01), f_time=1.26(1.26), b_time=1.28(1.27)  Time cost: 35:45/01:32 [4:58:22/17:25:23]  Acc_iter 14000       Data time: 0.02(0.01)  Forward time: 1.26(1.26)  Batch time: 1.28(1.27)
2025-09-04 18:47:27,617   INFO  Train:    8/36 ( 22%) [1736/1759 ( 99%)]  Loss: 3.343 (4.09)  LR: 1.881e-03  Grad: 3.8640  max=2.6226(module.vfe.pfn_layers.0.linear.weight)  min: -0.9327(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7987, loss_cls=0.1676, loss_bbox=0.9119, matched_ious=0.4794, loss_iou=0.1006, loss_iou_reg=0.2424, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 36:49/00:29 [4:59:26/17:24:31]  Acc_iter 14050       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-04 18:47:55,010   INFO  Train:    8/36 ( 22%) [1758/1759 (100%)]  Loss: 3.374 (4.09)  LR: 1.884e-03  Grad: 3.0587  max=1.4011(module.vfe.pfn_layers.0.linear.weight)  min: -0.6200(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7453, loss_cls=0.1585, loss_bbox=0.8368, matched_ious=0.4882, loss_iou=0.0966, loss_iou_reg=0.2412, d_time=0.01(0.01), f_time=0.74(1.26), b_time=0.75(1.27)  Time cost: 37:16/00:01 [4:59:53/17:23:47]  Acc_iter 14072       Data time: 0.01(0.01)  Forward time: 0.74(1.26)  Batch time: 0.75(1.27)

                                               [Aepochs:  22%|██▏       | 8/36 [4:59:54<17:26:59, 2243.57s/it]epochs:  22%|██▏       | 8/36 [4:59:54<17:27:01, 2243.62s/it]epochs:  22%|██▏       | 8/36 [4:59:54<17:27:01, 2243.63s/it]epochs:  22%|██▏       | 8/36 [4:59:54<17:27:00, 2243.60s/it]epochs:  22%|██▏       | 8/36 [4:59:54<17:27:00, 2243.60s/it]epochs:  22%|██▏       | 8/36 [4:59:54<17:27:00, 2243.60s/it]epochs:  22%|██▏       | 8/36 [4:59:54<17:27:00, 2243.60s/it]epochs:  22%|██▏       | 8/36 [4:59:54<17:27:01, 2243.64s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 18:48:00,404   INFO  Train:    9/36 ( 25%) [   0/1759 (  0%)]  Loss: 3.227 (3.23)  LR: 1.884e-03  Grad: 2.9256  max=1.7063(module.vfe.pfn_layers.0.linear.weight)  min: -0.4772(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6151, loss_cls=0.1435, loss_bbox=0.6093, matched_ious=0.4837, loss_iou=0.0897, loss_iou_reg=0.2514, d_time=1.47(1.47), f_time=2.61(2.61), b_time=4.08(4.08)  Time cost: 00:03/1:47:41 [4:59:59/50:15:13]  Acc_iter 14073       Data time: 1.47(1.47)  Forward time: 2.61(2.61)  Batch time: 4.08(4.08)
2025-09-04 18:48:34,409   INFO  Train:    9/36 ( 25%) [  27/1759 (  2%)]  Loss: 3.788 (3.90)  LR: 1.889e-03  Grad: 2.6265  max=0.5008(module.vfe.pfn_layers.0.linear.weight)  min: -1.0605(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7753, loss_cls=0.1594, loss_bbox=0.8563, matched_ious=0.4993, loss_iou=0.0968, loss_iou_reg=0.2323, d_time=0.00(0.06), f_time=1.30(1.30), b_time=1.30(1.36)  Time cost: 00:37/38:50 [5:00:33/18:23:59]  Acc_iter 14100       Data time: 0.00(0.06)  Forward time: 1.30(1.30)  Batch time: 1.30(1.36)
2025-09-04 18:49:37,558   INFO  Train:    9/36 ( 25%) [  77/1759 (  4%)]  Loss: 3.050 (4.05)  LR: 1.897e-03  Grad: 2.5003  max=1.2159(module.vfe.pfn_layers.0.linear.weight)  min: -0.8302(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7933, loss_cls=0.1622, loss_bbox=0.9385, matched_ious=0.4795, loss_iou=0.0995, loss_iou_reg=0.2438, d_time=0.00(0.02), f_time=1.30(1.27), b_time=1.31(1.30)  Time cost: 01:40/36:14 [5:01:36/17:39:26]  Acc_iter 14150       Data time: 0.00(0.02)  Forward time: 1.30(1.27)  Batch time: 1.31(1.30)
2025-09-04 18:50:41,104   INFO  Train:    9/36 ( 25%) [ 127/1759 (  7%)]  Loss: 4.174 (4.05)  LR: 1.905e-03  Grad: 3.0360  max=0.4992(module.vfe.pfn_layers.0.linear.weight)  min: -1.3840(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7854, loss_cls=0.1613, loss_bbox=0.9092, matched_ious=0.4824, loss_iou=0.0986, loss_iou_reg=0.2405, d_time=0.00(0.02), f_time=1.33(1.27), b_time=1.34(1.29)  Time cost: 02:44/34:55 [5:02:39/17:31:24]  Acc_iter 14200       Data time: 0.00(0.02)  Forward time: 1.33(1.27)  Batch time: 1.34(1.29)
2025-09-04 18:51:46,506   INFO  Train:    9/36 ( 25%) [ 177/1759 ( 10%)]  Loss: 4.759 (4.01)  LR: 1.914e-03  Grad: 3.3716  max=1.6560(module.vfe.pfn_layers.0.linear.weight)  min: -0.9213(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7776, loss_cls=0.1622, loss_bbox=0.8527, matched_ious=0.4912, loss_iou=0.0980, loss_iou_reg=0.2358, d_time=0.00(0.02), f_time=1.23(1.28), b_time=1.23(1.29)  Time cost: 03:49/34:02 [5:03:45/17:35:49]  Acc_iter 14250       Data time: 0.00(0.02)  Forward time: 1.23(1.28)  Batch time: 1.23(1.29)
2025-09-04 18:52:49,787   INFO  Train:    9/36 ( 25%) [ 227/1759 ( 13%)]  Loss: 3.813 (4.03)  LR: 1.922e-03  Grad: 2.9521  max=0.6648(module.vfe.pfn_layers.0.linear.weight)  min: -1.1590(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7698, loss_cls=0.1582, loss_bbox=0.9199, matched_ious=0.4839, loss_iou=0.1003, loss_iou_reg=0.2402, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.29)  Time cost: 04:53/32:49 [5:04:48/17:30:13]  Acc_iter 14300       Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.29)
2025-09-04 18:53:53,467   INFO  Train:    9/36 ( 25%) [ 277/1759 ( 16%)]  Loss: 4.071 (4.02)  LR: 1.930e-03  Grad: 2.0654  max=0.2303(module.vfe.pfn_layers.0.linear.weight)  min: -0.4979(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7677, loss_cls=0.1565, loss_bbox=0.8902, matched_ious=0.4931, loss_iou=0.0956, loss_iou_reg=0.2365, d_time=0.00(0.01), f_time=1.40(1.27), b_time=1.40(1.28)  Time cost: 05:56/31:41 [5:05:52/17:27:25]  Acc_iter 14350       Data time: 0.00(0.01)  Forward time: 1.40(1.27)  Batch time: 1.40(1.28)
2025-09-04 18:54:56,242   INFO  Train:    9/36 ( 25%) [ 327/1759 ( 19%)]  Loss: 3.742 (4.01)  LR: 1.938e-03  Grad: 2.5855  max=0.9212(module.vfe.pfn_layers.0.linear.weight)  min: -0.5374(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7482, loss_cls=0.1537, loss_bbox=0.8884, matched_ious=0.4807, loss_iou=0.0982, loss_iou_reg=0.2447, d_time=0.00(0.01), f_time=1.10(1.27), b_time=1.10(1.28)  Time cost: 06:59/30:31 [5:06:55/17:22:55]  Acc_iter 14400       Data time: 0.00(0.01)  Forward time: 1.10(1.27)  Batch time: 1.10(1.28)
2025-09-04 18:55:59,287   INFO  Train:    9/36 ( 25%) [ 377/1759 ( 21%)]  Loss: 3.054 (4.02)  LR: 1.946e-03  Grad: 2.7374  max=0.7434(module.vfe.pfn_layers.0.linear.weight)  min: -0.6769(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8106, loss_cls=0.1628, loss_bbox=0.9383, matched_ious=0.4830, loss_iou=0.0986, loss_iou_reg=0.2401, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.20(1.28)  Time cost: 08:02/29:24 [5:07:58/17:19:53]  Acc_iter 14450       Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.28)
2025-09-04 18:57:02,821   INFO  Train:    9/36 ( 25%) [ 427/1759 ( 24%)]  Loss: 4.378 (4.02)  LR: 1.955e-03  Grad: 3.0526  max=0.7396(module.vfe.pfn_layers.0.linear.weight)  min: -0.9910(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8005, loss_cls=0.1644, loss_bbox=0.9129, matched_ious=0.4827, loss_iou=0.0984, loss_iou_reg=0.2420, d_time=0.01(0.01), f_time=1.19(1.27), b_time=1.20(1.28)  Time cost: 09:06/28:19 [5:09:01/17:18:16]  Acc_iter 14500       Data time: 0.01(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.28)
2025-09-04 18:58:06,108   INFO  Train:    9/36 ( 25%) [ 477/1759 ( 27%)]  Loss: 4.063 (4.02)  LR: 1.963e-03  Grad: 3.2968  max=1.5052(module.vfe.pfn_layers.0.linear.weight)  min: -0.3465(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7630, loss_cls=0.1589, loss_bbox=0.8769, matched_ious=0.4908, loss_iou=0.0955, loss_iou_reg=0.2375, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.21(1.28)  Time cost: 10:09/27:14 [5:10:04/17:16:20]  Acc_iter 14550       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.21(1.28)
2025-09-04 18:59:09,099   INFO  Train:    9/36 ( 25%) [ 527/1759 ( 30%)]  Loss: 3.897 (4.01)  LR: 1.971e-03  Grad: 3.1697  max=0.6236(module.vfe.pfn_layers.0.linear.weight)  min: -1.0979(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7579, loss_cls=0.1521, loss_bbox=0.8625, matched_ious=0.4935, loss_iou=0.0992, loss_iou_reg=0.2374, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.21(1.27)  Time cost: 11:12/26:08 [5:11:07/17:14:07]  Acc_iter 14600       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.21(1.27)
2025-09-04 19:00:11,761   INFO  Train:    9/36 ( 25%) [ 577/1759 ( 33%)]  Loss: 3.278 (4.01)  LR: 1.979e-03  Grad: 3.4991  max=0.8597(module.vfe.pfn_layers.0.linear.weight)  min: -1.0132(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7659, loss_cls=0.1550, loss_bbox=0.9019, matched_ious=0.4866, loss_iou=0.0969, loss_iou_reg=0.2414, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.23(1.27)  Time cost: 12:15/25:03 [5:12:10/17:11:39]  Acc_iter 14650       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.27)
2025-09-04 19:01:14,326   INFO  Train:    9/36 ( 25%) [ 627/1759 ( 36%)]  Loss: 3.259 (4.00)  LR: 1.987e-03  Grad: 2.4040  max=0.5334(module.vfe.pfn_layers.0.linear.weight)  min: -0.8873(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7550, loss_cls=0.1569, loss_bbox=0.8583, matched_ious=0.4924, loss_iou=0.0950, loss_iou_reg=0.2365, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.20(1.27)  Time cost: 13:17/23:57 [5:13:13/17:09:16]  Acc_iter 14700       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.27)
2025-09-04 19:02:19,530   INFO  Train:    9/36 ( 25%) [ 677/1759 ( 38%)]  Loss: 5.008 (3.99)  LR: 1.995e-03  Grad: 2.4061  max=1.2177(module.vfe.pfn_layers.0.linear.weight)  min: -0.2815(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7454, loss_cls=0.1511, loss_bbox=0.8629, matched_ious=0.4956, loss_iou=0.0960, loss_iou_reg=0.2363, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.20(1.27)  Time cost: 14:22/22:56 [5:14:18/17:10:14]  Acc_iter 14750       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.20(1.27)
2025-09-04 19:03:23,206   INFO  Train:    9/36 ( 25%) [ 727/1759 ( 41%)]  Loss: 3.651 (3.99)  LR: 2.003e-03  Grad: 2.3407  max=0.8816(module.vfe.pfn_layers.0.linear.weight)  min: -0.8103(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7586, loss_cls=0.1550, loss_bbox=0.9028, matched_ious=0.4904, loss_iou=0.0955, loss_iou_reg=0.2385, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 15:26/21:53 [5:15:21/17:09:14]  Acc_iter 14800       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-04 19:04:25,882   INFO  Train:    9/36 ( 25%) [ 777/1759 ( 44%)]  Loss: 3.259 (4.00)  LR: 2.011e-03  Grad: 2.2347  max=0.4074(module.vfe.pfn_layers.0.linear.weight)  min: -0.3481(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8111, loss_cls=0.1612, loss_bbox=0.9587, matched_ious=0.4805, loss_iou=0.0974, loss_iou_reg=0.2419, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 16:29/20:48 [5:16:24/17:07:11]  Acc_iter 14850       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-04 19:05:29,309   INFO  Train:    9/36 ( 25%) [ 827/1759 ( 47%)]  Loss: 4.151 (4.00)  LR: 2.019e-03  Grad: 10.0000  max=8.9542(module.vfe.pfn_layers.0.linear.weight)  min: -0.1916(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7916, loss_cls=0.1622, loss_bbox=0.8778, matched_ious=0.4811, loss_iou=0.0983, loss_iou_reg=0.2426, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.20(1.27)  Time cost: 17:32/19:44 [5:17:28/17:05:59]  Acc_iter 14900       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.27)
2025-09-04 19:06:32,359   INFO  Train:    9/36 ( 25%) [ 877/1759 ( 50%)]  Loss: 3.385 (4.00)  LR: 2.027e-03  Grad: 2.3736  max=0.4544(module.vfe.pfn_layers.0.linear.weight)  min: -1.3396(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7520, loss_cls=0.1564, loss_bbox=0.8735, matched_ious=0.4932, loss_iou=0.0946, loss_iou_reg=0.2360, d_time=0.02(0.01), f_time=1.25(1.26), b_time=1.27(1.27)  Time cost: 18:35/18:40 [5:18:31/17:04:27]  Acc_iter 14950       Data time: 0.02(0.01)  Forward time: 1.25(1.26)  Batch time: 1.27(1.27)
2025-09-04 19:07:36,029   INFO  Train:    9/36 ( 25%) [ 927/1759 ( 53%)]  Loss: 4.065 (4.00)  LR: 2.035e-03  Grad: 2.4395  max=1.2735(module.vfe.pfn_layers.0.linear.weight)  min: -0.9512(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7791, loss_cls=0.1572, loss_bbox=0.9358, matched_ious=0.4957, loss_iou=0.0940, loss_iou_reg=0.2337, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.26(1.27)  Time cost: 19:39/17:37 [5:19:34/17:03:31]  Acc_iter 15000       Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.27)
2025-09-04 19:08:40,114   INFO  Train:    9/36 ( 25%) [ 977/1759 ( 56%)]  Loss: 4.162 (4.00)  LR: 2.043e-03  Grad: 2.7375  max=0.4745(module.vfe.pfn_layers.0.linear.weight)  min: -1.5111(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7704, loss_cls=0.1527, loss_bbox=0.9275, matched_ious=0.4916, loss_iou=0.0985, loss_iou_reg=0.2354, d_time=0.00(0.01), f_time=1.36(1.26), b_time=1.36(1.27)  Time cost: 20:43/16:34 [5:20:38/17:02:54]  Acc_iter 15050       Data time: 0.00(0.01)  Forward time: 1.36(1.26)  Batch time: 1.36(1.27)
2025-09-04 19:09:42,950   INFO  Train:    9/36 ( 25%) [1027/1759 ( 58%)]  Loss: 3.433 (4.00)  LR: 2.051e-03  Grad: 3.0131  max=0.9264(module.vfe.pfn_layers.0.linear.weight)  min: -2.0772(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7945, loss_cls=0.1610, loss_bbox=0.8354, matched_ious=0.4867, loss_iou=0.0971, loss_iou_reg=0.2396, d_time=0.02(0.01), f_time=1.22(1.26), b_time=1.23(1.27)  Time cost: 21:46/15:30 [5:21:41/17:01:16]  Acc_iter 15100       Data time: 0.02(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.27)
2025-09-04 19:10:45,840   INFO  Train:    9/36 ( 25%) [1077/1759 ( 61%)]  Loss: 4.867 (4.00)  LR: 2.059e-03  Grad: 2.5257  max=0.3365(module.dense_head.prediction_head.height.1.bias)  min: -0.8213(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7646, loss_cls=0.1577, loss_bbox=0.8509, matched_ious=0.4847, loss_iou=0.0979, loss_iou_reg=0.2407, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.25(1.27)  Time cost: 22:49/14:26 [5:22:44/16:59:44]  Acc_iter 15150       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.27)
2025-09-04 19:11:49,366   INFO  Train:    9/36 ( 25%) [1127/1759 ( 64%)]  Loss: 4.369 (4.00)  LR: 2.067e-03  Grad: 3.5150  max=2.4676(module.vfe.pfn_layers.0.linear.weight)  min: -0.7185(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8051, loss_cls=0.1609, loss_bbox=0.8460, matched_ious=0.4870, loss_iou=0.0997, loss_iou_reg=0.2409, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.21(1.27)  Time cost: 23:52/13:22 [5:23:48/16:58:41]  Acc_iter 15200       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.27)
2025-09-04 19:12:54,748   INFO  Train:    9/36 ( 25%) [1177/1759 ( 67%)]  Loss: 4.252 (4.00)  LR: 2.075e-03  Grad: 3.1569  max=2.2226(module.vfe.pfn_layers.0.linear.weight)  min: -0.3236(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7673, loss_cls=0.1602, loss_bbox=0.8863, matched_ious=0.4859, loss_iou=0.1005, loss_iou_reg=0.2386, d_time=0.00(0.01), f_time=1.30(1.26), b_time=1.31(1.27)  Time cost: 24:58/12:20 [5:24:53/16:58:55]  Acc_iter 15250       Data time: 0.00(0.01)  Forward time: 1.30(1.26)  Batch time: 1.31(1.27)
2025-09-04 19:13:58,567   INFO  Train:    9/36 ( 25%) [1227/1759 ( 70%)]  Loss: 4.398 (4.00)  LR: 2.083e-03  Grad: 2.5710  max=0.5736(module.vfe.pfn_layers.0.linear.weight)  min: -1.2099(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7700, loss_cls=0.1554, loss_bbox=0.9265, matched_ious=0.4819, loss_iou=0.0972, loss_iou_reg=0.2424, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 26:01/11:16 [5:25:57/16:58:00]  Acc_iter 15300       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-04 19:15:01,932   INFO  Train:    9/36 ( 25%) [1277/1759 ( 73%)]  Loss: 4.850 (4.00)  LR: 2.091e-03  Grad: 3.7522  max=1.4292(module.vfe.pfn_layers.0.linear.weight)  min: -1.8252(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7593, loss_cls=0.1555, loss_bbox=0.8568, matched_ious=0.4910, loss_iou=0.0966, loss_iou_reg=0.2382, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 27:05/10:12 [5:27:00/16:56:48]  Acc_iter 15350       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-04 19:16:05,193   INFO  Train:    9/36 ( 25%) [1327/1759 ( 75%)]  Loss: 4.347 (4.00)  LR: 2.099e-03  Grad: 3.1108  max=0.9076(module.vfe.pfn_layers.0.linear.weight)  min: -0.3834(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7836, loss_cls=0.1556, loss_bbox=0.9299, matched_ious=0.4863, loss_iou=0.0977, loss_iou_reg=0.2359, d_time=0.00(0.01), f_time=1.33(1.26), b_time=1.34(1.27)  Time cost: 28:08/09:09 [5:28:03/16:55:33]  Acc_iter 15400       Data time: 0.00(0.01)  Forward time: 1.33(1.26)  Batch time: 1.34(1.27)
2025-09-04 19:17:08,577   INFO  Train:    9/36 ( 25%) [1377/1759 ( 78%)]  Loss: 4.002 (4.00)  LR: 2.107e-03  Grad: 3.2704  max=1.8467(module.vfe.pfn_layers.0.linear.weight)  min: -0.9369(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7869, loss_cls=0.1588, loss_bbox=0.8967, matched_ious=0.4844, loss_iou=0.0993, loss_iou_reg=0.2397, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.20(1.27)  Time cost: 29:11/08:05 [5:29:07/16:54:23]  Acc_iter 15450       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.27)
2025-09-04 19:18:12,213   INFO  Train:    9/36 ( 25%) [1427/1759 ( 81%)]  Loss: 4.189 (4.00)  LR: 2.115e-03  Grad: 3.4997  max=1.7286(module.vfe.pfn_layers.0.linear.weight)  min: -1.5707(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7936, loss_cls=0.1646, loss_bbox=0.9025, matched_ious=0.4770, loss_iou=0.1001, loss_iou_reg=0.2457, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.28(1.27)  Time cost: 30:15/07:02 [5:30:10/16:53:22]  Acc_iter 15500       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.27)
2025-09-04 19:19:15,398   INFO  Train:    9/36 ( 25%) [1477/1759 ( 84%)]  Loss: 3.980 (4.00)  LR: 2.123e-03  Grad: 2.7168  max=0.0954(module.backbone_3d.cls_conv.3.weight)  min: -0.9267(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7527, loss_cls=0.1547, loss_bbox=0.8565, matched_ious=0.4957, loss_iou=0.0966, loss_iou_reg=0.2362, d_time=0.01(0.01), f_time=1.23(1.26), b_time=1.24(1.27)  Time cost: 31:18/05:58 [5:31:14/16:52:06]  Acc_iter 15550       Data time: 0.01(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.27)
2025-09-04 19:20:18,374   INFO  Train:    9/36 ( 25%) [1527/1759 ( 87%)]  Loss: 3.401 (4.00)  LR: 2.131e-03  Grad: 2.8383  max=0.6240(module.vfe.pfn_layers.0.linear.weight)  min: -0.2581(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7532, loss_cls=0.1527, loss_bbox=0.8440, matched_ious=0.4894, loss_iou=0.0956, loss_iou_reg=0.2413, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.23(1.27)  Time cost: 32:21/04:54 [5:32:17/16:50:44]  Acc_iter 15600       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.27)
2025-09-04 19:21:21,379   INFO  Train:    9/36 ( 25%) [1577/1759 ( 90%)]  Loss: 4.003 (3.99)  LR: 2.138e-03  Grad: 3.1886  max=1.1323(module.vfe.pfn_layers.0.linear.weight)  min: -0.2278(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7535, loss_cls=0.1586, loss_bbox=0.8400, matched_ious=0.4872, loss_iou=0.0990, loss_iou_reg=0.2369, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.20(1.27)  Time cost: 33:24/03:51 [5:33:20/16:49:25]  Acc_iter 15650       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.27)
2025-09-04 19:22:24,436   INFO  Train:    9/36 ( 25%) [1627/1759 ( 92%)]  Loss: 3.207 (3.99)  LR: 2.146e-03  Grad: 3.3506  max=0.8343(module.vfe.pfn_layers.0.linear.weight)  min: -0.4907(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7755, loss_cls=0.1595, loss_bbox=0.8747, matched_ious=0.4900, loss_iou=0.0965, loss_iou_reg=0.2385, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.23(1.27)  Time cost: 34:27/02:47 [5:34:23/16:48:07]  Acc_iter 15700       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.27)
2025-09-04 19:23:29,761   INFO  Train:    9/36 ( 25%) [1677/1759 ( 95%)]  Loss: 3.896 (3.99)  LR: 2.154e-03  Grad: 3.7321  max=0.4426(module.vfe.pfn_layers.0.linear.weight)  min: -0.8492(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7688, loss_cls=0.1519, loss_bbox=0.8423, matched_ious=0.4947, loss_iou=0.0962, loss_iou_reg=0.2357, d_time=0.01(0.01), f_time=1.23(1.26), b_time=1.24(1.27)  Time cost: 35:33/01:44 [5:35:28/16:47:56]  Acc_iter 15750       Data time: 0.01(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.27)
2025-09-04 19:24:33,247   INFO  Train:    9/36 ( 25%) [1727/1759 ( 98%)]  Loss: 3.671 (3.99)  LR: 2.162e-03  Grad: 3.7283  max=0.1348(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.4248(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7453, loss_cls=0.1511, loss_bbox=0.8667, matched_ious=0.4938, loss_iou=0.0963, loss_iou_reg=0.2375, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.28(1.27)  Time cost: 36:36/00:40 [5:36:32/16:46:50]  Acc_iter 15800       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.27)
2025-09-04 19:25:11,828   INFO  Train:    9/36 ( 25%) [1758/1759 (100%)]  Loss: 3.871 (3.98)  LR: 2.167e-03  Grad: 4.4006  max=1.5349(module.vfe.pfn_layers.0.linear.weight)  min: -0.1114(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.7320, loss_cls=0.1534, loss_bbox=0.8104, matched_ious=0.4964, loss_iou=0.0974, loss_iou_reg=0.2340, d_time=0.00(0.01), f_time=0.74(1.26), b_time=0.74(1.27)  Time cost: 37:15/00:01 [5:37:10/16:45:48]  Acc_iter 15831       Data time: 0.00(0.01)  Forward time: 0.74(1.26)  Batch time: 0.74(1.27)

                                               [Aepochs:  25%|██▌       | 9/36 [5:37:10<16:48:39, 2241.46s/it]epochs:  25%|██▌       | 9/36 [5:37:11<16:48:39, 2241.46s/it]epochs:  25%|██▌       | 9/36 [5:37:11<16:48:40, 2241.50s/it]epochs:  25%|██▌       | 9/36 [5:37:11<16:48:40, 2241.49s/it]epochs:  25%|██▌       | 9/36 [5:37:11<16:48:40, 2241.49s/it]epochs:  25%|██▌       | 9/36 [5:37:11<16:48:40, 2241.49s/it]epochs:  25%|██▌       | 9/36 [5:37:11<16:48:40, 2241.49s/it]epochs:  25%|██▌       | 9/36 [5:37:11<16:48:39, 2241.48s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 19:25:17,664   INFO  Train:   10/36 ( 28%) [   0/1759 (  0%)]  Loss: 3.927 (3.93)  LR: 2.167e-03  Grad: 3.9279  max=0.6891(module.vfe.pfn_layers.0.linear.weight)  min: -0.5382(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7760, loss_cls=0.1717, loss_bbox=0.6667, matched_ious=0.4801, loss_iou=0.0796, loss_iou_reg=0.2473, d_time=1.81(1.81), f_time=2.71(2.71), b_time=4.52(4.52)  Time cost: 00:04/2:05:13 [5:37:16/56:20:55]  Acc_iter 15832       Data time: 1.81(1.81)  Forward time: 2.71(2.71)  Batch time: 4.52(4.52)
2025-09-04 19:25:40,728   INFO  Train:   10/36 ( 28%) [  18/1759 (  1%)]  Loss: 3.942 (4.06)  LR: 2.169e-03  Grad: 3.7196  max=0.8110(module.vfe.pfn_layers.0.linear.weight)  min: -1.0811(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7854, loss_cls=0.1607, loss_bbox=0.9255, matched_ious=0.4858, loss_iou=0.0965, loss_iou_reg=0.2388, d_time=0.00(0.10), f_time=1.30(1.35), b_time=1.30(1.45)  Time cost: 00:27/41:44 [5:37:39/18:58:22]  Acc_iter 15850       Data time: 0.00(0.10)  Forward time: 1.30(1.35)  Batch time: 1.30(1.45)
2025-09-04 19:26:44,823   INFO  Train:   10/36 ( 28%) [  68/1759 (  4%)]  Loss: 4.181 (4.04)  LR: 2.177e-03  Grad: 3.7997  max=0.7904(module.vfe.pfn_layers.0.linear.weight)  min: -0.6876(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7958, loss_cls=0.1601, loss_bbox=0.8945, matched_ious=0.4796, loss_iou=0.0980, loss_iou_reg=0.2407, d_time=0.00(0.03), f_time=1.23(1.30), b_time=1.23(1.33)  Time cost: 01:31/37:20 [5:38:43/17:27:20]  Acc_iter 15900       Data time: 0.00(0.03)  Forward time: 1.23(1.30)  Batch time: 1.23(1.33)
2025-09-04 19:27:47,018   INFO  Train:   10/36 ( 28%) [ 118/1759 (  7%)]  Loss: 3.862 (3.99)  LR: 2.185e-03  Grad: 3.8358  max=0.3478(module.vfe.pfn_layers.0.linear.weight)  min: -0.6854(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7724, loss_cls=0.1576, loss_bbox=0.8759, matched_ious=0.4904, loss_iou=0.0968, loss_iou_reg=0.2372, d_time=0.00(0.02), f_time=1.19(1.27), b_time=1.19(1.29)  Time cost: 02:33/35:18 [5:39:45/16:59:19]  Acc_iter 15950       Data time: 0.00(0.02)  Forward time: 1.19(1.27)  Batch time: 1.19(1.29)
2025-09-04 19:28:50,476   INFO  Train:   10/36 ( 28%) [ 168/1759 ( 10%)]  Loss: 4.786 (3.97)  LR: 2.193e-03  Grad: 4.4723  max=1.4446(module.vfe.pfn_layers.0.linear.weight)  min: -1.0420(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7608, loss_cls=0.1557, loss_bbox=0.8413, matched_ious=0.4934, loss_iou=0.0977, loss_iou_reg=0.2372, d_time=0.00(0.02), f_time=1.25(1.27), b_time=1.25(1.29)  Time cost: 03:37/34:03 [5:40:49/16:53:09]  Acc_iter 16000       Data time: 0.00(0.02)  Forward time: 1.25(1.27)  Batch time: 1.25(1.29)
2025-09-04 19:29:54,350   INFO  Train:   10/36 ( 28%) [ 218/1759 ( 12%)]  Loss: 4.166 (3.97)  LR: 2.200e-03  Grad: 2.6208  max=1.1681(module.vfe.pfn_layers.0.linear.weight)  min: -0.5270(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7551, loss_cls=0.1457, loss_bbox=0.9148, matched_ious=0.4857, loss_iou=0.0979, loss_iou_reg=0.2380, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.26(1.28)  Time cost: 04:40/32:56 [5:41:53/16:50:49]  Acc_iter 16050       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.28)
2025-09-04 19:30:57,891   INFO  Train:   10/36 ( 28%) [ 268/1759 ( 15%)]  Loss: 4.633 (3.98)  LR: 2.208e-03  Grad: 2.7331  max=0.8463(module.vfe.pfn_layers.0.linear.weight)  min: -0.6885(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7723, loss_cls=0.1549, loss_bbox=0.9196, matched_ious=0.4818, loss_iou=0.0970, loss_iou_reg=0.2419, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.26(1.28)  Time cost: 05:44/31:49 [5:42:56/16:47:59]  Acc_iter 16100       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.28)
2025-09-04 19:32:01,515   INFO  Train:   10/36 ( 28%) [ 318/1759 ( 18%)]  Loss: 3.699 (3.97)  LR: 2.215e-03  Grad: 3.0418  max=0.7542(module.vfe.pfn_layers.0.linear.weight)  min: -0.9650(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7464, loss_cls=0.1493, loss_bbox=0.8659, matched_ious=0.4878, loss_iou=0.0969, loss_iou_reg=0.2380, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.21(1.28)  Time cost: 06:48/30:43 [5:44:00/16:45:54]  Acc_iter 16150       Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.28)
2025-09-04 19:33:04,106   INFO  Train:   10/36 ( 28%) [ 368/1759 ( 21%)]  Loss: 3.115 (3.96)  LR: 2.223e-03  Grad: 7.1583  max=6.1389(module.vfe.pfn_layers.0.linear.weight)  min: -2.2316(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7523, loss_cls=0.1530, loss_bbox=0.8953, matched_ious=0.4849, loss_iou=0.0990, loss_iou_reg=0.2391, d_time=0.00(0.01), f_time=1.17(1.27), b_time=1.17(1.28)  Time cost: 07:50/29:34 [5:45:02/16:41:54]  Acc_iter 16200       Data time: 0.00(0.01)  Forward time: 1.17(1.27)  Batch time: 1.17(1.28)
2025-09-04 19:34:08,333   INFO  Train:   10/36 ( 28%) [ 418/1759 ( 24%)]  Loss: 4.039 (3.95)  LR: 2.231e-03  Grad: 2.3585  max=1.1679(module.vfe.pfn_layers.0.linear.weight)  min: -0.7866(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7615, loss_cls=0.1511, loss_bbox=0.8632, matched_ious=0.4938, loss_iou=0.0967, loss_iou_reg=0.2376, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.28(1.28)  Time cost: 08:54/28:32 [5:46:07/16:41:40]  Acc_iter 16250       Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.28(1.28)
2025-09-04 19:35:11,300   INFO  Train:   10/36 ( 28%) [ 468/1759 ( 27%)]  Loss: 3.407 (3.94)  LR: 2.238e-03  Grad: 1.6834  max=0.5074(module.vfe.pfn_layers.0.linear.weight)  min: -0.8179(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7543, loss_cls=0.1497, loss_bbox=0.8625, matched_ious=0.4873, loss_iou=0.0987, loss_iou_reg=0.2381, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.18(1.28)  Time cost: 09:57/27:25 [5:47:10/16:39:09]  Acc_iter 16300       Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.18(1.28)
2025-09-04 19:36:15,232   INFO  Train:   10/36 ( 28%) [ 518/1759 ( 29%)]  Loss: 4.424 (3.93)  LR: 2.246e-03  Grad: 2.0604  max=1.1983(module.vfe.pfn_layers.0.linear.weight)  min: -0.3672(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7371, loss_cls=0.1480, loss_bbox=0.8097, matched_ious=0.4994, loss_iou=0.0979, loss_iou_reg=0.2349, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.28(1.28)  Time cost: 11:01/26:22 [5:48:13/16:38:21]  Acc_iter 16350       Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.28(1.28)
2025-09-04 19:37:18,091   INFO  Train:   10/36 ( 28%) [ 568/1759 ( 32%)]  Loss: 4.391 (3.94)  LR: 2.253e-03  Grad: 1.9937  max=1.1430(module.vfe.pfn_layers.0.linear.weight)  min: -0.6691(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7943, loss_cls=0.1583, loss_bbox=0.9406, matched_ious=0.4812, loss_iou=0.1004, loss_iou_reg=0.2422, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.21(1.27)  Time cost: 12:04/25:16 [5:49:16/16:36:05]  Acc_iter 16400       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.27)
2025-09-04 19:38:21,033   INFO  Train:   10/36 ( 28%) [ 618/1759 ( 35%)]  Loss: 3.513 (3.93)  LR: 2.261e-03  Grad: 2.2479  max=0.8224(module.vfe.pfn_layers.0.linear.weight)  min: -1.0665(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7525, loss_cls=0.1502, loss_bbox=0.8454, matched_ious=0.4857, loss_iou=0.0978, loss_iou_reg=0.2414, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 13:07/24:11 [5:50:19/16:34:05]  Acc_iter 16450       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-04 19:39:23,477   INFO  Train:   10/36 ( 28%) [ 668/1759 ( 38%)]  Loss: 4.244 (3.94)  LR: 2.268e-03  Grad: 2.2714  max=0.8391(module.vfe.pfn_layers.0.linear.weight)  min: -1.1343(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7863, loss_cls=0.1594, loss_bbox=0.8636, matched_ious=0.4817, loss_iou=0.0954, loss_iou_reg=0.2413, d_time=0.01(0.01), f_time=1.35(1.26), b_time=1.37(1.27)  Time cost: 14:10/23:06 [5:51:22/16:31:39]  Acc_iter 16500       Data time: 0.01(0.01)  Forward time: 1.35(1.26)  Batch time: 1.37(1.27)
2025-09-04 19:40:26,993   INFO  Train:   10/36 ( 28%) [ 718/1759 ( 41%)]  Loss: 4.201 (3.93)  LR: 2.276e-03  Grad: 2.4145  max=0.7765(module.vfe.pfn_layers.0.linear.weight)  min: -1.0631(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7298, loss_cls=0.1479, loss_bbox=0.8246, matched_ious=0.4968, loss_iou=0.0941, loss_iou_reg=0.2367, d_time=0.00(0.01), f_time=1.34(1.26), b_time=1.35(1.27)  Time cost: 15:13/22:02 [5:52:25/16:30:34]  Acc_iter 16550       Data time: 0.00(0.01)  Forward time: 1.34(1.26)  Batch time: 1.35(1.27)
2025-09-04 19:41:30,595   INFO  Train:   10/36 ( 28%) [ 768/1759 ( 44%)]  Loss: 3.826 (3.93)  LR: 2.283e-03  Grad: 2.4834  max=0.9956(module.vfe.pfn_layers.0.linear.weight)  min: -0.3428(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7605, loss_cls=0.1542, loss_bbox=0.8512, matched_ious=0.4963, loss_iou=0.0993, loss_iou_reg=0.2340, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.18(1.27)  Time cost: 16:17/20:59 [5:53:29/16:29:35]  Acc_iter 16600       Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.18(1.27)
2025-09-04 19:42:33,601   INFO  Train:   10/36 ( 28%) [ 818/1759 ( 47%)]  Loss: 3.946 (3.92)  LR: 2.290e-03  Grad: 3.3722  max=1.4983(module.vfe.pfn_layers.0.linear.weight)  min: -0.1644(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.7417, loss_cls=0.1479, loss_bbox=0.8372, matched_ious=0.4991, loss_iou=0.0971, loss_iou_reg=0.2342, d_time=0.00(0.01), f_time=1.29(1.26), b_time=1.29(1.27)  Time cost: 17:20/19:55 [5:54:32/16:28:01]  Acc_iter 16650       Data time: 0.00(0.01)  Forward time: 1.29(1.26)  Batch time: 1.29(1.27)
2025-09-04 19:43:36,678   INFO  Train:   10/36 ( 28%) [ 868/1759 ( 49%)]  Loss: 4.017 (3.92)  LR: 2.298e-03  Grad: 2.6812  max=0.8414(module.vfe.pfn_layers.0.linear.weight)  min: -0.2140(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7531, loss_cls=0.1473, loss_bbox=0.8502, matched_ious=0.4924, loss_iou=0.0962, loss_iou_reg=0.2376, d_time=0.00(0.01), f_time=1.30(1.26), b_time=1.30(1.27)  Time cost: 18:23/18:51 [5:55:35/16:26:35]  Acc_iter 16700       Data time: 0.00(0.01)  Forward time: 1.30(1.26)  Batch time: 1.30(1.27)
2025-09-04 19:44:42,068   INFO  Train:   10/36 ( 28%) [ 918/1759 ( 52%)]  Loss: 3.862 (3.91)  LR: 2.305e-03  Grad: 3.9501  max=0.7763(module.vfe.pfn_layers.0.linear.weight)  min: -2.5550(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7515, loss_cls=0.1498, loss_bbox=0.8205, matched_ious=0.4959, loss_iou=0.0969, loss_iou_reg=0.2346, d_time=0.01(0.01), f_time=1.28(1.26), b_time=1.28(1.27)  Time cost: 19:28/17:49 [5:56:40/16:27:08]  Acc_iter 16750       Data time: 0.01(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.27)
2025-09-04 19:45:46,888   INFO  Train:   10/36 ( 28%) [ 968/1759 ( 55%)]  Loss: 4.137 (3.91)  LR: 2.312e-03  Grad: 3.0049  max=0.3186(module.vfe.pfn_layers.0.linear.weight)  min: -0.8304(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7285, loss_cls=0.1452, loss_bbox=0.8663, matched_ious=0.4962, loss_iou=0.0989, loss_iou_reg=0.2340, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.18(1.27)  Time cost: 20:33/16:46 [5:57:45/16:27:04]  Acc_iter 16800       Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.18(1.27)
2025-09-04 19:46:50,014   INFO  Train:   10/36 ( 28%) [1018/1759 ( 58%)]  Loss: 4.147 (3.91)  LR: 2.320e-03  Grad: 3.2311  max=0.3977(module.vfe.pfn_layers.0.linear.weight)  min: -0.8506(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7308, loss_cls=0.1475, loss_bbox=0.8366, matched_ious=0.4898, loss_iou=0.0962, loss_iou_reg=0.2390, d_time=0.00(0.01), f_time=1.34(1.26), b_time=1.34(1.27)  Time cost: 21:36/15:42 [5:58:48/16:25:36]  Acc_iter 16850       Data time: 0.00(0.01)  Forward time: 1.34(1.26)  Batch time: 1.34(1.27)
2025-09-04 19:47:53,643   INFO  Train:   10/36 ( 28%) [1068/1759 ( 61%)]  Loss: 4.429 (3.91)  LR: 2.327e-03  Grad: 2.9099  max=0.5166(module.vfe.pfn_layers.0.linear.weight)  min: -0.1951(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.7457, loss_cls=0.1470, loss_bbox=0.8700, matched_ious=0.4947, loss_iou=0.0957, loss_iou_reg=0.2348, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.26(1.27)  Time cost: 22:40/14:39 [5:59:52/16:24:33]  Acc_iter 16900       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.27)
2025-09-04 19:48:56,950   INFO  Train:   10/36 ( 28%) [1118/1759 ( 64%)]  Loss: 4.051 (3.90)  LR: 2.334e-03  Grad: 3.3648  max=1.1107(module.vfe.pfn_layers.0.linear.weight)  min: -0.3523(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7504, loss_cls=0.1493, loss_bbox=0.8872, matched_ious=0.4987, loss_iou=0.0944, loss_iou_reg=0.2329, d_time=0.00(0.01), f_time=1.17(1.26), b_time=1.18(1.27)  Time cost: 23:43/13:35 [6:00:55/16:23:16]  Acc_iter 16950       Data time: 0.00(0.01)  Forward time: 1.17(1.26)  Batch time: 1.18(1.27)
2025-09-04 19:50:00,442   INFO  Train:   10/36 ( 28%) [1168/1759 ( 66%)]  Loss: 3.998 (3.90)  LR: 2.341e-03  Grad: 2.4490  max=0.5606(module.vfe.pfn_layers.0.linear.weight)  min: -0.6158(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7365, loss_cls=0.1496, loss_bbox=0.8495, matched_ious=0.4939, loss_iou=0.0970, loss_iou_reg=0.2364, d_time=0.01(0.01), f_time=1.20(1.26), b_time=1.21(1.27)  Time cost: 24:47/12:31 [6:01:59/16:22:08]  Acc_iter 17000       Data time: 0.01(0.01)  Forward time: 1.20(1.26)  Batch time: 1.21(1.27)
2025-09-04 19:51:03,500   INFO  Train:   10/36 ( 28%) [1218/1759 ( 69%)]  Loss: 3.811 (3.90)  LR: 2.348e-03  Grad: 2.4404  max=0.7711(module.vfe.pfn_layers.0.linear.weight)  min: -0.5430(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7601, loss_cls=0.1535, loss_bbox=0.8631, matched_ious=0.4988, loss_iou=0.0967, loss_iou_reg=0.2345, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.21(1.27)  Time cost: 25:50/11:27 [6:03:02/16:20:44]  Acc_iter 17050       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.27)
2025-09-04 19:52:06,429   INFO  Train:   10/36 ( 28%) [1268/1759 ( 72%)]  Loss: 3.172 (3.91)  LR: 2.356e-03  Grad: 3.2086  max=1.3679(module.vfe.pfn_layers.0.linear.weight)  min: -1.2433(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7477, loss_cls=0.1471, loss_bbox=0.9159, matched_ious=0.4842, loss_iou=0.0985, loss_iou_reg=0.2398, d_time=0.01(0.01), f_time=1.30(1.26), b_time=1.30(1.27)  Time cost: 26:53/10:24 [6:04:05/16:19:16]  Acc_iter 17100       Data time: 0.01(0.01)  Forward time: 1.30(1.26)  Batch time: 1.30(1.27)
2025-09-04 19:53:10,096   INFO  Train:   10/36 ( 28%) [1318/1759 ( 75%)]  Loss: 3.623 (3.91)  LR: 2.363e-03  Grad: 2.4781  max=0.4088(module.vfe.pfn_layers.0.linear.weight)  min: -0.2231(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7384, loss_cls=0.1513, loss_bbox=0.8317, matched_ious=0.4953, loss_iou=0.0974, loss_iou_reg=0.2348, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.25(1.27)  Time cost: 27:56/09:20 [6:05:08/16:18:17]  Acc_iter 17150       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.27)
2025-09-04 19:54:12,825   INFO  Train:   10/36 ( 28%) [1368/1759 ( 78%)]  Loss: 5.025 (3.90)  LR: 2.370e-03  Grad: 2.8557  max=0.7031(module.vfe.pfn_layers.0.linear.weight)  min: -0.6725(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7277, loss_cls=0.1467, loss_bbox=0.8395, matched_ious=0.4971, loss_iou=0.0961, loss_iou_reg=0.2344, d_time=0.00(0.01), f_time=1.33(1.26), b_time=1.33(1.27)  Time cost: 28:59/08:16 [6:06:11/16:16:45]  Acc_iter 17200       Data time: 0.00(0.01)  Forward time: 1.33(1.26)  Batch time: 1.33(1.27)
2025-09-04 19:55:19,461   INFO  Train:   10/36 ( 28%) [1418/1759 ( 81%)]  Loss: 3.154 (3.90)  LR: 2.377e-03  Grad: 3.0627  max=0.5311(module.vfe.pfn_layers.0.linear.weight)  min: -0.6316(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7492, loss_cls=0.1510, loss_bbox=0.8685, matched_ious=0.4913, loss_iou=0.0972, loss_iou_reg=0.2370, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.27)  Time cost: 30:06/07:14 [6:07:18/16:17:23]  Acc_iter 17250       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.27)
2025-09-04 19:56:23,511   INFO  Train:   10/36 ( 28%) [1468/1759 ( 83%)]  Loss: 4.407 (3.91)  LR: 2.384e-03  Grad: 3.3319  max=0.4592(module.vfe.pfn_layers.0.linear.weight)  min: -0.3918(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7734, loss_cls=0.1528, loss_bbox=0.8643, matched_ious=0.4857, loss_iou=0.0977, loss_iou_reg=0.2391, d_time=0.01(0.01), f_time=1.31(1.27), b_time=1.31(1.27)  Time cost: 31:10/06:10 [6:08:22/16:16:32]  Acc_iter 17300       Data time: 0.01(0.01)  Forward time: 1.31(1.27)  Batch time: 1.31(1.27)
2025-09-04 19:57:26,560   INFO  Train:   10/36 ( 28%) [1518/1759 ( 86%)]  Loss: 3.797 (3.91)  LR: 2.391e-03  Grad: 4.6562  max=2.5813(module.vfe.pfn_layers.0.linear.weight)  min: -1.9045(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7287, loss_cls=0.1475, loss_bbox=0.8645, matched_ious=0.4901, loss_iou=0.0972, loss_iou_reg=0.2359, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.26(1.27)  Time cost: 32:13/05:06 [6:09:25/16:15:10]  Acc_iter 17350       Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.27)
2025-09-04 19:58:29,633   INFO  Train:   10/36 ( 28%) [1568/1759 ( 89%)]  Loss: 4.114 (3.90)  LR: 2.398e-03  Grad: 3.9776  max=0.5087(module.vfe.pfn_layers.0.linear.weight)  min: -1.5538(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7310, loss_cls=0.1438, loss_bbox=0.8091, matched_ious=0.5010, loss_iou=0.0988, loss_iou_reg=0.2325, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.21(1.27)  Time cost: 33:16/04:03 [6:10:28/16:13:50]  Acc_iter 17400       Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.27)
2025-09-04 19:59:32,079   INFO  Train:   10/36 ( 28%) [1618/1759 ( 92%)]  Loss: 3.541 (3.90)  LR: 2.405e-03  Grad: 4.0928  max=0.3158(module.dense_head.prediction_head.height.1.weight)  min: -1.0362(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7255, loss_cls=0.1457, loss_bbox=0.8302, matched_ious=0.4953, loss_iou=0.0961, loss_iou_reg=0.2347, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.23(1.27)  Time cost: 34:18/02:59 [6:11:30/16:12:13]  Acc_iter 17450       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.27)
2025-09-04 20:00:35,945   INFO  Train:   10/36 ( 28%) [1668/1759 ( 95%)]  Loss: 3.590 (3.90)  LR: 2.412e-03  Grad: 2.8269  max=0.8970(module.vfe.pfn_layers.0.linear.weight)  min: -0.1091(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7335, loss_cls=0.1497, loss_bbox=0.8268, matched_ious=0.5003, loss_iou=0.0967, loss_iou_reg=0.2336, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.21(1.27)  Time cost: 35:22/01:55 [6:12:34/16:11:17]  Acc_iter 17500       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.21(1.27)
2025-09-04 20:01:39,435   INFO  Train:   10/36 ( 28%) [1718/1759 ( 98%)]  Loss: 3.291 (3.90)  LR: 2.419e-03  Grad: 3.3680  max=0.1115(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0926(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7490, loss_cls=0.1466, loss_bbox=0.8402, matched_ious=0.4946, loss_iou=0.0957, loss_iou_reg=0.2356, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.26(1.27)  Time cost: 36:26/00:52 [6:13:38/16:10:11]  Acc_iter 17550       Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.27)
2025-09-04 20:02:29,208   INFO  Train:   10/36 ( 28%) [1758/1759 (100%)]  Loss: 3.770 (3.89)  LR: 2.424e-03  Grad: 3.5823  max=0.8245(module.vfe.pfn_layers.0.linear.weight)  min: -1.1548(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7204, loss_cls=0.1447, loss_bbox=0.8012, matched_ious=0.5022, loss_iou=0.0952, loss_iou_reg=0.2317, d_time=0.01(0.01), f_time=0.73(1.26), b_time=0.74(1.27)  Time cost: 37:15/00:01 [6:14:27/16:08:52]  Acc_iter 17590       Data time: 0.01(0.01)  Forward time: 0.73(1.26)  Batch time: 0.74(1.27)

                                               [Aepochs:  28%|██▊       | 10/36 [6:14:28<16:10:45, 2240.20s/it]epochs:  28%|██▊       | 10/36 [6:14:28<16:10:45, 2240.19s/it]epochs:  28%|██▊       | 10/36 [6:14:28<16:10:46, 2240.24s/it]epochs:  28%|██▊       | 10/36 [6:14:28<16:10:46, 2240.23s/it]epochs:  28%|██▊       | 10/36 [6:14:28<16:10:46, 2240.23s/it]epochs:  28%|██▊       | 10/36 [6:14:28<16:10:46, 2240.23s/it]epochs:  28%|██▊       | 10/36 [6:14:28<16:10:46, 2240.24s/it]epochs:  28%|██▊       | 10/36 [6:14:28<16:10:46, 2240.25s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 20:02:34,649   INFO  Train:   11/36 ( 31%) [   0/1759 (  0%)]  Loss: 4.140 (4.14)  LR: 2.424e-03  Grad: 4.1728  max=1.6385(module.vfe.pfn_layers.0.linear.weight)  min: -1.3426(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8408, loss_cls=0.1626, loss_bbox=0.8567, matched_ious=0.4964, loss_iou=0.0944, loss_iou_reg=0.2244, d_time=1.51(1.51), f_time=2.55(2.55), b_time=4.06(4.06)  Time cost: 00:03/1:44:48 [6:14:33/45:25:12]  Acc_iter 17591       Data time: 1.51(1.51)  Forward time: 2.55(2.55)  Batch time: 4.06(4.06)
2025-09-04 20:02:46,299   INFO  Train:   11/36 ( 31%) [   9/1759 (  1%)]  Loss: 3.149 (3.71)  LR: 2.426e-03  Grad: 3.7963  max=0.9347(module.vfe.pfn_layers.0.linear.weight)  min: -1.5455(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7129, loss_cls=0.1450, loss_bbox=0.7504, matched_ious=0.5058, loss_iou=0.0978, loss_iou_reg=0.2391, d_time=0.00(0.16), f_time=1.31(1.41), b_time=1.31(1.57)  Time cost: 00:15/44:24 [6:14:45/19:20:21]  Acc_iter 17600       Data time: 0.00(0.16)  Forward time: 1.31(1.41)  Batch time: 1.31(1.57)
2025-09-04 20:03:50,395   INFO  Train:   11/36 ( 31%) [  59/1759 (  3%)]  Loss: 4.023 (3.89)  LR: 2.432e-03  Grad: 3.9199  max=1.3524(module.vfe.pfn_layers.0.linear.weight)  min: -0.8696(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7494, loss_cls=0.1485, loss_bbox=0.8816, matched_ious=0.4869, loss_iou=0.0957, loss_iou_reg=0.2394, d_time=0.00(0.03), f_time=1.29(1.30), b_time=1.29(1.33)  Time cost: 01:19/37:27 [6:15:49/16:46:23]  Acc_iter 17650       Data time: 0.00(0.03)  Forward time: 1.29(1.30)  Batch time: 1.29(1.33)
2025-09-04 20:04:55,153   INFO  Train:   11/36 ( 31%) [ 109/1759 (  6%)]  Loss: 4.296 (3.91)  LR: 2.439e-03  Grad: 3.1412  max=0.2637(module.vfe.pfn_layers.0.linear.weight)  min: -0.2397(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7664, loss_cls=0.1486, loss_bbox=0.8584, matched_ious=0.4959, loss_iou=0.0981, loss_iou_reg=0.2348, d_time=0.01(0.02), f_time=2.31(1.29), b_time=2.32(1.31)  Time cost: 02:24/36:01 [6:16:53/16:36:00]  Acc_iter 17700       Data time: 0.01(0.02)  Forward time: 2.31(1.29)  Batch time: 2.32(1.31)
2025-09-04 20:05:59,019   INFO  Train:   11/36 ( 31%) [ 159/1759 (  9%)]  Loss: 4.438 (3.86)  LR: 2.446e-03  Grad: 2.5590  max=0.7191(module.vfe.pfn_layers.0.linear.weight)  min: -0.5566(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7259, loss_cls=0.1445, loss_bbox=0.7943, matched_ious=0.4950, loss_iou=0.0961, loss_iou_reg=0.2357, d_time=0.00(0.02), f_time=1.28(1.28), b_time=1.29(1.30)  Time cost: 03:27/34:39 [6:17:57/16:27:12]  Acc_iter 17750       Data time: 0.00(0.02)  Forward time: 1.28(1.28)  Batch time: 1.29(1.30)
2025-09-04 20:07:02,403   INFO  Train:   11/36 ( 31%) [ 209/1759 ( 12%)]  Loss: 3.594 (3.87)  LR: 2.453e-03  Grad: 4.4942  max=2.4861(module.vfe.pfn_layers.0.linear.weight)  min: -2.3129(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7419, loss_cls=0.1461, loss_bbox=0.8762, matched_ious=0.4892, loss_iou=0.0975, loss_iou_reg=0.2381, d_time=0.00(0.02), f_time=1.31(1.28), b_time=1.31(1.29)  Time cost: 04:31/33:22 [6:19:01/16:20:20]  Acc_iter 17800       Data time: 0.00(0.02)  Forward time: 1.31(1.28)  Batch time: 1.31(1.29)
2025-09-04 20:08:06,556   INFO  Train:   11/36 ( 31%) [ 259/1759 ( 15%)]  Loss: 3.714 (3.87)  LR: 2.460e-03  Grad: 3.4699  max=0.4455(module.vfe.pfn_layers.0.linear.weight)  min: -1.5320(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7360, loss_cls=0.1415, loss_bbox=0.8534, matched_ious=0.5015, loss_iou=0.0971, loss_iou_reg=0.2332, d_time=0.00(0.02), f_time=1.33(1.28), b_time=1.34(1.29)  Time cost: 05:35/32:15 [6:20:05/16:17:57]  Acc_iter 17850       Data time: 0.00(0.02)  Forward time: 1.33(1.28)  Batch time: 1.34(1.29)
2025-09-04 20:09:10,012   INFO  Train:   11/36 ( 31%) [ 309/1759 ( 18%)]  Loss: 3.709 (3.86)  LR: 2.466e-03  Grad: 3.1808  max=0.5024(module.vfe.pfn_layers.0.linear.weight)  min: -0.6444(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7332, loss_cls=0.1452, loss_bbox=0.8531, matched_ious=0.4945, loss_iou=0.0954, loss_iou_reg=0.2353, d_time=0.01(0.01), f_time=1.38(1.27), b_time=1.38(1.29)  Time cost: 06:38/31:06 [6:21:08/16:14:17]  Acc_iter 17900       Data time: 0.01(0.01)  Forward time: 1.38(1.27)  Batch time: 1.38(1.29)
2025-09-04 20:10:13,647   INFO  Train:   11/36 ( 31%) [ 359/1759 ( 20%)]  Loss: 4.146 (3.86)  LR: 2.473e-03  Grad: 4.0359  max=1.2837(module.vfe.pfn_layers.0.linear.weight)  min: -1.3713(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7521, loss_cls=0.1479, loss_bbox=0.8358, matched_ious=0.5002, loss_iou=0.0958, loss_iou_reg=0.2315, d_time=0.00(0.01), f_time=1.32(1.27), b_time=1.32(1.29)  Time cost: 07:42/29:58 [6:22:12/16:11:43]  Acc_iter 17950       Data time: 0.00(0.01)  Forward time: 1.32(1.27)  Batch time: 1.32(1.29)
2025-09-04 20:11:16,373   INFO  Train:   11/36 ( 31%) [ 409/1759 ( 23%)]  Loss: 4.390 (3.87)  LR: 2.479e-03  Grad: 6.9754  max=1.8484(module.vfe.pfn_layers.0.linear.weight)  min: -5.6455(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7356, loss_cls=0.1440, loss_bbox=0.8497, matched_ious=0.4905, loss_iou=0.0963, loss_iou_reg=0.2381, d_time=0.00(0.01), f_time=1.14(1.27), b_time=1.14(1.28)  Time cost: 08:45/28:49 [6:23:15/16:07:51]  Acc_iter 18000       Data time: 0.00(0.01)  Forward time: 1.14(1.27)  Batch time: 1.14(1.28)
2025-09-04 20:12:19,480   INFO  Train:   11/36 ( 31%) [ 459/1759 ( 26%)]  Loss: 3.833 (3.86)  LR: 2.486e-03  Grad: 3.9343  max=0.3237(module.vfe.pfn_layers.0.linear.weight)  min: -0.8924(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7235, loss_cls=0.1437, loss_bbox=0.8500, matched_ious=0.4923, loss_iou=0.0967, loss_iou_reg=0.2367, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.28)  Time cost: 09:48/27:42 [6:24:18/16:05:13]  Acc_iter 18050       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.28)
2025-09-04 20:13:22,869   INFO  Train:   11/36 ( 31%) [ 509/1759 ( 29%)]  Loss: 4.106 (3.86)  LR: 2.493e-03  Grad: 3.2330  max=0.1941(module.vfe.pfn_layers.0.linear.weight)  min: -0.8560(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7417, loss_cls=0.1450, loss_bbox=0.8348, matched_ious=0.4978, loss_iou=0.0944, loss_iou_reg=0.2336, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.28)  Time cost: 10:51/26:37 [6:25:21/16:03:18]  Acc_iter 18100       Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.28)
2025-09-04 20:14:26,341   INFO  Train:   11/36 ( 31%) [ 559/1759 ( 32%)]  Loss: 4.191 (3.86)  LR: 2.499e-03  Grad: 3.3707  max=0.5794(module.vfe.pfn_layers.0.linear.weight)  min: -0.8829(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7417, loss_cls=0.1470, loss_bbox=0.8474, matched_ious=0.5009, loss_iou=0.0964, loss_iou_reg=0.2315, d_time=0.01(0.01), f_time=1.29(1.27), b_time=1.30(1.28)  Time cost: 11:55/25:32 [6:26:25/16:01:40]  Acc_iter 18150       Data time: 0.01(0.01)  Forward time: 1.29(1.27)  Batch time: 1.30(1.28)
2025-09-04 20:15:31,645   INFO  Train:   11/36 ( 31%) [ 609/1759 ( 35%)]  Loss: 3.255 (3.85)  LR: 2.506e-03  Grad: 2.1555  max=0.2407(module.vfe.pfn_layers.0.linear.weight)  min: -0.7919(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7241, loss_cls=0.1431, loss_bbox=0.8266, matched_ious=0.4962, loss_iou=0.0981, loss_iou_reg=0.2352, d_time=0.00(0.01), f_time=2.14(1.27), b_time=2.15(1.28)  Time cost: 13:00/24:31 [6:27:30/16:02:23]  Acc_iter 18200       Data time: 0.00(0.01)  Forward time: 2.14(1.27)  Batch time: 2.15(1.28)
2025-09-04 20:16:35,138   INFO  Train:   11/36 ( 31%) [ 659/1759 ( 37%)]  Loss: 3.751 (3.86)  LR: 2.512e-03  Grad: 1.6919  max=0.4335(module.vfe.pfn_layers.0.linear.weight)  min: -0.3284(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7631, loss_cls=0.1524, loss_bbox=0.8768, matched_ious=0.4918, loss_iou=0.0965, loss_iou_reg=0.2349, d_time=0.01(0.01), f_time=1.19(1.27), b_time=1.20(1.28)  Time cost: 14:04/23:26 [6:28:33/16:00:45]  Acc_iter 18250       Data time: 0.01(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.28)
2025-09-04 20:17:38,516   INFO  Train:   11/36 ( 31%) [ 709/1759 ( 40%)]  Loss: 3.506 (3.86)  LR: 2.519e-03  Grad: 2.6117  max=1.3131(module.vfe.pfn_layers.0.linear.weight)  min: -0.3902(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7468, loss_cls=0.1481, loss_bbox=0.8042, matched_ious=0.4996, loss_iou=0.0971, loss_iou_reg=0.2324, d_time=0.01(0.01), f_time=1.15(1.27), b_time=1.17(1.28)  Time cost: 15:07/22:21 [6:29:37/15:59:05]  Acc_iter 18300       Data time: 0.01(0.01)  Forward time: 1.15(1.27)  Batch time: 1.17(1.28)
2025-09-04 20:18:41,820   INFO  Train:   11/36 ( 31%) [ 759/1759 ( 43%)]  Loss: 4.181 (3.86)  LR: 2.525e-03  Grad: 2.6933  max=0.7462(module.vfe.pfn_layers.0.linear.weight)  min: -1.3329(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7372, loss_cls=0.1508, loss_bbox=0.8263, matched_ious=0.4942, loss_iou=0.0973, loss_iou_reg=0.2380, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.31(1.28)  Time cost: 16:10/21:17 [6:30:40/15:57:26]  Acc_iter 18350       Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.31(1.28)
2025-09-04 20:19:45,276   INFO  Train:   11/36 ( 31%) [ 809/1759 ( 46%)]  Loss: 3.801 (3.85)  LR: 2.531e-03  Grad: 2.5411  max=0.6280(module.vfe.pfn_layers.0.linear.weight)  min: -0.5919(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7175, loss_cls=0.1448, loss_bbox=0.8210, matched_ious=0.4958, loss_iou=0.0980, loss_iou_reg=0.2352, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.27(1.28)  Time cost: 17:14/20:12 [6:31:44/15:55:59]  Acc_iter 18400       Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.28)
2025-09-04 20:20:48,094   INFO  Train:   11/36 ( 31%) [ 859/1759 ( 49%)]  Loss: 3.632 (3.85)  LR: 2.538e-03  Grad: 3.0698  max=1.0443(module.vfe.pfn_layers.0.linear.weight)  min: -0.4582(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7412, loss_cls=0.1497, loss_bbox=0.8291, matched_ious=0.5009, loss_iou=0.0950, loss_iou_reg=0.2347, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.32(1.28)  Time cost: 18:17/19:08 [6:32:46/15:54:02]  Acc_iter 18450       Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.32(1.28)
2025-09-04 20:21:51,058   INFO  Train:   11/36 ( 31%) [ 909/1759 ( 52%)]  Loss: 3.143 (3.84)  LR: 2.544e-03  Grad: 3.1372  max=0.0978(module.dense_head.heatmap_head.0.bn.bias)  min: -1.0773(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7225, loss_cls=0.1447, loss_bbox=0.8290, matched_ious=0.4974, loss_iou=0.0954, loss_iou_reg=0.2353, d_time=0.00(0.01), f_time=1.17(1.27), b_time=1.18(1.28)  Time cost: 19:19/18:03 [6:33:49/15:52:18]  Acc_iter 18500       Data time: 0.00(0.01)  Forward time: 1.17(1.27)  Batch time: 1.18(1.28)
2025-09-04 20:22:54,663   INFO  Train:   11/36 ( 31%) [ 959/1759 ( 55%)]  Loss: 3.448 (3.84)  LR: 2.550e-03  Grad: 3.1649  max=0.6271(module.vfe.pfn_layers.0.linear.weight)  min: -0.7339(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7268, loss_cls=0.1428, loss_bbox=0.8259, matched_ious=0.5044, loss_iou=0.0957, loss_iou_reg=0.2314, d_time=0.00(0.01), f_time=1.35(1.27), b_time=1.36(1.28)  Time cost: 20:23/16:59 [6:34:53/15:51:08]  Acc_iter 18550       Data time: 0.00(0.01)  Forward time: 1.35(1.27)  Batch time: 1.36(1.28)
2025-09-04 20:23:57,766   INFO  Train:   11/36 ( 31%) [1009/1759 ( 57%)]  Loss: 4.285 (3.84)  LR: 2.556e-03  Grad: 3.1347  max=0.4687(module.vfe.pfn_layers.0.linear.weight)  min: -0.3225(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7345, loss_cls=0.1502, loss_bbox=0.8305, matched_ious=0.4929, loss_iou=0.0963, loss_iou_reg=0.2367, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.24(1.27)  Time cost: 21:26/15:55 [6:35:56/15:49:37]  Acc_iter 18600       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.27)
2025-09-04 20:25:02,098   INFO  Train:   11/36 ( 31%) [1059/1759 ( 60%)]  Loss: 3.623 (3.84)  LR: 2.563e-03  Grad: 3.3966  max=0.7885(module.vfe.pfn_layers.0.linear.weight)  min: -0.1609(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7509, loss_cls=0.1475, loss_bbox=0.8593, matched_ious=0.4954, loss_iou=0.0983, loss_iou_reg=0.2357, d_time=0.01(0.01), f_time=1.31(1.27), b_time=1.31(1.28)  Time cost: 22:31/14:52 [6:37:00/15:49:00]  Acc_iter 18650       Data time: 0.01(0.01)  Forward time: 1.31(1.27)  Batch time: 1.31(1.28)
2025-09-04 20:26:07,760   INFO  Train:   11/36 ( 31%) [1109/1759 ( 63%)]  Loss: 3.889 (3.84)  LR: 2.569e-03  Grad: 4.1185  max=1.5576(module.vfe.pfn_layers.0.linear.weight)  min: -0.7177(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7135, loss_cls=0.1486, loss_bbox=0.8084, matched_ious=0.4963, loss_iou=0.0973, loss_iou_reg=0.2340, d_time=0.00(0.01), f_time=1.20(1.27), b_time=1.21(1.28)  Time cost: 23:36/13:49 [6:38:06/15:49:14]  Acc_iter 18700       Data time: 0.00(0.01)  Forward time: 1.20(1.27)  Batch time: 1.21(1.28)
2025-09-04 20:27:11,099   INFO  Train:   11/36 ( 31%) [1159/1759 ( 66%)]  Loss: 4.064 (3.84)  LR: 2.575e-03  Grad: 3.9243  max=0.3380(module.vfe.pfn_layers.0.linear.weight)  min: -1.0779(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7385, loss_cls=0.1456, loss_bbox=0.8623, matched_ious=0.4902, loss_iou=0.0974, loss_iou_reg=0.2381, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.31(1.28)  Time cost: 24:40/12:45 [6:39:09/15:47:52]  Acc_iter 18750       Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.31(1.28)
2025-09-04 20:28:14,095   INFO  Train:   11/36 ( 31%) [1209/1759 ( 69%)]  Loss: 3.273 (3.84)  LR: 2.581e-03  Grad: 2.4157  max=0.6185(module.vfe.pfn_layers.0.linear.weight)  min: -0.7021(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7275, loss_cls=0.1482, loss_bbox=0.8378, matched_ious=0.5053, loss_iou=0.0985, loss_iou_reg=0.2314, d_time=0.01(0.01), f_time=1.24(1.27), b_time=1.24(1.28)  Time cost: 25:43/11:41 [6:40:12/15:46:19]  Acc_iter 18800       Data time: 0.01(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.28)
2025-09-04 20:29:17,064   INFO  Train:   11/36 ( 31%) [1259/1759 ( 72%)]  Loss: 3.640 (3.84)  LR: 2.587e-03  Grad: 2.2034  max=0.6962(module.vfe.pfn_layers.0.linear.weight)  min: -0.4975(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7164, loss_cls=0.1443, loss_bbox=0.8314, matched_ious=0.4904, loss_iou=0.0953, loss_iou_reg=0.2348, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.27)  Time cost: 26:45/10:37 [6:41:15/15:44:47]  Acc_iter 18850       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.27)
2025-09-04 20:30:19,959   INFO  Train:   11/36 ( 31%) [1309/1759 ( 74%)]  Loss: 3.995 (3.84)  LR: 2.593e-03  Grad: 2.4426  max=0.7598(module.vfe.pfn_layers.0.linear.weight)  min: -0.5979(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7275, loss_cls=0.1460, loss_bbox=0.8372, matched_ious=0.4913, loss_iou=0.0969, loss_iou_reg=0.2365, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.27(1.27)  Time cost: 27:48/09:33 [6:42:18/15:43:15]  Acc_iter 18900       Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.27)
2025-09-04 20:31:23,323   INFO  Train:   11/36 ( 31%) [1359/1759 ( 77%)]  Loss: 3.574 (3.84)  LR: 2.599e-03  Grad: 2.8403  max=0.5553(module.vfe.pfn_layers.0.linear.weight)  min: -1.1652(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7117, loss_cls=0.1422, loss_bbox=0.8271, matched_ious=0.5011, loss_iou=0.0938, loss_iou_reg=0.2341, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.27(1.27)  Time cost: 28:52/08:29 [6:43:22/15:42:01]  Acc_iter 18950       Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.27)
2025-09-04 20:32:26,496   INFO  Train:   11/36 ( 31%) [1409/1759 ( 80%)]  Loss: 4.219 (3.84)  LR: 2.605e-03  Grad: 2.7328  max=0.3904(module.vfe.pfn_layers.0.linear.weight)  min: -0.2199(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7428, loss_cls=0.1452, loss_bbox=0.9069, matched_ious=0.4854, loss_iou=0.0985, loss_iou_reg=0.2381, d_time=0.01(0.01), f_time=1.26(1.27), b_time=1.27(1.27)  Time cost: 29:55/07:25 [6:44:25/15:40:41]  Acc_iter 19000       Data time: 0.01(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.27)
2025-09-04 20:33:30,289   INFO  Train:   11/36 ( 31%) [1459/1759 ( 83%)]  Loss: 4.232 (3.84)  LR: 2.611e-03  Grad: 3.2800  max=0.9213(module.vfe.pfn_layers.0.linear.weight)  min: -0.6741(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7148, loss_cls=0.1437, loss_bbox=0.8439, matched_ious=0.4926, loss_iou=0.0948, loss_iou_reg=0.2358, d_time=0.00(0.01), f_time=1.35(1.27), b_time=1.35(1.27)  Time cost: 30:59/06:22 [6:45:29/15:39:41]  Acc_iter 19050       Data time: 0.00(0.01)  Forward time: 1.35(1.27)  Batch time: 1.35(1.27)
2025-09-04 20:34:34,356   INFO  Train:   11/36 ( 31%) [1509/1759 ( 86%)]  Loss: 4.279 (3.84)  LR: 2.617e-03  Grad: 3.4475  max=0.5834(module.vfe.pfn_layers.0.linear.weight)  min: -0.7788(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7115, loss_cls=0.1427, loss_bbox=0.8100, matched_ious=0.5014, loss_iou=0.0989, loss_iou_reg=0.2312, d_time=0.00(0.01), f_time=1.32(1.27), b_time=1.32(1.27)  Time cost: 32:03/05:18 [6:46:33/15:38:49]  Acc_iter 19100       Data time: 0.00(0.01)  Forward time: 1.32(1.27)  Batch time: 1.32(1.27)
2025-09-04 20:35:37,493   INFO  Train:   11/36 ( 31%) [1559/1759 ( 89%)]  Loss: 3.669 (3.84)  LR: 2.622e-03  Grad: 3.3968  max=0.5894(module.vfe.pfn_layers.0.linear.weight)  min: -1.7275(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7281, loss_cls=0.1415, loss_bbox=0.8307, matched_ious=0.5047, loss_iou=0.0965, loss_iou_reg=0.2287, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.31(1.27)  Time cost: 33:06/04:14 [6:47:36/15:37:30]  Acc_iter 19150       Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.31(1.27)
2025-09-04 20:36:42,768   INFO  Train:   11/36 ( 31%) [1609/1759 ( 91%)]  Loss: 4.040 (3.84)  LR: 2.628e-03  Grad: 3.1077  max=0.3349(module.vfe.pfn_layers.0.linear.weight)  min: -1.5478(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7457, loss_cls=0.1449, loss_bbox=0.8309, matched_ious=0.4946, loss_iou=0.0999, loss_iou_reg=0.2363, d_time=0.01(0.01), f_time=1.22(1.27), b_time=1.22(1.27)  Time cost: 34:11/03:11 [6:48:41/15:37:10]  Acc_iter 19200       Data time: 0.01(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.27)
2025-09-04 20:37:46,783   INFO  Train:   11/36 ( 31%) [1659/1759 ( 94%)]  Loss: 3.822 (3.84)  LR: 2.634e-03  Grad: 2.9686  max=0.1073(module.vfe.pfn_layers.0.linear.weight)  min: -0.5923(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7394, loss_cls=0.1446, loss_bbox=0.8547, matched_ious=0.5052, loss_iou=0.0922, loss_iou_reg=0.2274, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.29(1.27)  Time cost: 35:15/02:07 [6:49:45/15:36:14]  Acc_iter 19250       Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.27)
2025-09-04 20:38:49,156   INFO  Train:   11/36 ( 31%) [1709/1759 ( 97%)]  Loss: 3.481 (3.83)  LR: 2.640e-03  Grad: 3.4759  max=0.8583(module.vfe.pfn_layers.0.linear.weight)  min: -0.5655(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7051, loss_cls=0.1400, loss_bbox=0.7913, matched_ious=0.5091, loss_iou=0.0938, loss_iou_reg=0.2259, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.27)  Time cost: 36:18/01:03 [6:50:47/15:34:36]  Acc_iter 19300       Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.27)
2025-09-04 20:39:50,799   INFO  Train:   11/36 ( 31%) [1758/1759 (100%)]  Loss: 3.645 (3.83)  LR: 2.645e-03  Grad: 4.3697  max=0.3326(module.vfe.pfn_layers.0.linear.weight)  min: -2.4141(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7009, loss_cls=0.1356, loss_bbox=0.8544, matched_ious=0.5045, loss_iou=0.0966, loss_iou_reg=0.2282, d_time=0.00(0.01), f_time=0.82(1.27), b_time=0.82(1.27)  Time cost: 37:19/00:01 [6:51:49/15:33:14]  Acc_iter 19349       Data time: 0.00(0.01)  Forward time: 0.82(1.27)  Batch time: 0.82(1.27)

                                               [Aepochs:  31%|███       | 11/36 [6:51:50<15:33:35, 2240.62s/it]epochs:  31%|███       | 11/36 [6:51:50<15:33:36, 2240.65s/it]epochs:  31%|███       | 11/36 [6:51:50<15:33:36, 2240.66s/it]epochs:  31%|███       | 11/36 [6:51:50<15:33:35, 2240.63s/it]epochs:  31%|███       | 11/36 [6:51:50<15:33:35, 2240.63s/it]epochs:  31%|███       | 11/36 [6:51:50<15:33:35, 2240.63s/it]epochs:  31%|███       | 11/36 [6:51:50<15:33:35, 2240.63s/it]epochs:  31%|███       | 11/36 [6:51:50<15:33:35, 2240.64s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 20:39:56,292   INFO  Train:   12/36 ( 33%) [   0/1759 (  0%)]  Loss: 3.822 (3.82)  LR: 2.645e-03  Grad: 3.8498  max=1.2031(module.vfe.pfn_layers.0.linear.weight)  min: -0.7888(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7043, loss_cls=0.1393, loss_bbox=0.7898, matched_ious=0.4135, loss_iou=0.1128, loss_iou_reg=0.2800, d_time=1.22(1.22), f_time=2.95(2.95), b_time=4.16(4.16)  Time cost: 00:03/1:53:24 [6:51:55/47:15:07]  Acc_iter 19350       Data time: 1.22(1.22)  Forward time: 2.95(2.95)  Batch time: 4.16(4.16)
2025-09-04 20:40:59,635   INFO  Train:   12/36 ( 33%) [  50/1759 (  3%)]  Loss: 3.534 (3.85)  LR: 2.651e-03  Grad: 3.9101  max=0.6271(module.vfe.pfn_layers.0.linear.weight)  min: -0.9000(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7231, loss_cls=0.1414, loss_bbox=0.8499, matched_ious=0.4950, loss_iou=0.0965, loss_iou_reg=0.2358, d_time=0.00(0.03), f_time=1.22(1.30), b_time=1.22(1.32)  Time cost: 01:07/37:32 [6:52:58/16:04:47]  Acc_iter 19400       Data time: 0.00(0.03)  Forward time: 1.22(1.30)  Batch time: 1.22(1.32)
2025-09-04 20:42:03,149   INFO  Train:   12/36 ( 33%) [ 100/1759 (  6%)]  Loss: 3.699 (3.79)  LR: 2.657e-03  Grad: 3.9029  max=0.4267(module.vfe.pfn_layers.0.linear.weight)  min: -0.2759(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6976, loss_cls=0.1417, loss_bbox=0.8062, matched_ious=0.5104, loss_iou=0.0953, loss_iou_reg=0.2279, d_time=0.00(0.02), f_time=1.32(1.28), b_time=1.33(1.30)  Time cost: 02:10/35:47 [6:54:01/15:46:27]  Acc_iter 19450       Data time: 0.00(0.02)  Forward time: 1.32(1.28)  Batch time: 1.33(1.30)
2025-09-04 20:43:06,460   INFO  Train:   12/36 ( 33%) [ 150/1759 (  9%)]  Loss: 3.304 (3.80)  LR: 2.662e-03  Grad: 4.2914  max=0.1474(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0426(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7344, loss_cls=0.1449, loss_bbox=0.8199, matched_ious=0.4936, loss_iou=0.0970, loss_iou_reg=0.2351, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.31(1.29)  Time cost: 03:14/34:27 [6:55:05/15:38:35]  Acc_iter 19500       Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.31(1.29)
2025-09-04 20:44:09,935   INFO  Train:   12/36 ( 33%) [ 200/1759 ( 11%)]  Loss: 3.343 (3.79)  LR: 2.668e-03  Grad: 3.3780  max=0.3886(module.dense_head.prediction_head.height.1.bias)  min: -0.2721(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7070, loss_cls=0.1416, loss_bbox=0.7899, matched_ious=0.4972, loss_iou=0.0951, loss_iou_reg=0.2356, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.28)  Time cost: 04:17/33:17 [6:56:08/15:34:42]  Acc_iter 19550       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.28)
2025-09-04 20:45:13,879   INFO  Train:   12/36 ( 33%) [ 250/1759 ( 14%)]  Loss: 4.692 (3.80)  LR: 2.673e-03  Grad: 3.7746  max=1.0546(module.vfe.pfn_layers.0.linear.weight)  min: -0.8958(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7314, loss_cls=0.1476, loss_bbox=0.8156, matched_ious=0.4960, loss_iou=0.0965, loss_iou_reg=0.2348, d_time=0.01(0.01), f_time=1.27(1.27), b_time=1.29(1.28)  Time cost: 05:21/32:12 [6:57:12/15:33:18]  Acc_iter 19600       Data time: 0.01(0.01)  Forward time: 1.27(1.27)  Batch time: 1.29(1.28)
2025-09-04 20:46:17,282   INFO  Train:   12/36 ( 33%) [ 300/1759 ( 17%)]  Loss: 4.266 (3.81)  LR: 2.679e-03  Grad: 3.8992  max=0.4896(module.vfe.pfn_layers.0.linear.weight)  min: -0.3275(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.6964, loss_cls=0.1396, loss_bbox=0.8610, matched_ious=0.4875, loss_iou=0.0972, loss_iou_reg=0.2380, d_time=0.00(0.01), f_time=1.33(1.27), b_time=1.33(1.28)  Time cost: 06:24/31:05 [6:58:16/15:30:42]  Acc_iter 19650       Data time: 0.00(0.01)  Forward time: 1.33(1.27)  Batch time: 1.33(1.28)
2025-09-04 20:47:22,533   INFO  Train:   12/36 ( 33%) [ 350/1759 ( 20%)]  Loss: 3.647 (3.79)  LR: 2.684e-03  Grad: 4.2931  max=1.4683(module.vfe.pfn_layers.0.linear.weight)  min: -0.1810(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6941, loss_cls=0.1377, loss_bbox=0.7661, matched_ious=0.4976, loss_iou=0.0977, loss_iou_reg=0.2375, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.27(1.28)  Time cost: 07:30/30:06 [6:59:21/15:32:23]  Acc_iter 19700       Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.28)
2025-09-04 20:48:26,048   INFO  Train:   12/36 ( 33%) [ 400/1759 ( 23%)]  Loss: 4.101 (3.79)  LR: 2.689e-03  Grad: 4.0775  max=0.9542(module.vfe.pfn_layers.0.linear.weight)  min: -0.7057(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7119, loss_cls=0.1418, loss_bbox=0.7944, matched_ious=0.5004, loss_iou=0.0967, loss_iou_reg=0.2347, d_time=0.01(0.01), f_time=1.35(1.27), b_time=1.36(1.28)  Time cost: 08:33/29:00 [7:00:24/15:30:13]  Acc_iter 19750       Data time: 0.01(0.01)  Forward time: 1.35(1.27)  Batch time: 1.36(1.28)
2025-09-04 20:49:28,946   INFO  Train:   12/36 ( 33%) [ 450/1759 ( 26%)]  Loss: 3.210 (3.79)  LR: 2.695e-03  Grad: 4.2973  max=0.8497(module.vfe.pfn_layers.0.linear.weight)  min: -0.3305(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7192, loss_cls=0.1442, loss_bbox=0.8172, matched_ious=0.5056, loss_iou=0.0961, loss_iou_reg=0.2314, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.24(1.28)  Time cost: 09:36/27:53 [7:01:27/15:27:18]  Acc_iter 19800       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.28)
2025-09-04 20:50:31,807   INFO  Train:   12/36 ( 33%) [ 500/1759 ( 28%)]  Loss: 3.514 (3.79)  LR: 2.700e-03  Grad: 4.3049  max=0.2246(module.vfe.pfn_layers.0.linear.weight)  min: -0.2360(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7369, loss_cls=0.1471, loss_bbox=0.8195, matched_ious=0.5045, loss_iou=0.0933, loss_iou_reg=0.2310, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.23(1.28)  Time cost: 10:39/26:46 [7:02:30/15:24:43]  Acc_iter 19850       Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.23(1.28)
2025-09-04 20:51:35,493   INFO  Train:   12/36 ( 33%) [ 550/1759 ( 31%)]  Loss: 3.061 (3.79)  LR: 2.705e-03  Grad: 4.6128  max=0.4956(module.vfe.pfn_layers.0.linear.weight)  min: -0.6003(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7077, loss_cls=0.1395, loss_bbox=0.8160, matched_ious=0.5032, loss_iou=0.0934, loss_iou_reg=0.2322, d_time=0.00(0.01), f_time=1.35(1.27), b_time=1.36(1.28)  Time cost: 11:43/25:42 [7:03:34/15:23:29]  Acc_iter 19900       Data time: 0.00(0.01)  Forward time: 1.35(1.27)  Batch time: 1.36(1.28)
2025-09-04 20:52:38,731   INFO  Train:   12/36 ( 33%) [ 600/1759 ( 34%)]  Loss: 4.090 (3.78)  LR: 2.710e-03  Grad: 4.8560  max=0.8005(module.vfe.pfn_layers.0.linear.weight)  min: -0.2533(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7117, loss_cls=0.1424, loss_bbox=0.7940, matched_ious=0.4970, loss_iou=0.0958, loss_iou_reg=0.2345, d_time=0.00(0.01), f_time=1.17(1.27), b_time=1.17(1.28)  Time cost: 12:46/24:37 [7:04:37/15:21:45]  Acc_iter 19950       Data time: 0.00(0.01)  Forward time: 1.17(1.27)  Batch time: 1.17(1.28)
2025-09-04 20:53:41,772   INFO  Train:   12/36 ( 33%) [ 650/1759 ( 37%)]  Loss: 3.598 (3.78)  LR: 2.716e-03  Grad: 3.8407  max=0.3758(module.vfe.pfn_layers.0.linear.weight)  min: -0.5768(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7068, loss_cls=0.1414, loss_bbox=0.8461, matched_ious=0.5001, loss_iou=0.0970, loss_iou_reg=0.2323, d_time=0.00(0.01), f_time=1.35(1.27), b_time=1.36(1.27)  Time cost: 13:49/23:32 [7:05:40/15:19:54]  Acc_iter 20000       Data time: 0.00(0.01)  Forward time: 1.35(1.27)  Batch time: 1.36(1.27)
2025-09-04 20:54:44,985   INFO  Train:   12/36 ( 33%) [ 700/1759 ( 40%)]  Loss: 3.372 (3.78)  LR: 2.721e-03  Grad: 4.2773  max=1.2960(module.vfe.pfn_layers.0.linear.weight)  min: -0.5036(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7141, loss_cls=0.1451, loss_bbox=0.7751, matched_ious=0.5036, loss_iou=0.0952, loss_iou_reg=0.2314, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 14:52/22:28 [7:06:43/15:18:20]  Acc_iter 20050       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-04 20:55:48,359   INFO  Train:   12/36 ( 33%) [ 750/1759 ( 43%)]  Loss: 3.881 (3.77)  LR: 2.726e-03  Grad: 4.3369  max=0.6014(module.vfe.pfn_layers.0.linear.weight)  min: -0.1609(module.dense_head.prediction_head.dim.1.bias)  NaN: False  loss_hm=0.6943, loss_cls=0.1387, loss_bbox=0.7889, matched_ious=0.4947, loss_iou=0.0945, loss_iou_reg=0.2360, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 15:55/21:24 [7:07:47/15:17:00]  Acc_iter 20100       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-04 20:56:52,553   INFO  Train:   12/36 ( 33%) [ 800/1759 ( 45%)]  Loss: 3.408 (3.78)  LR: 2.731e-03  Grad: 4.5421  max=0.2991(module.vfe.pfn_layers.0.linear.weight)  min: -0.4643(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7498, loss_cls=0.1491, loss_bbox=0.8490, matched_ious=0.4957, loss_iou=0.0946, loss_iou_reg=0.2342, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.29(1.27)  Time cost: 17:00/20:21 [7:08:51/15:16:26]  Acc_iter 20150       Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.29(1.27)
2025-09-04 20:57:58,065   INFO  Train:   12/36 ( 33%) [ 850/1759 ( 48%)]  Loss: 3.733 (3.78)  LR: 2.736e-03  Grad: 2.0727  max=0.4094(module.vfe.pfn_layers.0.linear.weight)  min: -0.8523(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7084, loss_cls=0.1385, loss_bbox=0.8302, matched_ious=0.5026, loss_iou=0.0959, loss_iou_reg=0.2302, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.28)  Time cost: 18:05/19:19 [7:09:56/15:16:55]  Acc_iter 20200       Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.28)
2025-09-04 20:59:01,571   INFO  Train:   12/36 ( 33%) [ 900/1759 ( 51%)]  Loss: 3.803 (3.77)  LR: 2.741e-03  Grad: 2.3589  max=0.8016(module.vfe.pfn_layers.0.linear.weight)  min: -0.4414(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6962, loss_cls=0.1359, loss_bbox=0.7947, matched_ious=0.4988, loss_iou=0.0958, loss_iou_reg=0.2342, d_time=0.00(0.01), f_time=1.32(1.27), b_time=1.33(1.28)  Time cost: 19:09/18:15 [7:11:00/15:15:38]  Acc_iter 20250       Data time: 0.00(0.01)  Forward time: 1.32(1.27)  Batch time: 1.33(1.28)
2025-09-04 21:00:04,662   INFO  Train:   12/36 ( 33%) [ 950/1759 ( 54%)]  Loss: 3.665 (3.77)  LR: 2.746e-03  Grad: 2.5472  max=0.8413(module.vfe.pfn_layers.0.linear.weight)  min: -0.5760(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7223, loss_cls=0.1396, loss_bbox=0.8252, matched_ious=0.4990, loss_iou=0.0960, loss_iou_reg=0.2346, d_time=0.00(0.01), f_time=1.35(1.27), b_time=1.35(1.28)  Time cost: 20:12/17:11 [7:12:03/15:14:03]  Acc_iter 20300       Data time: 0.00(0.01)  Forward time: 1.35(1.27)  Batch time: 1.35(1.28)
2025-09-04 21:01:07,934   INFO  Train:   12/36 ( 33%) [1000/1759 ( 57%)]  Loss: 2.978 (3.77)  LR: 2.751e-03  Grad: 2.5522  max=0.4358(module.vfe.pfn_layers.0.linear.weight)  min: -0.3212(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6999, loss_cls=0.1424, loss_bbox=0.7735, matched_ious=0.5099, loss_iou=0.0952, loss_iou_reg=0.2275, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.27)  Time cost: 21:15/16:07 [7:13:06/15:12:40]  Acc_iter 20350       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.27)
2025-09-04 21:02:11,717   INFO  Train:   12/36 ( 33%) [1050/1759 ( 60%)]  Loss: 3.295 (3.77)  LR: 2.755e-03  Grad: 2.9499  max=0.2518(module.dense_head.prediction_head.height.1.bias)  min: -0.6718(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6990, loss_cls=0.1380, loss_bbox=0.8216, matched_ious=0.5059, loss_iou=0.0950, loss_iou_reg=0.2291, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.29(1.27)  Time cost: 22:19/15:03 [7:14:10/15:11:39]  Acc_iter 20400       Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.29(1.27)
2025-09-04 21:03:14,517   INFO  Train:   12/36 ( 33%) [1100/1759 ( 63%)]  Loss: 4.367 (3.77)  LR: 2.760e-03  Grad: 3.1606  max=0.4530(module.vfe.pfn_layers.0.linear.weight)  min: -0.5899(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6937, loss_cls=0.1366, loss_bbox=0.8111, matched_ious=0.5021, loss_iou=0.0968, loss_iou_reg=0.2324, d_time=0.00(0.01), f_time=1.18(1.27), b_time=1.18(1.27)  Time cost: 23:22/13:59 [7:15:13/15:10:00]  Acc_iter 20450       Data time: 0.00(0.01)  Forward time: 1.18(1.27)  Batch time: 1.18(1.27)
2025-09-04 21:04:17,877   INFO  Train:   12/36 ( 33%) [1150/1759 ( 65%)]  Loss: 3.064 (3.77)  LR: 2.765e-03  Grad: 3.0913  max=0.7007(module.vfe.pfn_layers.0.linear.weight)  min: -0.6755(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7535, loss_cls=0.1494, loss_bbox=0.8358, matched_ious=0.5013, loss_iou=0.0963, loss_iou_reg=0.2328, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.29(1.27)  Time cost: 24:25/12:55 [7:16:16/15:08:44]  Acc_iter 20500       Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.27)
2025-09-04 21:05:21,324   INFO  Train:   12/36 ( 33%) [1200/1759 ( 68%)]  Loss: 3.922 (3.77)  LR: 2.770e-03  Grad: 2.8095  max=0.7457(module.vfe.pfn_layers.0.linear.weight)  min: -0.5504(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7157, loss_cls=0.1433, loss_bbox=0.8224, matched_ious=0.4984, loss_iou=0.0989, loss_iou_reg=0.2338, d_time=0.00(0.01), f_time=1.20(1.27), b_time=1.20(1.27)  Time cost: 25:28/11:51 [7:17:20/15:07:33]  Acc_iter 20550       Data time: 0.00(0.01)  Forward time: 1.20(1.27)  Batch time: 1.20(1.27)
2025-09-04 21:06:24,890   INFO  Train:   12/36 ( 33%) [1250/1759 ( 71%)]  Loss: 3.766 (3.77)  LR: 2.774e-03  Grad: 3.0522  max=0.5807(module.vfe.pfn_layers.0.linear.weight)  min: -0.1465(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6918, loss_cls=0.1366, loss_bbox=0.8164, matched_ious=0.4944, loss_iou=0.0953, loss_iou_reg=0.2362, d_time=0.01(0.01), f_time=1.20(1.27), b_time=1.21(1.27)  Time cost: 26:32/10:47 [7:18:23/15:06:27]  Acc_iter 20600       Data time: 0.01(0.01)  Forward time: 1.20(1.27)  Batch time: 1.21(1.27)
2025-09-04 21:07:29,019   INFO  Train:   12/36 ( 33%) [1300/1759 ( 74%)]  Loss: 3.333 (3.77)  LR: 2.779e-03  Grad: 3.2240  max=0.3949(module.vfe.pfn_layers.0.linear.weight)  min: -0.3970(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7040, loss_cls=0.1384, loss_bbox=0.7952, matched_ious=0.5075, loss_iou=0.0938, loss_iou_reg=0.2308, d_time=0.00(0.01), f_time=1.33(1.27), b_time=1.34(1.27)  Time cost: 27:36/09:44 [7:19:27/15:05:39]  Acc_iter 20650       Data time: 0.00(0.01)  Forward time: 1.33(1.27)  Batch time: 1.34(1.27)
2025-09-04 21:08:34,173   INFO  Train:   12/36 ( 33%) [1350/1759 ( 77%)]  Loss: 3.871 (3.77)  LR: 2.783e-03  Grad: 3.4690  max=0.8175(module.vfe.pfn_layers.0.linear.weight)  min: -0.1818(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7176, loss_cls=0.1374, loss_bbox=0.8213, matched_ious=0.4933, loss_iou=0.0978, loss_iou_reg=0.2376, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.27)  Time cost: 28:41/08:41 [7:20:32/15:05:22]  Acc_iter 20700       Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.27)
2025-09-04 21:09:37,969   INFO  Train:   12/36 ( 33%) [1400/1759 ( 80%)]  Loss: 3.887 (3.77)  LR: 2.788e-03  Grad: 3.6980  max=0.4370(module.vfe.pfn_layers.0.linear.weight)  min: -0.5568(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7371, loss_cls=0.1433, loss_bbox=0.8215, matched_ious=0.5022, loss_iou=0.0967, loss_iou_reg=0.2317, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.27)  Time cost: 29:45/07:37 [7:21:36/15:04:19]  Acc_iter 20750       Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.27)
2025-09-04 21:10:41,595   INFO  Train:   12/36 ( 33%) [1450/1759 ( 82%)]  Loss: 3.743 (3.76)  LR: 2.792e-03  Grad: 3.8272  max=0.1291(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3802(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6928, loss_cls=0.1377, loss_bbox=0.7752, matched_ious=0.5051, loss_iou=0.0956, loss_iou_reg=0.2318, d_time=0.01(0.01), f_time=1.35(1.27), b_time=1.36(1.27)  Time cost: 30:49/06:33 [7:22:40/15:03:14]  Acc_iter 20800       Data time: 0.01(0.01)  Forward time: 1.35(1.27)  Batch time: 1.36(1.27)
2025-09-04 21:11:44,906   INFO  Train:   12/36 ( 33%) [1500/1759 ( 85%)]  Loss: 3.621 (3.77)  LR: 2.797e-03  Grad: 4.1713  max=0.6416(module.vfe.pfn_layers.0.linear.weight)  min: -0.5392(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7168, loss_cls=0.1449, loss_bbox=0.8205, matched_ious=0.5047, loss_iou=0.0949, loss_iou_reg=0.2319, d_time=0.00(0.01), f_time=1.39(1.27), b_time=1.39(1.27)  Time cost: 31:52/05:30 [7:23:43/15:01:59]  Acc_iter 20850       Data time: 0.00(0.01)  Forward time: 1.39(1.27)  Batch time: 1.39(1.27)
2025-09-04 21:12:48,401   INFO  Train:   12/36 ( 33%) [1550/1759 ( 88%)]  Loss: 3.057 (3.76)  LR: 2.801e-03  Grad: 4.5972  max=0.5401(module.vfe.pfn_layers.0.linear.weight)  min: -1.9894(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7107, loss_cls=0.1421, loss_bbox=0.7885, matched_ious=0.5103, loss_iou=0.0947, loss_iou_reg=0.2278, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.27)  Time cost: 32:55/04:26 [7:24:47/15:00:49]  Acc_iter 20900       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.27)
2025-09-04 21:13:52,653   INFO  Train:   12/36 ( 33%) [1600/1759 ( 91%)]  Loss: 4.053 (3.76)  LR: 2.806e-03  Grad: 4.0735  max=0.3150(module.vfe.pfn_layers.0.linear.weight)  min: -0.3524(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7020, loss_cls=0.1387, loss_bbox=0.8056, matched_ious=0.4969, loss_iou=0.0972, loss_iou_reg=0.2347, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.27)  Time cost: 34:00/03:22 [7:25:51/15:00:00]  Acc_iter 20950       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.27)
2025-09-04 21:14:56,422   INFO  Train:   12/36 ( 33%) [1650/1759 ( 94%)]  Loss: 3.597 (3.76)  LR: 2.810e-03  Grad: 4.3360  max=0.2322(module.vfe.pfn_layers.0.linear.weight)  min: -0.4972(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7091, loss_cls=0.1401, loss_bbox=0.8479, matched_ious=0.4985, loss_iou=0.0963, loss_iou_reg=0.2342, d_time=0.00(0.01), f_time=1.36(1.27), b_time=1.37(1.27)  Time cost: 35:03/02:18 [7:26:55/14:58:58]  Acc_iter 21000       Data time: 0.00(0.01)  Forward time: 1.36(1.27)  Batch time: 1.37(1.27)
2025-09-04 21:16:00,234   INFO  Train:   12/36 ( 33%) [1700/1759 ( 97%)]  Loss: 3.799 (3.76)  LR: 2.814e-03  Grad: 4.7820  max=0.1557(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9504(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7192, loss_cls=0.1391, loss_bbox=0.7904, matched_ious=0.5036, loss_iou=0.0980, loss_iou_reg=0.2307, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.27)  Time cost: 36:07/01:15 [7:27:59/14:57:56]  Acc_iter 21050       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.27)
2025-09-04 21:17:03,874   INFO  Train:   12/36 ( 33%) [1750/1759 ( 99%)]  Loss: 3.735 (3.76)  LR: 2.818e-03  Grad: 5.0952  max=1.2513(module.vfe.pfn_layers.0.linear.weight)  min: -0.2256(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7169, loss_cls=0.1418, loss_bbox=0.8158, matched_ious=0.4913, loss_iou=0.0981, loss_iou_reg=0.2354, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.30(1.27)  Time cost: 37:11/00:11 [7:29:02/14:56:50]  Acc_iter 21100       Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.30(1.27)
2025-09-04 21:17:13,314   INFO  Train:   12/36 ( 33%) [1758/1759 (100%)]  Loss: 4.896 (3.76)  LR: 2.819e-03  Grad: 5.0416  max=0.3459(module.vfe.pfn_layers.0.linear.weight)  min: -0.7041(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6904, loss_cls=0.1336, loss_bbox=0.9200, matched_ious=0.4876, loss_iou=0.0974, loss_iou_reg=0.2371, d_time=0.00(0.01), f_time=0.71(1.27), b_time=0.72(1.27)  Time cost: 37:20/00:01 [7:29:12/14:56:22]  Acc_iter 21108       Data time: 0.00(0.01)  Forward time: 0.71(1.27)  Batch time: 0.72(1.27)

                                               [Aepochs:  33%|███▎      | 12/36 [7:29:12<14:56:29, 2241.22s/it]epochs:  33%|███▎      | 12/36 [7:29:12<14:56:29, 2241.23s/it]epochs:  33%|███▎      | 12/36 [7:29:12<14:56:28, 2241.21s/it]epochs:  33%|███▎      | 12/36 [7:29:12<14:56:29, 2241.21s/it]epochs:  33%|███▎      | 12/36 [7:29:12<14:56:29, 2241.21s/it]epochs:  33%|███▎      | 12/36 [7:29:12<14:56:29, 2241.25s/it]epochs:  33%|███▎      | 12/36 [7:29:12<14:56:29, 2241.22s/it]epochs:  33%|███▎      | 12/36 [7:29:12<14:56:29, 2241.23s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 21:17:18,920   INFO  Train:   13/36 ( 36%) [   0/1759 (  0%)]  Loss: 4.163 (4.16)  LR: 2.819e-03  Grad: 5.0752  max=0.9806(module.vfe.pfn_layers.0.linear.weight)  min: -0.3638(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.6702, loss_cls=0.1538, loss_bbox=0.8954, matched_ious=0.4679, loss_iou=0.0884, loss_iou_reg=0.2490, d_time=1.80(1.80), f_time=2.39(2.39), b_time=4.20(4.20)  Time cost: 00:03/1:53:40 [7:29:17/45:28:18]  Acc_iter 21109       Data time: 1.80(1.80)  Forward time: 2.39(2.39)  Batch time: 4.20(4.20)
2025-09-04 21:18:11,278   INFO  Train:   13/36 ( 36%) [  41/1759 (  2%)]  Loss: 3.765 (3.71)  LR: 2.823e-03  Grad: 5.2328  max=1.1210(module.vfe.pfn_layers.0.linear.weight)  min: -0.5416(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6930, loss_cls=0.1341, loss_bbox=0.8281, matched_ious=0.5027, loss_iou=0.0959, loss_iou_reg=0.2337, d_time=0.00(0.05), f_time=1.25(1.30), b_time=1.25(1.35)  Time cost: 00:56/38:20 [7:30:10/15:41:08]  Acc_iter 21150       Data time: 0.00(0.05)  Forward time: 1.25(1.30)  Batch time: 1.25(1.35)
2025-09-04 21:19:17,695   INFO  Train:   13/36 ( 36%) [  91/1759 (  5%)]  Loss: 4.056 (3.76)  LR: 2.827e-03  Grad: 4.2259  max=0.8795(module.vfe.pfn_layers.0.linear.weight)  min: -1.0227(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7296, loss_cls=0.1423, loss_bbox=0.8298, matched_ious=0.4973, loss_iou=0.0951, loss_iou_reg=0.2334, d_time=0.01(0.03), f_time=1.36(1.31), b_time=1.37(1.34)  Time cost: 02:02/37:03 [7:31:16/15:36:00]  Acc_iter 21200       Data time: 0.01(0.03)  Forward time: 1.36(1.31)  Batch time: 1.37(1.34)
2025-09-04 21:20:21,592   INFO  Train:   13/36 ( 36%) [ 141/1759 (  8%)]  Loss: 3.195 (3.72)  LR: 2.831e-03  Grad: 4.3870  max=0.1512(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.6966(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6858, loss_cls=0.1355, loss_bbox=0.7763, matched_ious=0.5048, loss_iou=0.0952, loss_iou_reg=0.2332, d_time=0.00(0.02), f_time=1.36(1.29), b_time=1.37(1.32)  Time cost: 03:06/35:25 [7:32:20/15:21:14]  Acc_iter 21250       Data time: 0.00(0.02)  Forward time: 1.36(1.29)  Batch time: 1.37(1.32)
2025-09-04 21:21:24,866   INFO  Train:   13/36 ( 36%) [ 191/1759 ( 11%)]  Loss: 3.577 (3.71)  LR: 2.835e-03  Grad: 4.5654  max=0.7628(module.vfe.pfn_layers.0.linear.weight)  min: -0.5520(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6848, loss_cls=0.1337, loss_bbox=0.8063, matched_ious=0.5032, loss_iou=0.0961, loss_iou_reg=0.2333, d_time=0.01(0.02), f_time=1.27(1.29), b_time=1.28(1.30)  Time cost: 04:09/34:00 [7:33:23/15:11:21]  Acc_iter 21300       Data time: 0.01(0.02)  Forward time: 1.27(1.29)  Batch time: 1.28(1.30)
2025-09-04 21:22:28,707   INFO  Train:   13/36 ( 36%) [ 241/1759 ( 14%)]  Loss: 4.603 (3.73)  LR: 2.839e-03  Grad: 4.9583  max=1.1492(module.vfe.pfn_layers.0.linear.weight)  min: -0.3688(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7357, loss_cls=0.1437, loss_bbox=0.8375, matched_ious=0.4984, loss_iou=0.0953, loss_iou_reg=0.2321, d_time=0.00(0.02), f_time=1.21(1.28), b_time=1.22(1.30)  Time cost: 05:13/32:47 [7:34:27/15:06:45]  Acc_iter 21350       Data time: 0.00(0.02)  Forward time: 1.21(1.28)  Batch time: 1.22(1.30)
2025-09-04 21:23:33,233   INFO  Train:   13/36 ( 36%) [ 291/1759 ( 17%)]  Loss: 4.193 (3.72)  LR: 2.843e-03  Grad: 5.0954  max=0.9810(module.vfe.pfn_layers.0.linear.weight)  min: -0.5641(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7012, loss_cls=0.1409, loss_bbox=0.8060, matched_ious=0.5090, loss_iou=0.0953, loss_iou_reg=0.2282, d_time=0.00(0.01), f_time=1.28(1.28), b_time=1.29(1.30)  Time cost: 06:18/31:41 [7:35:32/15:05:00]  Acc_iter 21400       Data time: 0.00(0.01)  Forward time: 1.28(1.28)  Batch time: 1.29(1.30)
2025-09-04 21:24:36,672   INFO  Train:   13/36 ( 36%) [ 341/1759 ( 19%)]  Loss: 3.522 (3.72)  LR: 2.847e-03  Grad: 6.1262  max=1.9266(module.vfe.pfn_layers.0.linear.weight)  min: -2.0197(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6955, loss_cls=0.1380, loss_bbox=0.8168, matched_ious=0.5052, loss_iou=0.0956, loss_iou_reg=0.2315, d_time=0.00(0.01), f_time=1.21(1.28), b_time=1.21(1.29)  Time cost: 07:21/30:31 [7:36:35/15:01:13]  Acc_iter 21450       Data time: 0.00(0.01)  Forward time: 1.21(1.28)  Batch time: 1.21(1.29)
2025-09-04 21:25:40,568   INFO  Train:   13/36 ( 36%) [ 391/1759 ( 22%)]  Loss: 4.187 (3.71)  LR: 2.851e-03  Grad: 5.6920  max=0.7406(module.vfe.pfn_layers.0.linear.weight)  min: -1.1323(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6872, loss_cls=0.1324, loss_bbox=0.7824, matched_ious=0.5051, loss_iou=0.0959, loss_iou_reg=0.2311, d_time=0.00(0.01), f_time=1.24(1.28), b_time=1.24(1.29)  Time cost: 08:25/29:24 [7:37:39/14:58:57]  Acc_iter 21500       Data time: 0.00(0.01)  Forward time: 1.24(1.28)  Batch time: 1.24(1.29)
2025-09-04 21:26:43,363   INFO  Train:   13/36 ( 36%) [ 441/1759 ( 25%)]  Loss: 3.757 (3.72)  LR: 2.854e-03  Grad: 5.8559  max=0.4989(module.vfe.pfn_layers.0.linear.weight)  min: -1.0023(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7001, loss_cls=0.1345, loss_bbox=0.7981, matched_ious=0.5071, loss_iou=0.0955, loss_iou_reg=0.2281, d_time=0.00(0.01), f_time=1.24(1.28), b_time=1.24(1.29)  Time cost: 09:28/28:14 [7:38:42/14:55:14]  Acc_iter 21550       Data time: 0.00(0.01)  Forward time: 1.24(1.28)  Batch time: 1.24(1.29)
2025-09-04 21:27:46,936   INFO  Train:   13/36 ( 36%) [ 491/1759 ( 28%)]  Loss: 3.766 (3.73)  LR: 2.858e-03  Grad: 6.0576  max=0.3337(module.vfe.pfn_layers.0.linear.weight)  min: -0.7636(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7243, loss_cls=0.1395, loss_bbox=0.8563, matched_ious=0.4992, loss_iou=0.0982, loss_iou_reg=0.2331, d_time=0.00(0.01), f_time=1.32(1.27), b_time=1.32(1.28)  Time cost: 10:31/27:08 [7:39:45/14:53:08]  Acc_iter 21600       Data time: 0.00(0.01)  Forward time: 1.32(1.27)  Batch time: 1.32(1.28)
2025-09-04 21:28:49,984   INFO  Train:   13/36 ( 36%) [ 541/1759 ( 31%)]  Loss: 3.112 (3.74)  LR: 2.862e-03  Grad: 4.0826  max=0.3433(module.vfe.pfn_layers.0.linear.weight)  min: -0.7805(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7185, loss_cls=0.1396, loss_bbox=0.8497, matched_ious=0.5008, loss_iou=0.0964, loss_iou_reg=0.2299, d_time=0.00(0.01), f_time=1.18(1.27), b_time=1.19(1.28)  Time cost: 11:34/26:01 [7:40:48/14:50:34]  Acc_iter 21650       Data time: 0.00(0.01)  Forward time: 1.18(1.27)  Batch time: 1.19(1.28)
2025-09-04 21:29:55,274   INFO  Train:   13/36 ( 36%) [ 591/1759 ( 34%)]  Loss: 4.093 (3.73)  LR: 2.865e-03  Grad: 4.2364  max=0.4939(module.vfe.pfn_layers.0.linear.weight)  min: -0.2861(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7006, loss_cls=0.1395, loss_bbox=0.7764, matched_ious=0.5081, loss_iou=0.0922, loss_iou_reg=0.2284, d_time=0.00(0.01), f_time=1.23(1.28), b_time=1.24(1.28)  Time cost: 12:40/24:59 [7:41:54/14:50:53]  Acc_iter 21700       Data time: 0.00(0.01)  Forward time: 1.23(1.28)  Batch time: 1.24(1.28)
2025-09-04 21:30:58,365   INFO  Train:   13/36 ( 36%) [ 641/1759 ( 36%)]  Loss: 3.801 (3.73)  LR: 2.869e-03  Grad: 4.5505  max=0.6167(module.vfe.pfn_layers.0.linear.weight)  min: -0.4594(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6841, loss_cls=0.1377, loss_bbox=0.7170, matched_ious=0.5095, loss_iou=0.0964, loss_iou_reg=0.2313, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.24(1.28)  Time cost: 13:43/23:53 [7:42:57/14:48:37]  Acc_iter 21750       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.28)
2025-09-04 21:32:01,530   INFO  Train:   13/36 ( 36%) [ 691/1759 ( 39%)]  Loss: 3.253 (3.72)  LR: 2.873e-03  Grad: 4.7316  max=0.5953(module.vfe.pfn_layers.0.linear.weight)  min: -0.1682(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6997, loss_cls=0.1359, loss_bbox=0.8088, matched_ious=0.5049, loss_iou=0.0968, loss_iou_reg=0.2300, d_time=0.01(0.01), f_time=1.23(1.27), b_time=1.24(1.28)  Time cost: 14:46/22:48 [7:44:00/14:46:35]  Acc_iter 21800       Data time: 0.01(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.28)
2025-09-04 21:33:06,844   INFO  Train:   13/36 ( 36%) [ 741/1759 ( 42%)]  Loss: 3.673 (3.72)  LR: 2.876e-03  Grad: 4.3102  max=1.4736(module.vfe.pfn_layers.0.linear.weight)  min: -0.4078(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7228, loss_cls=0.1410, loss_bbox=0.7974, matched_ious=0.5015, loss_iou=0.0933, loss_iou_reg=0.2333, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.28)  Time cost: 15:51/21:45 [7:45:05/14:46:41]  Acc_iter 21850       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.28)
2025-09-04 21:34:09,110   INFO  Train:   13/36 ( 36%) [ 791/1759 ( 45%)]  Loss: 3.742 (3.72)  LR: 2.880e-03  Grad: 4.0896  max=0.3455(module.vfe.pfn_layers.0.linear.weight)  min: -0.3953(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6873, loss_cls=0.1328, loss_bbox=0.7891, matched_ious=0.5060, loss_iou=0.0952, loss_iou_reg=0.2320, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.28)  Time cost: 16:54/20:39 [7:46:07/14:43:59]  Acc_iter 21900       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.28)
2025-09-04 21:35:12,037   INFO  Train:   13/36 ( 36%) [ 841/1759 ( 48%)]  Loss: 4.323 (3.72)  LR: 2.883e-03  Grad: 4.6058  max=0.4457(module.vfe.pfn_layers.0.linear.weight)  min: -1.1954(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7071, loss_cls=0.1352, loss_bbox=0.7904, matched_ious=0.5167, loss_iou=0.0936, loss_iou_reg=0.2252, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.20(1.28)  Time cost: 17:56/19:34 [7:47:10/14:42:02]  Acc_iter 21950       Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.28)
2025-09-04 21:36:15,062   INFO  Train:   13/36 ( 36%) [ 891/1759 ( 51%)]  Loss: 3.113 (3.72)  LR: 2.886e-03  Grad: 5.4989  max=0.3665(module.vfe.pfn_layers.0.linear.weight)  min: -2.1087(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7140, loss_cls=0.1389, loss_bbox=0.7774, matched_ious=0.5114, loss_iou=0.0949, loss_iou_reg=0.2282, d_time=0.01(0.01), f_time=1.19(1.27), b_time=1.20(1.28)  Time cost: 19:00/18:29 [7:48:13/14:40:15]  Acc_iter 22000       Data time: 0.01(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.28)
2025-09-04 21:37:18,486   INFO  Train:   13/36 ( 36%) [ 941/1759 ( 53%)]  Loss: 3.413 (3.71)  LR: 2.890e-03  Grad: 5.5679  max=3.1029(module.vfe.pfn_layers.0.linear.weight)  min: -0.5361(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7016, loss_cls=0.1386, loss_bbox=0.7754, matched_ious=0.4991, loss_iou=0.0971, loss_iou_reg=0.2343, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.24(1.28)  Time cost: 20:03/17:25 [7:49:17/14:38:50]  Acc_iter 22050       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.28)
2025-09-04 21:38:21,258   INFO  Train:   13/36 ( 36%) [ 991/1759 ( 56%)]  Loss: 3.525 (3.72)  LR: 2.893e-03  Grad: 4.2303  max=0.2620(module.vfe.pfn_layers.0.linear.weight)  min: -0.4054(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7302, loss_cls=0.1456, loss_bbox=0.7924, matched_ious=0.4955, loss_iou=0.0973, loss_iou_reg=0.2373, d_time=0.01(0.01), f_time=1.27(1.27), b_time=1.28(1.28)  Time cost: 21:06/16:20 [7:50:20/14:37:00]  Acc_iter 22100       Data time: 0.01(0.01)  Forward time: 1.27(1.27)  Batch time: 1.28(1.28)
2025-09-04 21:39:24,168   INFO  Train:   13/36 ( 36%) [1041/1759 ( 59%)]  Loss: 4.048 (3.72)  LR: 2.896e-03  Grad: 4.3892  max=0.2409(module.backbone_3d.cls_conv.3.bias)  min: -0.4758(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6757, loss_cls=0.1332, loss_bbox=0.8124, matched_ious=0.5067, loss_iou=0.0970, loss_iou_reg=0.2305, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.27(1.28)  Time cost: 22:09/15:15 [7:51:22/14:35:20]  Acc_iter 22150       Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.28)
2025-09-04 21:40:29,222   INFO  Train:   13/36 ( 36%) [1091/1759 ( 62%)]  Loss: 3.631 (3.71)  LR: 2.900e-03  Grad: 4.5767  max=0.4438(module.vfe.pfn_layers.0.linear.weight)  min: -0.2073(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6894, loss_cls=0.1340, loss_bbox=0.7945, matched_ious=0.5084, loss_iou=0.0958, loss_iou_reg=0.2294, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.28)  Time cost: 23:14/14:12 [7:52:27/14:35:05]  Acc_iter 22200       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.28)
2025-09-04 21:41:32,699   INFO  Train:   13/36 ( 36%) [1141/1759 ( 65%)]  Loss: 4.191 (3.71)  LR: 2.903e-03  Grad: 4.9782  max=0.3732(module.vfe.pfn_layers.0.linear.weight)  min: -0.8549(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7039, loss_cls=0.1388, loss_bbox=0.8143, matched_ious=0.5034, loss_iou=0.0982, loss_iou_reg=0.2337, d_time=0.01(0.01), f_time=1.19(1.27), b_time=1.20(1.28)  Time cost: 24:17/13:08 [7:53:31/14:33:48]  Acc_iter 22250       Data time: 0.01(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.28)
2025-09-04 21:42:37,797   INFO  Train:   13/36 ( 36%) [1191/1759 ( 68%)]  Loss: 3.952 (3.72)  LR: 2.906e-03  Grad: 5.1408  max=0.3190(module.vfe.pfn_layers.0.linear.weight)  min: -0.5258(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7131, loss_cls=0.1375, loss_bbox=0.8137, matched_ious=0.4978, loss_iou=0.0960, loss_iou_reg=0.2353, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.21(1.28)  Time cost: 25:22/12:05 [7:54:36/14:33:28]  Acc_iter 22300       Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.28)
2025-09-04 21:43:40,561   INFO  Train:   13/36 ( 36%) [1241/1759 ( 71%)]  Loss: 3.460 (3.71)  LR: 2.909e-03  Grad: 5.4055  max=0.6578(module.vfe.pfn_layers.0.linear.weight)  min: -0.5621(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6887, loss_cls=0.1370, loss_bbox=0.7822, matched_ious=0.5080, loss_iou=0.0959, loss_iou_reg=0.2311, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.28)  Time cost: 26:25/11:01 [7:55:39/14:31:48]  Acc_iter 22350       Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.28)
2025-09-04 21:44:43,671   INFO  Train:   13/36 ( 36%) [1291/1759 ( 73%)]  Loss: 3.074 (3.71)  LR: 2.912e-03  Grad: 5.5803  max=0.7144(module.vfe.pfn_layers.0.linear.weight)  min: -0.2881(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6972, loss_cls=0.1339, loss_bbox=0.8118, matched_ious=0.5070, loss_iou=0.0955, loss_iou_reg=0.2287, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.28)  Time cost: 27:28/09:57 [7:56:42/14:30:21]  Acc_iter 22400       Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.28)
2025-09-04 21:45:46,330   INFO  Train:   13/36 ( 36%) [1341/1759 ( 76%)]  Loss: 4.070 (3.72)  LR: 2.915e-03  Grad: 4.2168  max=0.5278(module.vfe.pfn_layers.0.linear.weight)  min: -0.6103(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7315, loss_cls=0.1396, loss_bbox=0.8791, matched_ious=0.4955, loss_iou=0.0975, loss_iou_reg=0.2347, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.28)  Time cost: 28:31/08:53 [7:57:45/14:28:42]  Acc_iter 22450       Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.28)
2025-09-04 21:46:49,128   INFO  Train:   13/36 ( 36%) [1391/1759 ( 79%)]  Loss: 4.667 (3.72)  LR: 2.918e-03  Grad: 4.6359  max=1.1322(module.vfe.pfn_layers.0.linear.weight)  min: -0.8556(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7120, loss_cls=0.1380, loss_bbox=0.8070, matched_ious=0.5019, loss_iou=0.0928, loss_iou_reg=0.2338, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.23(1.27)  Time cost: 29:34/07:49 [7:58:47/14:27:10]  Acc_iter 22500       Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.23(1.27)
2025-09-04 21:47:52,763   INFO  Train:   13/36 ( 36%) [1441/1759 ( 82%)]  Loss: 3.547 (3.72)  LR: 2.921e-03  Grad: 4.6436  max=0.5088(module.vfe.pfn_layers.0.linear.weight)  min: -0.5360(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6750, loss_cls=0.1342, loss_bbox=0.8025, matched_ious=0.5011, loss_iou=0.0962, loss_iou_reg=0.2341, d_time=0.01(0.01), f_time=1.29(1.27), b_time=1.30(1.27)  Time cost: 30:37/06:45 [7:59:51/14:26:04]  Acc_iter 22550       Data time: 0.01(0.01)  Forward time: 1.29(1.27)  Batch time: 1.30(1.27)
2025-09-04 21:48:56,190   INFO  Train:   13/36 ( 36%) [1491/1759 ( 85%)]  Loss: 4.014 (3.72)  LR: 2.923e-03  Grad: 4.5499  max=0.2986(module.vfe.pfn_layers.0.linear.weight)  min: -0.8377(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7211, loss_cls=0.1394, loss_bbox=0.7861, matched_ious=0.5020, loss_iou=0.0963, loss_iou_reg=0.2323, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.30(1.27)  Time cost: 31:41/05:41 [8:00:54/14:24:52]  Acc_iter 22600       Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.30(1.27)
2025-09-04 21:49:59,080   INFO  Train:   13/36 ( 36%) [1541/1759 ( 88%)]  Loss: 3.928 (3.71)  LR: 2.926e-03  Grad: 4.9754  max=0.9145(module.vfe.pfn_layers.0.linear.weight)  min: -1.1420(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6918, loss_cls=0.1314, loss_bbox=0.7656, matched_ious=0.5048, loss_iou=0.0962, loss_iou_reg=0.2313, d_time=0.01(0.01), f_time=1.29(1.27), b_time=1.30(1.27)  Time cost: 32:44/04:37 [8:01:57/14:23:27]  Acc_iter 22650       Data time: 0.01(0.01)  Forward time: 1.29(1.27)  Batch time: 1.30(1.27)
2025-09-04 21:51:04,606   INFO  Train:   13/36 ( 36%) [1591/1759 ( 90%)]  Loss: 3.762 (3.71)  LR: 2.929e-03  Grad: 5.1011  max=0.1638(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.5023(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7088, loss_cls=0.1399, loss_bbox=0.7933, matched_ious=0.5021, loss_iou=0.0943, loss_iou_reg=0.2335, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.24(1.28)  Time cost: 33:49/03:34 [8:03:03/14:23:10]  Acc_iter 22700       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.28)
2025-09-04 21:52:08,144   INFO  Train:   13/36 ( 36%) [1641/1759 ( 93%)]  Loss: 3.499 (3.71)  LR: 2.931e-03  Grad: 5.1327  max=0.6897(module.vfe.pfn_layers.0.linear.weight)  min: -0.9284(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6876, loss_cls=0.1320, loss_bbox=0.7914, matched_ious=0.5105, loss_iou=0.0938, loss_iou_reg=0.2275, d_time=0.00(0.01), f_time=1.38(1.27), b_time=1.38(1.27)  Time cost: 34:53/02:30 [8:04:06/14:22:02]  Acc_iter 22750       Data time: 0.00(0.01)  Forward time: 1.38(1.27)  Batch time: 1.38(1.27)
2025-09-04 21:53:12,213   INFO  Train:   13/36 ( 36%) [1691/1759 ( 96%)]  Loss: 3.772 (3.71)  LR: 2.934e-03  Grad: 5.0946  max=0.5509(module.vfe.pfn_layers.0.linear.weight)  min: -0.9968(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7137, loss_cls=0.1341, loss_bbox=0.8512, matched_ious=0.5091, loss_iou=0.0960, loss_iou_reg=0.2285, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.28)  Time cost: 35:57/01:26 [8:05:10/14:21:06]  Acc_iter 22800       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.28)
2025-09-04 21:54:15,159   INFO  Train:   13/36 ( 36%) [1741/1759 ( 99%)]  Loss: 3.230 (3.71)  LR: 2.937e-03  Grad: 5.3188  max=1.0498(module.vfe.pfn_layers.0.linear.weight)  min: -0.6017(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7158, loss_cls=0.1355, loss_bbox=0.8023, matched_ious=0.5087, loss_iou=0.0949, loss_iou_reg=0.2282, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.27(1.27)  Time cost: 37:00/00:22 [8:06:13/14:19:43]  Acc_iter 22850       Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.27)
2025-09-04 21:54:36,037   INFO  Train:   13/36 ( 36%) [1758/1759 (100%)]  Loss: 3.445 (3.71)  LR: 2.937e-03  Grad: 5.3166  max=0.4018(module.vfe.pfn_layers.0.linear.weight)  min: -0.7721(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7053, loss_cls=0.1399, loss_bbox=0.7107, matched_ious=0.5087, loss_iou=0.0918, loss_iou_reg=0.2322, d_time=0.00(0.01), f_time=0.80(1.27), b_time=0.80(1.27)  Time cost: 37:20/00:01 [8:06:34/14:19:03]  Acc_iter 22867       Data time: 0.00(0.01)  Forward time: 0.80(1.27)  Batch time: 0.80(1.27)

                                               [Aepochs:  36%|███▌      | 13/36 [8:06:35<14:19:17, 2241.64s/it]epochs:  36%|███▌      | 13/36 [8:06:35<14:19:18, 2241.68s/it]epochs:  36%|███▌      | 13/36 [8:06:35<14:19:18, 2241.69s/it]epochs:  36%|███▌      | 13/36 [8:06:35<14:19:18, 2241.67s/it]epochs:  36%|███▌      | 13/36 [8:06:35<14:19:18, 2241.67s/it]epochs:  36%|███▌      | 13/36 [8:06:35<14:19:18, 2241.67s/it]epochs:  36%|███▌      | 13/36 [8:06:35<14:19:18, 2241.68s/it]epochs:  36%|███▌      | 13/36 [8:06:35<14:19:18, 2241.68s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 21:54:41,484   INFO  Train:   14/36 ( 39%) [   0/1759 (  0%)]  Loss: 3.750 (3.75)  LR: 2.938e-03  Grad: 5.6575  max=0.6195(module.vfe.pfn_layers.0.linear.weight)  min: -1.9017(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7052, loss_cls=0.1418, loss_bbox=0.5742, matched_ious=0.4979, loss_iou=0.0961, loss_iou_reg=0.2360, d_time=1.33(1.33), f_time=2.75(2.75), b_time=4.08(4.08)  Time cost: 00:03/1:48:23 [8:06:40/41:32:55]  Acc_iter 22868       Data time: 1.33(1.33)  Forward time: 2.75(2.75)  Batch time: 4.08(4.08)
2025-09-04 21:55:22,483   INFO  Train:   14/36 ( 39%) [  32/1759 (  2%)]  Loss: 4.005 (3.58)  LR: 2.939e-03  Grad: 5.5808  max=0.2920(module.backbone_3d.cls_conv.3.bias)  min: -1.3823(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6789, loss_cls=0.1366, loss_bbox=0.7210, matched_ious=0.4970, loss_iou=0.0959, loss_iou_reg=0.2377, d_time=0.00(0.04), f_time=1.22(1.32), b_time=1.22(1.37)  Time cost: 00:44/38:59 [8:07:21/15:12:33]  Acc_iter 22900       Data time: 0.00(0.04)  Forward time: 1.22(1.32)  Batch time: 1.22(1.37)
2025-09-04 21:56:25,225   INFO  Train:   14/36 ( 39%) [  82/1759 (  5%)]  Loss: 3.514 (3.63)  LR: 2.942e-03  Grad: 5.7849  max=0.8440(module.vfe.pfn_layers.0.linear.weight)  min: -0.2158(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.6995, loss_cls=0.1354, loss_bbox=0.7811, matched_ious=0.5087, loss_iou=0.0967, loss_iou_reg=0.2270, d_time=0.00(0.02), f_time=1.17(1.28), b_time=1.17(1.30)  Time cost: 01:47/36:10 [8:08:23/14:31:02]  Acc_iter 22950       Data time: 0.00(0.02)  Forward time: 1.17(1.28)  Batch time: 1.17(1.30)
2025-09-04 21:57:28,286   INFO  Train:   14/36 ( 39%) [ 132/1759 (  8%)]  Loss: 3.643 (3.60)  LR: 2.944e-03  Grad: 5.9738  max=0.4829(module.vfe.pfn_layers.0.linear.weight)  min: -0.3824(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6653, loss_cls=0.1302, loss_bbox=0.7542, matched_ious=0.5090, loss_iou=0.0958, loss_iou_reg=0.2310, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.29(1.28)  Time cost: 02:50/34:45 [8:09:27/14:21:34]  Acc_iter 23000       Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.28)
2025-09-04 21:58:31,380   INFO  Train:   14/36 ( 39%) [ 182/1759 ( 10%)]  Loss: 3.693 (3.62)  LR: 2.946e-03  Grad: 6.0920  max=0.2260(module.vfe.pfn_layers.0.linear.weight)  min: -0.3985(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6991, loss_cls=0.1343, loss_bbox=0.7767, matched_ious=0.5084, loss_iou=0.0970, loss_iou_reg=0.2285, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.29(1.28)  Time cost: 03:53/33:32 [8:10:30/14:16:49]  Acc_iter 23050       Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.29(1.28)
2025-09-04 21:59:34,893   INFO  Train:   14/36 ( 39%) [ 232/1759 ( 13%)]  Loss: 3.347 (3.64)  LR: 2.949e-03  Grad: 6.4706  max=0.2202(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.1109(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6885, loss_cls=0.1304, loss_bbox=0.8065, matched_ious=0.5071, loss_iou=0.0954, loss_iou_reg=0.2271, d_time=0.00(0.01), f_time=1.35(1.27), b_time=1.35(1.28)  Time cost: 04:57/32:27 [8:11:33/14:14:52]  Acc_iter 23100       Data time: 0.00(0.01)  Forward time: 1.35(1.27)  Batch time: 1.35(1.28)
2025-09-04 22:00:38,355   INFO  Train:   14/36 ( 39%) [ 282/1759 ( 16%)]  Loss: 3.623 (3.64)  LR: 2.951e-03  Grad: 3.8113  max=0.8253(module.vfe.pfn_layers.0.linear.weight)  min: -0.1344(module.dense_head.prediction_head.dim.0.1.weight)  NaN: False  loss_hm=0.6973, loss_cls=0.1344, loss_bbox=0.8055, matched_ious=0.5079, loss_iou=0.0944, loss_iou_reg=0.2282, d_time=0.00(0.01), f_time=1.32(1.27), b_time=1.32(1.28)  Time cost: 06:00/31:21 [8:12:37/14:13:06]  Acc_iter 23150       Data time: 0.00(0.01)  Forward time: 1.32(1.27)  Batch time: 1.32(1.28)
2025-09-04 22:01:43,326   INFO  Train:   14/36 ( 39%) [ 332/1759 ( 19%)]  Loss: 3.378 (3.65)  LR: 2.953e-03  Grad: 3.9401  max=0.2698(module.vfe.pfn_layers.0.linear.weight)  min: -0.1398(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7000, loss_cls=0.1382, loss_bbox=0.8145, matched_ious=0.4975, loss_iou=0.0973, loss_iou_reg=0.2339, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.28)  Time cost: 07:05/30:23 [8:13:42/14:14:34]  Acc_iter 23200       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.28)
2025-09-04 22:02:47,533   INFO  Train:   14/36 ( 39%) [ 382/1759 ( 22%)]  Loss: 4.123 (3.65)  LR: 2.955e-03  Grad: 3.7588  max=0.8214(module.vfe.pfn_layers.0.linear.weight)  min: -0.2009(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.6830, loss_cls=0.1346, loss_bbox=0.7837, matched_ious=0.5130, loss_iou=0.0965, loss_iou_reg=0.2260, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.24(1.28)  Time cost: 08:09/29:20 [8:14:46/14:14:04]  Acc_iter 23250       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.28)
2025-09-04 22:03:50,297   INFO  Train:   14/36 ( 39%) [ 432/1759 ( 25%)]  Loss: 3.831 (3.66)  LR: 2.957e-03  Grad: 3.9886  max=0.4848(module.vfe.pfn_layers.0.linear.weight)  min: -0.7788(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6916, loss_cls=0.1325, loss_bbox=0.8076, matched_ious=0.4947, loss_iou=0.0954, loss_iou_reg=0.2396, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.28)  Time cost: 09:12/28:13 [8:15:49/14:11:12]  Acc_iter 23300       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.28)
2025-09-04 22:04:53,406   INFO  Train:   14/36 ( 39%) [ 482/1759 ( 27%)]  Loss: 2.868 (3.65)  LR: 2.959e-03  Grad: 4.1544  max=0.3527(module.vfe.pfn_layers.0.linear.weight)  min: -0.5151(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6670, loss_cls=0.1294, loss_bbox=0.7738, matched_ious=0.5050, loss_iou=0.0961, loss_iou_reg=0.2326, d_time=0.01(0.01), f_time=1.26(1.27), b_time=1.27(1.28)  Time cost: 10:15/27:07 [8:16:52/14:09:11]  Acc_iter 23350       Data time: 0.01(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.28)
2025-09-04 22:05:56,930   INFO  Train:   14/36 ( 39%) [ 532/1759 ( 30%)]  Loss: 2.905 (3.65)  LR: 2.962e-03  Grad: 2.6873  max=0.4469(module.vfe.pfn_layers.0.linear.weight)  min: -0.4465(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6842, loss_cls=0.1297, loss_bbox=0.8325, matched_ious=0.5031, loss_iou=0.0976, loss_iou_reg=0.2327, d_time=0.01(0.01), f_time=1.39(1.27), b_time=1.40(1.27)  Time cost: 11:19/26:03 [8:17:55/14:07:52]  Acc_iter 23400       Data time: 0.01(0.01)  Forward time: 1.39(1.27)  Batch time: 1.40(1.27)
2025-09-04 22:07:00,718   INFO  Train:   14/36 ( 39%) [ 582/1759 ( 33%)]  Loss: 3.898 (3.66)  LR: 2.963e-03  Grad: 3.0113  max=0.4533(module.vfe.pfn_layers.0.linear.weight)  min: -0.4866(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7123, loss_cls=0.1380, loss_bbox=0.8367, matched_ious=0.4978, loss_iou=0.0942, loss_iou_reg=0.2348, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.26(1.27)  Time cost: 12:22/24:59 [8:18:59/14:06:53]  Acc_iter 23450       Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.27)
2025-09-04 22:08:04,413   INFO  Train:   14/36 ( 39%) [ 632/1759 ( 36%)]  Loss: 4.168 (3.66)  LR: 2.965e-03  Grad: 3.2067  max=0.5921(module.vfe.pfn_layers.0.linear.weight)  min: -0.5044(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6836, loss_cls=0.1362, loss_bbox=0.7631, matched_ious=0.5048, loss_iou=0.0948, loss_iou_reg=0.2329, d_time=0.00(0.01), f_time=1.42(1.27), b_time=1.42(1.27)  Time cost: 13:26/23:56 [8:20:03/14:05:48]  Acc_iter 23500       Data time: 0.00(0.01)  Forward time: 1.42(1.27)  Batch time: 1.42(1.27)
2025-09-04 22:09:08,348   INFO  Train:   14/36 ( 39%) [ 682/1759 ( 39%)]  Loss: 3.282 (3.66)  LR: 2.967e-03  Grad: 3.2438  max=0.4597(module.vfe.pfn_layers.0.linear.weight)  min: -0.2887(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6831, loss_cls=0.1354, loss_bbox=0.7907, matched_ious=0.5089, loss_iou=0.0956, loss_iou_reg=0.2299, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.31(1.28)  Time cost: 14:30/22:52 [8:21:07/14:04:57]  Acc_iter 23550       Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.31(1.28)
2025-09-04 22:10:12,142   INFO  Train:   14/36 ( 39%) [ 732/1759 ( 42%)]  Loss: 3.645 (3.66)  LR: 2.969e-03  Grad: 3.2650  max=0.2920(module.vfe.pfn_layers.0.linear.weight)  min: -0.8070(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6782, loss_cls=0.1325, loss_bbox=0.7686, matched_ious=0.4935, loss_iou=0.0963, loss_iou_reg=0.2376, d_time=0.01(0.01), f_time=1.27(1.27), b_time=1.27(1.28)  Time cost: 15:34/21:49 [8:22:10/14:03:57]  Acc_iter 23600       Data time: 0.01(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.28)
2025-09-04 22:11:15,764   INFO  Train:   14/36 ( 39%) [ 782/1759 ( 44%)]  Loss: 3.810 (3.66)  LR: 2.971e-03  Grad: 3.4311  max=0.4477(module.vfe.pfn_layers.0.linear.weight)  min: -0.2778(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6954, loss_cls=0.1351, loss_bbox=0.7926, matched_ious=0.5078, loss_iou=0.0934, loss_iou_reg=0.2302, d_time=0.01(0.01), f_time=1.22(1.27), b_time=1.23(1.28)  Time cost: 16:37/20:45 [8:23:14/14:02:48]  Acc_iter 23650       Data time: 0.01(0.01)  Forward time: 1.22(1.27)  Batch time: 1.23(1.28)
2025-09-04 22:12:22,502   INFO  Train:   14/36 ( 39%) [ 832/1759 ( 47%)]  Loss: 3.737 (3.66)  LR: 2.973e-03  Grad: 4.0240  max=1.0811(module.vfe.pfn_layers.0.linear.weight)  min: -0.5970(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6793, loss_cls=0.1309, loss_bbox=0.7947, matched_ious=0.5103, loss_iou=0.0976, loss_iou_reg=0.2279, d_time=0.01(0.01), f_time=1.41(1.27), b_time=1.42(1.28)  Time cost: 17:44/19:44 [8:24:21/14:04:07]  Acc_iter 23700       Data time: 0.01(0.01)  Forward time: 1.41(1.27)  Batch time: 1.42(1.28)
2025-09-04 22:13:25,403   INFO  Train:   14/36 ( 39%) [ 882/1759 ( 50%)]  Loss: 3.107 (3.66)  LR: 2.974e-03  Grad: 3.6931  max=0.8485(module.vfe.pfn_layers.0.linear.weight)  min: -0.7973(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7188, loss_cls=0.1355, loss_bbox=0.8291, matched_ious=0.5048, loss_iou=0.0945, loss_iou_reg=0.2304, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.27(1.28)  Time cost: 18:47/18:39 [8:25:24/14:02:18]  Acc_iter 23750       Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.28)
2025-09-04 22:14:28,832   INFO  Train:   14/36 ( 39%) [ 932/1759 ( 53%)]  Loss: 3.437 (3.67)  LR: 2.976e-03  Grad: 4.1889  max=0.9892(module.vfe.pfn_layers.0.linear.weight)  min: -0.4834(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7075, loss_cls=0.1363, loss_bbox=0.8212, matched_ious=0.5057, loss_iou=0.0954, loss_iou_reg=0.2303, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.31(1.28)  Time cost: 19:51/17:35 [8:26:27/14:00:56]  Acc_iter 23800       Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.31(1.28)
2025-09-04 22:15:32,370   INFO  Train:   14/36 ( 39%) [ 982/1759 ( 56%)]  Loss: 3.343 (3.66)  LR: 2.977e-03  Grad: 3.8571  max=0.4432(module.vfe.pfn_layers.0.linear.weight)  min: -0.1425(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6935, loss_cls=0.1361, loss_bbox=0.7733, matched_ious=0.5112, loss_iou=0.0934, loss_iou_reg=0.2254, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.28)  Time cost: 20:54/16:31 [8:27:31/13:59:41]  Acc_iter 23850       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.28)
2025-09-04 22:16:35,783   INFO  Train:   14/36 ( 39%) [1032/1759 ( 59%)]  Loss: 4.051 (3.67)  LR: 2.979e-03  Grad: 4.1153  max=0.3304(module.vfe.pfn_layers.0.linear.weight)  min: -0.5082(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7193, loss_cls=0.1412, loss_bbox=0.8395, matched_ious=0.5039, loss_iou=0.0950, loss_iou_reg=0.2305, d_time=0.01(0.01), f_time=1.32(1.27), b_time=1.33(1.28)  Time cost: 21:57/15:27 [8:28:34/13:58:22]  Acc_iter 23900       Data time: 0.01(0.01)  Forward time: 1.32(1.27)  Batch time: 1.33(1.28)
2025-09-04 22:17:39,492   INFO  Train:   14/36 ( 39%) [1082/1759 ( 62%)]  Loss: 3.400 (3.67)  LR: 2.980e-03  Grad: 4.4571  max=0.5452(module.vfe.pfn_layers.0.linear.weight)  min: -0.5484(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7068, loss_cls=0.1356, loss_bbox=0.7976, matched_ious=0.5113, loss_iou=0.0943, loss_iou_reg=0.2279, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.29(1.28)  Time cost: 23:01/14:23 [8:29:38/13:57:15]  Acc_iter 23950       Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.28)
2025-09-04 22:18:43,320   INFO  Train:   14/36 ( 39%) [1132/1759 ( 64%)]  Loss: 3.371 (3.67)  LR: 2.982e-03  Grad: 3.6269  max=0.7084(module.vfe.pfn_layers.0.linear.weight)  min: -0.9250(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6883, loss_cls=0.1317, loss_bbox=0.7960, matched_ious=0.5099, loss_iou=0.0975, loss_iou_reg=0.2291, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.28)  Time cost: 24:05/13:19 [8:30:42/13:56:12]  Acc_iter 24000       Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.28)
2025-09-04 22:19:46,693   INFO  Train:   14/36 ( 39%) [1182/1759 ( 67%)]  Loss: 3.141 (3.67)  LR: 2.983e-03  Grad: 3.6411  max=0.1968(module.dense_head.prediction_head.height.1.bias)  min: -0.4840(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6811, loss_cls=0.1371, loss_bbox=0.8080, matched_ious=0.4986, loss_iou=0.0959, loss_iou_reg=0.2347, d_time=0.00(0.01), f_time=1.18(1.27), b_time=1.18(1.28)  Time cost: 25:08/12:15 [8:31:45/13:54:54]  Acc_iter 24050       Data time: 0.00(0.01)  Forward time: 1.18(1.27)  Batch time: 1.18(1.28)
2025-09-04 22:20:48,984   INFO  Train:   14/36 ( 39%) [1232/1759 ( 70%)]  Loss: 3.520 (3.67)  LR: 2.984e-03  Grad: 3.9584  max=0.6046(module.vfe.pfn_layers.0.linear.weight)  min: -0.1409(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7112, loss_cls=0.1346, loss_bbox=0.8132, matched_ious=0.5043, loss_iou=0.0972, loss_iou_reg=0.2306, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.28(1.27)  Time cost: 26:11/11:11 [8:32:47/13:53:03]  Acc_iter 24100       Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.28(1.27)
2025-09-04 22:21:52,752   INFO  Train:   14/36 ( 39%) [1282/1759 ( 73%)]  Loss: 2.923 (3.68)  LR: 2.986e-03  Grad: 3.8963  max=1.7334(module.vfe.pfn_layers.0.linear.weight)  min: -0.1330(module.dense_head.prediction_head.dim.1.bias)  NaN: False  loss_hm=0.6890, loss_cls=0.1313, loss_bbox=0.7967, matched_ious=0.5046, loss_iou=0.0947, loss_iou_reg=0.2317, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.30(1.27)  Time cost: 27:14/10:07 [8:33:51/13:52:01]  Acc_iter 24150       Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.30(1.27)
2025-09-04 22:22:56,946   INFO  Train:   14/36 ( 39%) [1332/1759 ( 76%)]  Loss: 3.382 (3.68)  LR: 2.987e-03  Grad: 3.8809  max=0.3329(module.vfe.pfn_layers.0.linear.weight)  min: -0.7619(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7188, loss_cls=0.1376, loss_bbox=0.8172, matched_ious=0.5014, loss_iou=0.0953, loss_iou_reg=0.2328, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.27(1.27)  Time cost: 28:19/09:04 [8:34:55/13:51:12]  Acc_iter 24200       Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.27)
2025-09-04 22:23:59,699   INFO  Train:   14/36 ( 39%) [1382/1759 ( 79%)]  Loss: 4.175 (3.68)  LR: 2.988e-03  Grad: 3.9991  max=0.2176(module.dense_head.prediction_head.height.1.bias)  min: -0.2714(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7121, loss_cls=0.1361, loss_bbox=0.8131, matched_ious=0.5064, loss_iou=0.0933, loss_iou_reg=0.2289, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.27)  Time cost: 29:21/08:00 [8:35:58/13:49:40]  Acc_iter 24250       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.27)
2025-09-04 22:25:03,574   INFO  Train:   14/36 ( 39%) [1432/1759 ( 81%)]  Loss: 3.664 (3.69)  LR: 2.989e-03  Grad: 2.9370  max=0.3232(module.vfe.pfn_layers.0.linear.weight)  min: -0.9616(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7251, loss_cls=0.1412, loss_bbox=0.8416, matched_ious=0.5021, loss_iou=0.0970, loss_iou_reg=0.2329, d_time=0.01(0.01), f_time=1.31(1.27), b_time=1.32(1.27)  Time cost: 30:25/06:56 [8:37:02/13:48:41]  Acc_iter 24300       Data time: 0.01(0.01)  Forward time: 1.31(1.27)  Batch time: 1.32(1.27)
2025-09-04 22:26:06,682   INFO  Train:   14/36 ( 39%) [1482/1759 ( 84%)]  Loss: 4.024 (3.69)  LR: 2.990e-03  Grad: 1.5760  max=0.3426(module.vfe.pfn_layers.0.linear.weight)  min: -0.5471(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7085, loss_cls=0.1343, loss_bbox=0.8519, matched_ious=0.4955, loss_iou=0.0959, loss_iou_reg=0.2345, d_time=0.01(0.01), f_time=1.28(1.27), b_time=1.28(1.27)  Time cost: 31:28/05:52 [8:38:05/13:47:22]  Acc_iter 24350       Data time: 0.01(0.01)  Forward time: 1.28(1.27)  Batch time: 1.28(1.27)
2025-09-04 22:27:09,566   INFO  Train:   14/36 ( 39%) [1532/1759 ( 87%)]  Loss: 3.874 (3.69)  LR: 2.991e-03  Grad: 1.8633  max=0.5202(module.vfe.pfn_layers.0.linear.weight)  min: -0.2097(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.6982, loss_cls=0.1337, loss_bbox=0.7935, matched_ious=0.5124, loss_iou=0.0932, loss_iou_reg=0.2274, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.29(1.27)  Time cost: 32:31/04:49 [8:39:08/13:45:58]  Acc_iter 24400       Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.27)
2025-09-04 22:28:13,029   INFO  Train:   14/36 ( 39%) [1582/1759 ( 90%)]  Loss: 3.904 (3.69)  LR: 2.992e-03  Grad: 2.0680  max=0.3512(module.vfe.pfn_layers.0.linear.weight)  min: -0.3239(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6807, loss_cls=0.1320, loss_bbox=0.7714, matched_ious=0.5088, loss_iou=0.0937, loss_iou_reg=0.2287, d_time=0.03(0.01), f_time=1.32(1.27), b_time=1.34(1.27)  Time cost: 33:35/03:45 [8:40:11/13:44:49]  Acc_iter 24450       Data time: 0.03(0.01)  Forward time: 1.32(1.27)  Batch time: 1.34(1.27)
2025-09-04 22:29:16,707   INFO  Train:   14/36 ( 39%) [1632/1759 ( 93%)]  Loss: 2.978 (3.69)  LR: 2.993e-03  Grad: 2.4669  max=0.8381(module.vfe.pfn_layers.0.linear.weight)  min: -0.1620(module.backbone_3d.stage.embeddings.2.stem.0.bn1.bias)  NaN: False  loss_hm=0.7042, loss_cls=0.1347, loss_bbox=0.7911, matched_ious=0.5121, loss_iou=0.0942, loss_iou_reg=0.2268, d_time=0.00(0.01), f_time=1.39(1.27), b_time=1.39(1.27)  Time cost: 34:38/02:41 [8:41:15/13:43:46]  Acc_iter 24500       Data time: 0.00(0.01)  Forward time: 1.39(1.27)  Batch time: 1.39(1.27)
2025-09-04 22:30:20,066   INFO  Train:   14/36 ( 39%) [1682/1759 ( 96%)]  Loss: 4.246 (3.69)  LR: 2.994e-03  Grad: 2.9730  max=0.2013(module.vfe.pfn_layers.0.linear.weight)  min: -1.4432(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7055, loss_cls=0.1341, loss_bbox=0.7897, matched_ious=0.5047, loss_iou=0.0941, loss_iou_reg=0.2307, d_time=0.01(0.01), f_time=1.20(1.27), b_time=1.21(1.27)  Time cost: 35:42/01:38 [8:42:18/13:42:36]  Acc_iter 24550       Data time: 0.01(0.01)  Forward time: 1.20(1.27)  Batch time: 1.21(1.27)
2025-09-04 22:31:23,405   INFO  Train:   14/36 ( 39%) [1732/1759 ( 98%)]  Loss: 4.186 (3.69)  LR: 2.994e-03  Grad: 2.6136  max=0.1855(module.vfe.pfn_layers.0.linear.weight)  min: -0.3447(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6833, loss_cls=0.1341, loss_bbox=0.8148, matched_ious=0.5013, loss_iou=0.0935, loss_iou_reg=0.2334, d_time=0.01(0.01), f_time=1.31(1.27), b_time=1.32(1.27)  Time cost: 36:45/00:34 [8:43:22/13:41:25]  Acc_iter 24600       Data time: 0.01(0.01)  Forward time: 1.31(1.27)  Batch time: 1.32(1.27)
2025-09-04 22:31:56,289   INFO  Train:   14/36 ( 39%) [1758/1759 (100%)]  Loss: 3.364 (3.69)  LR: 2.995e-03  Grad: 3.2808  max=1.1072(module.vfe.pfn_layers.0.linear.weight)  min: -1.1356(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6884, loss_cls=0.1301, loss_bbox=0.8318, matched_ious=0.5021, loss_iou=0.0926, loss_iou_reg=0.2344, d_time=0.00(0.01), f_time=0.77(1.27), b_time=0.77(1.27)  Time cost: 37:18/00:01 [8:43:55/13:40:48]  Acc_iter 24626       Data time: 0.00(0.01)  Forward time: 0.77(1.27)  Batch time: 0.77(1.27)

                                               [Aepochs:  39%|███▉      | 14/36 [8:43:55<13:41:46, 2241.22s/it]epochs:  39%|███▉      | 14/36 [8:43:55<13:41:46, 2241.22s/it]epochs:  39%|███▉      | 14/36 [8:43:55<13:41:47, 2241.26s/it]epochs:  39%|███▉      | 14/36 [8:43:55<13:41:47, 2241.23s/it]epochs:  39%|███▉      | 14/36 [8:43:55<13:41:47, 2241.23s/it]epochs:  39%|███▉      | 14/36 [8:43:55<13:41:47, 2241.23s/it]epochs:  39%|███▉      | 14/36 [8:43:55<13:41:47, 2241.23s/it]epochs:  39%|███▉      | 14/36 [8:43:55<13:41:47, 2241.24s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 22:32:01,750   INFO  Train:   15/36 ( 42%) [   0/1759 (  0%)]  Loss: 4.127 (4.13)  LR: 2.995e-03  Grad: 2.9031  max=0.5217(module.vfe.pfn_layers.0.linear.weight)  min: -0.6078(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7278, loss_cls=0.1261, loss_bbox=0.9997, matched_ious=0.4461, loss_iou=0.1060, loss_iou_reg=0.2664, d_time=1.32(1.32), f_time=2.81(2.81), b_time=4.13(4.13)  Time cost: 00:03/1:47:29 [8:44:00/39:24:58]  Acc_iter 24627       Data time: 1.32(1.32)  Forward time: 2.81(2.81)  Batch time: 4.13(4.13)
2025-09-04 22:32:30,840   INFO  Train:   15/36 ( 42%) [  23/1759 (  1%)]  Loss: 3.890 (3.70)  LR: 2.995e-03  Grad: 3.0333  max=1.0298(module.vfe.pfn_layers.0.linear.weight)  min: -0.1016(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6870, loss_cls=0.1353, loss_bbox=0.8049, matched_ious=0.5136, loss_iou=0.0928, loss_iou_reg=0.2245, d_time=0.00(0.06), f_time=1.37(1.32), b_time=1.38(1.38)  Time cost: 00:32/39:29 [8:44:29/14:39:39]  Acc_iter 24650       Data time: 0.00(0.06)  Forward time: 1.37(1.32)  Batch time: 1.38(1.38)
2025-09-04 22:33:36,021   INFO  Train:   15/36 ( 42%) [  73/1759 (  4%)]  Loss: 2.962 (3.61)  LR: 2.996e-03  Grad: 3.1136  max=0.6653(module.vfe.pfn_layers.0.linear.weight)  min: -0.3331(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6822, loss_cls=0.1317, loss_bbox=0.7406, matched_ious=0.5110, loss_iou=0.0928, loss_iou_reg=0.2293, d_time=0.00(0.03), f_time=1.38(1.30), b_time=1.39(1.33)  Time cost: 01:37/37:11 [8:45:34/14:11:58]  Acc_iter 24700       Data time: 0.00(0.03)  Forward time: 1.38(1.30)  Batch time: 1.39(1.33)
2025-09-04 22:34:39,390   INFO  Train:   15/36 ( 42%) [ 123/1759 (  7%)]  Loss: 3.807 (3.65)  LR: 2.997e-03  Grad: 3.4374  max=0.1221(module.dense_head.prediction_head.dim.1.bias)  min: -0.9802(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7108, loss_cls=0.1354, loss_bbox=0.8393, matched_ious=0.5071, loss_iou=0.0958, loss_iou_reg=0.2292, d_time=0.00(0.02), f_time=1.24(1.28), b_time=1.25(1.30)  Time cost: 02:41/35:28 [8:46:38/13:56:20]  Acc_iter 24750       Data time: 0.00(0.02)  Forward time: 1.24(1.28)  Batch time: 1.25(1.30)
2025-09-04 22:35:42,794   INFO  Train:   15/36 ( 42%) [ 173/1759 ( 10%)]  Loss: 3.220 (3.67)  LR: 2.997e-03  Grad: 3.5058  max=0.1934(module.vfe.pfn_layers.0.linear.weight)  min: -0.1488(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7049, loss_cls=0.1371, loss_bbox=0.8195, matched_ious=0.5031, loss_iou=0.0960, loss_iou_reg=0.2306, d_time=0.00(0.02), f_time=1.24(1.28), b_time=1.25(1.29)  Time cost: 03:44/34:08 [8:47:41/13:49:12]  Acc_iter 24800       Data time: 0.00(0.02)  Forward time: 1.24(1.28)  Batch time: 1.25(1.29)
2025-09-04 22:36:46,056   INFO  Train:   15/36 ( 42%) [ 223/1759 ( 13%)]  Loss: 3.743 (3.65)  LR: 2.998e-03  Grad: 3.0869  max=0.3535(module.dense_head.prediction_head.height.1.bias)  min: -0.6795(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6814, loss_cls=0.1322, loss_bbox=0.7512, matched_ious=0.5112, loss_iou=0.0945, loss_iou_reg=0.2292, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.30(1.29)  Time cost: 04:47/32:54 [8:48:44/13:44:23]  Acc_iter 24850       Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.30(1.29)
2025-09-04 22:37:49,471   INFO  Train:   15/36 ( 42%) [ 273/1759 ( 16%)]  Loss: 3.411 (3.65)  LR: 2.998e-03  Grad: 3.0485  max=0.6076(module.vfe.pfn_layers.0.linear.weight)  min: -0.2081(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6723, loss_cls=0.1298, loss_bbox=0.8172, matched_ious=0.5075, loss_iou=0.0959, loss_iou_reg=0.2305, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.22(1.28)  Time cost: 05:51/31:45 [8:49:48/13:41:17]  Acc_iter 24900       Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.22(1.28)
2025-09-04 22:38:52,726   INFO  Train:   15/36 ( 42%) [ 323/1759 ( 18%)]  Loss: 3.596 (3.66)  LR: 2.999e-03  Grad: 3.2397  max=0.2131(module.vfe.pfn_layers.0.linear.weight)  min: -0.2676(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6934, loss_cls=0.1319, loss_bbox=0.7997, matched_ious=0.5045, loss_iou=0.0944, loss_iou_reg=0.2320, d_time=0.01(0.01), f_time=1.19(1.27), b_time=1.19(1.28)  Time cost: 06:54/30:37 [8:50:51/13:38:30]  Acc_iter 24950       Data time: 0.01(0.01)  Forward time: 1.19(1.27)  Batch time: 1.19(1.28)
2025-09-04 22:39:55,282   INFO  Train:   15/36 ( 42%) [ 373/1759 ( 21%)]  Loss: 3.446 (3.66)  LR: 2.999e-03  Grad: 3.5983  max=0.2419(module.vfe.pfn_layers.0.linear.weight)  min: -0.3084(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6866, loss_cls=0.1329, loss_bbox=0.8219, matched_ious=0.5080, loss_iou=0.0955, loss_iou_reg=0.2284, d_time=0.00(0.01), f_time=1.32(1.27), b_time=1.32(1.28)  Time cost: 07:57/29:28 [8:51:54/13:35:00]  Acc_iter 25000       Data time: 0.00(0.01)  Forward time: 1.32(1.27)  Batch time: 1.32(1.28)
2025-09-04 22:40:58,401   INFO  Train:   15/36 ( 42%) [ 423/1759 ( 24%)]  Loss: 3.067 (3.66)  LR: 2.999e-03  Grad: 3.7404  max=0.6582(module.vfe.pfn_layers.0.linear.weight)  min: -0.3346(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6859, loss_cls=0.1304, loss_bbox=0.7956, matched_ious=0.5056, loss_iou=0.0954, loss_iou_reg=0.2294, d_time=0.00(0.01), f_time=1.20(1.27), b_time=1.20(1.28)  Time cost: 09:00/28:22 [8:52:57/13:32:55]  Acc_iter 25050       Data time: 0.00(0.01)  Forward time: 1.20(1.27)  Batch time: 1.20(1.28)
2025-09-04 22:42:03,203   INFO  Train:   15/36 ( 42%) [ 473/1759 ( 27%)]  Loss: 3.028 (3.66)  LR: 2.999e-03  Grad: 4.0588  max=0.7991(module.vfe.pfn_layers.0.linear.weight)  min: -0.2371(module.backbone_3d.stage.embeddings.2.stem.0.bn1.bias)  NaN: False  loss_hm=0.6909, loss_cls=0.1343, loss_bbox=0.7859, matched_ious=0.5178, loss_iou=0.0927, loss_iou_reg=0.2244, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.24(1.28)  Time cost: 10:05/27:21 [8:54:01/13:33:18]  Acc_iter 25100       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.28)
2025-09-04 22:43:06,537   INFO  Train:   15/36 ( 42%) [ 523/1759 ( 30%)]  Loss: 4.121 (3.66)  LR: 3.000e-03  Grad: 4.2186  max=0.7566(module.vfe.pfn_layers.0.linear.weight)  min: -0.3165(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6850, loss_cls=0.1283, loss_bbox=0.8099, matched_ious=0.5090, loss_iou=0.0934, loss_iou_reg=0.2271, d_time=0.01(0.01), f_time=1.29(1.27), b_time=1.30(1.28)  Time cost: 11:08/26:16 [8:55:05/13:31:38]  Acc_iter 25150       Data time: 0.01(0.01)  Forward time: 1.29(1.27)  Batch time: 1.30(1.28)
2025-09-04 22:44:11,236   INFO  Train:   15/36 ( 42%) [ 573/1759 ( 33%)]  Loss: 3.645 (3.65)  LR: 3.000e-03  Grad: 9.5816  max=4.8005(module.vfe.pfn_layers.0.linear.weight)  min: -6.6175(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6720, loss_cls=0.1307, loss_bbox=0.7233, matched_ious=0.5207, loss_iou=0.0940, loss_iou_reg=0.2249, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.27(1.28)  Time cost: 12:13/25:14 [8:56:10/13:31:35]  Acc_iter 25200       Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.28)
2025-09-04 22:45:14,854   INFO  Train:   15/36 ( 42%) [ 623/1759 ( 35%)]  Loss: 3.808 (3.64)  LR: 3.000e-03  Grad: 3.3019  max=0.2523(module.dense_head.prediction_head.height.1.bias)  min: -0.4514(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6908, loss_cls=0.1336, loss_bbox=0.7911, matched_ious=0.5160, loss_iou=0.0963, loss_iou_reg=0.2274, d_time=0.01(0.01), f_time=1.20(1.27), b_time=1.21(1.28)  Time cost: 13:16/24:10 [8:57:13/13:30:16]  Acc_iter 25250       Data time: 0.01(0.01)  Forward time: 1.20(1.27)  Batch time: 1.21(1.28)
2025-09-04 22:46:18,195   INFO  Train:   15/36 ( 42%) [ 673/1759 ( 38%)]  Loss: 3.009 (3.63)  LR: 3.000e-03  Grad: 3.5036  max=0.5280(module.vfe.pfn_layers.0.linear.weight)  min: -0.2373(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6595, loss_cls=0.1287, loss_bbox=0.7536, matched_ious=0.5146, loss_iou=0.0946, loss_iou_reg=0.2288, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.28)  Time cost: 14:20/23:05 [8:58:16/13:28:44]  Acc_iter 25300       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.28)
2025-09-04 22:47:21,833   INFO  Train:   15/36 ( 42%) [ 723/1759 ( 41%)]  Loss: 4.216 (3.64)  LR: 3.000e-03  Grad: 3.0695  max=0.3631(module.vfe.pfn_layers.0.linear.weight)  min: -0.9077(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7000, loss_cls=0.1334, loss_bbox=0.7516, matched_ious=0.5066, loss_iou=0.0945, loss_iou_reg=0.2326, d_time=0.00(0.01), f_time=1.35(1.27), b_time=1.35(1.28)  Time cost: 15:23/22:01 [8:59:20/13:27:32]  Acc_iter 25350       Data time: 0.00(0.01)  Forward time: 1.35(1.27)  Batch time: 1.35(1.28)
2025-09-04 22:48:25,014   INFO  Train:   15/36 ( 42%) [ 773/1759 ( 44%)]  Loss: 3.202 (3.64)  LR: 3.000e-03  Grad: 3.1344  max=0.1469(module.vfe.pfn_layers.0.linear.weight)  min: -0.1541(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6848, loss_cls=0.1320, loss_bbox=0.7485, matched_ious=0.5094, loss_iou=0.0960, loss_iou_reg=0.2302, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.30(1.28)  Time cost: 16:26/20:57 [9:00:23/13:25:58]  Acc_iter 25400       Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.30(1.28)
2025-09-04 22:49:27,684   INFO  Train:   15/36 ( 42%) [ 823/1759 ( 47%)]  Loss: 3.615 (3.63)  LR: 3.000e-03  Grad: 3.4915  max=0.8029(module.vfe.pfn_layers.0.linear.weight)  min: -0.1243(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6697, loss_cls=0.1276, loss_bbox=0.7574, matched_ious=0.5120, loss_iou=0.0936, loss_iou_reg=0.2274, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.27)  Time cost: 17:29/19:52 [9:01:26/13:24:04]  Acc_iter 25450       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.27)
2025-09-04 22:50:30,348   INFO  Train:   15/36 ( 42%) [ 873/1759 ( 50%)]  Loss: 3.523 (3.63)  LR: 3.000e-03  Grad: 3.5992  max=0.3258(module.vfe.pfn_layers.0.linear.weight)  min: -0.2432(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6862, loss_cls=0.1312, loss_bbox=0.8129, matched_ious=0.5091, loss_iou=0.0969, loss_iou_reg=0.2295, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.23(1.27)  Time cost: 18:32/18:47 [9:02:29/13:22:16]  Acc_iter 25500       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.27)
2025-09-04 22:51:34,481   INFO  Train:   15/36 ( 42%) [ 923/1759 ( 52%)]  Loss: 3.269 (3.63)  LR: 3.000e-03  Grad: 3.1136  max=0.4089(module.vfe.pfn_layers.0.linear.weight)  min: -0.1529(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.6722, loss_cls=0.1292, loss_bbox=0.7680, matched_ious=0.5130, loss_iou=0.0920, loss_iou_reg=0.2266, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.26(1.27)  Time cost: 19:36/17:44 [9:03:33/13:21:32]  Acc_iter 25550       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.27)
2025-09-04 22:52:37,664   INFO  Train:   15/36 ( 42%) [ 973/1759 ( 55%)]  Loss: 3.394 (3.63)  LR: 3.000e-03  Grad: 3.4377  max=0.1376(module.backbone_3d.cls_conv.3.bias)  min: -0.5838(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6746, loss_cls=0.1285, loss_bbox=0.7519, matched_ious=0.5149, loss_iou=0.0948, loss_iou_reg=0.2278, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.27)  Time cost: 20:39/16:40 [9:04:36/13:20:11]  Acc_iter 25600       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.27)
2025-09-04 22:53:40,709   INFO  Train:   15/36 ( 42%) [1023/1759 ( 58%)]  Loss: 4.030 (3.63)  LR: 2.999e-03  Grad: 2.8457  max=0.2299(module.vfe.pfn_layers.0.linear.weight)  min: -1.3421(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7110, loss_cls=0.1346, loss_bbox=0.7975, matched_ious=0.5136, loss_iou=0.0932, loss_iou_reg=0.2255, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.27(1.27)  Time cost: 21:42/15:36 [9:05:39/13:18:46]  Acc_iter 25650       Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.27(1.27)
2025-09-04 22:54:46,206   INFO  Train:   15/36 ( 42%) [1073/1759 ( 61%)]  Loss: 3.636 (3.63)  LR: 2.999e-03  Grad: 2.2364  max=0.1751(module.backbone_3d.cls_conv.3.bias)  min: -0.6308(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6989, loss_cls=0.1327, loss_bbox=0.8056, matched_ious=0.5106, loss_iou=0.0972, loss_iou_reg=0.2277, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.27(1.27)  Time cost: 22:48/14:33 [9:06:44/13:18:48]  Acc_iter 25700       Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.27)
2025-09-04 22:55:48,858   INFO  Train:   15/36 ( 42%) [1123/1759 ( 64%)]  Loss: 3.952 (3.63)  LR: 2.999e-03  Grad: 2.3555  max=0.7472(module.vfe.pfn_layers.0.linear.weight)  min: -0.1745(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6767, loss_cls=0.1299, loss_bbox=0.7624, matched_ious=0.5109, loss_iou=0.0934, loss_iou_reg=0.2297, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.24(1.27)  Time cost: 23:50/13:29 [9:07:47/13:17:10]  Acc_iter 25750       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.27)
2025-09-04 22:56:52,082   INFO  Train:   15/36 ( 42%) [1173/1759 ( 67%)]  Loss: 3.889 (3.63)  LR: 2.999e-03  Grad: 2.4897  max=0.1694(module.vfe.pfn_layers.0.linear.weight)  min: -0.4503(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6759, loss_cls=0.1282, loss_bbox=0.7540, matched_ious=0.5136, loss_iou=0.0945, loss_iou_reg=0.2282, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.26(1.27)  Time cost: 24:53/12:25 [9:08:50/13:15:53]  Acc_iter 25800       Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.27)
2025-09-04 22:57:55,896   INFO  Train:   15/36 ( 42%) [1223/1759 ( 70%)]  Loss: 3.383 (3.63)  LR: 2.999e-03  Grad: 3.0366  max=1.0355(module.vfe.pfn_layers.0.linear.weight)  min: -0.2537(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.6776, loss_cls=0.1302, loss_bbox=0.7446, matched_ious=0.5130, loss_iou=0.0938, loss_iou_reg=0.2292, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.27)  Time cost: 25:57/11:22 [9:09:54/13:14:55]  Acc_iter 25850       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.27)
2025-09-04 22:58:59,047   INFO  Train:   15/36 ( 42%) [1273/1759 ( 72%)]  Loss: 3.977 (3.64)  LR: 2.998e-03  Grad: 3.3131  max=0.6010(module.vfe.pfn_layers.0.linear.weight)  min: -0.3197(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6946, loss_cls=0.1341, loss_bbox=0.7979, matched_ious=0.5039, loss_iou=0.0947, loss_iou_reg=0.2332, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.27)  Time cost: 27:00/10:18 [9:10:57/13:13:37]  Acc_iter 25900       Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.27)
2025-09-04 23:00:02,627   INFO  Train:   15/36 ( 42%) [1323/1759 ( 75%)]  Loss: 3.368 (3.64)  LR: 2.998e-03  Grad: 5.6546  max=2.5293(module.vfe.pfn_layers.0.linear.weight)  min: -3.8806(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6758, loss_cls=0.1297, loss_bbox=0.8023, matched_ious=0.5184, loss_iou=0.0931, loss_iou_reg=0.2272, d_time=0.00(0.01), f_time=1.07(1.27), b_time=1.08(1.27)  Time cost: 28:04/09:14 [9:12:01/13:12:32]  Acc_iter 25950       Data time: 0.00(0.01)  Forward time: 1.07(1.27)  Batch time: 1.08(1.27)
2025-09-04 23:01:06,615   INFO  Train:   15/36 ( 42%) [1373/1759 ( 78%)]  Loss: 3.637 (3.64)  LR: 2.998e-03  Grad: 3.8575  max=0.3137(module.vfe.pfn_layers.0.linear.weight)  min: -1.1751(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6806, loss_cls=0.1282, loss_bbox=0.7595, matched_ious=0.5094, loss_iou=0.0936, loss_iou_reg=0.2274, d_time=0.01(0.01), f_time=1.19(1.27), b_time=1.20(1.27)  Time cost: 29:08/08:11 [9:13:05/13:11:38]  Acc_iter 26000       Data time: 0.01(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.27)
2025-09-04 23:02:09,940   INFO  Train:   15/36 ( 42%) [1423/1759 ( 81%)]  Loss: 3.717 (3.63)  LR: 2.997e-03  Grad: 3.7840  max=0.6274(module.vfe.pfn_layers.0.linear.weight)  min: -0.1683(module.backbone_3d.stage.embeddings.2.stem.0.bn1.weight)  NaN: False  loss_hm=0.6650, loss_cls=0.1257, loss_bbox=0.7394, matched_ious=0.5150, loss_iou=0.0926, loss_iou_reg=0.2281, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.27)  Time cost: 30:11/07:07 [9:14:08/13:10:27]  Acc_iter 26050       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.27)
2025-09-04 23:03:13,617   INFO  Train:   15/36 ( 42%) [1473/1759 ( 84%)]  Loss: 3.196 (3.63)  LR: 2.997e-03  Grad: 3.0069  max=0.1811(module.dense_head.heatmap_head.0.bn.bias)  min: -0.2918(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.6946, loss_cls=0.1316, loss_bbox=0.7673, matched_ious=0.5120, loss_iou=0.0953, loss_iou_reg=0.2271, d_time=0.00(0.01), f_time=1.41(1.27), b_time=1.41(1.27)  Time cost: 31:15/06:03 [9:15:12/13:09:25]  Acc_iter 26100       Data time: 0.00(0.01)  Forward time: 1.41(1.27)  Batch time: 1.41(1.27)
2025-09-04 23:04:16,948   INFO  Train:   15/36 ( 42%) [1523/1759 ( 87%)]  Loss: 3.881 (3.63)  LR: 2.997e-03  Grad: 3.2052  max=0.7227(module.vfe.pfn_layers.0.linear.weight)  min: -0.1921(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.6606, loss_cls=0.1277, loss_bbox=0.7506, matched_ious=0.5224, loss_iou=0.0936, loss_iou_reg=0.2244, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.27)  Time cost: 32:18/05:00 [9:16:15/13:08:14]  Acc_iter 26150       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.27)
2025-09-04 23:05:22,230   INFO  Train:   15/36 ( 42%) [1573/1759 ( 89%)]  Loss: 3.913 (3.63)  LR: 2.996e-03  Grad: 3.3449  max=0.1790(module.backbone_3d.cls_conv.3.bias)  min: -0.2909(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6627, loss_cls=0.1266, loss_bbox=0.7510, matched_ious=0.5066, loss_iou=0.0932, loss_iou_reg=0.2319, d_time=0.01(0.01), f_time=1.22(1.27), b_time=1.23(1.27)  Time cost: 33:24/03:56 [9:17:20/13:07:50]  Acc_iter 26200       Data time: 0.01(0.01)  Forward time: 1.22(1.27)  Batch time: 1.23(1.27)
2025-09-04 23:06:25,296   INFO  Train:   15/36 ( 42%) [1623/1759 ( 92%)]  Loss: 3.809 (3.63)  LR: 2.996e-03  Grad: 3.8210  max=0.5930(module.vfe.pfn_layers.0.linear.weight)  min: -1.3306(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6716, loss_cls=0.1280, loss_bbox=0.7123, matched_ious=0.5211, loss_iou=0.0943, loss_iou_reg=0.2250, d_time=0.00(0.01), f_time=1.33(1.27), b_time=1.33(1.27)  Time cost: 34:27/02:53 [9:18:24/13:06:33]  Acc_iter 26250       Data time: 0.00(0.01)  Forward time: 1.33(1.27)  Batch time: 1.33(1.27)
2025-09-04 23:07:29,356   INFO  Train:   15/36 ( 42%) [1673/1759 ( 95%)]  Loss: 3.284 (3.63)  LR: 2.995e-03  Grad: 4.0499  max=0.5521(module.vfe.pfn_layers.0.linear.weight)  min: -1.0475(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6887, loss_cls=0.1334, loss_bbox=0.7461, matched_ious=0.5202, loss_iou=0.0937, loss_iou_reg=0.2239, d_time=0.00(0.01), f_time=1.32(1.27), b_time=1.32(1.27)  Time cost: 35:31/01:49 [9:19:28/13:05:38]  Acc_iter 26300       Data time: 0.00(0.01)  Forward time: 1.32(1.27)  Batch time: 1.32(1.27)
2025-09-04 23:08:31,830   INFO  Train:   15/36 ( 42%) [1723/1759 ( 98%)]  Loss: 2.950 (3.62)  LR: 2.995e-03  Grad: 6.7039  max=0.3351(module.vfe.pfn_layers.0.linear.weight)  min: -4.7511(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6483, loss_cls=0.1262, loss_bbox=0.7356, matched_ious=0.5127, loss_iou=0.0945, loss_iou_reg=0.2295, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.24(1.27)  Time cost: 36:33/00:45 [9:20:30/13:04:09]  Acc_iter 26350       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.27)
2025-09-04 23:09:15,237   INFO  Train:   15/36 ( 42%) [1758/1759 (100%)]  Loss: 4.002 (3.62)  LR: 2.994e-03  Grad: 4.0547  max=1.4835(module.vfe.pfn_layers.0.linear.weight)  min: -1.4657(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6721, loss_cls=0.1303, loss_bbox=0.7274, matched_ious=0.5084, loss_iou=0.0947, loss_iou_reg=0.2304, d_time=0.01(0.01), f_time=0.77(1.27), b_time=0.78(1.27)  Time cost: 37:17/00:01 [9:21:14/13:03:01]  Acc_iter 26385       Data time: 0.01(0.01)  Forward time: 0.77(1.27)  Batch time: 0.78(1.27)

                                               [Aepochs:  42%|████▏     | 15/36 [9:21:14<13:04:11, 2240.54s/it]epochs:  42%|████▏     | 15/36 [9:21:14<13:04:11, 2240.54s/it]epochs:  42%|████▏     | 15/36 [9:21:14<13:04:11, 2240.55s/it]epochs:  42%|████▏     | 15/36 [9:21:14<13:04:11, 2240.54s/it]epochs:  42%|████▏     | 15/36 [9:21:14<13:04:11, 2240.54s/it]epochs:  42%|████▏     | 15/36 [9:21:14<13:04:11, 2240.55s/it]epochs:  42%|████▏     | 15/36 [9:21:14<13:04:11, 2240.55s/it]epochs:  42%|████▏     | 15/36 [9:21:14<13:04:11, 2240.54s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 23:09:20,697   INFO  Train:   16/36 ( 44%) [   0/1759 (  0%)]  Loss: 3.707 (3.71)  LR: 2.994e-03  Grad: 3.6403  max=0.3176(module.vfe.pfn_layers.0.linear.weight)  min: -1.0056(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7013, loss_cls=0.1410, loss_bbox=0.6828, matched_ious=0.5160, loss_iou=0.0856, loss_iou_reg=0.2281, d_time=1.44(1.44), f_time=2.68(2.68), b_time=4.12(4.12)  Time cost: 00:03/1:48:53 [9:21:19/38:06:35]  Acc_iter 26386       Data time: 1.44(1.44)  Forward time: 2.68(2.68)  Batch time: 4.12(4.12)
2025-09-04 23:09:38,617   INFO  Train:   16/36 ( 44%) [  14/1759 (  1%)]  Loss: 3.389 (3.47)  LR: 2.994e-03  Grad: 3.6531  max=0.8194(module.vfe.pfn_layers.0.linear.weight)  min: -0.8637(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6311, loss_cls=0.1190, loss_bbox=0.7656, matched_ious=0.5267, loss_iou=0.0880, loss_iou_reg=0.2198, d_time=0.00(0.10), f_time=1.28(1.37), b_time=1.29(1.47)  Time cost: 00:21/41:57 [9:21:37/14:47:44]  Acc_iter 26400       Data time: 0.00(0.10)  Forward time: 1.28(1.37)  Batch time: 1.29(1.47)
2025-09-04 23:10:43,001   INFO  Train:   16/36 ( 44%) [  64/1759 (  4%)]  Loss: 3.449 (3.57)  LR: 2.994e-03  Grad: 3.8322  max=0.4340(module.vfe.pfn_layers.0.linear.weight)  min: -1.1145(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6732, loss_cls=0.1280, loss_bbox=0.7790, matched_ious=0.5150, loss_iou=0.0924, loss_iou_reg=0.2247, d_time=0.01(0.04), f_time=1.31(1.29), b_time=1.32(1.33)  Time cost: 01:26/37:23 [9:22:41/13:33:19]  Acc_iter 26450       Data time: 0.01(0.04)  Forward time: 1.31(1.29)  Batch time: 1.32(1.33)
2025-09-04 23:11:45,680   INFO  Train:   16/36 ( 44%) [ 114/1759 (  6%)]  Loss: 3.283 (3.56)  LR: 2.993e-03  Grad: 3.9834  max=0.8241(module.vfe.pfn_layers.0.linear.weight)  min: -0.2012(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6822, loss_cls=0.1315, loss_bbox=0.7512, matched_ious=0.5120, loss_iou=0.0938, loss_iou_reg=0.2282, d_time=0.01(0.02), f_time=1.33(1.27), b_time=1.34(1.30)  Time cost: 02:28/35:27 [9:23:44/13:13:36]  Acc_iter 26500       Data time: 0.01(0.02)  Forward time: 1.33(1.27)  Batch time: 1.34(1.30)
2025-09-04 23:12:48,466   INFO  Train:   16/36 ( 44%) [ 164/1759 (  9%)]  Loss: 3.220 (3.60)  LR: 2.992e-03  Grad: 4.9506  max=1.7590(module.vfe.pfn_layers.0.linear.weight)  min: -1.9033(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6864, loss_cls=0.1292, loss_bbox=0.7797, matched_ious=0.5120, loss_iou=0.0941, loss_iou_reg=0.2265, d_time=0.00(0.02), f_time=1.22(1.27), b_time=1.22(1.28)  Time cost: 03:31/34:04 [9:24:47/13:05:35]  Acc_iter 26550       Data time: 0.00(0.02)  Forward time: 1.22(1.27)  Batch time: 1.22(1.28)
2025-09-04 23:13:51,992   INFO  Train:   16/36 ( 44%) [ 214/1759 ( 12%)]  Loss: 3.011 (3.58)  LR: 2.992e-03  Grad: 4.5330  max=0.3975(module.vfe.pfn_layers.0.linear.weight)  min: -0.9886(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6428, loss_cls=0.1241, loss_bbox=0.7489, matched_ious=0.5086, loss_iou=0.0939, loss_iou_reg=0.2318, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.28)  Time cost: 04:35/32:56 [9:25:50/13:02:55]  Acc_iter 26600       Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.28)
2025-09-04 23:14:55,753   INFO  Train:   16/36 ( 44%) [ 264/1759 ( 15%)]  Loss: 2.937 (3.57)  LR: 2.991e-03  Grad: 10.0000  max=6.8707(module.vfe.pfn_layers.0.linear.weight)  min: -5.9036(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6501, loss_cls=0.1252, loss_bbox=0.7078, matched_ious=0.5177, loss_iou=0.0952, loss_iou_reg=0.2263, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.29(1.28)  Time cost: 05:38/31:51 [9:26:54/13:01:25]  Acc_iter 26650       Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.28)
2025-09-04 23:16:00,690   INFO  Train:   16/36 ( 44%) [ 314/1759 ( 18%)]  Loss: 3.392 (3.58)  LR: 2.990e-03  Grad: 4.4152  max=1.6131(module.vfe.pfn_layers.0.linear.weight)  min: -1.1409(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6763, loss_cls=0.1290, loss_bbox=0.7756, matched_ious=0.5105, loss_iou=0.0943, loss_iou_reg=0.2278, d_time=0.00(0.01), f_time=1.38(1.27), b_time=1.38(1.28)  Time cost: 06:43/30:51 [9:27:59/13:02:19]  Acc_iter 26700       Data time: 0.00(0.01)  Forward time: 1.38(1.27)  Batch time: 1.38(1.28)
2025-09-04 23:17:04,115   INFO  Train:   16/36 ( 44%) [ 364/1759 ( 21%)]  Loss: 3.360 (3.58)  LR: 2.990e-03  Grad: 4.2076  max=0.1987(module.vfe.pfn_layers.0.linear.weight)  min: -0.2962(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6658, loss_cls=0.1266, loss_bbox=0.7447, matched_ious=0.5105, loss_iou=0.0950, loss_iou_reg=0.2276, d_time=0.01(0.01), f_time=1.25(1.27), b_time=1.26(1.28)  Time cost: 07:47/29:45 [9:29:02/13:00:09]  Acc_iter 26750       Data time: 0.01(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.28)
2025-09-04 23:18:07,839   INFO  Train:   16/36 ( 44%) [ 414/1759 ( 24%)]  Loss: 3.791 (3.58)  LR: 2.989e-03  Grad: 8.0386  max=5.7551(module.vfe.pfn_layers.0.linear.weight)  min: -3.1769(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6813, loss_cls=0.1254, loss_bbox=0.7903, matched_ious=0.5170, loss_iou=0.0955, loss_iou_reg=0.2237, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.31(1.28)  Time cost: 08:50/28:40 [9:30:06/12:58:42]  Acc_iter 26800       Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.31(1.28)
2025-09-04 23:19:12,005   INFO  Train:   16/36 ( 44%) [ 464/1759 ( 26%)]  Loss: 3.413 (3.59)  LR: 2.988e-03  Grad: 4.6182  max=0.4552(module.vfe.pfn_layers.0.linear.weight)  min: -0.3126(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7041, loss_cls=0.1378, loss_bbox=0.7662, matched_ious=0.5086, loss_iou=0.0949, loss_iou_reg=0.2314, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.24(1.28)  Time cost: 09:55/27:37 [9:31:10/12:57:54]  Acc_iter 26850       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.28)
2025-09-04 23:20:16,144   INFO  Train:   16/36 ( 44%) [ 514/1759 ( 29%)]  Loss: 3.103 (3.59)  LR: 2.987e-03  Grad: 5.0417  max=0.5291(module.vfe.pfn_layers.0.linear.weight)  min: -1.3444(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6661, loss_cls=0.1296, loss_bbox=0.7495, matched_ious=0.5102, loss_iou=0.0949, loss_iou_reg=0.2300, d_time=0.00(0.01), f_time=1.33(1.27), b_time=1.33(1.28)  Time cost: 10:59/26:33 [9:32:14/12:57:01]  Acc_iter 26900       Data time: 0.00(0.01)  Forward time: 1.33(1.27)  Batch time: 1.33(1.28)
2025-09-04 23:21:19,094   INFO  Train:   16/36 ( 44%) [ 564/1759 ( 32%)]  Loss: 3.627 (3.59)  LR: 2.987e-03  Grad: 5.0705  max=0.2984(module.vfe.pfn_layers.0.linear.weight)  min: -0.3399(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6773, loss_cls=0.1289, loss_bbox=0.7460, matched_ious=0.5243, loss_iou=0.0916, loss_iou_reg=0.2209, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.23(1.28)  Time cost: 12:02/25:27 [9:33:17/12:54:50]  Acc_iter 26950       Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.23(1.28)
2025-09-04 23:22:22,055   INFO  Train:   16/36 ( 44%) [ 614/1759 ( 35%)]  Loss: 3.723 (3.58)  LR: 2.986e-03  Grad: 5.3879  max=0.2188(module.vfe.pfn_layers.0.linear.weight)  min: -0.8050(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6680, loss_cls=0.1248, loss_bbox=0.7537, matched_ious=0.5118, loss_iou=0.0966, loss_iou_reg=0.2266, d_time=0.01(0.01), f_time=1.36(1.27), b_time=1.36(1.28)  Time cost: 13:05/24:21 [9:34:20/12:52:50]  Acc_iter 27000       Data time: 0.01(0.01)  Forward time: 1.36(1.27)  Batch time: 1.36(1.28)
2025-09-04 23:23:25,411   INFO  Train:   16/36 ( 44%) [ 664/1759 ( 38%)]  Loss: 3.237 (3.59)  LR: 2.985e-03  Grad: 5.6952  max=0.9852(module.vfe.pfn_layers.0.linear.weight)  min: -0.5600(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6912, loss_cls=0.1265, loss_bbox=0.7939, matched_ious=0.5134, loss_iou=0.0942, loss_iou_reg=0.2250, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.28(1.28)  Time cost: 14:08/23:17 [9:35:24/12:51:20]  Acc_iter 27050       Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.28(1.28)
2025-09-04 23:24:29,432   INFO  Train:   16/36 ( 44%) [ 714/1759 ( 41%)]  Loss: 4.131 (3.59)  LR: 2.984e-03  Grad: 5.0457  max=0.4353(module.vfe.pfn_layers.0.linear.weight)  min: -0.3156(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6633, loss_cls=0.1290, loss_bbox=0.7547, matched_ious=0.5185, loss_iou=0.0928, loss_iou_reg=0.2252, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.28(1.28)  Time cost: 15:12/22:13 [9:36:28/12:50:28]  Acc_iter 27100       Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.28(1.28)
2025-09-04 23:25:31,956   INFO  Train:   16/36 ( 44%) [ 764/1759 ( 43%)]  Loss: 4.645 (3.58)  LR: 2.983e-03  Grad: 5.3156  max=0.5499(module.vfe.pfn_layers.0.linear.weight)  min: -0.4219(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6770, loss_cls=0.1315, loss_bbox=0.7279, matched_ious=0.5140, loss_iou=0.0944, loss_iou_reg=0.2275, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.20(1.28)  Time cost: 16:14/21:08 [9:37:30/12:48:24]  Acc_iter 27150       Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.28)
2025-09-04 23:26:36,477   INFO  Train:   16/36 ( 44%) [ 814/1759 ( 46%)]  Loss: 3.941 (3.59)  LR: 2.982e-03  Grad: 8.1640  max=4.4441(module.vfe.pfn_layers.0.linear.weight)  min: -1.0957(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6803, loss_cls=0.1248, loss_bbox=0.8117, matched_ious=0.5101, loss_iou=0.0914, loss_iou_reg=0.2261, d_time=0.01(0.01), f_time=1.14(1.27), b_time=1.14(1.28)  Time cost: 17:19/20:05 [9:38:35/12:47:55]  Acc_iter 27200       Data time: 0.01(0.01)  Forward time: 1.14(1.27)  Batch time: 1.14(1.28)
2025-09-04 23:27:39,941   INFO  Train:   16/36 ( 44%) [ 864/1759 ( 49%)]  Loss: 3.333 (3.58)  LR: 2.981e-03  Grad: 5.7808  max=0.6222(module.vfe.pfn_layers.0.linear.weight)  min: -0.2215(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6667, loss_cls=0.1279, loss_bbox=0.7629, matched_ious=0.5170, loss_iou=0.0914, loss_iou_reg=0.2243, d_time=0.00(0.01), f_time=1.34(1.27), b_time=1.35(1.28)  Time cost: 18:22/19:01 [9:39:38/12:46:39]  Acc_iter 27250       Data time: 0.00(0.01)  Forward time: 1.34(1.27)  Batch time: 1.35(1.28)
2025-09-04 23:28:43,030   INFO  Train:   16/36 ( 44%) [ 914/1759 ( 52%)]  Loss: 3.596 (3.58)  LR: 2.980e-03  Grad: 5.9976  max=0.4339(module.vfe.pfn_layers.0.linear.weight)  min: -0.5636(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6810, loss_cls=0.1298, loss_bbox=0.7939, matched_ious=0.5096, loss_iou=0.0931, loss_iou_reg=0.2297, d_time=0.00(0.01), f_time=1.18(1.27), b_time=1.19(1.27)  Time cost: 19:26/17:56 [9:40:41/12:45:09]  Acc_iter 27300       Data time: 0.00(0.01)  Forward time: 1.18(1.27)  Batch time: 1.19(1.27)
2025-09-04 23:29:47,234   INFO  Train:   16/36 ( 44%) [ 964/1759 ( 55%)]  Loss: 3.539 (3.59)  LR: 2.979e-03  Grad: 5.8145  max=0.2230(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3895(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6775, loss_cls=0.1287, loss_bbox=0.7829, matched_ious=0.5137, loss_iou=0.0933, loss_iou_reg=0.2284, d_time=0.00(0.01), f_time=1.34(1.27), b_time=1.35(1.28)  Time cost: 20:30/16:53 [9:41:46/12:44:23]  Acc_iter 27350       Data time: 0.00(0.01)  Forward time: 1.34(1.27)  Batch time: 1.35(1.28)
2025-09-04 23:30:51,470   INFO  Train:   16/36 ( 44%) [1014/1759 ( 58%)]  Loss: 3.012 (3.58)  LR: 2.978e-03  Grad: 5.5668  max=0.3074(module.vfe.pfn_layers.0.linear.weight)  min: -0.4632(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6765, loss_cls=0.1281, loss_bbox=0.7688, matched_ious=0.5170, loss_iou=0.0936, loss_iou_reg=0.2256, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.28(1.28)  Time cost: 21:34/15:50 [9:42:50/12:43:37]  Acc_iter 27400       Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.28(1.28)
2025-09-04 23:31:55,427   INFO  Train:   16/36 ( 44%) [1064/1759 ( 60%)]  Loss: 3.821 (3.59)  LR: 2.977e-03  Grad: 5.8028  max=0.3245(module.vfe.pfn_layers.0.linear.weight)  min: -0.6801(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6872, loss_cls=0.1310, loss_bbox=0.7672, matched_ious=0.5117, loss_iou=0.0941, loss_iou_reg=0.2276, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.20(1.28)  Time cost: 22:38/14:46 [9:43:54/12:42:39]  Acc_iter 27450       Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.28)
2025-09-04 23:32:59,132   INFO  Train:   16/36 ( 44%) [1114/1759 ( 63%)]  Loss: 3.724 (3.58)  LR: 2.976e-03  Grad: 4.4008  max=0.3065(module.vfe.pfn_layers.0.linear.weight)  min: -1.6961(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6597, loss_cls=0.1283, loss_bbox=0.7301, matched_ious=0.5107, loss_iou=0.0926, loss_iou_reg=0.2289, d_time=0.00(0.01), f_time=1.34(1.27), b_time=1.35(1.28)  Time cost: 23:42/13:42 [9:44:57/12:41:33]  Acc_iter 27500       Data time: 0.00(0.01)  Forward time: 1.34(1.27)  Batch time: 1.35(1.28)
2025-09-04 23:34:02,955   INFO  Train:   16/36 ( 44%) [1164/1759 ( 66%)]  Loss: 3.700 (3.58)  LR: 2.975e-03  Grad: 4.3452  max=0.5819(module.vfe.pfn_layers.0.linear.weight)  min: -0.6174(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6571, loss_cls=0.1276, loss_bbox=0.7513, matched_ious=0.5080, loss_iou=0.0943, loss_iou_reg=0.2333, d_time=0.00(0.01), f_time=1.18(1.27), b_time=1.19(1.28)  Time cost: 24:45/12:38 [9:46:01/12:40:31]  Acc_iter 27550       Data time: 0.00(0.01)  Forward time: 1.18(1.27)  Batch time: 1.19(1.28)
2025-09-04 23:35:06,398   INFO  Train:   16/36 ( 44%) [1214/1759 ( 69%)]  Loss: 3.880 (3.58)  LR: 2.974e-03  Grad: 4.4644  max=0.9194(module.vfe.pfn_layers.0.linear.weight)  min: -1.6042(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6587, loss_cls=0.1253, loss_bbox=0.7224, matched_ious=0.5157, loss_iou=0.0968, loss_iou_reg=0.2276, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.27(1.28)  Time cost: 25:49/11:35 [9:47:05/12:39:18]  Acc_iter 27600       Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.28)
2025-09-04 23:36:10,596   INFO  Train:   16/36 ( 44%) [1264/1759 ( 72%)]  Loss: 3.843 (3.58)  LR: 2.972e-03  Grad: 4.3228  max=0.6378(module.vfe.pfn_layers.0.linear.weight)  min: -0.5794(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6498, loss_cls=0.1262, loss_bbox=0.7238, matched_ious=0.5176, loss_iou=0.0943, loss_iou_reg=0.2273, d_time=0.00(0.01), f_time=2.17(1.27), b_time=2.18(1.28)  Time cost: 26:53/10:31 [9:48:09/12:38:26]  Acc_iter 27650       Data time: 0.00(0.01)  Forward time: 2.17(1.27)  Batch time: 2.18(1.28)
2025-09-04 23:37:13,582   INFO  Train:   16/36 ( 44%) [1314/1759 ( 75%)]  Loss: 3.876 (3.58)  LR: 2.971e-03  Grad: 4.6073  max=0.1762(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.9928(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6882, loss_cls=0.1292, loss_bbox=0.7693, matched_ious=0.5097, loss_iou=0.0930, loss_iou_reg=0.2290, d_time=0.01(0.01), f_time=1.20(1.27), b_time=1.22(1.28)  Time cost: 27:56/09:27 [9:49:12/12:37:01]  Acc_iter 27700       Data time: 0.01(0.01)  Forward time: 1.20(1.27)  Batch time: 1.22(1.28)
2025-09-04 23:38:16,621   INFO  Train:   16/36 ( 44%) [1364/1759 ( 78%)]  Loss: 3.013 (3.58)  LR: 2.970e-03  Grad: 8.8122  max=0.3178(module.vfe.pfn_layers.0.linear.weight)  min: -6.8925(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6516, loss_cls=0.1250, loss_bbox=0.7712, matched_ious=0.5206, loss_iou=0.0936, loss_iou_reg=0.2246, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.29(1.27)  Time cost: 28:59/08:23 [9:50:15/12:35:39]  Acc_iter 27750       Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.27)
2025-09-04 23:39:20,365   INFO  Train:   16/36 ( 44%) [1414/1759 ( 80%)]  Loss: 3.498 (3.58)  LR: 2.969e-03  Grad: 4.2191  max=0.1539(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.3184(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6842, loss_cls=0.1306, loss_bbox=0.7604, matched_ious=0.5145, loss_iou=0.0956, loss_iou_reg=0.2279, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.27)  Time cost: 30:03/07:19 [9:51:19/12:34:35]  Acc_iter 27800       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.27)
2025-09-04 23:40:23,969   INFO  Train:   16/36 ( 44%) [1464/1759 ( 83%)]  Loss: 3.626 (3.59)  LR: 2.968e-03  Grad: 4.6080  max=0.8477(module.vfe.pfn_layers.0.linear.weight)  min: -0.5275(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7290, loss_cls=0.1358, loss_bbox=0.8532, matched_ious=0.5090, loss_iou=0.0966, loss_iou_reg=0.2281, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.29(1.27)  Time cost: 31:06/06:15 [9:52:22/12:33:29]  Acc_iter 27850       Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.29(1.27)
2025-09-04 23:41:26,699   INFO  Train:   16/36 ( 44%) [1514/1759 ( 86%)]  Loss: 3.677 (3.59)  LR: 2.966e-03  Grad: 4.7255  max=0.1837(module.vfe.pfn_layers.0.linear.weight)  min: -0.3075(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6770, loss_cls=0.1265, loss_bbox=0.7492, matched_ious=0.5198, loss_iou=0.0935, loss_iou_reg=0.2241, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.27)  Time cost: 32:09/05:12 [9:53:25/12:32:02]  Acc_iter 27900       Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.27)
2025-09-04 23:42:30,174   INFO  Train:   16/36 ( 44%) [1564/1759 ( 89%)]  Loss: 3.704 (3.58)  LR: 2.965e-03  Grad: 5.6174  max=0.2473(module.vfe.pfn_layers.0.linear.weight)  min: -2.5235(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6355, loss_cls=0.1260, loss_bbox=0.7054, matched_ious=0.5151, loss_iou=0.0930, loss_iou_reg=0.2309, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.30(1.27)  Time cost: 33:13/04:08 [9:54:28/12:30:53]  Acc_iter 27950       Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.30(1.27)
2025-09-04 23:43:32,939   INFO  Train:   16/36 ( 44%) [1614/1759 ( 92%)]  Loss: 4.233 (3.58)  LR: 2.964e-03  Grad: 5.3003  max=0.3737(module.vfe.pfn_layers.0.linear.weight)  min: -0.4125(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6758, loss_cls=0.1265, loss_bbox=0.7512, matched_ious=0.5063, loss_iou=0.0950, loss_iou_reg=0.2311, d_time=0.00(0.01), f_time=1.34(1.27), b_time=1.34(1.27)  Time cost: 34:15/03:04 [9:55:31/12:29:30]  Acc_iter 28000       Data time: 0.00(0.01)  Forward time: 1.34(1.27)  Batch time: 1.34(1.27)
2025-09-04 23:44:36,180   INFO  Train:   16/36 ( 44%) [1664/1759 ( 95%)]  Loss: 3.789 (3.58)  LR: 2.962e-03  Grad: 5.5135  max=0.5465(module.vfe.pfn_layers.0.linear.weight)  min: -0.4882(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6723, loss_cls=0.1225, loss_bbox=0.7817, matched_ious=0.5142, loss_iou=0.0959, loss_iou_reg=0.2260, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.19(1.27)  Time cost: 35:19/02:00 [9:56:34/12:28:17]  Acc_iter 28050       Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.19(1.27)
2025-09-04 23:45:39,161   INFO  Train:   16/36 ( 44%) [1714/1759 ( 97%)]  Loss: 3.766 (3.58)  LR: 2.961e-03  Grad: 5.9357  max=0.2877(module.vfe.pfn_layers.0.linear.weight)  min: -0.8709(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6811, loss_cls=0.1297, loss_bbox=0.7750, matched_ious=0.5122, loss_iou=0.0958, loss_iou_reg=0.2274, d_time=0.00(0.01), f_time=1.17(1.27), b_time=1.18(1.27)  Time cost: 36:22/00:57 [9:57:37/12:27:00]  Acc_iter 28100       Data time: 0.00(0.01)  Forward time: 1.17(1.27)  Batch time: 1.18(1.27)
2025-09-04 23:46:34,834   INFO  Train:   16/36 ( 44%) [1758/1759 (100%)]  Loss: 3.721 (3.58)  LR: 2.960e-03  Grad: 6.0681  max=0.6366(module.vfe.pfn_layers.0.linear.weight)  min: -0.5104(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6822, loss_cls=0.1306, loss_bbox=0.8021, matched_ious=0.5175, loss_iou=0.0944, loss_iou_reg=0.2242, d_time=0.00(0.01), f_time=0.78(1.27), b_time=0.78(1.27)  Time cost: 37:17/00:01 [9:58:33/12:25:58]  Acc_iter 28144       Data time: 0.00(0.01)  Forward time: 0.78(1.27)  Batch time: 0.78(1.27)

                                               [Aepochs:  44%|████▍     | 16/36 [9:58:34<12:26:44, 2240.24s/it]epochs:  44%|████▍     | 16/36 [9:58:34<12:26:44, 2240.24s/it]epochs:  44%|████▍     | 16/36 [9:58:34<12:26:45, 2240.28s/it]epochs:  44%|████▍     | 16/36 [9:58:34<12:26:45, 2240.26s/it]epochs:  44%|████▍     | 16/36 [9:58:34<12:26:45, 2240.27s/it]epochs:  44%|████▍     | 16/36 [9:58:34<12:26:45, 2240.27s/it]epochs:  44%|████▍     | 16/36 [9:58:34<12:26:45, 2240.27s/it]epochs:  44%|████▍     | 16/36 [9:58:34<12:26:45, 2240.25s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-04 23:46:40,372   INFO  Train:   17/36 ( 47%) [   0/1759 (  0%)]  Loss: 3.272 (3.27)  LR: 2.960e-03  Grad: 6.0500  max=0.2645(module.vfe.pfn_layers.0.linear.weight)  min: -1.0089(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6887, loss_cls=0.1149, loss_bbox=0.6139, matched_ious=0.5566, loss_iou=0.0998, loss_iou_reg=0.2113, d_time=1.64(1.64), f_time=2.56(2.56), b_time=4.20(4.20)  Time cost: 00:03/1:49:26 [9:58:39/36:28:49]  Acc_iter 28145       Data time: 1.64(1.64)  Forward time: 2.56(2.56)  Batch time: 4.20(4.20)
2025-09-04 23:46:49,017   INFO  Train:   17/36 ( 47%) [   5/1759 (  0%)]  Loss: 4.078 (3.51)  LR: 2.959e-03  Grad: 5.9954  max=0.2431(module.vfe.pfn_layers.0.linear.weight)  min: -0.2362(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.7191, loss_cls=0.1354, loss_bbox=0.7794, matched_ious=0.4914, loss_iou=0.1061, loss_iou_reg=0.2347, d_time=0.01(0.28), f_time=1.43(1.86), b_time=1.44(2.14)  Time cost: 00:12/1:00:18 [9:58:47/20:09:27]  Acc_iter 28150       Data time: 0.01(0.28)  Forward time: 1.43(1.86)  Batch time: 1.44(2.14)
2025-09-04 23:47:52,219   INFO  Train:   17/36 ( 47%) [  55/1759 (  3%)]  Loss: 3.180 (3.52)  LR: 2.958e-03  Grad: 3.0350  max=0.3411(module.vfe.pfn_layers.0.linear.weight)  min: -0.3940(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6572, loss_cls=0.1242, loss_bbox=0.7535, matched_ious=0.5184, loss_iou=0.0951, loss_iou_reg=0.2262, d_time=0.00(0.03), f_time=1.20(1.32), b_time=1.20(1.36)  Time cost: 01:15/38:19 [9:59:50/13:10:05]  Acc_iter 28200       Data time: 0.00(0.03)  Forward time: 1.20(1.32)  Batch time: 1.20(1.36)
2025-09-04 23:48:55,476   INFO  Train:   17/36 ( 47%) [ 105/1759 (  6%)]  Loss: 3.173 (3.57)  LR: 2.956e-03  Grad: 4.0009  max=1.9645(module.vfe.pfn_layers.0.linear.weight)  min: -0.3818(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6858, loss_cls=0.1263, loss_bbox=0.7841, matched_ious=0.5103, loss_iou=0.0944, loss_iou_reg=0.2263, d_time=0.00(0.02), f_time=1.34(1.29), b_time=1.35(1.31)  Time cost: 02:18/36:06 [10:00:54/12:45:40]  Acc_iter 28250       Data time: 0.00(0.02)  Forward time: 1.34(1.29)  Batch time: 1.35(1.31)
2025-09-04 23:49:59,513   INFO  Train:   17/36 ( 47%) [ 155/1759 (  9%)]  Loss: 3.741 (3.56)  LR: 2.955e-03  Grad: 3.5999  max=0.2622(module.dense_head.prediction_head.height.1.bias)  min: -0.6243(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6649, loss_cls=0.1254, loss_bbox=0.7371, matched_ious=0.5244, loss_iou=0.0946, loss_iou_reg=0.2245, d_time=0.01(0.02), f_time=1.28(1.28), b_time=1.28(1.30)  Time cost: 03:22/34:45 [10:01:58/12:39:09]  Acc_iter 28300       Data time: 0.01(0.02)  Forward time: 1.28(1.28)  Batch time: 1.28(1.30)
2025-09-04 23:51:03,748   INFO  Train:   17/36 ( 47%) [ 205/1759 ( 12%)]  Loss: 3.501 (3.55)  LR: 2.953e-03  Grad: 4.2812  max=1.4077(module.vfe.pfn_layers.0.linear.weight)  min: -0.6983(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6568, loss_cls=0.1223, loss_bbox=0.7488, matched_ious=0.5180, loss_iou=0.0930, loss_iou_reg=0.2241, d_time=0.00(0.02), f_time=1.29(1.28), b_time=1.29(1.30)  Time cost: 04:27/33:34 [10:03:02/12:35:49]  Acc_iter 28350       Data time: 0.00(0.02)  Forward time: 1.29(1.28)  Batch time: 1.29(1.30)
2025-09-04 23:52:06,920   INFO  Train:   17/36 ( 47%) [ 255/1759 ( 14%)]  Loss: 3.517 (3.55)  LR: 2.952e-03  Grad: 2.5539  max=0.6126(module.vfe.pfn_layers.0.linear.weight)  min: -0.6621(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6398, loss_cls=0.1199, loss_bbox=0.7530, matched_ious=0.5136, loss_iou=0.0946, loss_iou_reg=0.2247, d_time=0.00(0.01), f_time=1.16(1.28), b_time=1.17(1.29)  Time cost: 05:30/32:20 [10:04:05/12:30:58]  Acc_iter 28400       Data time: 0.00(0.01)  Forward time: 1.16(1.28)  Batch time: 1.17(1.29)
2025-09-04 23:53:10,331   INFO  Train:   17/36 ( 47%) [ 305/1759 ( 17%)]  Loss: 4.013 (3.55)  LR: 2.950e-03  Grad: 2.7279  max=0.5363(module.vfe.pfn_layers.0.linear.weight)  min: -0.4284(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6658, loss_cls=0.1237, loss_bbox=0.7943, matched_ious=0.5108, loss_iou=0.0960, loss_iou_reg=0.2273, d_time=0.00(0.01), f_time=1.26(1.28), b_time=1.26(1.29)  Time cost: 06:33/31:10 [10:05:09/12:27:49]  Acc_iter 28450       Data time: 0.00(0.01)  Forward time: 1.26(1.28)  Batch time: 1.26(1.29)
2025-09-04 23:54:13,409   INFO  Train:   17/36 ( 47%) [ 355/1759 ( 20%)]  Loss: 3.920 (3.54)  LR: 2.949e-03  Grad: 2.1423  max=0.4746(module.vfe.pfn_layers.0.linear.weight)  min: -0.4241(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6472, loss_cls=0.1253, loss_bbox=0.7081, matched_ious=0.5183, loss_iou=0.0984, loss_iou_reg=0.2267, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.29(1.28)  Time cost: 07:36/30:01 [10:06:12/12:24:42]  Acc_iter 28500       Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.29(1.28)
2025-09-04 23:55:15,964   INFO  Train:   17/36 ( 47%) [ 405/1759 ( 23%)]  Loss: 3.400 (3.54)  LR: 2.947e-03  Grad: 2.9715  max=1.4454(module.vfe.pfn_layers.0.linear.weight)  min: -1.1076(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6639, loss_cls=0.1257, loss_bbox=0.7415, matched_ious=0.5129, loss_iou=0.0944, loss_iou_reg=0.2265, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.24(1.28)  Time cost: 08:39/28:51 [10:07:14/12:21:21]  Acc_iter 28550       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.28)
2025-09-04 23:56:18,673   INFO  Train:   17/36 ( 47%) [ 455/1759 ( 26%)]  Loss: 3.729 (3.53)  LR: 2.946e-03  Grad: 2.4181  max=0.3361(module.vfe.pfn_layers.0.linear.weight)  min: -0.2545(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6744, loss_cls=0.1258, loss_bbox=0.7182, matched_ious=0.5228, loss_iou=0.0931, loss_iou_reg=0.2232, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.32(1.28)  Time cost: 09:42/27:44 [10:08:17/12:18:42]  Acc_iter 28600       Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.32(1.28)
2025-09-04 23:57:23,960   INFO  Train:   17/36 ( 47%) [ 505/1759 ( 29%)]  Loss: 3.529 (3.54)  LR: 2.944e-03  Grad: 2.7460  max=0.5860(module.vfe.pfn_layers.0.linear.weight)  min: -0.1404(module.dense_head.prediction_head.dim.0.1.weight)  NaN: False  loss_hm=0.6810, loss_cls=0.1286, loss_bbox=0.7399, matched_ious=0.5211, loss_iou=0.0921, loss_iou_reg=0.2230, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.26(1.28)  Time cost: 10:47/26:44 [10:09:22/12:19:19]  Acc_iter 28650       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.28)
2025-09-04 23:58:27,299   INFO  Train:   17/36 ( 47%) [ 555/1759 ( 32%)]  Loss: 3.569 (3.54)  LR: 2.942e-03  Grad: 2.9197  max=0.1435(module.backbone_3d.stage.embeddings.2.stem.0.bn1.bias)  min: -0.3988(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6694, loss_cls=0.1243, loss_bbox=0.7518, matched_ious=0.5134, loss_iou=0.0960, loss_iou_reg=0.2274, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.26(1.28)  Time cost: 11:50/25:38 [10:10:26/12:17:36]  Acc_iter 28700       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.28)
2025-09-04 23:59:30,633   INFO  Train:   17/36 ( 47%) [ 605/1759 ( 34%)]  Loss: 2.991 (3.54)  LR: 2.940e-03  Grad: 3.1214  max=0.1615(module.vfe.pfn_layers.0.linear.weight)  min: -0.3710(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6417, loss_cls=0.1216, loss_bbox=0.7448, matched_ious=0.5145, loss_iou=0.0937, loss_iou_reg=0.2266, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.23(1.28)  Time cost: 12:53/24:33 [10:11:29/12:15:59]  Acc_iter 28750       Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.23(1.28)
2025-09-05 00:00:33,773   INFO  Train:   17/36 ( 47%) [ 655/1759 ( 37%)]  Loss: 3.176 (3.54)  LR: 2.939e-03  Grad: 3.2146  max=0.1299(module.backbone_3d.cls_conv.1.bias)  min: -0.5565(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6796, loss_cls=0.1260, loss_bbox=0.7820, matched_ious=0.5107, loss_iou=0.0959, loss_iou_reg=0.2264, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.23(1.28)  Time cost: 13:57/23:28 [10:12:32/12:14:17]  Acc_iter 28800       Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.23(1.28)
2025-09-05 00:01:37,180   INFO  Train:   17/36 ( 47%) [ 705/1759 ( 40%)]  Loss: 3.308 (3.55)  LR: 2.937e-03  Grad: 3.3827  max=0.3700(module.vfe.pfn_layers.0.linear.weight)  min: -0.3867(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6568, loss_cls=0.1267, loss_bbox=0.7635, matched_ious=0.5079, loss_iou=0.0956, loss_iou_reg=0.2302, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.28(1.28)  Time cost: 15:00/22:24 [10:13:35/12:12:54]  Acc_iter 28850       Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.28(1.28)
2025-09-05 00:02:40,416   INFO  Train:   17/36 ( 47%) [ 755/1759 ( 43%)]  Loss: 2.864 (3.55)  LR: 2.935e-03  Grad: 3.5950  max=0.6337(module.vfe.pfn_layers.0.linear.weight)  min: -0.1319(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6601, loss_cls=0.1263, loss_bbox=0.7397, matched_ious=0.5171, loss_iou=0.0927, loss_iou_reg=0.2257, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.28(1.28)  Time cost: 16:03/21:19 [10:14:39/12:11:26]  Acc_iter 28900       Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.28(1.28)
2025-09-05 00:03:44,134   INFO  Train:   17/36 ( 47%) [ 805/1759 ( 46%)]  Loss: 3.567 (3.53)  LR: 2.933e-03  Grad: 2.7882  max=0.1959(module.vfe.pfn_layers.0.linear.weight)  min: -0.3488(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6325, loss_cls=0.1231, loss_bbox=0.6748, matched_ious=0.5245, loss_iou=0.0942, loss_iou_reg=0.2247, d_time=0.01(0.01), f_time=1.34(1.27), b_time=1.35(1.28)  Time cost: 17:07/20:16 [10:15:42/12:10:21]  Acc_iter 28950       Data time: 0.01(0.01)  Forward time: 1.34(1.27)  Batch time: 1.35(1.28)
2025-09-05 00:04:48,020   INFO  Train:   17/36 ( 47%) [ 855/1759 ( 49%)]  Loss: 3.610 (3.54)  LR: 2.931e-03  Grad: 2.9068  max=0.6625(module.vfe.pfn_layers.0.linear.weight)  min: -0.9613(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6863, loss_cls=0.1268, loss_bbox=0.8352, matched_ious=0.5111, loss_iou=0.0946, loss_iou_reg=0.2285, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.28)  Time cost: 18:11/19:12 [10:16:46/12:09:23]  Acc_iter 29000       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.28)
2025-09-05 00:05:50,675   INFO  Train:   17/36 ( 47%) [ 905/1759 ( 51%)]  Loss: 3.555 (3.54)  LR: 2.930e-03  Grad: 5.9942  max=2.0260(module.vfe.pfn_layers.0.linear.weight)  min: -4.7308(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6745, loss_cls=0.1270, loss_bbox=0.7488, matched_ious=0.5198, loss_iou=0.0940, loss_iou_reg=0.2232, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.26(1.27)  Time cost: 19:14/18:07 [10:17:49/12:07:38]  Acc_iter 29050       Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.27)
2025-09-05 00:06:54,079   INFO  Train:   17/36 ( 47%) [ 955/1759 ( 54%)]  Loss: 3.832 (3.54)  LR: 2.928e-03  Grad: 3.2640  max=0.6025(module.vfe.pfn_layers.0.linear.weight)  min: -0.6388(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6692, loss_cls=0.1260, loss_bbox=0.7636, matched_ious=0.5216, loss_iou=0.0950, loss_iou_reg=0.2233, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.31(1.27)  Time cost: 20:17/17:03 [10:18:52/12:06:24]  Acc_iter 29100       Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.31(1.27)
2025-09-05 00:07:58,508   INFO  Train:   17/36 ( 47%) [1005/1759 ( 57%)]  Loss: 3.614 (3.54)  LR: 2.926e-03  Grad: 3.3963  max=0.1941(module.dense_head.prediction_head.dim.0.1.weight)  min: -0.6144(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6529, loss_cls=0.1240, loss_bbox=0.7678, matched_ious=0.5097, loss_iou=0.0953, loss_iou_reg=0.2296, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.27(1.27)  Time cost: 21:21/16:00 [10:19:57/12:05:46]  Acc_iter 29150       Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.27)
2025-09-05 00:09:02,194   INFO  Train:   17/36 ( 47%) [1055/1759 ( 60%)]  Loss: 3.086 (3.54)  LR: 2.924e-03  Grad: 3.8556  max=0.9747(module.vfe.pfn_layers.0.linear.weight)  min: -1.6697(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6549, loss_cls=0.1272, loss_bbox=0.7270, matched_ious=0.5164, loss_iou=0.0951, loss_iou_reg=0.2282, d_time=0.00(0.01), f_time=1.18(1.27), b_time=1.18(1.27)  Time cost: 22:25/14:57 [10:21:00/12:04:42]  Acc_iter 29200       Data time: 0.00(0.01)  Forward time: 1.18(1.27)  Batch time: 1.18(1.27)
2025-09-05 00:10:05,250   INFO  Train:   17/36 ( 47%) [1105/1759 ( 63%)]  Loss: 3.647 (3.55)  LR: 2.922e-03  Grad: 3.7209  max=1.1727(module.vfe.pfn_layers.0.linear.weight)  min: -0.6630(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6923, loss_cls=0.1312, loss_bbox=0.8197, matched_ious=0.5169, loss_iou=0.0946, loss_iou_reg=0.2262, d_time=0.00(0.01), f_time=1.32(1.27), b_time=1.32(1.27)  Time cost: 23:28/13:52 [10:22:04/12:03:18]  Acc_iter 29250       Data time: 0.00(0.01)  Forward time: 1.32(1.27)  Batch time: 1.32(1.27)
2025-09-05 00:11:08,867   INFO  Train:   17/36 ( 47%) [1155/1759 ( 66%)]  Loss: 3.314 (3.55)  LR: 2.920e-03  Grad: 3.8762  max=0.7233(module.vfe.pfn_layers.0.linear.weight)  min: -0.4252(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6565, loss_cls=0.1248, loss_bbox=0.7730, matched_ious=0.5147, loss_iou=0.0941, loss_iou_reg=0.2265, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.27)  Time cost: 24:32/12:49 [10:23:07/12:02:12]  Acc_iter 29300       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.27)
2025-09-05 00:12:12,143   INFO  Train:   17/36 ( 47%) [1205/1759 ( 69%)]  Loss: 3.708 (3.55)  LR: 2.918e-03  Grad: 6.1632  max=0.1385(module.dense_head.decoder.self_attn.in_proj_weight)  min: -3.4633(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6515, loss_cls=0.1238, loss_bbox=0.7053, matched_ious=0.5261, loss_iou=0.0926, loss_iou_reg=0.2232, d_time=0.00(0.01), f_time=1.18(1.27), b_time=1.18(1.27)  Time cost: 25:35/11:45 [10:24:10/12:00:57]  Acc_iter 29350       Data time: 0.00(0.01)  Forward time: 1.18(1.27)  Batch time: 1.18(1.27)
2025-09-05 00:13:15,780   INFO  Train:   17/36 ( 47%) [1255/1759 ( 71%)]  Loss: 2.985 (3.55)  LR: 2.916e-03  Grad: 4.2307  max=0.3659(module.vfe.pfn_layers.0.linear.weight)  min: -0.6419(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6786, loss_cls=0.1294, loss_bbox=0.7603, matched_ious=0.5080, loss_iou=0.0938, loss_iou_reg=0.2300, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.27)  Time cost: 26:39/10:41 [10:25:14/11:59:53]  Acc_iter 29400       Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.27)
2025-09-05 00:14:18,650   INFO  Train:   17/36 ( 47%) [1305/1759 ( 74%)]  Loss: 4.105 (3.55)  LR: 2.914e-03  Grad: 3.5559  max=0.6070(module.vfe.pfn_layers.0.linear.weight)  min: -0.2475(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6533, loss_cls=0.1215, loss_bbox=0.7262, matched_ious=0.5207, loss_iou=0.0926, loss_iou_reg=0.2238, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.27)  Time cost: 27:42/09:37 [10:26:17/11:58:29]  Acc_iter 29450       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.27)
2025-09-05 00:15:22,530   INFO  Train:   17/36 ( 47%) [1355/1759 ( 77%)]  Loss: 3.011 (3.55)  LR: 2.912e-03  Grad: 3.8411  max=0.1385(module.backbone_3d.stage.embeddings.2.stem.0.bn1.bias)  min: -0.4934(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6530, loss_cls=0.1249, loss_bbox=0.7333, matched_ious=0.5064, loss_iou=0.0960, loss_iou_reg=0.2340, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.27(1.27)  Time cost: 28:45/08:34 [10:27:21/11:57:31]  Acc_iter 29500       Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.27)
2025-09-05 00:16:26,098   INFO  Train:   17/36 ( 47%) [1405/1759 ( 80%)]  Loss: 3.174 (3.55)  LR: 2.910e-03  Grad: 3.1137  max=0.2929(module.vfe.pfn_layers.0.linear.weight)  min: -0.1177(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6945, loss_cls=0.1303, loss_bbox=0.7586, matched_ious=0.5178, loss_iou=0.0945, loss_iou_reg=0.2232, d_time=0.01(0.01), f_time=1.21(1.27), b_time=1.21(1.27)  Time cost: 29:49/07:30 [10:28:24/11:56:26]  Acc_iter 29550       Data time: 0.01(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.27)
2025-09-05 00:17:29,293   INFO  Train:   17/36 ( 47%) [1455/1759 ( 83%)]  Loss: 3.797 (3.55)  LR: 2.907e-03  Grad: 3.4651  max=0.6634(module.vfe.pfn_layers.0.linear.weight)  min: -0.2561(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6485, loss_cls=0.1230, loss_bbox=0.7394, matched_ious=0.5112, loss_iou=0.0933, loss_iou_reg=0.2287, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.24(1.27)  Time cost: 30:52/06:26 [10:29:28/11:55:12]  Acc_iter 29600       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.27)
2025-09-05 00:18:34,842   INFO  Train:   17/36 ( 47%) [1505/1759 ( 86%)]  Loss: 2.843 (3.55)  LR: 2.905e-03  Grad: 3.7652  max=0.4431(module.vfe.pfn_layers.0.linear.weight)  min: -0.6480(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6666, loss_cls=0.1233, loss_bbox=0.7905, matched_ious=0.5160, loss_iou=0.0942, loss_iou_reg=0.2258, d_time=0.01(0.01), f_time=1.25(1.27), b_time=1.26(1.27)  Time cost: 31:58/05:23 [10:30:33/11:54:52]  Acc_iter 29650       Data time: 0.01(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.27)
2025-09-05 00:19:37,424   INFO  Train:   17/36 ( 47%) [1555/1759 ( 88%)]  Loss: 2.956 (3.55)  LR: 2.903e-03  Grad: 4.1998  max=0.3836(module.vfe.pfn_layers.0.linear.weight)  min: -1.5442(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6968, loss_cls=0.1286, loss_bbox=0.7905, matched_ious=0.5137, loss_iou=0.0942, loss_iou_reg=0.2265, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.27)  Time cost: 33:00/04:19 [10:31:36/11:53:24]  Acc_iter 29700       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.27)
2025-09-05 00:20:40,805   INFO  Train:   17/36 ( 47%) [1605/1759 ( 91%)]  Loss: 3.128 (3.55)  LR: 2.901e-03  Grad: 4.1360  max=0.4324(module.vfe.pfn_layers.0.linear.weight)  min: -0.2306(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6613, loss_cls=0.1234, loss_bbox=0.7697, matched_ious=0.5245, loss_iou=0.0920, loss_iou_reg=0.2236, d_time=0.00(0.01), f_time=1.32(1.27), b_time=1.33(1.27)  Time cost: 34:04/03:16 [10:32:39/11:52:15]  Acc_iter 29750       Data time: 0.00(0.01)  Forward time: 1.32(1.27)  Batch time: 1.33(1.27)
2025-09-05 00:21:44,175   INFO  Train:   17/36 ( 47%) [1655/1759 ( 94%)]  Loss: 3.270 (3.55)  LR: 2.899e-03  Grad: 2.9767  max=0.4668(module.vfe.pfn_layers.0.linear.weight)  min: -0.4711(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6251, loss_cls=0.1229, loss_bbox=0.6742, matched_ious=0.5183, loss_iou=0.0933, loss_iou_reg=0.2268, d_time=0.01(0.01), f_time=1.27(1.27), b_time=1.28(1.27)  Time cost: 35:07/02:12 [10:33:42/11:51:06]  Acc_iter 29800       Data time: 0.01(0.01)  Forward time: 1.27(1.27)  Batch time: 1.28(1.27)
2025-09-05 00:22:47,090   INFO  Train:   17/36 ( 47%) [1705/1759 ( 97%)]  Loss: 3.582 (3.55)  LR: 2.896e-03  Grad: 3.2176  max=0.2148(module.vfe.pfn_layers.0.linear.weight)  min: -0.3239(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6544, loss_cls=0.1267, loss_bbox=0.7045, matched_ious=0.5225, loss_iou=0.0931, loss_iou_reg=0.2241, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.27)  Time cost: 36:10/01:08 [10:34:45/11:49:48]  Acc_iter 29850       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.27)
2025-09-05 00:23:50,478   INFO  Train:   17/36 ( 47%) [1755/1759 (100%)]  Loss: 2.783 (3.54)  LR: 2.894e-03  Grad: 2.9481  max=0.5016(module.vfe.pfn_layers.0.linear.weight)  min: -0.9427(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6426, loss_cls=0.1234, loss_bbox=0.7252, matched_ious=0.5259, loss_iou=0.0924, loss_iou_reg=0.2224, d_time=0.00(0.01), f_time=1.16(1.27), b_time=1.17(1.27)  Time cost: 37:13/00:05 [10:35:49/11:48:40]  Acc_iter 29900       Data time: 0.00(0.01)  Forward time: 1.16(1.27)  Batch time: 1.17(1.27)
2025-09-05 00:23:53,778   INFO  Train:   17/36 ( 47%) [1758/1759 (100%)]  Loss: 4.519 (3.54)  LR: 2.894e-03  Grad: 3.2074  max=1.0727(module.vfe.pfn_layers.0.linear.weight)  min: -0.6079(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6694, loss_cls=0.1227, loss_bbox=0.9671, matched_ious=0.5175, loss_iou=0.0924, loss_iou_reg=0.2265, d_time=0.00(0.01), f_time=0.70(1.27), b_time=0.70(1.27)  Time cost: 37:17/00:01 [10:35:52/11:48:26]  Acc_iter 29903       Data time: 0.00(0.01)  Forward time: 0.70(1.27)  Batch time: 0.70(1.27)

                                               [Aepochs:  47%|████▋     | 17/36 [10:35:52<11:49:16, 2239.83s/it]epochs:  47%|████▋     | 17/36 [10:35:53<11:49:17, 2239.89s/it]epochs:  47%|████▋     | 17/36 [10:35:53<11:49:18, 2239.90s/it]epochs:  47%|████▋     | 17/36 [10:35:53<11:49:17, 2239.88s/it]epochs:  47%|████▋     | 17/36 [10:35:53<11:49:17, 2239.88s/it]epochs:  47%|████▋     | 17/36 [10:35:53<11:49:17, 2239.88s/it]epochs:  47%|████▋     | 17/36 [10:35:53<11:49:17, 2239.88s/it]epochs:  47%|████▋     | 17/36 [10:35:53<11:49:17, 2239.87s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-05 00:23:59,291   INFO  Train:   18/36 ( 50%) [   0/1759 (  0%)]  Loss: 3.635 (3.63)  LR: 2.894e-03  Grad: 3.3703  max=1.3812(module.vfe.pfn_layers.0.linear.weight)  min: -1.0416(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5802, loss_cls=0.1081, loss_bbox=0.8483, matched_ious=0.5113, loss_iou=0.1035, loss_iou_reg=0.2182, d_time=1.47(1.47), f_time=2.66(2.66), b_time=4.13(4.13)  Time cost: 00:03/1:50:22 [10:35:58/34:57:02]  Acc_iter 29904       Data time: 1.47(1.47)  Forward time: 2.66(2.66)  Batch time: 4.13(4.13)
2025-09-05 00:24:57,452   INFO  Train:   18/36 ( 50%) [  46/1759 (  3%)]  Loss: 3.576 (3.54)  LR: 2.892e-03  Grad: 2.9325  max=0.5392(module.vfe.pfn_layers.0.linear.weight)  min: -0.1252(module.dense_head.prediction_head.dim.0.1.weight)  NaN: False  loss_hm=0.6536, loss_cls=0.1214, loss_bbox=0.7327, matched_ious=0.5198, loss_iou=0.0916, loss_iou_reg=0.2264, d_time=0.00(0.04), f_time=1.11(1.29), b_time=1.11(1.32)  Time cost: 01:01/37:36 [10:36:56/12:12:39]  Acc_iter 29950       Data time: 0.00(0.04)  Forward time: 1.11(1.29)  Batch time: 1.11(1.32)
2025-09-05 00:26:01,153   INFO  Train:   18/36 ( 50%) [  96/1759 (  5%)]  Loss: 3.577 (3.51)  LR: 2.890e-03  Grad: 4.1817  max=2.5867(module.vfe.pfn_layers.0.linear.weight)  min: -0.6221(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6511, loss_cls=0.1267, loss_bbox=0.7106, matched_ious=0.5169, loss_iou=0.0947, loss_iou_reg=0.2275, d_time=0.00(0.02), f_time=1.25(1.28), b_time=1.25(1.30)  Time cost: 02:05/35:53 [10:37:59/11:59:19]  Acc_iter 30000       Data time: 0.00(0.02)  Forward time: 1.25(1.28)  Batch time: 1.25(1.30)
2025-09-05 00:27:03,984   INFO  Train:   18/36 ( 50%) [ 146/1759 (  8%)]  Loss: 3.513 (3.51)  LR: 2.887e-03  Grad: 3.4213  max=0.2242(module.vfe.pfn_layers.0.linear.weight)  min: -0.4477(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6445, loss_cls=0.1224, loss_bbox=0.7612, matched_ious=0.5176, loss_iou=0.0921, loss_iou_reg=0.2252, d_time=0.00(0.02), f_time=1.36(1.27), b_time=1.36(1.28)  Time cost: 03:08/34:27 [10:39:02/11:50:59]  Acc_iter 30050       Data time: 0.00(0.02)  Forward time: 1.36(1.27)  Batch time: 1.36(1.28)
