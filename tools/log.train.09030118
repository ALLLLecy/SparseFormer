/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
2025-09-03 01:19:10,202   INFO  **********************Start logging**********************
2025-09-03 01:19:10,202   INFO  CUDA_VISIBLE_DEVICES=ALL
2025-09-03 01:19:10,202   INFO  Training in distributed mode : total_batch_size: 16
2025-09-03 01:19:10,203   INFO  cfg_file         cfgs/sparse_models/sparse_former_base.yaml
2025-09-03 01:19:10,203   INFO  batch_size       2
2025-09-03 01:19:10,203   INFO  epochs           36
2025-09-03 01:19:10,203   INFO  workers          12
2025-09-03 01:19:10,203   INFO  extra_tag        default
2025-09-03 01:19:10,203   INFO  ckpt             None
2025-09-03 01:19:10,203   INFO  pretrained_model None
2025-09-03 01:19:10,203   INFO  launcher         pytorch
2025-09-03 01:19:10,203   INFO  tcp_port         18888
2025-09-03 01:19:10,203   INFO  sync_bn          True
2025-09-03 01:19:10,203   INFO  fix_random_seed  False
2025-09-03 01:19:10,203   INFO  ckpt_save_interval 1
2025-09-03 01:19:10,203   INFO  local_rank       0
2025-09-03 01:19:10,203   INFO  max_ckpt_save_num 30
2025-09-03 01:19:10,203   INFO  merge_all_iters_to_one_epoch False
2025-09-03 01:19:10,203   INFO  set_cfgs         None
2025-09-03 01:19:10,203   INFO  max_waiting_mins 0
2025-09-03 01:19:10,203   INFO  start_epoch      0
2025-09-03 01:19:10,204   INFO  num_epochs_to_eval 0
2025-09-03 01:19:10,204   INFO  save_to_file     False
2025-09-03 01:19:10,204   INFO  use_tqdm_to_record False
2025-09-03 01:19:10,204   INFO  logger_iter_interval 50
2025-09-03 01:19:10,204   INFO  ckpt_save_time_interval 300
2025-09-03 01:19:10,204   INFO  wo_gpu_stat      True
2025-09-03 01:19:10,204   INFO  use_amp          False
2025-09-03 01:19:10,204   INFO  eval_map         False
2025-09-03 01:19:10,204   INFO  dataset          nuscenes
2025-09-03 01:19:10,204   INFO  root_dir         /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2
2025-09-03 01:19:10,204   INFO  output_dir       /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_.0diff_dense_hm_.1lr_.1wd
2025-09-03 01:19:10,204   INFO  cfg.ROOT_DIR: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2
2025-09-03 01:19:10,204   INFO  cfg.LOCAL_RANK: 0
2025-09-03 01:19:10,204   INFO  cfg.CLASS_NAMES: ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']
2025-09-03 01:19:10,204   INFO  ----------- DATA_CONFIG -----------
2025-09-03 01:19:10,204   INFO  cfg.DATA_CONFIG.DATASET: NuScenesDataset
2025-09-03 01:19:10,204   INFO  cfg.DATA_CONFIG.DATA_PATH: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/data/nuscenes
2025-09-03 01:19:10,204   INFO  cfg.DATA_CONFIG.VERSION: v1.0-trainval
2025-09-03 01:19:10,204   INFO  cfg.DATA_CONFIG.MAX_SWEEPS: 10
2025-09-03 01:19:10,205   INFO  cfg.DATA_CONFIG.PRED_VELOCITY: True
2025-09-03 01:19:10,205   INFO  cfg.DATA_CONFIG.SET_NAN_VELOCITY_TO_ZEROS: True
2025-09-03 01:19:10,205   INFO  cfg.DATA_CONFIG.FILTER_MIN_POINTS_IN_GT: 1
2025-09-03 01:19:10,205   INFO  ----------- DATA_SPLIT -----------
2025-09-03 01:19:10,205   INFO  cfg.DATA_CONFIG.DATA_SPLIT.train: train
2025-09-03 01:19:10,205   INFO  cfg.DATA_CONFIG.DATA_SPLIT.test: val
2025-09-03 01:19:10,205   INFO  ----------- INFO_PATH -----------
2025-09-03 01:19:10,205   INFO  cfg.DATA_CONFIG.INFO_PATH.train: ['nuscenes_infos_10sweeps_train.pkl']
2025-09-03 01:19:10,205   INFO  cfg.DATA_CONFIG.INFO_PATH.test: ['nuscenes_infos_10sweeps_val.pkl']
2025-09-03 01:19:10,205   INFO  cfg.DATA_CONFIG.POINT_CLOUD_RANGE: [-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]
2025-09-03 01:19:10,205   INFO  cfg.DATA_CONFIG.BALANCED_RESAMPLING: False
2025-09-03 01:19:10,205   INFO  ----------- DATA_AUGMENTOR -----------
2025-09-03 01:19:10,205   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.DISABLE_AUG_LIST: ['placeholder']
2025-09-03 01:19:10,205   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.AUG_CONFIG_LIST: [{'NAME': 'gt_sampling', 'DB_INFO_PATH': ['nuscenes_dbinfos_10sweeps_withvelo.pkl'], 'USE_SHARED_MEMORY': True, 'DB_DATA_PATH': ['nuscenes_10sweeps_withvelo_lidar.npy'], 'PREPARE': {'filter_by_min_points': ['car:5', 'truck:5', 'construction_vehicle:5', 'bus:5', 'trailer:5', 'barrier:5', 'motorcycle:5', 'bicycle:5', 'pedestrian:5', 'traffic_cone:5']}, 'SAMPLE_GROUPS': ['car:2', 'truck:3', 'construction_vehicle:7', 'bus:4', 'trailer:6', 'barrier:2', 'motorcycle:6', 'bicycle:6', 'pedestrian:2', 'traffic_cone:2'], 'NUM_POINT_FEATURES': 5, 'DATABASE_WITH_FAKELIDAR': False, 'REMOVE_EXTRA_WIDTH': [0.0, 0.0, 0.0], 'LIMIT_WHOLE_SCENE': True}, {'NAME': 'random_world_flip', 'ALONG_AXIS_LIST': ['x', 'y']}, {'NAME': 'random_world_rotation', 'WORLD_ROT_ANGLE': [-0.78539816, 0.78539816]}, {'NAME': 'random_world_scaling', 'WORLD_SCALE_RANGE': [0.9, 1.1]}, {'NAME': 'random_world_translation', 'NOISE_TRANSLATE_STD': [0.5, 0.5, 0.5]}]
2025-09-03 01:19:10,236   INFO  ----------- POINT_FEATURE_ENCODING -----------
2025-09-03 01:19:10,237   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.encoding_type: absolute_coordinates_encoding
2025-09-03 01:19:10,237   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.used_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-09-03 01:19:10,237   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.src_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-09-03 01:19:10,237   INFO  cfg.DATA_CONFIG.DATA_PROCESSOR: [{'NAME': 'mask_points_and_boxes_outside_range', 'REMOVE_OUTSIDE_BOXES': True}, {'NAME': 'shuffle_points', 'SHUFFLE_ENABLED': {'train': True, 'test': True}}, {'NAME': 'transform_points_to_voxels_placeholder', 'VOXEL_SIZE': [0.3, 0.3, 0.2]}]
2025-09-03 01:19:10,237   INFO  cfg.DATA_CONFIG._BASE_CONFIG_: cfgs/dataset_configs/nuscenes_dataset.yaml
2025-09-03 01:19:10,237   INFO  ----------- SAMPLED_INTERVAL -----------
2025-09-03 01:19:10,237   INFO  cfg.DATA_CONFIG.SAMPLED_INTERVAL.train: 1
2025-09-03 01:19:10,237   INFO  cfg.DATA_CONFIG.SAMPLED_INTERVAL.test: 1
2025-09-03 01:19:10,237   INFO  ----------- MODEL -----------
2025-09-03 01:19:10,238   INFO  cfg.MODEL.NAME: TransFusion
2025-09-03 01:19:10,238   INFO  ----------- VFE -----------
2025-09-03 01:19:10,238   INFO  cfg.MODEL.VFE.NAME: DynamicVoxelVFE
2025-09-03 01:19:10,238   INFO  cfg.MODEL.VFE.WITH_DISTANCE: False
2025-09-03 01:19:10,238   INFO  cfg.MODEL.VFE.USE_ABSLOTE_XYZ: True
2025-09-03 01:19:10,238   INFO  cfg.MODEL.VFE.USE_NORM: True
2025-09-03 01:19:10,238   INFO  cfg.MODEL.VFE.NUM_FILTERS: [64, 64]
2025-09-03 01:19:10,238   INFO  ----------- BACKBONE_3D -----------
2025-09-03 01:19:10,238   INFO  cfg.MODEL.BACKBONE_3D.NAME: FSHNet_nusc
2025-09-03 01:19:10,238   INFO  cfg.MODEL.BACKBONE_3D.FEATURE_DIM: 128
2025-09-03 01:19:10,238   INFO  ----------- SPENCODER -----------
2025-09-03 01:19:10,238   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.DIM: 3
2025-09-03 01:19:10,238   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.NUM_LAYERS: 3
2025-09-03 01:19:10,238   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.ORDERS: [['z', 'z-trans'], ['hilbert', 'hilbert-trans'], ['x', 'y']]
2025-09-03 01:19:10,238   INFO  ----------- SMSA -----------
2025-09-03 01:19:10,239   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.EMBED_DIM: 128
2025-09-03 01:19:10,239   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DEPTH: 9
2025-09-03 01:19:10,239   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.NUM_LEVELS: 3
2025-09-03 01:19:10,239   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.WINDOW_SHAPE: [9, 9, 5]
2025-09-03 01:19:10,239   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DROP_PATH: 0.2
2025-09-03 01:19:10,239   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.SPATIAL_ENHANCE: True
2025-09-03 01:19:10,239   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.FUSED_ADD_NORM: True
2025-09-03 01:19:10,239   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.RMS_NORM: True
2025-09-03 01:19:10,239   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.NORM_EPSILON: 1e-05
2025-09-03 01:19:10,239   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.RESIDUAL_IN_FP32: True
2025-09-03 01:19:10,239   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DIFF_COEF: 0.0
2025-09-03 01:19:10,239   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DIFF_KERNEL: [7, 5, 3]
2025-09-03 01:19:10,239   INFO  cfg.MODEL.BACKBONE_3D.AFD_FEATURE_DIM: 128
2025-09-03 01:19:10,239   INFO  cfg.MODEL.BACKBONE_3D.AFD_NUM_LAYERS: 4
2025-09-03 01:19:10,240   INFO  cfg.MODEL.BACKBONE_3D.AFD_NUM_SBB: [2, 1, 1]
2025-09-03 01:19:10,240   INFO  cfg.MODEL.BACKBONE_3D.AFD_DOWN_STRIDE: [1, 2, 2]
2025-09-03 01:19:10,240   INFO  cfg.MODEL.BACKBONE_3D.AFD_DOWN_KERNEL_SIZE: [3, 3, 3]
2025-09-03 01:19:10,240   INFO  cfg.MODEL.BACKBONE_3D.AFD: True
2025-09-03 01:19:10,240   INFO  cfg.MODEL.BACKBONE_3D.FG_THRESHOLD: 0.2
2025-09-03 01:19:10,240   INFO  cfg.MODEL.BACKBONE_3D.FEATMAP_STRIDE: 2
2025-09-03 01:19:10,240   INFO  cfg.MODEL.BACKBONE_3D.DETACH_FEATURE: True
2025-09-03 01:19:10,240   INFO  cfg.MODEL.BACKBONE_3D.GREOUP_POOLING_KERNEL_SIZE: [9, 15, 5, 5]
2025-09-03 01:19:10,240   INFO  cfg.MODEL.BACKBONE_3D.GROUP_CLASS_NAMES: [['car', 'truck', 'construction_vehicle'], ['bus', 'trailer'], ['barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']]
2025-09-03 01:19:10,240   INFO  ----------- DENSE_HEAD -----------
2025-09-03 01:19:10,240   INFO  cfg.MODEL.DENSE_HEAD.CLASS_AGNOSTIC: False
2025-09-03 01:19:10,240   INFO  cfg.MODEL.DENSE_HEAD.NAME: TransFusionHead
2025-09-03 01:19:10,240   INFO  cfg.MODEL.DENSE_HEAD.QUERY_RADIUS: 20
2025-09-03 01:19:10,240   INFO  cfg.MODEL.DENSE_HEAD.QUERY_LOCAL: True
2025-09-03 01:19:10,240   INFO  cfg.MODEL.DENSE_HEAD.USE_BIAS_BEFORE_NORM: True
2025-09-03 01:19:10,240   INFO  cfg.MODEL.DENSE_HEAD.INPUT_FEATURES: 128
2025-09-03 01:19:10,241   INFO  cfg.MODEL.DENSE_HEAD.NUM_PROPOSALS: 600
2025-09-03 01:19:10,241   INFO  cfg.MODEL.DENSE_HEAD.HIDDEN_CHANNEL: 128
2025-09-03 01:19:10,241   INFO  cfg.MODEL.DENSE_HEAD.NUM_CLASSES: 10
2025-09-03 01:19:10,241   INFO  cfg.MODEL.DENSE_HEAD.NUM_HEADS: 8
2025-09-03 01:19:10,241   INFO  cfg.MODEL.DENSE_HEAD.NMS_KERNEL_SIZE: 3
2025-09-03 01:19:10,241   INFO  cfg.MODEL.DENSE_HEAD.FFN_CHANNEL: 256
2025-09-03 01:19:10,241   INFO  cfg.MODEL.DENSE_HEAD.DROPOUT: 0.1
2025-09-03 01:19:10,241   INFO  cfg.MODEL.DENSE_HEAD.BN_MOMENTUM: 0.1
2025-09-03 01:19:10,241   INFO  cfg.MODEL.DENSE_HEAD.ACTIVATION: relu
2025-09-03 01:19:10,241   INFO  cfg.MODEL.DENSE_HEAD.NUM_HM_CONV: 2
2025-09-03 01:19:10,241   INFO  ----------- SEPARATE_HEAD_CFG -----------
2025-09-03 01:19:10,241   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_ORDER: ['center', 'height', 'dim', 'rot', 'vel', 'iou']
2025-09-03 01:19:10,241   INFO  ----------- HEAD_DICT -----------
2025-09-03 01:19:10,241   INFO  ----------- center -----------
2025-09-03 01:19:10,241   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.out_channels: 2
2025-09-03 01:19:10,241   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.num_conv: 2
2025-09-03 01:19:10,241   INFO  ----------- height -----------
2025-09-03 01:19:10,242   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.height.out_channels: 1
2025-09-03 01:19:10,242   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.height.num_conv: 2
2025-09-03 01:19:10,242   INFO  ----------- dim -----------
2025-09-03 01:19:10,242   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.out_channels: 3
2025-09-03 01:19:10,242   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.num_conv: 2
2025-09-03 01:19:10,242   INFO  ----------- rot -----------
2025-09-03 01:19:10,242   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.out_channels: 2
2025-09-03 01:19:10,242   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.num_conv: 2
2025-09-03 01:19:10,242   INFO  ----------- vel -----------
2025-09-03 01:19:10,242   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.out_channels: 2
2025-09-03 01:19:10,242   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.num_conv: 2
2025-09-03 01:19:10,242   INFO  ----------- iou -----------
2025-09-03 01:19:10,242   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.iou.out_channels: 1
2025-09-03 01:19:10,242   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.iou.num_conv: 2
2025-09-03 01:19:10,242   INFO  ----------- TARGET_ASSIGNER_CONFIG -----------
2025-09-03 01:19:10,242   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.FEATURE_MAP_STRIDE: 2
2025-09-03 01:19:10,243   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.DATASET: nuScenes
2025-09-03 01:19:10,243   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.GAUSSIAN_OVERLAP: 0.1
2025-09-03 01:19:10,243   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.MIN_RADIUS: 2
2025-09-03 01:19:10,243   INFO  ----------- HUNGARIAN_ASSIGNER -----------
2025-09-03 01:19:10,243   INFO  ----------- cls_cost -----------
2025-09-03 01:19:10,243   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.gamma: 2.0
2025-09-03 01:19:10,243   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.alpha: 0.25
2025-09-03 01:19:10,243   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.weight: 0.15
2025-09-03 01:19:10,243   INFO  ----------- reg_cost -----------
2025-09-03 01:19:10,243   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.reg_cost.weight: 0.25
2025-09-03 01:19:10,243   INFO  ----------- iou_cost -----------
2025-09-03 01:19:10,243   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.iou_cost.weight: 0.25
2025-09-03 01:19:10,243   INFO  ----------- LOSS_CONFIG -----------
2025-09-03 01:19:10,243   INFO  ----------- LOSS_WEIGHTS -----------
2025-09-03 01:19:10,243   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.cls_weight: 1.0
2025-09-03 01:19:10,243   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.bbox_weight: 0.25
2025-09-03 01:19:10,243   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.hm_weight: 1.0
2025-09-03 01:19:10,244   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.iou_weight: 0.5
2025-09-03 01:19:10,244   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.iou_reg_weight: 0.5
2025-09-03 01:19:10,244   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.code_weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2]
2025-09-03 01:19:10,244   INFO  ----------- LOSS_CLS -----------
2025-09-03 01:19:10,244   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.use_sigmoid: True
2025-09-03 01:19:10,244   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.gamma: 2.0
2025-09-03 01:19:10,244   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.alpha: 0.25
2025-09-03 01:19:10,244   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_IOU: True
2025-09-03 01:19:10,244   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_IOU_REG: True
2025-09-03 01:19:10,244   INFO  ----------- POST_PROCESSING -----------
2025-09-03 01:19:10,244   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.SCORE_THRESH: 0.0
2025-09-03 01:19:10,244   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.POST_CENTER_RANGE: [-61.2, -61.2, -10.0, 61.2, 61.2, 10.0]
2025-09-03 01:19:10,244   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.USE_IOU_TO_RECTIFY_SCORE: True
2025-09-03 01:19:10,244   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.IOU_RECTIFIER: [0.5]
2025-09-03 01:19:10,244   INFO  ----------- NMS_CONFIG -----------
2025-09-03 01:19:10,244   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_TYPE: nms_gpu
2025-09-03 01:19:10,245   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_THRESH: 0.2
2025-09-03 01:19:10,245   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_PRE_MAXSIZE: 1000
2025-09-03 01:19:10,245   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_POST_MAXSIZE: 100
2025-09-03 01:19:10,245   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.SCORE_THRES: 0.0
2025-09-03 01:19:10,245   INFO  ----------- POST_PROCESSING -----------
2025-09-03 01:19:10,245   INFO  cfg.MODEL.POST_PROCESSING.RECALL_THRESH_LIST: [0.3, 0.5, 0.7]
2025-09-03 01:19:10,245   INFO  cfg.MODEL.POST_PROCESSING.SCORE_THRESH: 0.1
2025-09-03 01:19:10,245   INFO  cfg.MODEL.POST_PROCESSING.OUTPUT_RAW_SCORE: False
2025-09-03 01:19:10,245   INFO  cfg.MODEL.POST_PROCESSING.EVAL_METRIC: kitti
2025-09-03 01:19:10,245   INFO  ----------- NMS_CONFIG -----------
2025-09-03 01:19:10,245   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.MULTI_CLASSES_NMS: True
2025-09-03 01:19:10,245   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_TYPE: nms_gpu
2025-09-03 01:19:10,245   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_THRESH: 0.2
2025-09-03 01:19:10,245   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_PRE_MAXSIZE: 1000
2025-09-03 01:19:10,245   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_POST_MAXSIZE: 83
2025-09-03 01:19:10,246   INFO  ----------- OPTIMIZATION -----------
2025-09-03 01:19:10,246   INFO  cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU: 2
2025-09-03 01:19:10,246   INFO  cfg.OPTIMIZATION.NUM_EPOCHS: 36
2025-09-03 01:19:10,246   INFO  cfg.OPTIMIZATION.OPTIMIZER: adam_onecycle
2025-09-03 01:19:10,246   INFO  cfg.OPTIMIZATION.LR: 0.003
2025-09-03 01:19:10,246   INFO  cfg.OPTIMIZATION.WEIGHT_DECAY: 0.03
2025-09-03 01:19:10,246   INFO  cfg.OPTIMIZATION.MOMENTUM: 0.9
2025-09-03 01:19:10,246   INFO  cfg.OPTIMIZATION.MOMS: [0.95, 0.85]
2025-09-03 01:19:10,246   INFO  cfg.OPTIMIZATION.PCT_START: 0.4
2025-09-03 01:19:10,246   INFO  cfg.OPTIMIZATION.DIV_FACTOR: 10
2025-09-03 01:19:10,246   INFO  cfg.OPTIMIZATION.DECAY_STEP_LIST: [35, 45]
2025-09-03 01:19:10,246   INFO  cfg.OPTIMIZATION.LR_DECAY: 0.1
2025-09-03 01:19:10,246   INFO  cfg.OPTIMIZATION.LR_CLIP: 1e-07
2025-09-03 01:19:10,247   INFO  cfg.OPTIMIZATION.LR_WARMUP: False
2025-09-03 01:19:10,247   INFO  cfg.OPTIMIZATION.WARMUP_EPOCH: 1
2025-09-03 01:19:10,247   INFO  cfg.OPTIMIZATION.GRAD_NORM_CLIP: 10
2025-09-03 01:19:10,247   INFO  cfg.OPTIMIZATION.LOSS_SCALE_FP16: 32
2025-09-03 01:19:10,247   INFO  ----------- HOOK -----------
2025-09-03 01:19:10,247   INFO  ----------- DisableAugmentationHook -----------
2025-09-03 01:19:10,247   INFO  cfg.HOOK.DisableAugmentationHook.DISABLE_AUG_LIST: ['gt_sampling']
2025-09-03 01:19:10,247   INFO  cfg.HOOK.DisableAugmentationHook.NUM_LAST_EPOCHS: 5
2025-09-03 01:19:10,247   INFO  cfg.TAG: sparse_former_base
2025-09-03 01:19:10,247   INFO  cfg.EXP_GROUP_PATH: sparse_models
2025-09-03 01:19:10,247   INFO  cfg.OUTPUT_DIR: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_.0diff_dense_hm_.1lr_.1wd
2025-09-03 01:19:10,257   INFO  ----------- Create dataloader & network & optimizer -----------
2025-09-03 01:19:15,630   INFO  Database filter by min points car: 339949 => 294532
2025-09-03 01:19:15,642   INFO  Database filter by min points truck: 65262 => 60344
2025-09-03 01:19:15,644   INFO  Database filter by min points construction_vehicle: 11050 => 10589
2025-09-03 01:19:15,645   INFO  Database filter by min points bus: 12286 => 11619
2025-09-03 01:19:15,649   INFO  Database filter by min points trailer: 19202 => 17934
2025-09-03 01:19:15,664   INFO  Database filter by min points barrier: 107507 => 101993
2025-09-03 01:19:15,665   INFO  Database filter by min points motorcycle: 8846 => 8055
2025-09-03 01:19:15,666   INFO  Database filter by min points bicycle: 8185 => 7531
2025-09-03 01:19:15,687   INFO  Database filter by min points pedestrian: 161928 => 148520
2025-09-03 01:19:15,696   INFO  Database filter by min points traffic_cone: 62964 => 55504
2025-09-03 01:19:15,696   INFO  Loading GT database to shared memory
eflops66:23:23 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:23:23 [0] NCCL INFO Bootstrap : Using bond0:10.16.10.214<0>
eflops66:23:23 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops66:23:23 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops66:23:23 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.18.5+cuda11.8
eflops66:27:27 [4] NCCL INFO cudaDriverVersion 12050
eflops66:31:31 [7] NCCL INFO cudaDriverVersion 12050
eflops66:29:29 [6] NCCL INFO cudaDriverVersion 12050
eflops66:24:24 [1] NCCL INFO cudaDriverVersion 12050
eflops66:27:27 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:28:28 [5] NCCL INFO cudaDriverVersion 12050
eflops66:29:29 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:31:31 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:26:26 [3] NCCL INFO cudaDriverVersion 12050
eflops66:24:24 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:25:25 [2] NCCL INFO cudaDriverVersion 12050
eflops66:28:28 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:26:26 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:25:25 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:29:29 [6] NCCL INFO Bootstrap : Using bond0:10.16.10.214<0>
eflops66:24:24 [1] NCCL INFO Bootstrap : Using bond0:10.16.10.214<0>
eflops66:26:26 [3] NCCL INFO Bootstrap : Using bond0:10.16.10.214<0>
eflops66:31:31 [7] NCCL INFO Bootstrap : Using bond0:10.16.10.214<0>
eflops66:27:27 [4] NCCL INFO Bootstrap : Using bond0:10.16.10.214<0>
eflops66:28:28 [5] NCCL INFO Bootstrap : Using bond0:10.16.10.214<0>
eflops66:24:24 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops66:24:24 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops66:25:25 [2] NCCL INFO Bootstrap : Using bond0:10.16.10.214<0>
eflops66:31:31 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops66:26:26 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops66:31:31 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops66:26:26 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops66:29:29 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops66:29:29 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops66:27:27 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops66:27:27 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops66:28:28 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops66:28:28 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops66:25:25 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops66:25:25 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops66:23:101 [0] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops66:23:101 [0] NCCL INFO P2P plugin IBext
eflops66:23:101 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:26:102 [3] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops66:26:102 [3] NCCL INFO P2P plugin IBext
eflops66:26:102 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops66:23:101 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops66:23:101 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops66:23:101 [0] NCCL INFO NET/IB : No device found.
eflops66:23:101 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops66:23:101 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:23:101 [0] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.214<0>
eflops66:23:101 [0] NCCL INFO Using network Socket
eflops66:27:100 [4] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops66:27:100 [4] NCCL INFO P2P plugin IBext
eflops66:27:100 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops66:26:102 [3] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops66:26:102 [3] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops66:26:102 [3] NCCL INFO NET/IB : No device found.
eflops66:26:102 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops66:26:102 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:24:99 [1] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops66:24:99 [1] NCCL INFO P2P plugin IBext
eflops66:24:99 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:25:106 [2] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops66:25:106 [2] NCCL INFO P2P plugin IBext
eflops66:25:106 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:28:105 [5] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops66:28:105 [5] NCCL INFO P2P plugin IBext
eflops66:28:105 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops66:27:100 [4] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops66:27:100 [4] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops66:27:100 [4] NCCL INFO NET/IB : No device found.
eflops66:27:100 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops66:27:100 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:29:104 [6] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops66:29:104 [6] NCCL INFO P2P plugin IBext
eflops66:29:104 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:26:102 [3] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.214<0>
eflops66:26:102 [3] NCCL INFO Using network Socket
eflops66:27:100 [4] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.214<0>
eflops66:27:100 [4] NCCL INFO Using network Socket

eflops66:25:106 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops66:24:99 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops66:25:106 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0

eflops66:24:99 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops66:25:106 [2] NCCL INFO NET/IB : No device found.
eflops66:24:99 [1] NCCL INFO NET/IB : No device found.
eflops66:25:106 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops66:25:106 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops66:28:105 [5] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops66:28:105 [5] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops66:28:105 [5] NCCL INFO NET/IB : No device found.
eflops66:24:99 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops66:28:105 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops66:24:99 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:28:105 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:25:106 [2] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.214<0>
eflops66:25:106 [2] NCCL INFO Using network Socket
eflops66:28:105 [5] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.214<0>
eflops66:28:105 [5] NCCL INFO Using network Socket

eflops66:29:104 [6] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed
eflops66:24:99 [1] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.214<0>
eflops66:24:99 [1] NCCL INFO Using network Socket

eflops66:29:104 [6] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops66:29:104 [6] NCCL INFO NET/IB : No device found.
eflops66:29:104 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops66:29:104 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:29:104 [6] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.214<0>
eflops66:29:104 [6] NCCL INFO Using network Socket
eflops66:31:103 [7] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops66:31:103 [7] NCCL INFO P2P plugin IBext
eflops66:31:103 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops66:31:103 [7] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops66:31:103 [7] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops66:31:103 [7] NCCL INFO NET/IB : No device found.
eflops66:31:103 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops66:31:103 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops66:31:103 [7] NCCL INFO NET/Socket : Using [0]bond0:10.16.10.214<0>
eflops66:31:103 [7] NCCL INFO Using network Socket
eflops66:28:105 [5] NCCL INFO comm 0x123db070 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId a6000 commId 0x891c06cd4d55505d - Init START
eflops66:27:100 [4] NCCL INFO comm 0x11f89a10 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId a5000 commId 0x891c06cd4d55505d - Init START
eflops66:31:103 [7] NCCL INFO comm 0x110d6240 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId ad000 commId 0x891c06cd4d55505d - Init START
eflops66:29:104 [6] NCCL INFO comm 0x12352860 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a9000 commId 0x891c06cd4d55505d - Init START
eflops66:26:102 [3] NCCL INFO comm 0x120e7520 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 2d000 commId 0x891c06cd4d55505d - Init START
eflops66:25:106 [2] NCCL INFO comm 0x10796e20 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 29000 commId 0x891c06cd4d55505d - Init START
eflops66:24:99 [1] NCCL INFO comm 0x109d7630 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 26000 commId 0x891c06cd4d55505d - Init START
eflops66:23:101 [0] NCCL INFO comm 0x1119ede0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 25000 commId 0x891c06cd4d55505d - Init START
eflops66:24:99 [1] NCCL INFO Setting affinity for GPU 1 to ff0000,00000000,00ff0000
eflops66:27:100 [4] NCCL INFO Setting affinity for GPU 4 to ff0000,00000000,00ff0000,00000000
eflops66:28:105 [5] NCCL INFO Setting affinity for GPU 5 to ff0000,00000000,00ff0000,00000000
eflops66:25:106 [2] NCCL INFO Setting affinity for GPU 2 to ff0000,00000000,00ff0000
eflops66:26:102 [3] NCCL INFO Setting affinity for GPU 3 to ff0000,00000000,00ff0000
eflops66:23:101 [0] NCCL INFO Setting affinity for GPU 0 to ff0000,00000000,00ff0000
eflops66:29:104 [6] NCCL INFO Setting affinity for GPU 6 to ff0000,00000000,00ff0000,00000000
eflops66:31:103 [7] NCCL INFO Setting affinity for GPU 7 to ff0000,00000000,00ff0000,00000000
eflops66:31:103 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6
eflops66:31:103 [7] NCCL INFO P2P Chunksize set to 131072
eflops66:28:105 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4
eflops66:29:104 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5
eflops66:28:105 [5] NCCL INFO P2P Chunksize set to 131072
eflops66:23:101 [0] NCCL INFO Channel 00/04 :    0   1   2   3   4   5   6   7
eflops66:24:99 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0
eflops66:29:104 [6] NCCL INFO P2P Chunksize set to 131072
eflops66:23:101 [0] NCCL INFO Channel 01/04 :    0   1   2   3   4   5   6   7
eflops66:23:101 [0] NCCL INFO Channel 02/04 :    0   1   2   3   4   5   6   7
eflops66:24:99 [1] NCCL INFO P2P Chunksize set to 131072
eflops66:23:101 [0] NCCL INFO Channel 03/04 :    0   1   2   3   4   5   6   7
eflops66:23:101 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1
eflops66:23:101 [0] NCCL INFO P2P Chunksize set to 131072
eflops66:26:102 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2
eflops66:25:106 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1
eflops66:26:102 [3] NCCL INFO P2P Chunksize set to 131072
eflops66:25:106 [2] NCCL INFO P2P Chunksize set to 131072
eflops66:27:100 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3
eflops66:27:100 [4] NCCL INFO P2P Chunksize set to 131072
eflops66:31:103 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/IPC
eflops66:29:104 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC
eflops66:31:103 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/IPC
eflops66:23:101 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
eflops66:26:102 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC
eflops66:24:99 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
eflops66:29:104 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC
eflops66:31:103 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/IPC
eflops66:27:100 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
eflops66:23:101 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
eflops66:26:102 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC
eflops66:24:99 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
eflops66:29:104 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/IPC
eflops66:31:103 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/IPC
eflops66:27:100 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
eflops66:23:101 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC
eflops66:26:102 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/IPC
eflops66:24:99 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/IPC
eflops66:29:104 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/IPC
eflops66:27:100 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/IPC
eflops66:23:101 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC
eflops66:26:102 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/IPC
eflops66:28:105 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC
eflops66:24:99 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC
eflops66:27:100 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/IPC
eflops66:28:105 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC
eflops66:25:106 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
eflops66:28:105 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/IPC
eflops66:25:106 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
eflops66:28:105 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/IPC
eflops66:25:106 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC
eflops66:25:106 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/IPC
eflops66:31:103 [7] NCCL INFO Connected all rings
eflops66:31:103 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC
eflops66:31:103 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC
eflops66:31:103 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/IPC
eflops66:31:103 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/IPC
eflops66:23:101 [0] NCCL INFO Connected all rings
eflops66:28:105 [5] NCCL INFO Connected all rings
eflops66:29:104 [6] NCCL INFO Connected all rings
eflops66:27:100 [4] NCCL INFO Connected all rings
eflops66:25:106 [2] NCCL INFO Connected all rings
eflops66:26:102 [3] NCCL INFO Connected all rings
eflops66:28:105 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
eflops66:29:104 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC
eflops66:27:100 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC
eflops66:25:106 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
eflops66:28:105 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
eflops66:29:104 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC
eflops66:27:100 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC
eflops66:25:106 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
eflops66:28:105 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/IPC
eflops66:29:104 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/IPC
eflops66:27:100 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/IPC
eflops66:28:105 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/IPC
eflops66:25:106 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC
eflops66:26:102 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
eflops66:27:100 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/IPC
eflops66:29:104 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/IPC
eflops66:25:106 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC
eflops66:26:102 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
eflops66:24:99 [1] NCCL INFO Connected all rings
eflops66:26:102 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC
eflops66:26:102 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/IPC
eflops66:31:103 [7] NCCL INFO Connected all trees
eflops66:31:103 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops66:31:103 [7] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
eflops66:24:99 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
eflops66:29:104 [6] NCCL INFO Connected all trees
eflops66:29:104 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops66:29:104 [6] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
eflops66:28:105 [5] NCCL INFO Connected all trees
eflops66:28:105 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops66:28:105 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
eflops66:24:99 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
eflops66:24:99 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC
eflops66:24:99 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC
eflops66:26:102 [3] NCCL INFO Connected all trees
eflops66:26:102 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops66:26:102 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
eflops66:27:100 [4] NCCL INFO Connected all trees
eflops66:27:100 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops66:27:100 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
eflops66:23:101 [0] NCCL INFO Connected all trees
eflops66:23:101 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops66:23:101 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
eflops66:25:106 [2] NCCL INFO Connected all trees
eflops66:25:106 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops66:25:106 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
eflops66:24:99 [1] NCCL INFO Connected all trees
eflops66:24:99 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops66:24:99 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
eflops66:23:101 [0] NCCL INFO comm 0x1119ede0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 25000 commId 0x891c06cd4d55505d - Init COMPLETE
eflops66:27:100 [4] NCCL INFO comm 0x11f89a10 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId a5000 commId 0x891c06cd4d55505d - Init COMPLETE
eflops66:26:102 [3] NCCL INFO comm 0x120e7520 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 2d000 commId 0x891c06cd4d55505d - Init COMPLETE
eflops66:25:106 [2] NCCL INFO comm 0x10796e20 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 29000 commId 0x891c06cd4d55505d - Init COMPLETE
eflops66:31:103 [7] NCCL INFO comm 0x110d6240 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId ad000 commId 0x891c06cd4d55505d - Init COMPLETE
eflops66:29:104 [6] NCCL INFO comm 0x12352860 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a9000 commId 0x891c06cd4d55505d - Init COMPLETE
eflops66:24:99 [1] NCCL INFO comm 0x109d7630 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 26000 commId 0x891c06cd4d55505d - Init COMPLETE
eflops66:28:105 [5] NCCL INFO comm 0x123db070 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId a6000 commId 0x891c06cd4d55505d - Init COMPLETE
2025-09-03 01:19:29,879   INFO  GT database has been saved to shared memory
2025-09-03 01:19:30,154   INFO  Loading NuScenes dataset
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2025-09-03 01:19:32,160   INFO  Total samples for NuScenes dataset: 28130
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]2025-09-03 01:19:32,574   INFO  ----------- Model TransFusion created, param count: 27099970 -----------
2025-09-03 01:19:32,575   INFO  DistributedDataParallel(
  (module): TransFusion(
    (vfe): DynamicVoxelVFE(
      (pfn_layers): ModuleList(
        (0): PFNLayerV2(
          (linear): Linear(in_features=11, out_features=32, bias=False)
          (norm): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
        )
        (1): PFNLayerV2(
          (linear): Linear(in_features=64, out_features=64, bias=False)
          (norm): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
        )
      )
    )
    (backbone_3d): FSHNet_nusc(
      (cpe): SparseSequential(
        (0): SparseSequential(
          (0): SubMConv3d(64, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (2): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
      )
      (down1): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (2): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (down2): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (2): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (down3): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (2): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (stage): SPEncoder(
        (blocks): ModuleList(
          (0): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): ZOrderSerialization()
            )
          )
          (1): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): HilbertSerialization()
            )
          )
          (2): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): FlattenWindowsSerialization()
            )
          )
        )
        (embeddings): ModuleList(
          (0-2): 3 x MSSubConvEmbeddingLearned(
            (stem): ModuleList(
              (0-2): 3 x SparseBasicBlock(
                (conv1): SubMConv3d(3, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
              )
            )
          )
        )
      )
      (app): AttnPillarPool(
        (query_func): SparseMaxPool3d(kernel_size=[10, 1, 1], stride=[10, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], algo=ConvAlgo.MaskImplicitGemm)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (pos_embedding): Embedding(10, 128)
      )
      (cls_conv): SparseSequential(
        (0): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): SubMConv2d(128, 3, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
      )
      (afd_layers): ModuleList(
        (0-3): 4 x SEDLayer(
          (encoder): ModuleList(
            (0): SparseSequential(
              (0): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
              (1): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
            )
            (1-2): 2 x SparseSequential(
              (0): SparseSequential(
                (0): SparseConv2d(128, 128, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
            )
          )
          (decoder): ModuleList(
            (0-1): 2 x SparseSequential(
              (0): SparseInverseConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
              (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (decoder_norm): ModuleList(
            (0-1): 2 x SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (shared_conv): SparseSequential(
        (0): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (map_to_bev_module): None
    (pfe): None
    (backbone_2d): None
    (dense_head): TransFusionHead(
      (loss_cls): SigmoidFocalClassificationLoss()
      (loss_bbox): L1Loss()
      (loss_heatmap): GaussianFocalLoss()
      (shared_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (heatmap_head): Sequential(
        (0): BasicBlock2D(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): Conv2d(128, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (class_encoding): Conv1d(10, 128, kernel_size=(1,), stride=(1,))
      (decoder): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_posembed): PositionEmbeddingLearned(
          (position_embedding_head): Sequential(
            (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
        )
        (cross_posembed): PositionEmbeddingLearned(
          (position_embedding_head): Sequential(
            (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
        )
      )
      (prediction_head): SeparateHead_Transfusion(
        (center): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (height): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
        (dim): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 3, kernel_size=(1,), stride=(1,))
        )
        (rot): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (vel): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (iou): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
        (heatmap): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 10, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (point_head): None
    (roi_head): None
  )
)
2025-09-03 01:19:32,581   INFO  **********************Start training sparse_models/sparse_former_base(default)**********************
epochs:   0%|          | 0/36 [00:00<?, ?it/s]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
2025-09-03 01:19:47,029   INFO  Train:    1/36 (  3%) [   0/1759 (  0%)]  Loss: 1421. (1.42e+03)  LR: 3.000e-04  Grad: 10.0000  max=0.3381(module.dense_head.heatmap_head.1.bias)  min: -0.1722(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1384.3699, loss_cls=18.4023, loss_bbox=8.3481, matched_ious=0.0081, loss_iou=0.4242, loss_iou_reg=0.4708, d_time=1.55(1.55), f_time=11.11(11.11), b_time=12.66(12.66)  Time cost: 00:12/6:16:07 [00:14/225:40:39]  Acc_iter 1           Data time: 1.55(1.55)  Forward time: 11.11(11.11)  Batch time: 12.66(12.66)
2025-09-03 01:20:51,443   INFO  Train:    1/36 (  3%) [  49/1759 (  3%)]  Loss: 17.51 (128.)  LR: 3.000e-04  Grad: 10.0000  max=1.1681(module.dense_head.prediction_head.heatmap.1.bias)  min: -0.2737(module.dense_head.prediction_head.heatmap.0.1.bias)  NaN: False  loss_hm=75.5152, loss_cls=12.0100, loss_bbox=6.8907, matched_ious=0.0091, loss_iou=0.2603, loss_iou_reg=0.2958, d_time=0.01(0.05), f_time=1.29(1.49), b_time=1.30(1.54)  Time cost: 01:17/44:01 [01:18/27:09:12]  Acc_iter 50          Data time: 0.01(0.05)  Forward time: 1.29(1.49)  Batch time: 1.30(1.54)
2025-09-03 01:21:58,555   INFO  Train:    1/36 (  3%) [  99/1759 (  6%)]  Loss: 13.67 (71.9)  LR: 3.001e-04  Grad: 8.1905  max=0.7734(module.dense_head.prediction_head.heatmap.1.bias)  min: -0.1751(module.dense_head.prediction_head.heatmap.0.1.bias)  NaN: False  loss_hm=2.8685, loss_cls=4.4085, loss_bbox=3.9394, matched_ious=0.0552, loss_iou=0.1208, loss_iou_reg=0.4282, d_time=0.01(0.04), f_time=1.35(1.40), b_time=1.35(1.44)  Time cost: 02:24/39:56 [02:25/25:21:07]  Acc_iter 100         Data time: 0.01(0.04)  Forward time: 1.35(1.40)  Batch time: 1.35(1.44)
2025-09-03 01:23:06,203   INFO  Train:    1/36 (  3%) [ 149/1759 (  8%)]  Loss: 10.99 (51.6)  LR: 3.002e-04  Grad: 4.8228  max=0.3292(module.dense_head.prediction_head.heatmap.1.bias)  min: -0.1225(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=2.5848, loss_cls=1.4471, loss_bbox=2.8791, matched_ious=0.0984, loss_iou=0.1037, loss_iou_reg=0.4070, d_time=0.01(0.03), f_time=1.44(1.38), b_time=1.45(1.41)  Time cost: 03:32/37:55 [03:33/24:48:09]  Acc_iter 150         Data time: 0.01(0.03)  Forward time: 1.44(1.38)  Batch time: 1.45(1.41)
2025-09-03 01:24:11,987   INFO  Train:    1/36 (  3%) [ 199/1759 ( 11%)]  Loss: 10.14 (41.3)  LR: 3.004e-04  Grad: 5.0331  max=0.1355(module.dense_head.prediction_head.heatmap.1.bias)  min: -0.1648(module.dense_head.heatmap_head.1.weight)  NaN: False  loss_hm=2.4543, loss_cls=0.8413, loss_bbox=2.8631, matched_ious=0.1115, loss_iou=0.1046, loss_iou_reg=0.4025, d_time=0.01(0.03), f_time=1.23(1.36), b_time=1.23(1.39)  Time cost: 04:37/36:06 [04:39/24:21:16]  Acc_iter 200         Data time: 0.01(0.03)  Forward time: 1.23(1.36)  Batch time: 1.23(1.39)
2025-09-03 01:25:17,551   INFO  Train:    1/36 (  3%) [ 249/1759 ( 14%)]  Loss: 9.412 (34.9)  LR: 3.006e-04  Grad: 4.1809  max=0.1056(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.1655(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=2.3188, loss_cls=0.6808, loss_bbox=2.6110, matched_ious=0.1274, loss_iou=0.1077, loss_iou_reg=0.3905, d_time=0.01(0.02), f_time=1.31(1.35), b_time=1.32(1.37)  Time cost: 05:43/34:33 [05:44/24:03:47]  Acc_iter 250         Data time: 0.01(0.02)  Forward time: 1.31(1.35)  Batch time: 1.32(1.37)
2025-09-03 01:26:27,320   INFO  Train:    1/36 (  3%) [ 299/1759 ( 17%)]  Loss: 8.761 (30.6)  LR: 3.009e-04  Grad: 4.3612  max=0.1607(module.backbone_3d.cls_conv.3.bias)  min: -0.2635(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=2.1523, loss_cls=0.5910, loss_bbox=2.4077, matched_ious=0.1478, loss_iou=0.1054, loss_iou_reg=0.3873, d_time=0.01(0.02), f_time=1.44(1.35), b_time=1.45(1.38)  Time cost: 06:53/33:30 [06:54/24:06:29]  Acc_iter 300         Data time: 0.01(0.02)  Forward time: 1.44(1.35)  Batch time: 1.45(1.38)
2025-09-03 01:27:34,363   INFO  Train:    1/36 (  3%) [ 349/1759 ( 20%)]  Loss: 7.406 (27.4)  LR: 3.013e-04  Grad: 4.8928  max=0.1036(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.1684(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=2.0615, loss_cls=0.5707, loss_bbox=2.2077, matched_ious=0.1688, loss_iou=0.1068, loss_iou_reg=0.3740, d_time=0.01(0.02), f_time=1.20(1.35), b_time=1.20(1.37)  Time cost: 08:00/32:14 [08:01/23:59:55]  Acc_iter 350         Data time: 0.01(0.02)  Forward time: 1.20(1.35)  Batch time: 1.20(1.37)
2025-09-03 01:28:39,939   INFO  Train:    1/36 (  3%) [ 399/1759 ( 23%)]  Loss: 7.907 (25.0)  LR: 3.017e-04  Grad: 5.6102  max=0.2262(module.backbone_3d.cls_conv.3.weight)  min: -0.3160(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.9697, loss_cls=0.5387, loss_bbox=2.1208, matched_ious=0.1843, loss_iou=0.1062, loss_iou_reg=0.3638, d_time=0.01(0.02), f_time=1.28(1.35), b_time=1.28(1.36)  Time cost: 09:05/30:55 [09:07/23:50:51]  Acc_iter 400         Data time: 0.01(0.02)  Forward time: 1.28(1.35)  Batch time: 1.28(1.36)
2025-09-03 01:29:44,775   INFO  Train:    1/36 (  3%) [ 449/1759 ( 26%)]  Loss: 7.442 (23.1)  LR: 3.021e-04  Grad: 6.7999  max=0.1869(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2962(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.8681, loss_cls=0.5265, loss_bbox=2.0530, matched_ious=0.2145, loss_iou=0.1102, loss_iou_reg=0.3512, d_time=0.01(0.02), f_time=1.31(1.34), b_time=1.32(1.36)  Time cost: 10:10/29:37 [10:12/23:41:51]  Acc_iter 450         Data time: 0.01(0.02)  Forward time: 1.31(1.34)  Batch time: 1.32(1.36)
2025-09-03 01:30:49,978   INFO  Train:    1/36 (  3%) [ 499/1759 ( 28%)]  Loss: 8.181 (21.6)  LR: 3.026e-04  Grad: 7.8288  max=0.3578(module.backbone_3d.cls_conv.3.bias)  min: -0.3147(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.7810, loss_cls=0.4981, loss_bbox=1.8470, matched_ious=0.2372, loss_iou=0.1095, loss_iou_reg=0.3388, d_time=0.01(0.02), f_time=1.38(1.33), b_time=1.39(1.35)  Time cost: 11:15/28:22 [11:17/23:35:11]  Acc_iter 500         Data time: 0.01(0.02)  Forward time: 1.38(1.33)  Batch time: 1.39(1.35)
2025-09-03 01:31:55,088   INFO  Train:    1/36 (  3%) [ 549/1759 ( 31%)]  Loss: 6.663 (20.3)  LR: 3.031e-04  Grad: 8.2874  max=1.0810(module.backbone_3d.cls_conv.3.weight)  min: -0.1808(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.7398, loss_cls=0.4728, loss_bbox=1.8320, matched_ious=0.2542, loss_iou=0.1125, loss_iou_reg=0.3328, d_time=0.01(0.02), f_time=1.26(1.33), b_time=1.27(1.35)  Time cost: 12:20/27:09 [12:22/23:29:21]  Acc_iter 550         Data time: 0.01(0.02)  Forward time: 1.26(1.33)  Batch time: 1.27(1.35)
2025-09-03 01:32:59,979   INFO  Train:    1/36 (  3%) [ 599/1759 ( 34%)]  Loss: 6.672 (19.2)  LR: 3.037e-04  Grad: 8.0982  max=0.2132(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.1995(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.7045, loss_cls=0.4597, loss_bbox=1.7482, matched_ious=0.2661, loss_iou=0.1130, loss_iou_reg=0.3265, d_time=0.01(0.02), f_time=1.33(1.33), b_time=1.34(1.34)  Time cost: 13:25/25:57 [13:27/23:23:57]  Acc_iter 600         Data time: 0.01(0.02)  Forward time: 1.33(1.33)  Batch time: 1.34(1.34)
2025-09-03 01:34:04,378   INFO  Train:    1/36 (  3%) [ 649/1759 ( 37%)]  Loss: 7.475 (18.3)  LR: 3.044e-04  Grad: 8.4299  max=0.2060(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.1995(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.6478, loss_cls=0.4322, loss_bbox=1.7994, matched_ious=0.2703, loss_iou=0.1125, loss_iou_reg=0.3205, d_time=0.01(0.01), f_time=1.26(1.32), b_time=1.26(1.34)  Time cost: 14:30/24:45 [14:31/23:18:25]  Acc_iter 650         Data time: 0.01(0.01)  Forward time: 1.26(1.32)  Batch time: 1.26(1.34)
2025-09-03 01:35:09,225   INFO  Train:    1/36 (  3%) [ 699/1759 ( 40%)]  Loss: 7.932 (17.5)  LR: 3.051e-04  Grad: 8.8953  max=0.3239(module.backbone_3d.cls_conv.3.weight)  min: -0.2168(module.dense_head.heatmap_head.1.weight)  NaN: False  loss_hm=1.6517, loss_cls=0.4331, loss_bbox=1.8151, matched_ious=0.2815, loss_iou=0.1123, loss_iou_reg=0.3203, d_time=0.01(0.01), f_time=1.30(1.32), b_time=1.31(1.34)  Time cost: 15:35/23:35 [15:36/23:14:11]  Acc_iter 700         Data time: 0.01(0.01)  Forward time: 1.30(1.32)  Batch time: 1.31(1.34)
2025-09-03 01:36:14,826   INFO  Train:    1/36 (  3%) [ 749/1759 ( 43%)]  Loss: 7.790 (16.8)  LR: 3.058e-04  Grad: 9.2443  max=0.2247(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2210(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.6120, loss_cls=0.4178, loss_bbox=1.8055, matched_ious=0.2850, loss_iou=0.1132, loss_iou_reg=0.3170, d_time=0.01(0.01), f_time=1.23(1.32), b_time=1.24(1.33)  Time cost: 16:40/22:27 [16:42/23:11:25]  Acc_iter 750         Data time: 0.01(0.01)  Forward time: 1.23(1.32)  Batch time: 1.24(1.33)
2025-09-03 01:37:21,306   INFO  Train:    1/36 (  3%) [ 799/1759 ( 45%)]  Loss: 5.714 (16.2)  LR: 3.066e-04  Grad: 9.6787  max=0.2256(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2121(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.5503, loss_cls=0.3999, loss_bbox=1.7412, matched_ious=0.2937, loss_iou=0.1133, loss_iou_reg=0.3122, d_time=0.01(0.01), f_time=1.30(1.32), b_time=1.31(1.33)  Time cost: 17:47/21:20 [17:48/23:10:00]  Acc_iter 800         Data time: 0.01(0.01)  Forward time: 1.30(1.32)  Batch time: 1.31(1.33)
2025-09-03 01:38:26,305   INFO  Train:    1/36 (  3%) [ 849/1759 ( 48%)]  Loss: 7.811 (15.6)  LR: 3.075e-04  Grad: 9.8966  max=0.9493(module.backbone_3d.cls_conv.3.weight)  min: -0.2220(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.5626, loss_cls=0.3974, loss_bbox=1.7538, matched_ious=0.2988, loss_iou=0.1136, loss_iou_reg=0.3098, d_time=0.01(0.01), f_time=1.35(1.32), b_time=1.36(1.33)  Time cost: 18:52/20:12 [18:53/23:06:49]  Acc_iter 850         Data time: 0.01(0.01)  Forward time: 1.35(1.32)  Batch time: 1.36(1.33)
2025-09-03 01:39:31,174   INFO  Train:    1/36 (  3%) [ 899/1759 ( 51%)]  Loss: 6.172 (15.1)  LR: 3.084e-04  Grad: 9.5168  max=0.2199(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2193(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.4699, loss_cls=0.3849, loss_bbox=1.6359, matched_ious=0.3105, loss_iou=0.1105, loss_iou_reg=0.3037, d_time=0.01(0.01), f_time=1.30(1.32), b_time=1.31(1.33)  Time cost: 19:56/19:03 [19:58/23:03:43]  Acc_iter 900         Data time: 0.01(0.01)  Forward time: 1.30(1.32)  Batch time: 1.31(1.33)
2025-09-03 01:40:36,500   INFO  Train:    1/36 (  3%) [ 949/1759 ( 54%)]  Loss: 6.325 (14.7)  LR: 3.093e-04  Grad: 9.8331  max=0.2177(module.dense_head.heatmap_head.1.weight)  min: -0.2366(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.5386, loss_cls=0.3890, loss_bbox=1.7656, matched_ious=0.3105, loss_iou=0.1155, loss_iou_reg=0.3037, d_time=0.01(0.01), f_time=1.28(1.32), b_time=1.29(1.33)  Time cost: 21:02/17:56 [21:03/23:01:20]  Acc_iter 950         Data time: 0.01(0.01)  Forward time: 1.28(1.32)  Batch time: 1.29(1.33)
2025-09-03 01:41:42,148   INFO  Train:    1/36 (  3%) [ 999/1759 ( 57%)]  Loss: 6.808 (14.3)  LR: 3.104e-04  Grad: 9.8326  max=0.6562(module.backbone_3d.cls_conv.3.weight)  min: -0.2457(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.4739, loss_cls=0.3853, loss_bbox=1.7122, matched_ious=0.3165, loss_iou=0.1115, loss_iou_reg=0.3037, d_time=0.01(0.01), f_time=1.42(1.31), b_time=1.43(1.33)  Time cost: 22:07/16:49 [22:09/22:59:24]  Acc_iter 1000        Data time: 0.01(0.01)  Forward time: 1.42(1.31)  Batch time: 1.43(1.33)
2025-09-03 01:42:48,348   INFO  Train:    1/36 (  3%) [1049/1759 ( 60%)]  Loss: 6.611 (13.9)  LR: 3.114e-04  Grad: 9.7869  max=0.3742(module.vfe.pfn_layers.0.linear.weight)  min: -0.2295(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.4191, loss_cls=0.3641, loss_bbox=1.6371, matched_ious=0.3175, loss_iou=0.1093, loss_iou_reg=0.3004, d_time=0.01(0.01), f_time=1.27(1.31), b_time=1.27(1.33)  Time cost: 23:14/15:42 [23:15/22:58:06]  Acc_iter 1050        Data time: 0.01(0.01)  Forward time: 1.27(1.31)  Batch time: 1.27(1.33)
2025-09-03 01:43:52,861   INFO  Train:    1/36 (  3%) [1099/1759 ( 62%)]  Loss: 6.063 (13.6)  LR: 3.125e-04  Grad: 9.5169  max=0.1960(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2153(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.4621, loss_cls=0.3646, loss_bbox=1.7403, matched_ious=0.3220, loss_iou=0.1106, loss_iou_reg=0.2932, d_time=0.01(0.01), f_time=1.28(1.31), b_time=1.29(1.33)  Time cost: 24:18/14:35 [24:20/22:55:13]  Acc_iter 1100        Data time: 0.01(0.01)  Forward time: 1.28(1.31)  Batch time: 1.29(1.33)
2025-09-03 01:44:58,474   INFO  Train:    1/36 (  3%) [1149/1759 ( 65%)]  Loss: 6.872 (13.3)  LR: 3.137e-04  Grad: 9.8936  max=0.3578(module.backbone_3d.cls_conv.3.bias)  min: -0.2189(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.4155, loss_cls=0.3648, loss_bbox=1.6506, matched_ious=0.3305, loss_iou=0.1095, loss_iou_reg=0.2960, d_time=0.01(0.01), f_time=1.46(1.31), b_time=1.47(1.33)  Time cost: 25:24/13:28 [25:25/22:53:30]  Acc_iter 1150        Data time: 0.01(0.01)  Forward time: 1.46(1.31)  Batch time: 1.47(1.33)
2025-09-03 01:46:05,218   INFO  Train:    1/36 (  3%) [1199/1759 ( 68%)]  Loss: 6.559 (13.0)  LR: 3.149e-04  Grad: 9.7721  max=0.2875(module.backbone_3d.cls_conv.3.weight)  min: -0.2909(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.4296, loss_cls=0.3599, loss_bbox=1.6702, matched_ious=0.3295, loss_iou=0.1100, loss_iou_reg=0.2909, d_time=0.01(0.01), f_time=1.31(1.31), b_time=1.32(1.33)  Time cost: 26:31/12:22 [26:32/22:52:48]  Acc_iter 1200        Data time: 0.01(0.01)  Forward time: 1.31(1.31)  Batch time: 1.32(1.33)
2025-09-03 01:47:10,795   INFO  Train:    1/36 (  3%) [1249/1759 ( 71%)]  Loss: 5.822 (12.7)  LR: 3.162e-04  Grad: 9.8148  max=0.2058(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.3973(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.3460, loss_cls=0.3534, loss_bbox=1.6544, matched_ious=0.3297, loss_iou=0.1116, loss_iou_reg=0.2974, d_time=0.01(0.01), f_time=1.25(1.31), b_time=1.26(1.33)  Time cost: 27:36/11:15 [27:38/22:51:06]  Acc_iter 1250        Data time: 0.01(0.01)  Forward time: 1.25(1.31)  Batch time: 1.26(1.33)
2025-09-03 01:48:17,812   INFO  Train:    1/36 (  3%) [1299/1759 ( 74%)]  Loss: 7.427 (12.5)  LR: 3.175e-04  Grad: 9.9486  max=0.5315(module.backbone_3d.cls_conv.3.weight)  min: -0.2193(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.3990, loss_cls=0.3544, loss_bbox=1.6976, matched_ious=0.3289, loss_iou=0.1086, loss_iou_reg=0.2926, d_time=0.01(0.01), f_time=1.32(1.31), b_time=1.32(1.33)  Time cost: 28:43/10:09 [28:45/22:50:36]  Acc_iter 1300        Data time: 0.01(0.01)  Forward time: 1.32(1.31)  Batch time: 1.32(1.33)
2025-09-03 01:49:22,711   INFO  Train:    1/36 (  3%) [1349/1759 ( 77%)]  Loss: 6.167 (12.3)  LR: 3.189e-04  Grad: 9.7814  max=0.2182(module.backbone_3d.cls_conv.3.weight)  min: -0.2473(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.3664, loss_cls=0.3519, loss_bbox=1.6129, matched_ious=0.3345, loss_iou=0.1093, loss_iou_reg=0.2937, d_time=0.01(0.01), f_time=1.34(1.31), b_time=1.34(1.32)  Time cost: 29:48/09:03 [29:50/22:48:25]  Acc_iter 1350        Data time: 0.01(0.01)  Forward time: 1.34(1.31)  Batch time: 1.34(1.32)
2025-09-03 01:50:28,718   INFO  Train:    1/36 (  3%) [1399/1759 ( 80%)]  Loss: 5.792 (12.1)  LR: 3.203e-04  Grad: 9.7671  max=0.2009(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2205(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.3513, loss_cls=0.3497, loss_bbox=1.6184, matched_ious=0.3434, loss_iou=0.1092, loss_iou_reg=0.2880, d_time=0.01(0.01), f_time=1.35(1.31), b_time=1.36(1.32)  Time cost: 30:54/07:56 [30:56/22:47:09]  Acc_iter 1400        Data time: 0.01(0.01)  Forward time: 1.35(1.31)  Batch time: 1.36(1.32)
2025-09-03 01:51:34,427   INFO  Train:    1/36 (  3%) [1449/1759 ( 82%)]  Loss: 5.670 (11.8)  LR: 3.217e-04  Grad: 9.6691  max=0.2051(module.dense_head.prediction_head.height.1.bias)  min: -0.2171(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.3509, loss_cls=0.3523, loss_bbox=1.5776, matched_ious=0.3444, loss_iou=0.1084, loss_iou_reg=0.2917, d_time=0.01(0.01), f_time=1.28(1.31), b_time=1.28(1.32)  Time cost: 32:00/06:50 [32:01/22:45:40]  Acc_iter 1450        Data time: 0.01(0.01)  Forward time: 1.28(1.31)  Batch time: 1.28(1.32)
2025-09-03 01:52:42,321   INFO  Train:    1/36 (  3%) [1499/1759 ( 85%)]  Loss: 5.783 (11.7)  LR: 3.233e-04  Grad: 9.7008  max=0.1869(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2146(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.3093, loss_cls=0.3439, loss_bbox=1.5811, matched_ious=0.3492, loss_iou=0.1074, loss_iou_reg=0.2868, d_time=0.01(0.01), f_time=1.40(1.31), b_time=1.41(1.33)  Time cost: 33:08/05:44 [33:09/22:45:43]  Acc_iter 1500        Data time: 0.01(0.01)  Forward time: 1.40(1.31)  Batch time: 1.41(1.33)
2025-09-03 01:53:46,708   INFO  Train:    1/36 (  3%) [1549/1759 ( 88%)]  Loss: 4.952 (11.5)  LR: 3.248e-04  Grad: 9.4962  max=0.1930(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2697(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.3575, loss_cls=0.3479, loss_bbox=1.6672, matched_ious=0.3352, loss_iou=0.1066, loss_iou_reg=0.2916, d_time=0.01(0.01), f_time=1.28(1.31), b_time=1.29(1.32)  Time cost: 34:12/04:38 [34:14/22:43:22]  Acc_iter 1550        Data time: 0.01(0.01)  Forward time: 1.28(1.31)  Batch time: 1.29(1.32)
2025-09-03 01:54:52,028   INFO  Train:    1/36 (  3%) [1599/1759 ( 91%)]  Loss: 5.645 (11.3)  LR: 3.265e-04  Grad: 9.7567  max=0.1926(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2088(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.2925, loss_cls=0.3437, loss_bbox=1.5033, matched_ious=0.3639, loss_iou=0.1083, loss_iou_reg=0.2802, d_time=0.01(0.01), f_time=1.31(1.31), b_time=1.32(1.32)  Time cost: 35:17/03:31 [35:19/22:41:41]  Acc_iter 1600        Data time: 0.01(0.01)  Forward time: 1.31(1.31)  Batch time: 1.32(1.32)
2025-09-03 01:55:57,787   INFO  Train:    1/36 (  3%) [1649/1759 ( 94%)]  Loss: 5.144 (11.2)  LR: 3.281e-04  Grad: 9.6412  max=0.1995(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2414(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.3223, loss_cls=0.3433, loss_bbox=1.5993, matched_ious=0.3477, loss_iou=0.1064, loss_iou_reg=0.2889, d_time=0.01(0.01), f_time=1.29(1.31), b_time=1.30(1.32)  Time cost: 36:23/02:25 [36:25/22:40:19]  Acc_iter 1650        Data time: 0.01(0.01)  Forward time: 1.29(1.31)  Batch time: 1.30(1.32)
2025-09-03 01:57:02,939   INFO  Train:    1/36 (  3%) [1699/1759 ( 97%)]  Loss: 6.941 (11.0)  LR: 3.299e-04  Grad: 9.9835  max=0.8066(module.backbone_3d.cls_conv.3.weight)  min: -0.2286(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.3225, loss_cls=0.3495, loss_bbox=1.5899, matched_ious=0.3628, loss_iou=0.1076, loss_iou_reg=0.2815, d_time=0.01(0.01), f_time=1.22(1.31), b_time=1.23(1.32)  Time cost: 37:28/01:19 [37:30/22:38:36]  Acc_iter 1700        Data time: 0.01(0.01)  Forward time: 1.22(1.31)  Batch time: 1.23(1.32)
2025-09-03 01:58:07,159   INFO  Train:    1/36 (  3%) [1749/1759 ( 99%)]  Loss: 7.869 (10.9)  LR: 3.316e-04  Grad: 9.7741  max=0.2020(module.vfe.pfn_layers.1.linear.weight)  min: -0.3794(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.3219, loss_cls=0.3417, loss_bbox=1.5953, matched_ious=0.3605, loss_iou=0.1047, loss_iou_reg=0.2827, d_time=0.01(0.01), f_time=1.24(1.31), b_time=1.25(1.32)  Time cost: 38:32/00:13 [38:34/22:36:23]  Acc_iter 1750        Data time: 0.01(0.01)  Forward time: 1.24(1.31)  Batch time: 1.25(1.32)
2025-09-03 01:58:18,090   INFO  Train:    1/36 (  3%) [1758/1759 (100%)]  Loss: 6.669 (10.9)  LR: 3.320e-04  Grad: 9.9973  max=0.3938(module.backbone_3d.cls_conv.3.weight)  min: -0.2815(module.vfe.pfn_layers.1.linear.weight)  NaN: False  loss_hm=1.2815, loss_cls=0.3318, loss_bbox=1.5596, matched_ious=0.3585, loss_iou=0.1068, loss_iou_reg=0.2814, d_time=0.01(0.01), f_time=0.75(1.31), b_time=0.75(1.32)  Time cost: 38:43/00:01 [38:45/22:35:37]  Acc_iter 1759        Data time: 0.01(0.01)  Forward time: 0.75(1.31)  Batch time: 0.75(1.32)

                                               [Aepochs:   3%|▎         | 1/36 [38:45<22:36:34, 2325.55s/it]epochs:   3%|▎         | 1/36 [38:45<22:36:37, 2325.63s/it]epochs:   3%|▎         | 1/36 [38:45<22:36:38, 2325.68s/it]epochs:   3%|▎         | 1/36 [38:45<22:36:40, 2325.72s/it]epochs:   3%|▎         | 1/36 [38:45<22:36:40, 2325.72s/it]epochs:   3%|▎         | 1/36 [38:45<22:36:40, 2325.73s/it]epochs:   3%|▎         | 1/36 [38:45<22:36:41, 2325.74s/it]epochs:   3%|▎         | 1/36 [38:45<22:36:49, 2325.98s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 01:58:22,989   INFO  Train:    2/36 (  6%) [   0/1759 (  0%)]  Loss: 4.931 (4.93)  LR: 3.320e-04  Grad: 9.7964  max=0.2055(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2226(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.1113, loss_cls=0.3263, loss_bbox=0.9665, matched_ious=0.3910, loss_iou=0.1075, loss_iou_reg=0.2977, d_time=0.92(0.92), f_time=3.01(3.01), b_time=3.93(3.93)  Time cost: 00:03/1:40:59 [38:50/58:54:43]  Acc_iter 1760        Data time: 0.92(0.92)  Forward time: 3.01(3.01)  Batch time: 3.93(3.93)
2025-09-03 01:59:15,703   INFO  Train:    2/36 (  6%) [  40/1759 (  2%)]  Loss: 6.735 (6.08)  LR: 3.335e-04  Grad: 9.9621  max=0.2904(module.backbone_3d.cls_conv.3.weight)  min: -0.2376(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.2610, loss_cls=0.3315, loss_bbox=1.6158, matched_ious=0.3573, loss_iou=0.1095, loss_iou_reg=0.2845, d_time=0.01(0.03), f_time=1.22(1.35), b_time=1.23(1.38)  Time cost: 00:56/39:14 [39:43/23:24:40]  Acc_iter 1800        Data time: 0.01(0.03)  Forward time: 1.22(1.35)  Batch time: 1.23(1.38)
2025-09-03 02:00:21,549   INFO  Train:    2/36 (  6%) [  90/1759 (  5%)]  Loss: 5.298 (6.05)  LR: 3.353e-04  Grad: 9.8037  max=0.2111(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2234(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.2763, loss_cls=0.3376, loss_bbox=1.5400, matched_ious=0.3691, loss_iou=0.1104, loss_iou_reg=0.2783, d_time=0.01(0.02), f_time=1.33(1.33), b_time=1.34(1.35)  Time cost: 02:02/37:17 [40:48/22:53:43]  Acc_iter 1850        Data time: 0.01(0.02)  Forward time: 1.33(1.33)  Batch time: 1.34(1.35)
2025-09-03 02:01:26,768   INFO  Train:    2/36 (  6%) [ 140/1759 (  8%)]  Loss: 6.664 (6.04)  LR: 3.373e-04  Grad: 9.8662  max=0.2889(module.vfe.pfn_layers.0.linear.weight)  min: -0.2631(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2685, loss_cls=0.3303, loss_bbox=1.5337, matched_ious=0.3593, loss_iou=0.1067, loss_iou_reg=0.2830, d_time=0.01(0.01), f_time=1.27(1.32), b_time=1.28(1.33)  Time cost: 03:07/35:49 [41:54/22:39:23]  Acc_iter 1900        Data time: 0.01(0.01)  Forward time: 1.27(1.32)  Batch time: 1.28(1.33)
2025-09-03 02:02:31,815   INFO  Train:    2/36 (  6%) [ 190/1759 ( 11%)]  Loss: 6.423 (6.03)  LR: 3.393e-04  Grad: 9.7123  max=0.5271(module.backbone_3d.cls_conv.3.weight)  min: -0.2335(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.2516, loss_cls=0.3304, loss_bbox=1.5427, matched_ious=0.3673, loss_iou=0.1084, loss_iou_reg=0.2818, d_time=0.01(0.01), f_time=1.31(1.31), b_time=1.32(1.32)  Time cost: 04:12/34:32 [42:59/22:31:04]  Acc_iter 1950        Data time: 0.01(0.01)  Forward time: 1.31(1.31)  Batch time: 1.32(1.32)
2025-09-03 02:03:37,427   INFO  Train:    2/36 (  6%) [ 240/1759 ( 14%)]  Loss: 5.522 (5.99)  LR: 3.413e-04  Grad: 9.7517  max=0.4150(module.backbone_3d.cls_conv.3.weight)  min: -0.3208(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2510, loss_cls=0.3287, loss_bbox=1.4383, matched_ious=0.3722, loss_iou=0.1082, loss_iou_reg=0.2796, d_time=0.01(0.01), f_time=1.33(1.31), b_time=1.34(1.32)  Time cost: 05:17/33:23 [44:04/22:28:10]  Acc_iter 2000        Data time: 0.01(0.01)  Forward time: 1.33(1.31)  Batch time: 1.34(1.32)
2025-09-03 02:04:43,368   INFO  Train:    2/36 (  6%) [ 290/1759 ( 16%)]  Loss: 5.927 (5.99)  LR: 3.434e-04  Grad: 9.7109  max=0.3977(module.backbone_3d.cls_conv.3.weight)  min: -0.2229(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.2622, loss_cls=0.3308, loss_bbox=1.5147, matched_ious=0.3648, loss_iou=0.1062, loss_iou_reg=0.2831, d_time=0.01(0.01), f_time=1.30(1.31), b_time=1.30(1.32)  Time cost: 06:23/32:17 [45:10/22:27:02]  Acc_iter 2050        Data time: 0.01(0.01)  Forward time: 1.30(1.31)  Batch time: 1.30(1.32)
2025-09-03 02:05:49,409   INFO  Train:    2/36 (  6%) [ 340/1759 ( 19%)]  Loss: 6.347 (6.00)  LR: 3.455e-04  Grad: 9.4776  max=0.2119(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2113(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.2741, loss_cls=0.3293, loss_bbox=1.5495, matched_ious=0.3695, loss_iou=0.1058, loss_iou_reg=0.2793, d_time=0.01(0.01), f_time=1.26(1.31), b_time=1.26(1.32)  Time cost: 07:29/31:12 [46:16/22:26:11]  Acc_iter 2100        Data time: 0.01(0.01)  Forward time: 1.26(1.31)  Batch time: 1.26(1.32)
2025-09-03 02:06:54,446   INFO  Train:    2/36 (  6%) [ 390/1759 ( 22%)]  Loss: 6.304 (5.98)  LR: 3.477e-04  Grad: 9.8271  max=0.2156(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.4144(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.2423, loss_cls=0.3250, loss_bbox=1.4772, matched_ious=0.3702, loss_iou=0.1079, loss_iou_reg=0.2805, d_time=0.01(0.01), f_time=1.25(1.31), b_time=1.26(1.32)  Time cost: 08:34/30:02 [47:21/22:22:41]  Acc_iter 2150        Data time: 0.01(0.01)  Forward time: 1.25(1.31)  Batch time: 1.26(1.32)
2025-09-03 02:07:59,480   INFO  Train:    2/36 (  6%) [ 440/1759 ( 25%)]  Loss: 5.475 (5.95)  LR: 3.499e-04  Grad: 9.7363  max=0.2412(module.vfe.pfn_layers.0.linear.weight)  min: -0.2377(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.1946, loss_cls=0.3219, loss_bbox=1.4333, matched_ious=0.3850, loss_iou=0.1056, loss_iou_reg=0.2736, d_time=0.01(0.01), f_time=1.31(1.31), b_time=1.32(1.32)  Time cost: 09:39/28:54 [48:26/22:19:42]  Acc_iter 2200        Data time: 0.01(0.01)  Forward time: 1.31(1.31)  Batch time: 1.32(1.32)
2025-09-03 02:09:05,248   INFO  Train:    2/36 (  6%) [ 490/1759 ( 28%)]  Loss: 8.059 (5.94)  LR: 3.522e-04  Grad: 9.8895  max=0.3417(module.vfe.pfn_layers.0.linear.weight)  min: -0.2248(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.2274, loss_cls=0.3170, loss_bbox=1.5036, matched_ious=0.3650, loss_iou=0.1066, loss_iou_reg=0.2821, d_time=0.01(0.01), f_time=1.25(1.31), b_time=1.25(1.32)  Time cost: 10:45/27:48 [49:32/22:18:38]  Acc_iter 2250        Data time: 0.01(0.01)  Forward time: 1.25(1.31)  Batch time: 1.25(1.32)
2025-09-03 02:10:11,723   INFO  Train:    2/36 (  6%) [ 540/1759 ( 31%)]  Loss: 6.390 (5.93)  LR: 3.545e-04  Grad: 9.7537  max=0.2283(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2316(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.2170, loss_cls=0.3149, loss_bbox=1.4663, matched_ious=0.3889, loss_iou=0.1041, loss_iou_reg=0.2711, d_time=0.01(0.01), f_time=1.28(1.31), b_time=1.29(1.32)  Time cost: 11:52/26:44 [50:39/22:18:54]  Acc_iter 2300        Data time: 0.01(0.01)  Forward time: 1.28(1.31)  Batch time: 1.29(1.32)
2025-09-03 02:11:17,028   INFO  Train:    2/36 (  6%) [ 590/1759 ( 34%)]  Loss: 5.885 (5.92)  LR: 3.569e-04  Grad: 9.5020  max=0.2863(module.dense_head.prediction_head.height.1.bias)  min: -0.3050(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.2339, loss_cls=0.3126, loss_bbox=1.5385, matched_ious=0.3810, loss_iou=0.1056, loss_iou_reg=0.2740, d_time=0.01(0.01), f_time=1.35(1.31), b_time=1.36(1.32)  Time cost: 12:57/25:37 [51:44/22:16:55]  Acc_iter 2350        Data time: 0.01(0.01)  Forward time: 1.35(1.31)  Batch time: 1.36(1.32)
2025-09-03 02:12:21,791   INFO  Train:    2/36 (  6%) [ 640/1759 ( 36%)]  Loss: 5.740 (5.92)  LR: 3.593e-04  Grad: 9.7014  max=0.4083(module.backbone_3d.cls_conv.3.weight)  min: -0.2053(module.backbone_3d.app.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2269, loss_cls=0.3150, loss_bbox=1.4861, matched_ious=0.3829, loss_iou=0.1038, loss_iou_reg=0.2729, d_time=0.01(0.01), f_time=1.25(1.31), b_time=1.26(1.31)  Time cost: 14:02/24:30 [52:49/22:14:13]  Acc_iter 2400        Data time: 0.01(0.01)  Forward time: 1.25(1.31)  Batch time: 1.26(1.31)
2025-09-03 02:13:27,713   INFO  Train:    2/36 (  6%) [ 690/1759 ( 39%)]  Loss: 5.292 (5.91)  LR: 3.618e-04  Grad: 9.4815  max=0.2362(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.3547(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2194, loss_cls=0.3129, loss_bbox=1.5261, matched_ious=0.3837, loss_iou=0.1052, loss_iou_reg=0.2725, d_time=0.01(0.01), f_time=1.36(1.31), b_time=1.37(1.31)  Time cost: 15:08/23:24 [53:55/22:13:27]  Acc_iter 2450        Data time: 0.01(0.01)  Forward time: 1.36(1.31)  Batch time: 1.37(1.31)
2025-09-03 02:14:33,577   INFO  Train:    2/36 (  6%) [ 740/1759 ( 42%)]  Loss: 5.222 (5.91)  LR: 3.643e-04  Grad: 9.6982  max=0.2500(module.backbone_3d.cls_conv.3.weight)  min: -0.2141(module.backbone_3d.app.self_attn.in_proj_weight)  NaN: False  loss_hm=1.2086, loss_cls=0.3120, loss_bbox=1.4761, matched_ious=0.3830, loss_iou=0.1062, loss_iou_reg=0.2733, d_time=0.01(0.01), f_time=1.33(1.31), b_time=1.34(1.32)  Time cost: 16:14/22:19 [55:00/22:12:33]  Acc_iter 2500        Data time: 0.01(0.01)  Forward time: 1.33(1.31)  Batch time: 1.34(1.32)
2025-09-03 02:15:39,851   INFO  Train:    2/36 (  6%) [ 790/1759 ( 45%)]  Loss: 5.350 (5.89)  LR: 3.669e-04  Grad: 9.8518  max=0.7420(module.vfe.pfn_layers.0.linear.weight)  min: -0.5923(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1951, loss_cls=0.3075, loss_bbox=1.4400, matched_ious=0.3937, loss_iou=0.1044, loss_iou_reg=0.2689, d_time=0.01(0.01), f_time=1.39(1.31), b_time=1.40(1.32)  Time cost: 17:20/21:14 [56:07/22:12:10]  Acc_iter 2550        Data time: 0.01(0.01)  Forward time: 1.39(1.31)  Batch time: 1.40(1.32)
2025-09-03 02:16:45,283   INFO  Train:    2/36 (  6%) [ 840/1759 ( 48%)]  Loss: 5.610 (5.88)  LR: 3.695e-04  Grad: 9.7050  max=0.2330(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2417(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2197, loss_cls=0.3175, loss_bbox=1.4635, matched_ious=0.3861, loss_iou=0.1088, loss_iou_reg=0.2745, d_time=0.01(0.01), f_time=1.24(1.31), b_time=1.25(1.32)  Time cost: 18:25/20:08 [57:12/22:10:40]  Acc_iter 2600        Data time: 0.01(0.01)  Forward time: 1.24(1.31)  Batch time: 1.25(1.32)
2025-09-03 02:17:50,121   INFO  Train:    2/36 (  6%) [ 890/1759 ( 51%)]  Loss: 5.439 (5.87)  LR: 3.722e-04  Grad: 9.8736  max=0.3723(module.backbone_3d.cls_conv.3.weight)  min: -0.2211(module.backbone_3d.app.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1857, loss_cls=0.3094, loss_bbox=1.4199, matched_ious=0.3901, loss_iou=0.1083, loss_iou_reg=0.2751, d_time=0.01(0.01), f_time=1.35(1.31), b_time=1.35(1.31)  Time cost: 19:30/19:01 [58:17/22:08:33]  Acc_iter 2650        Data time: 0.01(0.01)  Forward time: 1.35(1.31)  Batch time: 1.35(1.31)
2025-09-03 02:18:55,271   INFO  Train:    2/36 (  6%) [ 940/1759 ( 53%)]  Loss: 4.978 (5.86)  LR: 3.749e-04  Grad: 9.9491  max=0.5931(module.backbone_3d.cls_conv.3.weight)  min: -0.2552(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1597, loss_cls=0.3022, loss_bbox=1.3636, matched_ious=0.3983, loss_iou=0.1038, loss_iou_reg=0.2700, d_time=0.01(0.01), f_time=1.40(1.31), b_time=1.40(1.31)  Time cost: 20:35/17:55 [59:22/22:06:53]  Acc_iter 2700        Data time: 0.01(0.01)  Forward time: 1.40(1.31)  Batch time: 1.40(1.31)
2025-09-03 02:20:02,305   INFO  Train:    2/36 (  6%) [ 990/1759 ( 56%)]  Loss: 5.836 (5.85)  LR: 3.777e-04  Grad: 9.7019  max=0.2725(module.vfe.pfn_layers.1.linear.weight)  min: -0.4130(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1930, loss_cls=0.3021, loss_bbox=1.5423, matched_ious=0.3886, loss_iou=0.1077, loss_iou_reg=0.2712, d_time=0.01(0.01), f_time=1.38(1.31), b_time=1.38(1.32)  Time cost: 21:42/16:50 [1:00:29/22:07:11]  Acc_iter 2750        Data time: 0.01(0.01)  Forward time: 1.38(1.31)  Batch time: 1.38(1.32)
2025-09-03 02:21:07,799   INFO  Train:    2/36 (  6%) [1040/1759 ( 59%)]  Loss: 5.472 (5.84)  LR: 3.805e-04  Grad: 9.8890  max=0.2692(module.vfe.pfn_layers.0.linear.weight)  min: -0.5053(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1789, loss_cls=0.3066, loss_bbox=1.4615, matched_ious=0.3883, loss_iou=0.1047, loss_iou_reg=0.2757, d_time=0.01(0.01), f_time=1.24(1.31), b_time=1.25(1.31)  Time cost: 22:48/15:45 [1:01:35/22:05:52]  Acc_iter 2800        Data time: 0.01(0.01)  Forward time: 1.24(1.31)  Batch time: 1.25(1.31)
2025-09-03 02:22:12,772   INFO  Train:    2/36 (  6%) [1090/1759 ( 62%)]  Loss: 6.076 (5.83)  LR: 3.834e-04  Grad: 9.8688  max=0.3386(module.vfe.pfn_layers.1.linear.weight)  min: -0.7357(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1657, loss_cls=0.3050, loss_bbox=1.4097, matched_ious=0.4031, loss_iou=0.1031, loss_iou_reg=0.2677, d_time=0.01(0.01), f_time=1.19(1.31), b_time=1.19(1.31)  Time cost: 23:53/14:38 [1:02:40/22:04:05]  Acc_iter 2850        Data time: 0.01(0.01)  Forward time: 1.19(1.31)  Batch time: 1.19(1.31)
2025-09-03 02:23:19,407   INFO  Train:    2/36 (  6%) [1140/1759 ( 65%)]  Loss: 6.362 (5.83)  LR: 3.863e-04  Grad: 9.7592  max=0.2741(module.backbone_3d.cls_conv.3.weight)  min: -0.4686(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.1622, loss_cls=0.2964, loss_bbox=1.5186, matched_ious=0.3887, loss_iou=0.1056, loss_iou_reg=0.2713, d_time=0.01(0.01), f_time=1.31(1.31), b_time=1.32(1.31)  Time cost: 24:59/13:33 [1:03:46/22:03:49]  Acc_iter 2900        Data time: 0.01(0.01)  Forward time: 1.31(1.31)  Batch time: 1.32(1.31)
2025-09-03 02:24:24,931   INFO  Train:    2/36 (  6%) [1190/1759 ( 68%)]  Loss: 5.287 (5.82)  LR: 3.893e-04  Grad: 9.7851  max=0.2573(module.vfe.pfn_layers.0.linear.weight)  min: -0.3310(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1734, loss_cls=0.2938, loss_bbox=1.4836, matched_ious=0.3968, loss_iou=0.1037, loss_iou_reg=0.2692, d_time=0.01(0.01), f_time=1.33(1.31), b_time=1.34(1.31)  Time cost: 26:05/12:27 [1:04:52/22:02:33]  Acc_iter 2950        Data time: 0.01(0.01)  Forward time: 1.33(1.31)  Batch time: 1.34(1.31)
2025-09-03 02:25:30,783   INFO  Train:    2/36 (  6%) [1240/1759 ( 70%)]  Loss: 5.225 (5.81)  LR: 3.923e-04  Grad: 9.9756  max=0.8988(module.vfe.pfn_layers.0.linear.weight)  min: -0.8370(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1078, loss_cls=0.2922, loss_bbox=1.3130, matched_ious=0.4095, loss_iou=0.1033, loss_iou_reg=0.2668, d_time=0.01(0.01), f_time=1.48(1.31), b_time=1.49(1.31)  Time cost: 27:11/11:22 [1:05:58/22:01:34]  Acc_iter 3000        Data time: 0.01(0.01)  Forward time: 1.48(1.31)  Batch time: 1.49(1.31)
2025-09-03 02:26:35,371   INFO  Train:    2/36 (  6%) [1290/1759 ( 73%)]  Loss: 5.588 (5.79)  LR: 3.954e-04  Grad: 9.8127  max=0.5731(module.vfe.pfn_layers.0.linear.weight)  min: -0.6600(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1413, loss_cls=0.2961, loss_bbox=1.3949, matched_ious=0.3996, loss_iou=0.1053, loss_iou_reg=0.2678, d_time=0.01(0.01), f_time=1.32(1.30), b_time=1.33(1.31)  Time cost: 28:15/10:16 [1:07:02/21:59:36]  Acc_iter 3050        Data time: 0.01(0.01)  Forward time: 1.32(1.30)  Batch time: 1.33(1.31)
2025-09-03 02:27:40,583   INFO  Train:    2/36 (  6%) [1340/1759 ( 76%)]  Loss: 5.527 (5.78)  LR: 3.985e-04  Grad: 9.6389  max=0.5403(module.vfe.pfn_layers.0.linear.weight)  min: -0.3433(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1148, loss_cls=0.2856, loss_bbox=1.3578, matched_ious=0.4086, loss_iou=0.1060, loss_iou_reg=0.2689, d_time=0.01(0.01), f_time=1.41(1.30), b_time=1.42(1.31)  Time cost: 29:21/09:10 [1:08:07/21:58:09]  Acc_iter 3100        Data time: 0.01(0.01)  Forward time: 1.41(1.30)  Batch time: 1.42(1.31)
2025-09-03 02:28:45,523   INFO  Train:    2/36 (  6%) [1390/1759 ( 79%)]  Loss: 4.817 (5.76)  LR: 4.017e-04  Grad: 9.7950  max=0.6797(module.vfe.pfn_layers.0.linear.weight)  min: -0.6630(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0858, loss_cls=0.2865, loss_bbox=1.3181, matched_ious=0.4139, loss_iou=0.1051, loss_iou_reg=0.2652, d_time=0.01(0.01), f_time=1.26(1.30), b_time=1.26(1.31)  Time cost: 30:25/08:04 [1:09:12/21:56:32]  Acc_iter 3150        Data time: 0.01(0.01)  Forward time: 1.26(1.30)  Batch time: 1.26(1.31)
2025-09-03 02:29:51,081   INFO  Train:    2/36 (  6%) [1440/1759 ( 82%)]  Loss: 5.872 (5.76)  LR: 4.049e-04  Grad: 9.6492  max=0.3370(module.backbone_3d.cls_conv.3.weight)  min: -0.4565(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1401, loss_cls=0.2901, loss_bbox=1.4016, matched_ious=0.4014, loss_iou=0.1067, loss_iou_reg=0.2677, d_time=0.01(0.01), f_time=1.40(1.30), b_time=1.41(1.31)  Time cost: 31:31/06:58 [1:10:18/21:55:23]  Acc_iter 3200        Data time: 0.01(0.01)  Forward time: 1.40(1.30)  Batch time: 1.41(1.31)
2025-09-03 02:30:57,810   INFO  Train:    2/36 (  6%) [1490/1759 ( 85%)]  Loss: 7.382 (5.74)  LR: 4.081e-04  Grad: 9.7953  max=0.2482(module.vfe.pfn_layers.0.linear.weight)  min: -0.3339(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1066, loss_cls=0.2890, loss_bbox=1.3282, matched_ious=0.4094, loss_iou=0.1053, loss_iou_reg=0.2684, d_time=0.01(0.01), f_time=1.38(1.30), b_time=1.38(1.31)  Time cost: 32:38/05:53 [1:11:25/21:55:02]  Acc_iter 3250        Data time: 0.01(0.01)  Forward time: 1.38(1.30)  Batch time: 1.38(1.31)
2025-09-03 02:32:02,809   INFO  Train:    2/36 (  6%) [1540/1759 ( 88%)]  Loss: 5.024 (5.73)  LR: 4.114e-04  Grad: 9.7292  max=0.3987(module.vfe.pfn_layers.0.linear.weight)  min: -0.4977(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1776, loss_cls=0.2962, loss_bbox=1.3750, matched_ious=0.4022, loss_iou=0.1059, loss_iou_reg=0.2687, d_time=0.01(0.01), f_time=1.35(1.30), b_time=1.36(1.31)  Time cost: 33:43/04:47 [1:12:30/21:53:30]  Acc_iter 3300        Data time: 0.01(0.01)  Forward time: 1.35(1.30)  Batch time: 1.36(1.31)
2025-09-03 02:33:07,180   INFO  Train:    2/36 (  6%) [1590/1759 ( 90%)]  Loss: 4.874 (5.72)  LR: 4.148e-04  Grad: 9.7903  max=0.8676(module.vfe.pfn_layers.0.linear.weight)  min: -0.2305(module.backbone_3d.app.self_attn.in_proj_weight)  NaN: False  loss_hm=1.1334, loss_cls=0.2933, loss_bbox=1.3702, matched_ious=0.4114, loss_iou=0.1029, loss_iou_reg=0.2664, d_time=0.01(0.01), f_time=1.39(1.30), b_time=1.40(1.31)  Time cost: 34:47/03:41 [1:13:34/21:51:36]  Acc_iter 3350        Data time: 0.01(0.01)  Forward time: 1.39(1.30)  Batch time: 1.40(1.31)
2025-09-03 02:34:13,505   INFO  Train:    2/36 (  6%) [1640/1759 ( 93%)]  Loss: 5.864 (5.72)  LR: 4.182e-04  Grad: 10.0000  max=0.7025(module.backbone_3d.cls_conv.3.weight)  min: -0.4612(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1329, loss_cls=0.2892, loss_bbox=1.3606, matched_ious=0.3981, loss_iou=0.1042, loss_iou_reg=0.2701, d_time=0.01(0.01), f_time=1.27(1.30), b_time=1.27(1.31)  Time cost: 35:53/02:36 [1:14:40/21:50:57]  Acc_iter 3400        Data time: 0.01(0.01)  Forward time: 1.27(1.30)  Batch time: 1.27(1.31)
2025-09-03 02:35:19,028   INFO  Train:    2/36 (  6%) [1690/1759 ( 96%)]  Loss: 5.824 (5.71)  LR: 4.217e-04  Grad: 9.7803  max=0.8949(module.vfe.pfn_layers.0.linear.weight)  min: -1.0249(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1218, loss_cls=0.2843, loss_bbox=1.3997, matched_ious=0.4041, loss_iou=0.1047, loss_iou_reg=0.2677, d_time=0.01(0.01), f_time=1.32(1.30), b_time=1.32(1.31)  Time cost: 36:59/01:30 [1:15:46/21:49:47]  Acc_iter 3450        Data time: 0.01(0.01)  Forward time: 1.32(1.30)  Batch time: 1.32(1.31)
2025-09-03 02:36:23,952   INFO  Train:    2/36 (  6%) [1740/1759 ( 99%)]  Loss: 10.36 (5.70)  LR: 4.251e-04  Grad: 10.0000  max=1.4253(module.backbone_3d.cls_conv.3.weight)  min: -0.7738(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0905, loss_cls=0.2754, loss_bbox=1.3329, matched_ious=0.4050, loss_iou=0.1045, loss_iou_reg=0.2697, d_time=0.01(0.01), f_time=1.28(1.30), b_time=1.29(1.31)  Time cost: 38:04/00:24 [1:16:51/21:48:17]  Acc_iter 3500        Data time: 0.01(0.01)  Forward time: 1.28(1.30)  Batch time: 1.29(1.31)
2025-09-03 02:36:45,882   INFO  Train:    2/36 (  6%) [1758/1759 (100%)]  Loss: 4.647 (5.70)  LR: 4.264e-04  Grad: 8.4380  max=1.1288(module.vfe.pfn_layers.0.linear.weight)  min: -1.1640(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0903, loss_cls=0.2782, loss_bbox=1.2588, matched_ious=0.4301, loss_iou=0.0995, loss_iou_reg=0.2552, d_time=0.01(0.01), f_time=0.67(1.30), b_time=0.68(1.31)  Time cost: 38:26/00:01 [1:17:13/21:46:57]  Acc_iter 3518        Data time: 0.01(0.01)  Forward time: 0.67(1.30)  Batch time: 0.68(1.31)

                                               [Aepochs:   6%|▌         | 2/36 [1:17:13<21:51:54, 2315.13s/it]epochs:   6%|▌         | 2/36 [1:17:13<21:51:54, 2315.12s/it]epochs:   6%|▌         | 2/36 [1:17:13<21:51:56, 2315.19s/it]epochs:   6%|▌         | 2/36 [1:17:13<21:51:55, 2315.17s/it]epochs:   6%|▌         | 2/36 [1:17:13<21:51:56, 2315.18s/it]epochs:   6%|▌         | 2/36 [1:17:13<21:51:56, 2315.19s/it]epochs:   6%|▌         | 2/36 [1:17:13<21:51:56, 2315.20s/it]epochs:   6%|▌         | 2/36 [1:17:13<21:52:00, 2315.30s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 02:36:50,137   INFO  Train:    3/36 (  8%) [   0/1759 (  0%)]  Loss: 5.044 (5.04)  LR: 4.265e-04  Grad: 8.1572  max=0.7059(module.vfe.pfn_layers.0.linear.weight)  min: -0.6179(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0573, loss_cls=0.2770, loss_bbox=1.3417, matched_ious=0.4271, loss_iou=0.0861, loss_iou_reg=0.2514, d_time=0.98(0.98), f_time=2.28(2.28), b_time=3.27(3.27)  Time cost: 00:02/1:24:03 [1:17:17/47:37:50]  Acc_iter 3519        Data time: 0.98(0.98)  Forward time: 2.28(2.28)  Batch time: 3.27(3.27)
2025-09-03 02:37:30,989   INFO  Train:    3/36 (  8%) [  31/1759 (  2%)]  Loss: 4.958 (5.43)  LR: 4.287e-04  Grad: 8.3436  max=0.4661(module.vfe.pfn_layers.0.linear.weight)  min: -0.5620(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1232, loss_cls=0.2904, loss_bbox=1.3493, matched_ious=0.3974, loss_iou=0.1061, loss_iou_reg=0.2758, d_time=0.01(0.04), f_time=1.21(1.34), b_time=1.22(1.38)  Time cost: 00:43/39:20 [1:17:58/22:41:06]  Acc_iter 3550        Data time: 0.01(0.04)  Forward time: 1.21(1.34)  Batch time: 1.22(1.38)
2025-09-03 02:38:36,351   INFO  Train:    3/36 (  8%) [  81/1759 (  5%)]  Loss: 5.280 (5.40)  LR: 4.323e-04  Grad: 8.6602  max=0.8000(module.vfe.pfn_layers.0.linear.weight)  min: -0.9910(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0982, loss_cls=0.2774, loss_bbox=1.3535, matched_ious=0.4029, loss_iou=0.1048, loss_iou_reg=0.2700, d_time=0.01(0.02), f_time=1.27(1.32), b_time=1.28(1.34)  Time cost: 01:49/37:12 [1:19:03/22:04:09]  Acc_iter 3600        Data time: 0.01(0.02)  Forward time: 1.27(1.32)  Batch time: 1.28(1.34)
2025-09-03 02:39:40,935   INFO  Train:    3/36 (  8%) [ 131/1759 (  7%)]  Loss: 4.544 (5.38)  LR: 4.359e-04  Grad: 8.6053  max=0.2048(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.5565(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0845, loss_cls=0.2739, loss_bbox=1.3525, matched_ious=0.4129, loss_iou=0.1069, loss_iou_reg=0.2630, d_time=0.01(0.02), f_time=1.26(1.30), b_time=1.27(1.32)  Time cost: 02:53/35:41 [1:20:08/21:48:30]  Acc_iter 3650        Data time: 0.01(0.02)  Forward time: 1.26(1.30)  Batch time: 1.27(1.32)
2025-09-03 02:40:46,639   INFO  Train:    3/36 (  8%) [ 181/1759 ( 10%)]  Loss: 5.060 (5.37)  LR: 4.396e-04  Grad: 8.7781  max=0.2731(module.backbone_3d.cls_conv.3.weight)  min: -0.6780(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0854, loss_cls=0.2718, loss_bbox=1.3251, matched_ious=0.4133, loss_iou=0.1032, loss_iou_reg=0.2623, d_time=0.01(0.01), f_time=1.38(1.30), b_time=1.38(1.32)  Time cost: 03:59/34:35 [1:21:13/21:47:00]  Acc_iter 3700        Data time: 0.01(0.01)  Forward time: 1.38(1.30)  Batch time: 1.38(1.32)
2025-09-03 02:41:54,505   INFO  Train:    3/36 (  8%) [ 231/1759 ( 13%)]  Loss: 5.202 (5.37)  LR: 4.433e-04  Grad: 8.9564  max=0.2346(module.vfe.pfn_layers.0.linear.weight)  min: -0.4505(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0955, loss_cls=0.2767, loss_bbox=1.4189, matched_ious=0.4144, loss_iou=0.1045, loss_iou_reg=0.2647, d_time=0.01(0.01), f_time=1.26(1.31), b_time=1.27(1.33)  Time cost: 05:07/33:43 [1:22:21/21:54:54]  Acc_iter 3750        Data time: 0.01(0.01)  Forward time: 1.26(1.31)  Batch time: 1.27(1.33)
2025-09-03 02:42:59,412   INFO  Train:    3/36 (  8%) [ 281/1759 ( 16%)]  Loss: 5.321 (5.38)  LR: 4.471e-04  Grad: 7.4413  max=0.1838(module.backbone_3d.cls_conv.3.weight)  min: -0.3579(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1025, loss_cls=0.2759, loss_bbox=1.3571, matched_ious=0.4131, loss_iou=0.1054, loss_iou_reg=0.2661, d_time=0.01(0.01), f_time=1.27(1.31), b_time=1.28(1.32)  Time cost: 06:12/32:30 [1:23:26/21:49:12]  Acc_iter 3800        Data time: 0.01(0.01)  Forward time: 1.27(1.31)  Batch time: 1.28(1.32)
2025-09-03 02:44:04,517   INFO  Train:    3/36 (  8%) [ 331/1759 ( 19%)]  Loss: 5.347 (5.35)  LR: 4.509e-04  Grad: 7.7116  max=0.7763(module.vfe.pfn_layers.0.linear.weight)  min: -0.1864(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.0900, loss_cls=0.2781, loss_bbox=1.2425, matched_ious=0.4203, loss_iou=0.1040, loss_iou_reg=0.2647, d_time=0.01(0.01), f_time=1.23(1.31), b_time=1.24(1.32)  Time cost: 07:17/31:20 [1:24:31/21:45:29]  Acc_iter 3850        Data time: 0.01(0.01)  Forward time: 1.23(1.31)  Batch time: 1.24(1.32)
2025-09-03 02:45:11,047   INFO  Train:    3/36 (  8%) [ 381/1759 ( 22%)]  Loss: 4.736 (5.35)  LR: 4.548e-04  Grad: 7.9968  max=1.0400(module.vfe.pfn_layers.0.linear.weight)  min: -0.2782(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0936, loss_cls=0.2740, loss_bbox=1.3526, matched_ious=0.4096, loss_iou=0.1035, loss_iou_reg=0.2696, d_time=0.01(0.01), f_time=2.38(1.31), b_time=2.38(1.32)  Time cost: 08:23/30:17 [1:25:38/21:46:08]  Acc_iter 3900        Data time: 0.01(0.01)  Forward time: 2.38(1.31)  Batch time: 2.38(1.32)
2025-09-03 02:46:16,494   INFO  Train:    3/36 (  8%) [ 431/1759 ( 25%)]  Loss: 5.246 (5.33)  LR: 4.587e-04  Grad: 8.1782  max=0.7703(module.vfe.pfn_layers.0.linear.weight)  min: -0.2091(module.vfe.pfn_layers.1.linear.weight)  NaN: False  loss_hm=1.0687, loss_cls=0.2646, loss_bbox=1.2998, matched_ious=0.4113, loss_iou=0.1016, loss_iou_reg=0.2655, d_time=0.01(0.01), f_time=1.35(1.31), b_time=1.36(1.32)  Time cost: 09:29/29:09 [1:26:43/21:43:55]  Acc_iter 3950        Data time: 0.01(0.01)  Forward time: 1.35(1.31)  Batch time: 1.36(1.32)
2025-09-03 02:47:21,844   INFO  Train:    3/36 (  8%) [ 481/1759 ( 27%)]  Loss: 7.613 (5.32)  LR: 4.627e-04  Grad: 8.3383  max=0.3561(module.vfe.pfn_layers.0.linear.weight)  min: -0.4909(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0788, loss_cls=0.2700, loss_bbox=1.2860, matched_ious=0.4148, loss_iou=0.1051, loss_iou_reg=0.2634, d_time=0.01(0.01), f_time=1.30(1.31), b_time=1.31(1.32)  Time cost: 10:34/28:02 [1:27:49/21:41:44]  Acc_iter 4000        Data time: 0.01(0.01)  Forward time: 1.30(1.31)  Batch time: 1.31(1.32)
2025-09-03 02:48:26,626   INFO  Train:    3/36 (  8%) [ 531/1759 ( 30%)]  Loss: 6.054 (5.31)  LR: 4.667e-04  Grad: 8.6843  max=0.8369(module.vfe.pfn_layers.0.linear.weight)  min: -1.0206(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0644, loss_cls=0.2644, loss_bbox=1.2807, matched_ious=0.4101, loss_iou=0.1064, loss_iou_reg=0.2695, d_time=0.01(0.01), f_time=1.33(1.30), b_time=1.34(1.32)  Time cost: 11:39/26:54 [1:28:53/21:38:41]  Acc_iter 4050        Data time: 0.01(0.01)  Forward time: 1.33(1.30)  Batch time: 1.34(1.32)
2025-09-03 02:49:31,318   INFO  Train:    3/36 (  8%) [ 581/1759 ( 33%)]  Loss: 4.906 (5.29)  LR: 4.707e-04  Grad: 8.8218  max=1.3710(module.vfe.pfn_layers.0.linear.weight)  min: -0.8332(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0530, loss_cls=0.2596, loss_bbox=1.2616, matched_ious=0.4142, loss_iou=0.1047, loss_iou_reg=0.2687, d_time=0.01(0.01), f_time=1.17(1.30), b_time=1.18(1.31)  Time cost: 12:44/25:46 [1:29:58/21:35:50]  Acc_iter 4100        Data time: 0.01(0.01)  Forward time: 1.17(1.30)  Batch time: 1.18(1.31)
2025-09-03 02:50:36,687   INFO  Train:    3/36 (  8%) [ 631/1759 ( 36%)]  Loss: 5.088 (5.29)  LR: 4.748e-04  Grad: 8.8790  max=0.4055(module.vfe.pfn_layers.0.linear.weight)  min: -0.5386(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0809, loss_cls=0.2694, loss_bbox=1.3048, matched_ious=0.4092, loss_iou=0.1043, loss_iou_reg=0.2680, d_time=0.01(0.01), f_time=1.25(1.30), b_time=1.26(1.31)  Time cost: 13:49/24:40 [1:31:04/21:34:19]  Acc_iter 4150        Data time: 0.01(0.01)  Forward time: 1.25(1.30)  Batch time: 1.26(1.31)
2025-09-03 02:51:42,104   INFO  Train:    3/36 (  8%) [ 681/1759 ( 39%)]  Loss: 6.479 (5.29)  LR: 4.790e-04  Grad: 9.1514  max=1.4857(module.vfe.pfn_layers.0.linear.weight)  min: -2.8654(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0528, loss_cls=0.2605, loss_bbox=1.3022, matched_ious=0.4161, loss_iou=0.1069, loss_iou_reg=0.2662, d_time=0.01(0.01), f_time=1.34(1.30), b_time=1.35(1.31)  Time cost: 14:54/23:34 [1:32:09/21:32:56]  Acc_iter 4200        Data time: 0.01(0.01)  Forward time: 1.34(1.30)  Batch time: 1.35(1.31)
2025-09-03 02:52:50,503   INFO  Train:    3/36 (  8%) [ 731/1759 ( 42%)]  Loss: 5.376 (5.27)  LR: 4.832e-04  Grad: 8.8087  max=0.4150(module.vfe.pfn_layers.0.linear.weight)  min: -1.6927(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0066, loss_cls=0.2553, loss_bbox=1.2518, matched_ious=0.4204, loss_iou=0.1051, loss_iou_reg=0.2633, d_time=0.01(0.01), f_time=1.30(1.31), b_time=1.31(1.32)  Time cost: 16:03/22:32 [1:33:17/21:35:36]  Acc_iter 4250        Data time: 0.01(0.01)  Forward time: 1.30(1.31)  Batch time: 1.31(1.32)
2025-09-03 02:53:55,886   INFO  Train:    3/36 (  8%) [ 781/1759 ( 44%)]  Loss: 5.759 (5.27)  LR: 4.874e-04  Grad: 8.7685  max=0.2627(module.vfe.pfn_layers.0.linear.weight)  min: -0.8337(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0665, loss_cls=0.2591, loss_bbox=1.2767, matched_ious=0.4243, loss_iou=0.1025, loss_iou_reg=0.2627, d_time=0.01(0.01), f_time=1.24(1.31), b_time=1.25(1.32)  Time cost: 17:08/21:26 [1:34:23/21:33:59]  Acc_iter 4300        Data time: 0.01(0.01)  Forward time: 1.24(1.31)  Batch time: 1.25(1.32)
2025-09-03 02:55:01,286   INFO  Train:    3/36 (  8%) [ 831/1759 ( 47%)]  Loss: 6.076 (5.25)  LR: 4.917e-04  Grad: 7.7183  max=2.7811(module.vfe.pfn_layers.0.linear.weight)  min: -0.3149(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0233, loss_cls=0.2527, loss_bbox=1.2713, matched_ious=0.4237, loss_iou=0.1005, loss_iou_reg=0.2603, d_time=0.01(0.01), f_time=1.21(1.31), b_time=1.22(1.32)  Time cost: 18:14/20:20 [1:35:28/21:32:27]  Acc_iter 4350        Data time: 0.01(0.01)  Forward time: 1.21(1.31)  Batch time: 1.22(1.32)
2025-09-03 02:56:07,607   INFO  Train:    3/36 (  8%) [ 881/1759 ( 50%)]  Loss: 5.115 (5.25)  LR: 4.960e-04  Grad: 7.8016  max=1.7544(module.vfe.pfn_layers.0.linear.weight)  min: -0.2858(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=1.0299, loss_cls=0.2567, loss_bbox=1.2843, matched_ious=0.4268, loss_iou=0.1018, loss_iou_reg=0.2618, d_time=0.01(0.01), f_time=1.17(1.31), b_time=1.18(1.32)  Time cost: 19:20/19:15 [1:36:34/21:31:59]  Acc_iter 4400        Data time: 0.01(0.01)  Forward time: 1.17(1.31)  Batch time: 1.18(1.32)
2025-09-03 02:57:13,572   INFO  Train:    3/36 (  8%) [ 931/1759 ( 53%)]  Loss: 5.402 (5.24)  LR: 5.004e-04  Grad: 8.2271  max=3.0788(module.vfe.pfn_layers.0.linear.weight)  min: -1.3564(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0479, loss_cls=0.2595, loss_bbox=1.2099, matched_ious=0.4311, loss_iou=0.1047, loss_iou_reg=0.2610, d_time=0.01(0.01), f_time=1.32(1.31), b_time=1.33(1.32)  Time cost: 20:26/18:09 [1:37:40/21:31:06]  Acc_iter 4450        Data time: 0.01(0.01)  Forward time: 1.32(1.31)  Batch time: 1.33(1.32)
2025-09-03 02:58:18,051   INFO  Train:    3/36 (  8%) [ 981/1759 ( 56%)]  Loss: 4.820 (5.23)  LR: 5.048e-04  Grad: 7.4412  max=2.8178(module.vfe.pfn_layers.0.linear.weight)  min: -1.1862(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0379, loss_cls=0.2525, loss_bbox=1.2885, matched_ious=0.4274, loss_iou=0.1046, loss_iou_reg=0.2611, d_time=0.01(0.01), f_time=1.26(1.31), b_time=1.26(1.31)  Time cost: 21:30/17:02 [1:38:45/21:28:42]  Acc_iter 4500        Data time: 0.01(0.01)  Forward time: 1.26(1.31)  Batch time: 1.26(1.31)
2025-09-03 02:59:23,445   INFO  Train:    3/36 (  8%) [1031/1759 ( 59%)]  Loss: 5.531 (5.23)  LR: 5.092e-04  Grad: 7.4775  max=1.1105(module.vfe.pfn_layers.0.linear.weight)  min: -2.1953(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0533, loss_cls=0.2491, loss_bbox=1.3278, matched_ious=0.4157, loss_iou=0.1080, loss_iou_reg=0.2656, d_time=0.01(0.01), f_time=1.32(1.30), b_time=1.33(1.31)  Time cost: 22:36/15:56 [1:39:50/21:27:17]  Acc_iter 4550        Data time: 0.01(0.01)  Forward time: 1.32(1.30)  Batch time: 1.33(1.31)
2025-09-03 03:00:29,484   INFO  Train:    3/36 (  8%) [1081/1759 ( 61%)]  Loss: 4.447 (5.22)  LR: 5.137e-04  Grad: 7.6466  max=1.7863(module.vfe.pfn_layers.0.linear.weight)  min: -4.6310(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0314, loss_cls=0.2484, loss_bbox=1.2705, matched_ious=0.4226, loss_iou=0.1035, loss_iou_reg=0.2643, d_time=0.01(0.01), f_time=1.36(1.31), b_time=1.37(1.31)  Time cost: 23:42/14:51 [1:40:56/21:26:29]  Acc_iter 4600        Data time: 0.01(0.01)  Forward time: 1.36(1.31)  Batch time: 1.37(1.31)
2025-09-03 03:01:35,230   INFO  Train:    3/36 (  8%) [1131/1759 ( 64%)]  Loss: 4.936 (5.21)  LR: 5.183e-04  Grad: 7.2653  max=4.0164(module.vfe.pfn_layers.0.linear.weight)  min: -0.1577(module.backbone_3d.app.self_attn.in_proj_weight)  NaN: False  loss_hm=0.9948, loss_cls=0.2430, loss_bbox=1.1947, matched_ious=0.4361, loss_iou=0.1020, loss_iou_reg=0.2591, d_time=0.01(0.01), f_time=1.37(1.31), b_time=1.38(1.31)  Time cost: 24:47/13:45 [1:42:02/21:25:25]  Acc_iter 4650        Data time: 0.01(0.01)  Forward time: 1.37(1.31)  Batch time: 1.38(1.31)
2025-09-03 03:02:42,153   INFO  Train:    3/36 (  8%) [1181/1759 ( 67%)]  Loss: 5.004 (5.21)  LR: 5.229e-04  Grad: 6.1933  max=1.3122(module.vfe.pfn_layers.0.linear.weight)  min: -2.2312(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0461, loss_cls=0.2556, loss_bbox=1.2910, matched_ious=0.4269, loss_iou=0.1036, loss_iou_reg=0.2636, d_time=0.01(0.01), f_time=1.29(1.31), b_time=1.30(1.32)  Time cost: 25:54/12:40 [1:43:09/21:25:19]  Acc_iter 4700        Data time: 0.01(0.01)  Forward time: 1.29(1.31)  Batch time: 1.30(1.32)
2025-09-03 03:03:50,916   INFO  Train:    3/36 (  8%) [1231/1759 ( 70%)]  Loss: 6.095 (5.20)  LR: 5.275e-04  Grad: 7.0400  max=3.6147(module.vfe.pfn_layers.0.linear.weight)  min: -1.9010(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0154, loss_cls=0.2481, loss_bbox=1.2370, matched_ious=0.4307, loss_iou=0.1034, loss_iou_reg=0.2596, d_time=0.01(0.01), f_time=1.31(1.31), b_time=1.32(1.32)  Time cost: 27:03/11:35 [1:44:18/21:26:35]  Acc_iter 4750        Data time: 0.01(0.01)  Forward time: 1.31(1.31)  Batch time: 1.32(1.32)
2025-09-03 03:04:57,299   INFO  Train:    3/36 (  8%) [1281/1759 ( 73%)]  Loss: 5.386 (5.20)  LR: 5.322e-04  Grad: 7.6200  max=3.2732(module.vfe.pfn_layers.0.linear.weight)  min: -0.2576(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.0467, loss_cls=0.2469, loss_bbox=1.3340, matched_ious=0.4233, loss_iou=0.1060, loss_iou_reg=0.2597, d_time=0.01(0.01), f_time=1.36(1.31), b_time=1.36(1.32)  Time cost: 28:10/10:30 [1:45:24/21:25:52]  Acc_iter 4800        Data time: 0.01(0.01)  Forward time: 1.36(1.31)  Batch time: 1.36(1.32)
2025-09-03 03:06:03,179   INFO  Train:    3/36 (  8%) [1331/1759 ( 76%)]  Loss: 4.975 (5.20)  LR: 5.369e-04  Grad: 7.2653  max=3.2802(module.vfe.pfn_layers.0.linear.weight)  min: -0.2831(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.0385, loss_cls=0.2540, loss_bbox=1.2123, matched_ious=0.4165, loss_iou=0.1063, loss_iou_reg=0.2668, d_time=0.01(0.01), f_time=1.39(1.31), b_time=1.39(1.32)  Time cost: 29:15/09:24 [1:46:30/21:24:44]  Acc_iter 4850        Data time: 0.01(0.01)  Forward time: 1.39(1.31)  Batch time: 1.39(1.32)
2025-09-03 03:07:08,669   INFO  Train:    3/36 (  8%) [1381/1759 ( 79%)]  Loss: 4.555 (5.19)  LR: 5.416e-04  Grad: 5.7984  max=0.8016(module.vfe.pfn_layers.0.linear.weight)  min: -0.7729(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0020, loss_cls=0.2456, loss_bbox=1.2325, matched_ious=0.4320, loss_iou=0.1028, loss_iou_reg=0.2605, d_time=0.01(0.01), f_time=1.21(1.31), b_time=1.21(1.32)  Time cost: 30:21/08:18 [1:47:36/21:23:20]  Acc_iter 4900        Data time: 0.01(0.01)  Forward time: 1.21(1.31)  Batch time: 1.21(1.32)
2025-09-03 03:08:13,604   INFO  Train:    3/36 (  8%) [1431/1759 ( 81%)]  Loss: 4.863 (5.18)  LR: 5.464e-04  Grad: 6.5462  max=1.9311(module.vfe.pfn_layers.0.linear.weight)  min: -1.6266(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9902, loss_cls=0.2432, loss_bbox=1.1610, matched_ious=0.4334, loss_iou=0.1019, loss_iou_reg=0.2601, d_time=0.01(0.01), f_time=1.15(1.31), b_time=1.15(1.32)  Time cost: 31:26/07:12 [1:48:40/21:21:35]  Acc_iter 4950        Data time: 0.01(0.01)  Forward time: 1.15(1.31)  Batch time: 1.15(1.32)
2025-09-03 03:09:19,059   INFO  Train:    3/36 (  8%) [1481/1759 ( 84%)]  Loss: 4.747 (5.18)  LR: 5.513e-04  Grad: 9.8585  max=6.5150(module.vfe.pfn_layers.0.linear.weight)  min: -2.0230(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0407, loss_cls=0.2510, loss_bbox=1.2208, matched_ious=0.4245, loss_iou=0.1037, loss_iou_reg=0.2633, d_time=0.01(0.01), f_time=1.26(1.31), b_time=1.27(1.32)  Time cost: 32:31/06:06 [1:49:46/21:20:13]  Acc_iter 5000        Data time: 0.01(0.01)  Forward time: 1.26(1.31)  Batch time: 1.27(1.32)
2025-09-03 03:10:24,229   INFO  Train:    3/36 (  8%) [1531/1759 ( 87%)]  Loss: 5.219 (5.17)  LR: 5.562e-04  Grad: 7.0554  max=2.0198(module.vfe.pfn_layers.0.linear.weight)  min: -2.6503(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0067, loss_cls=0.2485, loss_bbox=1.2238, matched_ious=0.4329, loss_iou=0.1033, loss_iou_reg=0.2573, d_time=0.01(0.01), f_time=1.32(1.31), b_time=1.33(1.32)  Time cost: 33:36/05:00 [1:50:51/21:18:42]  Acc_iter 5050        Data time: 0.01(0.01)  Forward time: 1.32(1.31)  Batch time: 1.33(1.32)
2025-09-03 03:11:29,108   INFO  Train:    3/36 (  8%) [1581/1759 ( 90%)]  Loss: 5.356 (5.17)  LR: 5.611e-04  Grad: 7.7961  max=2.8678(module.vfe.pfn_layers.0.linear.weight)  min: -4.8079(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0103, loss_cls=0.2445, loss_bbox=1.2677, matched_ious=0.4331, loss_iou=0.1011, loss_iou_reg=0.2609, d_time=0.01(0.01), f_time=1.30(1.31), b_time=1.30(1.32)  Time cost: 34:41/03:54 [1:51:56/21:17:01]  Acc_iter 5100        Data time: 0.01(0.01)  Forward time: 1.30(1.31)  Batch time: 1.30(1.32)
2025-09-03 03:12:34,037   INFO  Train:    3/36 (  8%) [1631/1759 ( 93%)]  Loss: 3.947 (5.16)  LR: 5.661e-04  Grad: 5.9638  max=2.1545(module.vfe.pfn_layers.0.linear.weight)  min: -0.1810(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.9785, loss_cls=0.2436, loss_bbox=1.1852, matched_ious=0.4372, loss_iou=0.1029, loss_iou_reg=0.2565, d_time=0.01(0.01), f_time=1.26(1.31), b_time=1.27(1.32)  Time cost: 35:46/02:48 [1:53:01/21:15:24]  Acc_iter 5150        Data time: 0.01(0.01)  Forward time: 1.26(1.31)  Batch time: 1.27(1.32)
2025-09-03 03:13:39,056   INFO  Train:    3/36 (  8%) [1681/1759 ( 96%)]  Loss: 5.631 (5.15)  LR: 5.711e-04  Grad: 7.6663  max=0.2662(module.vfe.pfn_layers.0.linear.weight)  min: -3.6162(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9879, loss_cls=0.2425, loss_bbox=1.1794, matched_ious=0.4355, loss_iou=0.1034, loss_iou_reg=0.2580, d_time=0.01(0.01), f_time=1.30(1.31), b_time=1.31(1.32)  Time cost: 36:51/01:42 [1:54:06/21:13:52]  Acc_iter 5200        Data time: 0.01(0.01)  Forward time: 1.30(1.31)  Batch time: 1.31(1.32)
2025-09-03 03:14:46,649   INFO  Train:    3/36 (  8%) [1731/1759 ( 98%)]  Loss: 4.483 (5.14)  LR: 5.761e-04  Grad: 6.5553  max=3.2859(module.vfe.pfn_layers.0.linear.weight)  min: -0.7838(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9816, loss_cls=0.2367, loss_bbox=1.2002, matched_ious=0.4294, loss_iou=0.1024, loss_iou_reg=0.2605, d_time=0.01(0.01), f_time=1.26(1.31), b_time=1.26(1.32)  Time cost: 37:59/00:36 [1:55:13/21:13:48]  Acc_iter 5250        Data time: 0.01(0.01)  Forward time: 1.26(1.31)  Batch time: 1.26(1.32)
2025-09-03 03:15:21,776   INFO  Train:    3/36 (  8%) [1758/1759 (100%)]  Loss: 5.569 (5.14)  LR: 5.789e-04  Grad: 6.4403  max=0.1299(module.vfe.pfn_layers.0.linear.weight)  min: -3.8451(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0365, loss_cls=0.2488, loss_bbox=1.2273, matched_ious=0.4308, loss_iou=0.1017, loss_iou_reg=0.2617, d_time=0.01(0.01), f_time=0.73(1.31), b_time=0.74(1.32)  Time cost: 38:34/00:01 [1:55:49/21:13:00]  Acc_iter 5277        Data time: 0.01(0.01)  Forward time: 0.73(1.31)  Batch time: 0.74(1.32)

                                               [Aepochs:   8%|▊         | 3/36 [1:55:49<21:13:29, 2315.45s/it]epochs:   8%|▊         | 3/36 [1:55:49<21:13:31, 2315.49s/it]epochs:   8%|▊         | 3/36 [1:55:49<21:13:30, 2315.46s/it]epochs:   8%|▊         | 3/36 [1:55:49<21:13:30, 2315.47s/it]epochs:   8%|▊         | 3/36 [1:55:49<21:13:31, 2315.50s/it]epochs:   8%|▊         | 3/36 [1:55:49<21:13:30, 2315.48s/it]epochs:   8%|▊         | 3/36 [1:55:49<21:13:31, 2315.50s/it]epochs:   8%|▊         | 3/36 [1:55:49<21:13:33, 2315.56s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 03:15:26,052   INFO  Train:    4/36 ( 11%) [   0/1759 (  0%)]  Loss: 4.756 (4.76)  LR: 5.790e-04  Grad: 7.6053  max=0.4057(module.vfe.pfn_layers.0.linear.weight)  min: -5.8747(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9898, loss_cls=0.2358, loss_bbox=1.0131, matched_ious=0.4486, loss_iou=0.1194, loss_iou_reg=0.2426, d_time=1.23(1.23), f_time=2.04(2.04), b_time=3.27(3.27)  Time cost: 00:02/1:24:44 [1:55:53/46:36:37]  Acc_iter 5278        Data time: 1.23(1.23)  Forward time: 2.04(2.04)  Batch time: 3.27(3.27)
2025-09-03 03:15:55,139   INFO  Train:    4/36 ( 11%) [  22/1759 (  1%)]  Loss: 5.423 (4.88)  LR: 5.812e-04  Grad: 7.0527  max=0.7600(module.vfe.pfn_layers.0.linear.weight)  min: -4.3434(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0210, loss_cls=0.2408, loss_bbox=1.1610, matched_ious=0.4397, loss_iou=0.1054, loss_iou_reg=0.2599, d_time=0.01(0.06), f_time=1.34(1.35), b_time=1.34(1.41)  Time cost: 00:31/40:15 [1:56:22/22:24:55]  Acc_iter 5300        Data time: 0.01(0.06)  Forward time: 1.34(1.35)  Batch time: 1.34(1.41)
2025-09-03 03:17:00,554   INFO  Train:    4/36 ( 11%) [  72/1759 (  4%)]  Loss: 4.647 (4.89)  LR: 5.864e-04  Grad: 6.6506  max=1.6212(module.vfe.pfn_layers.0.linear.weight)  min: -4.8338(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9793, loss_cls=0.2373, loss_bbox=1.2522, matched_ious=0.4285, loss_iou=0.1023, loss_iou_reg=0.2621, d_time=0.01(0.02), f_time=1.39(1.31), b_time=1.40(1.34)  Time cost: 01:37/37:30 [1:57:27/21:29:12]  Acc_iter 5350        Data time: 0.01(0.02)  Forward time: 1.39(1.31)  Batch time: 1.40(1.34)
2025-09-03 03:18:05,459   INFO  Train:    4/36 ( 11%) [ 122/1759 (  7%)]  Loss: 5.433 (4.90)  LR: 5.915e-04  Grad: 4.3050  max=1.8236(module.vfe.pfn_layers.0.linear.weight)  min: -0.5154(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0033, loss_cls=0.2412, loss_bbox=1.1266, matched_ious=0.4386, loss_iou=0.1033, loss_iou_reg=0.2583, d_time=0.01(0.02), f_time=1.30(1.30), b_time=1.31(1.32)  Time cost: 02:42/36:00 [1:58:32/21:13:56]  Acc_iter 5400        Data time: 0.01(0.02)  Forward time: 1.30(1.30)  Batch time: 1.31(1.32)
2025-09-03 03:19:11,169   INFO  Train:    4/36 ( 11%) [ 172/1759 ( 10%)]  Loss: 4.942 (4.88)  LR: 5.968e-04  Grad: 8.8759  max=6.5769(module.vfe.pfn_layers.0.linear.weight)  min: -4.3757(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9666, loss_cls=0.2401, loss_bbox=1.1635, matched_ious=0.4407, loss_iou=0.0996, loss_iou_reg=0.2558, d_time=0.01(0.01), f_time=1.29(1.31), b_time=1.30(1.32)  Time cost: 03:48/34:51 [1:59:38/21:11:19]  Acc_iter 5450        Data time: 0.01(0.01)  Forward time: 1.29(1.31)  Batch time: 1.30(1.32)
2025-09-03 03:20:16,278   INFO  Train:    4/36 ( 11%) [ 222/1759 ( 13%)]  Loss: 4.821 (4.91)  LR: 6.020e-04  Grad: 5.6300  max=0.2800(module.backbone_3d.cls_conv.3.weight)  min: -3.7420(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0206, loss_cls=0.2384, loss_bbox=1.2231, matched_ious=0.4341, loss_iou=0.1019, loss_iou_reg=0.2591, d_time=0.01(0.01), f_time=1.38(1.30), b_time=1.39(1.32)  Time cost: 04:53/33:40 [2:00:43/21:06:49]  Acc_iter 5500        Data time: 0.01(0.01)  Forward time: 1.38(1.30)  Batch time: 1.39(1.32)
2025-09-03 03:21:21,718   INFO  Train:    4/36 ( 11%) [ 272/1759 ( 15%)]  Loss: 4.912 (4.91)  LR: 6.073e-04  Grad: 5.5092  max=0.2904(module.dense_head.prediction_head.height.1.weight)  min: -3.0157(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9958, loss_cls=0.2413, loss_bbox=1.1397, matched_ious=0.4307, loss_iou=0.1023, loss_iou_reg=0.2626, d_time=0.01(0.01), f_time=1.32(1.30), b_time=1.33(1.31)  Time cost: 05:58/32:33 [2:01:49/21:04:43]  Acc_iter 5550        Data time: 0.01(0.01)  Forward time: 1.32(1.30)  Batch time: 1.33(1.31)
2025-09-03 03:22:26,610   INFO  Train:    4/36 ( 11%) [ 322/1759 ( 18%)]  Loss: 4.830 (4.92)  LR: 6.127e-04  Grad: 4.4775  max=1.7036(module.vfe.pfn_layers.0.linear.weight)  min: -1.9225(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0165, loss_cls=0.2376, loss_bbox=1.2101, matched_ious=0.4300, loss_iou=0.1046, loss_iou_reg=0.2612, d_time=0.01(0.01), f_time=1.21(1.30), b_time=1.22(1.31)  Time cost: 07:03/31:23 [2:02:53/21:01:18]  Acc_iter 5600        Data time: 0.01(0.01)  Forward time: 1.21(1.30)  Batch time: 1.22(1.31)
2025-09-03 03:23:31,501   INFO  Train:    4/36 ( 11%) [ 372/1759 ( 21%)]  Loss: 4.649 (4.92)  LR: 6.180e-04  Grad: 6.4447  max=1.2744(module.vfe.pfn_layers.0.linear.weight)  min: -4.5394(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0007, loss_cls=0.2405, loss_bbox=1.1572, matched_ious=0.4413, loss_iou=0.1049, loss_iou_reg=0.2556, d_time=0.01(0.01), f_time=1.21(1.30), b_time=1.22(1.31)  Time cost: 08:08/30:15 [2:03:58/20:58:31]  Acc_iter 5650        Data time: 0.01(0.01)  Forward time: 1.21(1.30)  Batch time: 1.22(1.31)
2025-09-03 03:24:40,070   INFO  Train:    4/36 ( 11%) [ 422/1759 ( 24%)]  Loss: 4.916 (4.91)  LR: 6.234e-04  Grad: 2.9056  max=0.8614(module.vfe.pfn_layers.0.linear.weight)  min: -0.4699(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.9718, loss_cls=0.2313, loss_bbox=1.1521, matched_ious=0.4413, loss_iou=0.1023, loss_iou_reg=0.2587, d_time=0.01(0.01), f_time=2.14(1.31), b_time=2.14(1.32)  Time cost: 09:16/29:20 [2:05:07/21:04:28]  Acc_iter 5700        Data time: 0.01(0.01)  Forward time: 2.14(1.31)  Batch time: 2.14(1.32)
2025-09-03 03:25:46,402   INFO  Train:    4/36 ( 11%) [ 472/1759 ( 27%)]  Loss: 4.483 (4.89)  LR: 6.289e-04  Grad: 10.0000  max=0.2342(module.vfe.pfn_layers.0.linear.weight)  min: -7.0237(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9472, loss_cls=0.2261, loss_bbox=1.1576, matched_ious=0.4430, loss_iou=0.1011, loss_iou_reg=0.2546, d_time=0.01(0.01), f_time=1.21(1.31), b_time=1.22(1.32)  Time cost: 10:23/28:15 [2:06:13/21:04:24]  Acc_iter 5750        Data time: 0.01(0.01)  Forward time: 1.21(1.31)  Batch time: 1.22(1.32)
2025-09-03 03:26:51,163   INFO  Train:    4/36 ( 11%) [ 522/1759 ( 30%)]  Loss: 4.396 (4.89)  LR: 6.344e-04  Grad: 3.0052  max=1.9513(module.vfe.pfn_layers.0.linear.weight)  min: -0.2430(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.9895, loss_cls=0.2328, loss_bbox=1.1959, matched_ious=0.4364, loss_iou=0.0990, loss_iou_reg=0.2562, d_time=0.01(0.01), f_time=1.24(1.30), b_time=1.25(1.32)  Time cost: 11:28/27:07 [2:07:18/21:01:14]  Acc_iter 5800        Data time: 0.01(0.01)  Forward time: 1.24(1.30)  Batch time: 1.25(1.32)
2025-09-03 03:27:55,559   INFO  Train:    4/36 ( 11%) [ 572/1759 ( 33%)]  Loss: 5.341 (4.89)  LR: 6.399e-04  Grad: 4.8625  max=0.1273(module.vfe.pfn_layers.0.linear.weight)  min: -2.9855(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9583, loss_cls=0.2268, loss_bbox=1.1678, matched_ious=0.4321, loss_iou=0.1054, loss_iou_reg=0.2615, d_time=0.01(0.01), f_time=1.25(1.30), b_time=1.26(1.31)  Time cost: 12:32/25:58 [2:08:22/20:57:50]  Acc_iter 5850        Data time: 0.01(0.01)  Forward time: 1.25(1.30)  Batch time: 1.26(1.31)
2025-09-03 03:29:00,497   INFO  Train:    4/36 ( 11%) [ 622/1759 ( 35%)]  Loss: 5.441 (4.89)  LR: 6.455e-04  Grad: 5.4429  max=3.8320(module.vfe.pfn_layers.0.linear.weight)  min: -0.2177(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9971, loss_cls=0.2391, loss_bbox=1.1516, matched_ious=0.4375, loss_iou=0.1028, loss_iou_reg=0.2586, d_time=0.01(0.01), f_time=1.29(1.30), b_time=1.29(1.31)  Time cost: 13:37/24:51 [2:09:27/20:55:38]  Acc_iter 5900        Data time: 0.01(0.01)  Forward time: 1.29(1.30)  Batch time: 1.29(1.31)
2025-09-03 03:30:06,754   INFO  Train:    4/36 ( 11%) [ 672/1759 ( 38%)]  Loss: 4.915 (4.89)  LR: 6.511e-04  Grad: 3.3920  max=2.3861(module.vfe.pfn_layers.0.linear.weight)  min: -1.2530(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0204, loss_cls=0.2303, loss_bbox=1.2461, matched_ious=0.4389, loss_iou=0.1028, loss_iou_reg=0.2546, d_time=0.01(0.01), f_time=1.26(1.30), b_time=1.27(1.31)  Time cost: 14:43/23:47 [2:10:34/20:55:29]  Acc_iter 5950        Data time: 0.01(0.01)  Forward time: 1.26(1.30)  Batch time: 1.27(1.31)
2025-09-03 03:31:12,167   INFO  Train:    4/36 ( 11%) [ 722/1759 ( 41%)]  Loss: 5.810 (4.89)  LR: 6.568e-04  Grad: 4.0936  max=3.4551(module.vfe.pfn_layers.0.linear.weight)  min: -0.0775(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9904, loss_cls=0.2310, loss_bbox=1.2035, matched_ious=0.4338, loss_iou=0.1024, loss_iou_reg=0.2588, d_time=0.01(0.01), f_time=1.32(1.30), b_time=1.32(1.31)  Time cost: 15:49/22:41 [2:11:39/20:54:05]  Acc_iter 6000        Data time: 0.01(0.01)  Forward time: 1.32(1.30)  Batch time: 1.32(1.31)
2025-09-03 03:32:17,762   INFO  Train:    4/36 ( 11%) [ 772/1759 ( 44%)]  Loss: 4.118 (4.89)  LR: 6.625e-04  Grad: 2.8988  max=0.8991(module.vfe.pfn_layers.0.linear.weight)  min: -0.8662(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0061, loss_cls=0.2383, loss_bbox=1.1728, matched_ious=0.4351, loss_iou=0.1003, loss_iou_reg=0.2584, d_time=0.01(0.01), f_time=1.33(1.30), b_time=1.34(1.31)  Time cost: 16:54/21:35 [2:12:45/20:52:56]  Acc_iter 6050        Data time: 0.01(0.01)  Forward time: 1.33(1.30)  Batch time: 1.34(1.31)
2025-09-03 03:33:23,189   INFO  Train:    4/36 ( 11%) [ 822/1759 ( 47%)]  Loss: 4.285 (4.89)  LR: 6.682e-04  Grad: 9.3666  max=2.2658(module.vfe.pfn_layers.0.linear.weight)  min: -6.6154(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9831, loss_cls=0.2351, loss_bbox=1.1107, matched_ious=0.4401, loss_iou=0.1010, loss_iou_reg=0.2592, d_time=0.01(0.01), f_time=1.42(1.30), b_time=1.43(1.31)  Time cost: 18:00/20:29 [2:13:50/20:51:37]  Acc_iter 6100        Data time: 0.01(0.01)  Forward time: 1.42(1.30)  Batch time: 1.43(1.31)
2025-09-03 03:34:27,919   INFO  Train:    4/36 ( 11%) [ 872/1759 ( 50%)]  Loss: 4.010 (4.89)  LR: 6.740e-04  Grad: 3.1373  max=0.9232(module.vfe.pfn_layers.0.linear.weight)  min: -1.9036(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9918, loss_cls=0.2298, loss_bbox=1.1895, matched_ious=0.4318, loss_iou=0.1053, loss_iou_reg=0.2619, d_time=0.01(0.01), f_time=1.31(1.30), b_time=1.32(1.31)  Time cost: 19:04/19:23 [2:14:55/20:49:33]  Acc_iter 6150        Data time: 0.01(0.01)  Forward time: 1.31(1.30)  Batch time: 1.32(1.31)
2025-09-03 03:35:37,637   INFO  Train:    4/36 ( 11%) [ 922/1759 ( 52%)]  Loss: 4.862 (4.88)  LR: 6.798e-04  Grad: 4.9753  max=3.5752(module.vfe.pfn_layers.0.linear.weight)  min: -1.7528(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9616, loss_cls=0.2294, loss_bbox=1.1176, matched_ious=0.4300, loss_iou=0.1050, loss_iou_reg=0.2642, d_time=0.01(0.01), f_time=1.21(1.31), b_time=1.22(1.32)  Time cost: 20:14/18:21 [2:16:04/20:52:45]  Acc_iter 6200        Data time: 0.01(0.01)  Forward time: 1.21(1.31)  Batch time: 1.22(1.32)
2025-09-03 03:36:42,274   INFO  Train:    4/36 ( 11%) [ 972/1759 ( 55%)]  Loss: 4.125 (4.88)  LR: 6.856e-04  Grad: 3.6090  max=0.2070(module.backbone_3d.cls_conv.3.weight)  min: -1.9358(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9553, loss_cls=0.2253, loss_bbox=1.1354, matched_ious=0.4427, loss_iou=0.1026, loss_iou_reg=0.2548, d_time=0.01(0.01), f_time=1.18(1.31), b_time=1.19(1.32)  Time cost: 21:19/17:14 [2:17:09/20:50:31]  Acc_iter 6250        Data time: 0.01(0.01)  Forward time: 1.18(1.31)  Batch time: 1.19(1.32)
2025-09-03 03:37:47,276   INFO  Train:    4/36 ( 11%) [1022/1759 ( 58%)]  Loss: 5.036 (4.86)  LR: 6.915e-04  Grad: 8.3196  max=6.9338(module.vfe.pfn_layers.0.linear.weight)  min: -3.3500(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9469, loss_cls=0.2220, loss_bbox=1.0904, matched_ious=0.4364, loss_iou=0.1043, loss_iou_reg=0.2596, d_time=0.01(0.01), f_time=1.47(1.30), b_time=1.48(1.31)  Time cost: 22:24/16:08 [2:18:14/20:48:45]  Acc_iter 6300        Data time: 0.01(0.01)  Forward time: 1.47(1.30)  Batch time: 1.48(1.31)
2025-09-03 03:38:51,706   INFO  Train:    4/36 ( 11%) [1072/1759 ( 61%)]  Loss: 4.580 (4.86)  LR: 6.974e-04  Grad: 5.9549  max=4.2683(module.vfe.pfn_layers.0.linear.weight)  min: -3.3353(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9742, loss_cls=0.2280, loss_bbox=1.0685, matched_ious=0.4482, loss_iou=0.1052, loss_iou_reg=0.2539, d_time=0.01(0.01), f_time=1.28(1.30), b_time=1.29(1.31)  Time cost: 23:28/15:01 [2:19:19/20:46:32]  Acc_iter 6350        Data time: 0.01(0.01)  Forward time: 1.28(1.30)  Batch time: 1.29(1.31)
2025-09-03 03:39:56,632   INFO  Train:    4/36 ( 11%) [1122/1759 ( 64%)]  Loss: 4.268 (4.85)  LR: 7.033e-04  Grad: 9.5532  max=9.1792(module.vfe.pfn_layers.0.linear.weight)  min: -0.1098(module.dense_head.prediction_head.dim.1.weight)  NaN: False  loss_hm=0.9502, loss_cls=0.2271, loss_bbox=1.1127, matched_ious=0.4559, loss_iou=0.1006, loss_iou_reg=0.2521, d_time=0.01(0.01), f_time=1.26(1.30), b_time=1.27(1.31)  Time cost: 24:33/13:55 [2:20:23/20:44:50]  Acc_iter 6400        Data time: 0.01(0.01)  Forward time: 1.26(1.30)  Batch time: 1.27(1.31)
2025-09-03 03:41:01,796   INFO  Train:    4/36 ( 11%) [1172/1759 ( 67%)]  Loss: 4.191 (4.84)  LR: 7.093e-04  Grad: 6.5494  max=1.3911(module.vfe.pfn_layers.0.linear.weight)  min: -5.9042(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9339, loss_cls=0.2183, loss_bbox=1.1466, matched_ious=0.4322, loss_iou=0.1058, loss_iou_reg=0.2625, d_time=0.01(0.01), f_time=1.27(1.30), b_time=1.28(1.31)  Time cost: 25:38/12:49 [2:21:29/20:43:23]  Acc_iter 6450        Data time: 0.01(0.01)  Forward time: 1.27(1.30)  Batch time: 1.28(1.31)
2025-09-03 03:42:06,993   INFO  Train:    4/36 ( 11%) [1222/1759 ( 69%)]  Loss: 3.791 (4.84)  LR: 7.154e-04  Grad: 5.9786  max=0.3017(module.dense_head.prediction_head.height.1.weight)  min: -5.5306(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9158, loss_cls=0.2183, loss_bbox=1.0598, matched_ious=0.4502, loss_iou=0.1028, loss_iou_reg=0.2535, d_time=0.01(0.01), f_time=1.30(1.30), b_time=1.31(1.31)  Time cost: 26:43/11:44 [2:22:34/20:42:00]  Acc_iter 6500        Data time: 0.01(0.01)  Forward time: 1.30(1.30)  Batch time: 1.31(1.31)
2025-09-03 03:43:11,997   INFO  Train:    4/36 ( 11%) [1272/1759 ( 72%)]  Loss: 4.564 (4.83)  LR: 7.214e-04  Grad: 3.1489  max=2.2292(module.vfe.pfn_layers.0.linear.weight)  min: -0.8729(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9335, loss_cls=0.2159, loss_bbox=1.1193, matched_ious=0.4476, loss_iou=0.1010, loss_iou_reg=0.2540, d_time=0.01(0.01), f_time=1.24(1.30), b_time=1.24(1.31)  Time cost: 27:48/10:38 [2:23:39/20:40:29]  Acc_iter 6550        Data time: 0.01(0.01)  Forward time: 1.24(1.30)  Batch time: 1.24(1.31)
2025-09-03 03:44:17,063   INFO  Train:    4/36 ( 11%) [1322/1759 ( 75%)]  Loss: 4.512 (4.82)  LR: 7.275e-04  Grad: 3.6743  max=2.8313(module.vfe.pfn_layers.0.linear.weight)  min: -0.9615(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9288, loss_cls=0.2208, loss_bbox=1.1339, matched_ious=0.4466, loss_iou=0.1019, loss_iou_reg=0.2557, d_time=0.01(0.01), f_time=1.19(1.30), b_time=1.20(1.31)  Time cost: 28:53/09:32 [2:24:44/20:39:03]  Acc_iter 6600        Data time: 0.01(0.01)  Forward time: 1.19(1.30)  Batch time: 1.20(1.31)
2025-09-03 03:45:22,456   INFO  Train:    4/36 ( 11%) [1372/1759 ( 78%)]  Loss: 4.615 (4.82)  LR: 7.336e-04  Grad: 2.4946  max=0.3045(module.dense_head.prediction_head.height.1.weight)  min: -0.8387(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9756, loss_cls=0.2257, loss_bbox=1.1256, matched_ious=0.4381, loss_iou=0.1016, loss_iou_reg=0.2605, d_time=0.01(0.01), f_time=1.29(1.30), b_time=1.29(1.31)  Time cost: 29:59/08:27 [2:25:49/20:37:52]  Acc_iter 6650        Data time: 0.01(0.01)  Forward time: 1.29(1.30)  Batch time: 1.29(1.31)
2025-09-03 03:46:30,273   INFO  Train:    4/36 ( 11%) [1422/1759 ( 81%)]  Loss: 3.798 (4.81)  LR: 7.398e-04  Grad: 3.8245  max=1.1155(module.vfe.pfn_layers.0.linear.weight)  min: -2.8559(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9400, loss_cls=0.2170, loss_bbox=1.1088, matched_ious=0.4481, loss_iou=0.1029, loss_iou_reg=0.2524, d_time=0.01(0.01), f_time=1.27(1.30), b_time=1.28(1.31)  Time cost: 31:07/07:22 [2:26:57/20:38:17]  Acc_iter 6700        Data time: 0.01(0.01)  Forward time: 1.27(1.30)  Batch time: 1.28(1.31)
2025-09-03 03:47:34,930   INFO  Train:    4/36 ( 11%) [1472/1759 ( 84%)]  Loss: 4.783 (4.81)  LR: 7.460e-04  Grad: 2.8792  max=1.2089(module.vfe.pfn_layers.0.linear.weight)  min: -1.4003(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9619, loss_cls=0.2217, loss_bbox=1.1306, matched_ious=0.4406, loss_iou=0.1036, loss_iou_reg=0.2558, d_time=0.01(0.01), f_time=1.28(1.30), b_time=1.29(1.31)  Time cost: 32:11/06:16 [2:28:02/20:36:35]  Acc_iter 6750        Data time: 0.01(0.01)  Forward time: 1.28(1.30)  Batch time: 1.29(1.31)
2025-09-03 03:48:39,376   INFO  Train:    4/36 ( 11%) [1522/1759 ( 87%)]  Loss: 4.586 (4.80)  LR: 7.522e-04  Grad: 5.1669  max=3.3882(module.vfe.pfn_layers.0.linear.weight)  min: -0.1689(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9264, loss_cls=0.2211, loss_bbox=1.0790, matched_ious=0.4476, loss_iou=0.1036, loss_iou_reg=0.2557, d_time=0.01(0.01), f_time=1.31(1.30), b_time=1.32(1.31)  Time cost: 33:16/05:10 [2:29:06/20:34:48]  Acc_iter 6800        Data time: 0.01(0.01)  Forward time: 1.31(1.30)  Batch time: 1.32(1.31)
2025-09-03 03:49:44,741   INFO  Train:    4/36 ( 11%) [1572/1759 ( 89%)]  Loss: 5.239 (4.80)  LR: 7.585e-04  Grad: 2.1157  max=0.1012(module.vfe.pfn_layers.0.linear.weight)  min: -0.3632(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.9361, loss_cls=0.2172, loss_bbox=1.1153, matched_ious=0.4459, loss_iou=0.1022, loss_iou_reg=0.2563, d_time=0.01(0.01), f_time=1.16(1.30), b_time=1.17(1.31)  Time cost: 34:21/04:05 [2:30:12/20:33:36]  Acc_iter 6850        Data time: 0.01(0.01)  Forward time: 1.16(1.30)  Batch time: 1.17(1.31)
2025-09-03 03:50:50,240   INFO  Train:    4/36 ( 11%) [1622/1759 ( 92%)]  Loss: 5.251 (4.80)  LR: 7.648e-04  Grad: 6.3604  max=4.3686(module.vfe.pfn_layers.0.linear.weight)  min: -1.2595(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9866, loss_cls=0.2240, loss_bbox=1.1753, matched_ious=0.4398, loss_iou=0.1025, loss_iou_reg=0.2582, d_time=0.01(0.01), f_time=1.35(1.30), b_time=1.36(1.31)  Time cost: 35:27/02:59 [2:31:17/20:32:30]  Acc_iter 6900        Data time: 0.01(0.01)  Forward time: 1.35(1.30)  Batch time: 1.36(1.31)
2025-09-03 03:51:55,186   INFO  Train:    4/36 ( 11%) [1672/1759 ( 95%)]  Loss: 4.871 (4.80)  LR: 7.711e-04  Grad: 6.6313  max=5.2486(module.vfe.pfn_layers.0.linear.weight)  min: -0.2782(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.9707, loss_cls=0.2186, loss_bbox=1.1680, matched_ious=0.4515, loss_iou=0.1007, loss_iou_reg=0.2486, d_time=0.01(0.01), f_time=1.27(1.30), b_time=1.28(1.31)  Time cost: 36:32/01:53 [2:32:22/20:31:04]  Acc_iter 6950        Data time: 0.01(0.01)  Forward time: 1.27(1.30)  Batch time: 1.28(1.31)
2025-09-03 03:52:59,634   INFO  Train:    4/36 ( 11%) [1722/1759 ( 98%)]  Loss: 5.000 (4.80)  LR: 7.775e-04  Grad: 3.0753  max=1.6968(module.vfe.pfn_layers.0.linear.weight)  min: -0.5283(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9611, loss_cls=0.2168, loss_bbox=1.1315, matched_ious=0.4463, loss_iou=0.1033, loss_iou_reg=0.2540, d_time=0.01(0.01), f_time=1.28(1.30), b_time=1.28(1.31)  Time cost: 37:36/00:48 [2:33:26/20:29:24]  Acc_iter 7000        Data time: 0.01(0.01)  Forward time: 1.28(1.30)  Batch time: 1.28(1.31)
2025-09-03 03:53:44,896   INFO  Train:    4/36 ( 11%) [1758/1759 (100%)]  Loss: 4.165 (4.80)  LR: 7.821e-04  Grad: 2.7095  max=0.6643(module.vfe.pfn_layers.0.linear.weight)  min: -0.4518(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9181, loss_cls=0.2149, loss_bbox=1.1074, matched_ious=0.4494, loss_iou=0.1032, loss_iou_reg=0.2567, d_time=0.01(0.01), f_time=0.73(1.30), b_time=0.74(1.31)  Time cost: 38:21/00:01 [2:34:12/20:27:37]  Acc_iter 7036        Data time: 0.01(0.01)  Forward time: 0.73(1.30)  Batch time: 0.74(1.31)

                                               [Aepochs:  11%|█         | 4/36 [2:34:12<20:32:18, 2310.58s/it]epochs:  11%|█         | 4/36 [2:34:12<20:32:19, 2310.59s/it]epochs:  11%|█         | 4/36 [2:34:12<20:32:18, 2310.59s/it]epochs:  11%|█         | 4/36 [2:34:12<20:32:19, 2310.62s/it]epochs:  11%|█         | 4/36 [2:34:12<20:32:20, 2310.63s/it]epochs:  11%|█         | 4/36 [2:34:12<20:32:19, 2310.62s/it]epochs:  11%|█         | 4/36 [2:34:12<20:32:19, 2310.61s/it]epochs:  11%|█         | 4/36 [2:34:12<20:32:20, 2310.65s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 03:53:49,189   INFO  Train:    5/36 ( 14%) [   0/1759 (  0%)]  Loss: 5.202 (5.20)  LR: 7.823e-04  Grad: 4.4682  max=0.6649(module.vfe.pfn_layers.0.linear.weight)  min: -3.8084(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0924, loss_cls=0.2031, loss_bbox=1.3646, matched_ious=0.4227, loss_iou=0.1005, loss_iou_reg=0.2421, d_time=1.14(1.14), f_time=2.12(2.12), b_time=3.27(3.27)  Time cost: 00:02/1:24:29 [2:34:16/45:03:53]  Acc_iter 7037        Data time: 1.14(1.14)  Forward time: 2.12(2.12)  Batch time: 3.27(3.27)
2025-09-03 03:54:06,131   INFO  Train:    5/36 ( 14%) [  13/1759 (  1%)]  Loss: 4.869 (4.53)  LR: 7.839e-04  Grad: 3.9267  max=2.7589(module.vfe.pfn_layers.0.linear.weight)  min: -0.1278(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.8968, loss_cls=0.2176, loss_bbox=0.9501, matched_ious=0.4477, loss_iou=0.1048, loss_iou_reg=0.2615, d_time=0.01(0.09), f_time=1.24(1.35), b_time=1.24(1.44)  Time cost: 00:19/41:12 [2:34:33/22:08:16]  Acc_iter 7050        Data time: 0.01(0.09)  Forward time: 1.24(1.35)  Batch time: 1.24(1.44)
2025-09-03 03:55:13,369   INFO  Train:    5/36 ( 14%) [  63/1759 (  4%)]  Loss: 4.399 (4.69)  LR: 7.904e-04  Grad: 3.2571  max=0.9375(module.vfe.pfn_layers.0.linear.weight)  min: -2.3022(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9536, loss_cls=0.2141, loss_bbox=1.1391, matched_ious=0.4433, loss_iou=0.1035, loss_iou_reg=0.2552, d_time=0.01(0.03), f_time=1.34(1.34), b_time=1.35(1.37)  Time cost: 01:27/38:27 [2:35:40/21:14:47]  Acc_iter 7100        Data time: 0.01(0.03)  Forward time: 1.34(1.34)  Batch time: 1.35(1.37)
2025-09-03 03:56:17,909   INFO  Train:    5/36 ( 14%) [ 113/1759 (  6%)]  Loss: 3.691 (4.64)  LR: 7.968e-04  Grad: 3.5644  max=3.0400(module.vfe.pfn_layers.0.linear.weight)  min: -0.6642(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9210, loss_cls=0.2098, loss_bbox=1.0619, matched_ious=0.4516, loss_iou=0.1027, loss_iou_reg=0.2515, d_time=0.01(0.02), f_time=1.24(1.32), b_time=1.25(1.33)  Time cost: 02:31/36:28 [2:36:45/20:45:05]  Acc_iter 7150        Data time: 0.01(0.02)  Forward time: 1.24(1.32)  Batch time: 1.25(1.33)
2025-09-03 03:57:25,658   INFO  Train:    5/36 ( 14%) [ 163/1759 (  9%)]  Loss: 5.078 (4.61)  LR: 8.033e-04  Grad: 4.6597  max=2.8190(module.vfe.pfn_layers.0.linear.weight)  min: -2.2396(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9199, loss_cls=0.2121, loss_bbox=1.0311, matched_ious=0.4477, loss_iou=0.1033, loss_iou_reg=0.2588, d_time=0.01(0.02), f_time=1.15(1.32), b_time=1.16(1.34)  Time cost: 03:39/35:34 [2:37:52/20:51:08]  Acc_iter 7200        Data time: 0.01(0.02)  Forward time: 1.15(1.32)  Batch time: 1.16(1.34)
2025-09-03 03:58:30,571   INFO  Train:    5/36 ( 14%) [ 213/1759 ( 12%)]  Loss: 5.080 (4.61)  LR: 8.099e-04  Grad: 5.5901  max=4.1803(module.vfe.pfn_layers.0.linear.weight)  min: -3.3714(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9290, loss_cls=0.2093, loss_bbox=1.1227, matched_ious=0.4548, loss_iou=0.1013, loss_iou_reg=0.2493, d_time=0.01(0.02), f_time=1.25(1.31), b_time=1.26(1.33)  Time cost: 04:44/34:13 [2:38:57/20:41:27]  Acc_iter 7250        Data time: 0.01(0.02)  Forward time: 1.25(1.31)  Batch time: 1.26(1.33)
2025-09-03 03:59:35,869   INFO  Train:    5/36 ( 14%) [ 263/1759 ( 15%)]  Loss: 3.932 (4.58)  LR: 8.164e-04  Grad: 3.7429  max=0.1836(module.backbone_3d.cls_conv.3.weight)  min: -3.0634(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8914, loss_cls=0.2123, loss_bbox=0.9971, matched_ious=0.4517, loss_iou=0.1042, loss_iou_reg=0.2544, d_time=0.01(0.01), f_time=1.27(1.31), b_time=1.28(1.33)  Time cost: 05:49/33:00 [2:40:03/20:36:23]  Acc_iter 7300        Data time: 0.01(0.01)  Forward time: 1.27(1.31)  Batch time: 1.28(1.33)
2025-09-03 04:00:41,054   INFO  Train:    5/36 ( 14%) [ 313/1759 ( 18%)]  Loss: 4.461 (4.57)  LR: 8.231e-04  Grad: 9.9009  max=1.7116(module.vfe.pfn_layers.0.linear.weight)  min: -9.5464(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8962, loss_cls=0.2037, loss_bbox=1.0626, matched_ious=0.4514, loss_iou=0.1043, loss_iou_reg=0.2523, d_time=0.01(0.01), f_time=1.31(1.31), b_time=1.32(1.32)  Time cost: 06:54/31:49 [2:41:08/20:32:14]  Acc_iter 7350        Data time: 0.01(0.01)  Forward time: 1.31(1.31)  Batch time: 1.32(1.32)
2025-09-03 04:01:45,787   INFO  Train:    5/36 ( 14%) [ 363/1759 ( 21%)]  Loss: 5.987 (4.57)  LR: 8.297e-04  Grad: 3.0742  max=1.2107(module.vfe.pfn_layers.0.linear.weight)  min: -1.5301(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9068, loss_cls=0.2050, loss_bbox=1.0938, matched_ious=0.4553, loss_iou=0.1015, loss_iou_reg=0.2514, d_time=0.01(0.01), f_time=1.22(1.31), b_time=1.22(1.32)  Time cost: 07:59/30:38 [2:42:13/20:27:47]  Acc_iter 7400        Data time: 0.01(0.01)  Forward time: 1.22(1.31)  Batch time: 1.22(1.32)
2025-09-03 04:02:51,044   INFO  Train:    5/36 ( 14%) [ 413/1759 ( 23%)]  Loss: 4.190 (4.57)  LR: 8.363e-04  Grad: 7.3139  max=1.6856(module.vfe.pfn_layers.0.linear.weight)  min: -6.5739(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9490, loss_cls=0.2103, loss_bbox=1.0812, matched_ious=0.4556, loss_iou=0.0982, loss_iou_reg=0.2518, d_time=0.01(0.01), f_time=1.19(1.30), b_time=1.20(1.32)  Time cost: 09:04/29:31 [2:43:18/20:25:19]  Acc_iter 7450        Data time: 0.01(0.01)  Forward time: 1.19(1.30)  Batch time: 1.20(1.32)
2025-09-03 04:03:57,143   INFO  Train:    5/36 ( 14%) [ 463/1759 ( 26%)]  Loss: 3.533 (4.57)  LR: 8.430e-04  Grad: 3.6608  max=1.1339(module.vfe.pfn_layers.0.linear.weight)  min: -2.6143(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9064, loss_cls=0.2043, loss_bbox=1.0685, matched_ious=0.4492, loss_iou=0.1020, loss_iou_reg=0.2542, d_time=0.01(0.01), f_time=1.20(1.31), b_time=1.20(1.32)  Time cost: 10:10/28:26 [2:44:24/20:24:51]  Acc_iter 7500        Data time: 0.01(0.01)  Forward time: 1.20(1.31)  Batch time: 1.20(1.32)
2025-09-03 04:05:03,621   INFO  Train:    5/36 ( 14%) [ 513/1759 ( 29%)]  Loss: 4.518 (4.59)  LR: 8.498e-04  Grad: 2.6265  max=1.3115(module.vfe.pfn_layers.0.linear.weight)  min: -0.1841(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9626, loss_cls=0.2121, loss_bbox=1.1630, matched_ious=0.4437, loss_iou=0.1041, loss_iou_reg=0.2539, d_time=0.01(0.01), f_time=1.37(1.31), b_time=1.38(1.32)  Time cost: 11:17/27:21 [2:45:30/20:24:56]  Acc_iter 7550        Data time: 0.01(0.01)  Forward time: 1.37(1.31)  Batch time: 1.38(1.32)
2025-09-03 04:06:08,216   INFO  Train:    5/36 ( 14%) [ 563/1759 ( 32%)]  Loss: 4.269 (4.58)  LR: 8.565e-04  Grad: 3.1189  max=1.9861(module.vfe.pfn_layers.0.linear.weight)  min: -0.2894(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9073, loss_cls=0.2017, loss_bbox=1.0585, matched_ious=0.4468, loss_iou=0.1017, loss_iou_reg=0.2547, d_time=0.01(0.01), f_time=1.31(1.31), b_time=1.32(1.32)  Time cost: 12:21/26:13 [2:46:35/20:21:43]  Acc_iter 7600        Data time: 0.01(0.01)  Forward time: 1.31(1.31)  Batch time: 1.32(1.32)
2025-09-03 04:07:12,917   INFO  Train:    5/36 ( 14%) [ 613/1759 ( 35%)]  Loss: 3.826 (4.57)  LR: 8.633e-04  Grad: 3.3297  max=0.6078(module.vfe.pfn_layers.0.linear.weight)  min: -2.1666(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9067, loss_cls=0.2059, loss_bbox=1.0302, matched_ious=0.4529, loss_iou=0.1005, loss_iou_reg=0.2552, d_time=0.01(0.01), f_time=1.28(1.30), b_time=1.29(1.31)  Time cost: 13:26/25:05 [2:47:40/20:19:00]  Acc_iter 7650        Data time: 0.01(0.01)  Forward time: 1.28(1.30)  Batch time: 1.29(1.31)
2025-09-03 04:08:19,764   INFO  Train:    5/36 ( 14%) [ 663/1759 ( 38%)]  Loss: 5.061 (4.56)  LR: 8.701e-04  Grad: 4.7382  max=0.6803(module.vfe.pfn_layers.0.linear.weight)  min: -3.7926(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9075, loss_cls=0.2028, loss_bbox=1.0329, matched_ious=0.4605, loss_iou=0.1009, loss_iou_reg=0.2503, d_time=0.01(0.01), f_time=1.30(1.31), b_time=1.31(1.32)  Time cost: 14:33/24:01 [2:48:47/20:19:31]  Acc_iter 7700        Data time: 0.01(0.01)  Forward time: 1.30(1.31)  Batch time: 1.31(1.32)
2025-09-03 04:09:25,033   INFO  Train:    5/36 ( 14%) [ 713/1759 ( 41%)]  Loss: 4.279 (4.57)  LR: 8.770e-04  Grad: 6.7506  max=1.1258(module.vfe.pfn_layers.0.linear.weight)  min: -6.0301(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9393, loss_cls=0.2060, loss_bbox=1.0978, matched_ious=0.4370, loss_iou=0.1038, loss_iou_reg=0.2609, d_time=0.01(0.01), f_time=1.35(1.31), b_time=1.35(1.32)  Time cost: 15:38/22:55 [2:49:52/20:17:47]  Acc_iter 7750        Data time: 0.01(0.01)  Forward time: 1.35(1.31)  Batch time: 1.35(1.32)
2025-09-03 04:10:29,766   INFO  Train:    5/36 ( 14%) [ 763/1759 ( 43%)]  Loss: 4.749 (4.57)  LR: 8.839e-04  Grad: 3.8036  max=1.7324(module.vfe.pfn_layers.0.linear.weight)  min: -1.5118(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8985, loss_cls=0.2026, loss_bbox=1.0405, matched_ious=0.4539, loss_iou=0.0997, loss_iou_reg=0.2531, d_time=0.01(0.01), f_time=1.26(1.30), b_time=1.27(1.31)  Time cost: 16:43/21:48 [2:50:57/20:15:28]  Acc_iter 7800        Data time: 0.01(0.01)  Forward time: 1.26(1.30)  Batch time: 1.27(1.31)
2025-09-03 04:11:34,402   INFO  Train:    5/36 ( 14%) [ 813/1759 ( 46%)]  Loss: 4.347 (4.57)  LR: 8.908e-04  Grad: 4.2259  max=2.2754(module.vfe.pfn_layers.0.linear.weight)  min: -0.2957(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9436, loss_cls=0.2109, loss_bbox=1.0756, matched_ious=0.4490, loss_iou=0.1034, loss_iou_reg=0.2535, d_time=0.01(0.01), f_time=1.25(1.30), b_time=1.26(1.31)  Time cost: 17:48/20:41 [2:52:01/20:13:12]  Acc_iter 7850        Data time: 0.01(0.01)  Forward time: 1.25(1.30)  Batch time: 1.26(1.31)
2025-09-03 04:12:38,981   INFO  Train:    5/36 ( 14%) [ 863/1759 ( 49%)]  Loss: 4.030 (4.56)  LR: 8.977e-04  Grad: 3.0868  max=0.1568(module.vfe.pfn_layers.0.linear.weight)  min: -1.4790(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8967, loss_cls=0.1997, loss_bbox=1.0420, matched_ious=0.4482, loss_iou=0.1021, loss_iou_reg=0.2551, d_time=0.01(0.01), f_time=1.25(1.30), b_time=1.26(1.31)  Time cost: 18:52/19:34 [2:53:06/20:11:00]  Acc_iter 7900        Data time: 0.01(0.01)  Forward time: 1.25(1.30)  Batch time: 1.26(1.31)
2025-09-03 04:13:43,796   INFO  Train:    5/36 ( 14%) [ 913/1759 ( 52%)]  Loss: 4.540 (4.56)  LR: 9.047e-04  Grad: 10.0000  max=7.4944(module.vfe.pfn_layers.0.linear.weight)  min: -3.2989(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9061, loss_cls=0.2029, loss_bbox=1.0468, matched_ious=0.4505, loss_iou=0.1008, loss_iou_reg=0.2548, d_time=0.01(0.01), f_time=1.27(1.30), b_time=1.27(1.31)  Time cost: 19:57/18:28 [2:54:11/20:09:10]  Acc_iter 7950        Data time: 0.01(0.01)  Forward time: 1.27(1.30)  Batch time: 1.27(1.31)
2025-09-03 04:14:50,144   INFO  Train:    5/36 ( 14%) [ 963/1759 ( 55%)]  Loss: 4.671 (4.56)  LR: 9.117e-04  Grad: 6.1894  max=5.1850(module.vfe.pfn_layers.0.linear.weight)  min: -2.2086(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9130, loss_cls=0.2028, loss_bbox=1.1062, matched_ious=0.4399, loss_iou=0.1044, loss_iou_reg=0.2581, d_time=0.04(0.01), f_time=1.26(1.30), b_time=1.29(1.31)  Time cost: 21:03/17:23 [2:55:17/20:08:53]  Acc_iter 8000        Data time: 0.04(0.01)  Forward time: 1.26(1.30)  Batch time: 1.29(1.31)
2025-09-03 04:15:55,728   INFO  Train:    5/36 ( 14%) [1013/1759 ( 58%)]  Loss: 4.587 (4.56)  LR: 9.187e-04  Grad: 4.7495  max=2.6259(module.vfe.pfn_layers.0.linear.weight)  min: -2.8394(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9023, loss_cls=0.1966, loss_bbox=1.1513, matched_ious=0.4432, loss_iou=0.1022, loss_iou_reg=0.2577, d_time=0.01(0.01), f_time=1.27(1.30), b_time=1.28(1.31)  Time cost: 22:09/16:18 [2:56:23/20:07:49]  Acc_iter 8050        Data time: 0.01(0.01)  Forward time: 1.27(1.30)  Batch time: 1.28(1.31)
2025-09-03 04:17:00,250   INFO  Train:    5/36 ( 14%) [1063/1759 ( 60%)]  Loss: 4.332 (4.55)  LR: 9.257e-04  Grad: 5.1406  max=3.6460(module.vfe.pfn_layers.0.linear.weight)  min: -2.0327(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8903, loss_cls=0.2001, loss_bbox=0.9922, matched_ious=0.4694, loss_iou=0.1016, loss_iou_reg=0.2459, d_time=0.01(0.01), f_time=1.18(1.30), b_time=1.18(1.31)  Time cost: 23:13/15:11 [2:57:27/20:05:50]  Acc_iter 8100        Data time: 0.01(0.01)  Forward time: 1.18(1.30)  Batch time: 1.18(1.31)
2025-09-03 04:18:05,137   INFO  Train:    5/36 ( 14%) [1113/1759 ( 63%)]  Loss: 4.546 (4.55)  LR: 9.328e-04  Grad: 6.3716  max=4.2429(module.vfe.pfn_layers.0.linear.weight)  min: -2.6345(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8890, loss_cls=0.2012, loss_bbox=0.9927, matched_ious=0.4632, loss_iou=0.0978, loss_iou_reg=0.2484, d_time=0.01(0.01), f_time=1.26(1.30), b_time=1.27(1.31)  Time cost: 24:18/14:05 [2:58:32/20:04:14]  Acc_iter 8150        Data time: 0.01(0.01)  Forward time: 1.26(1.30)  Batch time: 1.27(1.31)
2025-09-03 04:19:13,062   INFO  Train:    5/36 ( 14%) [1163/1759 ( 66%)]  Loss: 4.231 (4.54)  LR: 9.399e-04  Grad: 8.7747  max=4.3985(module.vfe.pfn_layers.0.linear.weight)  min: -5.1823(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8703, loss_cls=0.1910, loss_bbox=1.0771, matched_ious=0.4474, loss_iou=0.1016, loss_iou_reg=0.2554, d_time=0.01(0.01), f_time=1.37(1.30), b_time=1.38(1.31)  Time cost: 25:26/13:01 [2:59:40/20:05:04]  Acc_iter 8200        Data time: 0.01(0.01)  Forward time: 1.37(1.30)  Batch time: 1.38(1.31)
2025-09-03 04:20:17,970   INFO  Train:    5/36 ( 14%) [1213/1759 ( 69%)]  Loss: 5.157 (4.54)  LR: 9.471e-04  Grad: 7.9201  max=3.0558(module.vfe.pfn_layers.0.linear.weight)  min: -6.1545(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9007, loss_cls=0.2006, loss_bbox=1.0667, matched_ious=0.4566, loss_iou=0.1018, loss_iou_reg=0.2497, d_time=0.01(0.01), f_time=1.24(1.30), b_time=1.25(1.31)  Time cost: 26:31/11:55 [3:00:45/20:03:28]  Acc_iter 8250        Data time: 0.01(0.01)  Forward time: 1.24(1.30)  Batch time: 1.25(1.31)
2025-09-03 04:21:23,317   INFO  Train:    5/36 ( 14%) [1263/1759 ( 72%)]  Loss: 4.106 (4.54)  LR: 9.542e-04  Grad: 6.8229  max=0.9292(module.vfe.pfn_layers.0.linear.weight)  min: -4.9626(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8826, loss_cls=0.1936, loss_bbox=1.0739, matched_ious=0.4529, loss_iou=0.1017, loss_iou_reg=0.2527, d_time=0.01(0.01), f_time=1.36(1.30), b_time=1.37(1.31)  Time cost: 27:37/10:50 [3:01:50/20:02:13]  Acc_iter 8300        Data time: 0.01(0.01)  Forward time: 1.36(1.30)  Batch time: 1.37(1.31)
2025-09-03 04:22:28,196   INFO  Train:    5/36 ( 14%) [1313/1759 ( 75%)]  Loss: 4.655 (4.53)  LR: 9.614e-04  Grad: 5.4387  max=1.4243(module.vfe.pfn_layers.0.linear.weight)  min: -3.3694(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8828, loss_cls=0.1978, loss_bbox=1.0678, matched_ious=0.4470, loss_iou=0.1017, loss_iou_reg=0.2577, d_time=0.01(0.01), f_time=1.23(1.30), b_time=1.24(1.31)  Time cost: 28:41/09:44 [3:02:55/20:00:40]  Acc_iter 8350        Data time: 0.01(0.01)  Forward time: 1.23(1.30)  Batch time: 1.24(1.31)
2025-09-03 04:23:33,588   INFO  Train:    5/36 ( 14%) [1363/1759 ( 77%)]  Loss: 4.856 (4.53)  LR: 9.686e-04  Grad: 9.8119  max=7.0454(module.vfe.pfn_layers.0.linear.weight)  min: -0.9839(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8849, loss_cls=0.1974, loss_bbox=1.0242, matched_ious=0.4543, loss_iou=0.1002, loss_iou_reg=0.2536, d_time=0.01(0.01), f_time=1.39(1.30), b_time=1.40(1.31)  Time cost: 29:47/08:38 [3:04:00/19:59:29]  Acc_iter 8400        Data time: 0.01(0.01)  Forward time: 1.39(1.30)  Batch time: 1.40(1.31)
2025-09-03 04:24:40,098   INFO  Train:    5/36 ( 14%) [1413/1759 ( 80%)]  Loss: 4.070 (4.53)  LR: 9.759e-04  Grad: 8.3621  max=7.7270(module.vfe.pfn_layers.0.linear.weight)  min: -1.2075(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8799, loss_cls=0.1936, loss_bbox=1.0165, matched_ious=0.4575, loss_iou=0.1013, loss_iou_reg=0.2515, d_time=0.01(0.01), f_time=1.35(1.30), b_time=1.36(1.31)  Time cost: 30:53/07:33 [3:05:07/19:59:02]  Acc_iter 8450        Data time: 0.01(0.01)  Forward time: 1.35(1.30)  Batch time: 1.36(1.31)
2025-09-03 04:25:44,890   INFO  Train:    5/36 ( 14%) [1463/1759 ( 83%)]  Loss: 4.297 (4.52)  LR: 9.831e-04  Grad: 2.7639  max=0.2729(module.vfe.pfn_layers.0.linear.weight)  min: -0.2386(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.9045, loss_cls=0.1948, loss_bbox=1.0681, matched_ious=0.4581, loss_iou=0.0983, loss_iou_reg=0.2501, d_time=0.01(0.01), f_time=1.21(1.30), b_time=1.21(1.31)  Time cost: 31:58/06:27 [3:06:12/19:57:28]  Acc_iter 8500        Data time: 0.01(0.01)  Forward time: 1.21(1.30)  Batch time: 1.21(1.31)
2025-09-03 04:26:50,169   INFO  Train:    5/36 ( 14%) [1513/1759 ( 86%)]  Loss: 5.041 (4.52)  LR: 9.904e-04  Grad: 4.6724  max=3.2007(module.vfe.pfn_layers.0.linear.weight)  min: -0.3552(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.9090, loss_cls=0.1958, loss_bbox=1.0474, matched_ious=0.4535, loss_iou=0.1030, loss_iou_reg=0.2517, d_time=0.01(0.01), f_time=1.30(1.30), b_time=1.31(1.31)  Time cost: 33:03/05:22 [3:07:17/19:56:13]  Acc_iter 8550        Data time: 0.01(0.01)  Forward time: 1.30(1.30)  Batch time: 1.31(1.31)
2025-09-03 04:27:55,487   INFO  Train:    5/36 ( 14%) [1563/1759 ( 89%)]  Loss: 4.099 (4.52)  LR: 9.977e-04  Grad: 3.4981  max=2.4608(module.vfe.pfn_layers.0.linear.weight)  min: -1.6829(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8899, loss_cls=0.1954, loss_bbox=1.1044, matched_ious=0.4488, loss_iou=0.1036, loss_iou_reg=0.2532, d_time=0.01(0.01), f_time=1.25(1.30), b_time=1.26(1.31)  Time cost: 34:09/04:16 [3:08:22/19:55:01]  Acc_iter 8600        Data time: 0.01(0.01)  Forward time: 1.25(1.30)  Batch time: 1.26(1.31)
2025-09-03 04:29:01,393   INFO  Train:    5/36 ( 14%) [1613/1759 ( 92%)]  Loss: 4.166 (4.52)  LR: 1.005e-03  Grad: 6.3402  max=3.7824(module.vfe.pfn_layers.0.linear.weight)  min: -3.5403(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8888, loss_cls=0.1948, loss_bbox=1.0775, matched_ious=0.4554, loss_iou=0.1007, loss_iou_reg=0.2515, d_time=0.01(0.01), f_time=1.29(1.30), b_time=1.30(1.31)  Time cost: 35:15/03:11 [3:09:28/19:54:09]  Acc_iter 8650        Data time: 0.01(0.01)  Forward time: 1.29(1.30)  Batch time: 1.30(1.31)
2025-09-03 04:30:06,269   INFO  Train:    5/36 ( 14%) [1663/1759 ( 95%)]  Loss: 4.178 (4.52)  LR: 1.012e-03  Grad: 4.7689  max=0.0733(module.backbone_3d.cls_conv.3.weight)  min: -4.2380(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8857, loss_cls=0.1941, loss_bbox=1.0239, matched_ious=0.4548, loss_iou=0.1036, loss_iou_reg=0.2517, d_time=0.01(0.01), f_time=1.27(1.30), b_time=1.27(1.31)  Time cost: 36:19/02:05 [3:10:33/19:52:42]  Acc_iter 8700        Data time: 0.01(0.01)  Forward time: 1.27(1.30)  Batch time: 1.27(1.31)
2025-09-03 04:31:11,452   INFO  Train:    5/36 ( 14%) [1713/1759 ( 97%)]  Loss: 4.350 (4.52)  LR: 1.020e-03  Grad: 9.7039  max=0.8901(module.vfe.pfn_layers.0.linear.weight)  min: -9.3956(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9036, loss_cls=0.1941, loss_bbox=1.0529, matched_ious=0.4540, loss_iou=0.1011, loss_iou_reg=0.2516, d_time=0.01(0.01), f_time=1.38(1.30), b_time=1.38(1.31)  Time cost: 37:25/01:00 [3:11:38/19:51:27]  Acc_iter 8750        Data time: 0.01(0.01)  Forward time: 1.38(1.30)  Batch time: 1.38(1.31)
2025-09-03 04:32:09,365   INFO  Train:    5/36 ( 14%) [1758/1759 (100%)]  Loss: 4.465 (4.51)  LR: 1.027e-03  Grad: 8.6340  max=6.3005(module.vfe.pfn_layers.0.linear.weight)  min: -0.6744(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8689, loss_cls=0.1917, loss_bbox=1.0111, matched_ious=0.4636, loss_iou=0.0999, loss_iou_reg=0.2470, d_time=0.01(0.01), f_time=0.73(1.30), b_time=0.74(1.31)  Time cost: 38:23/00:01 [3:12:36/19:49:56]  Acc_iter 8795        Data time: 0.01(0.01)  Forward time: 0.73(1.30)  Batch time: 0.74(1.31)

                                               [Aepochs:  14%|█▍        | 5/36 [3:12:36<19:52:39, 2308.36s/it]epochs:  14%|█▍        | 5/36 [3:12:36<19:52:39, 2308.38s/it]epochs:  14%|█▍        | 5/36 [3:12:36<19:52:39, 2308.37s/it]epochs:  14%|█▍        | 5/36 [3:12:36<19:52:40, 2308.40s/it]epochs:  14%|█▍        | 5/36 [3:12:36<19:52:40, 2308.39s/it]epochs:  14%|█▍        | 5/36 [3:12:36<19:52:40, 2308.39s/it]epochs:  14%|█▍        | 5/36 [3:12:36<19:52:40, 2308.41s/it]epochs:  14%|█▍        | 5/36 [3:12:37<19:52:40, 2308.42s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 04:32:13,513   INFO  Train:    6/36 ( 17%) [   0/1759 (  0%)]  Loss: 4.766 (4.77)  LR: 1.027e-03  Grad: 4.2417  max=3.6146(module.vfe.pfn_layers.0.linear.weight)  min: -0.2576(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.8761, loss_cls=0.1902, loss_bbox=1.3160, matched_ious=0.4785, loss_iou=0.1020, loss_iou_reg=0.2377, d_time=1.02(1.02), f_time=2.11(2.11), b_time=3.13(3.13)  Time cost: 00:02/1:20:47 [3:12:40/41:44:24]  Acc_iter 8796        Data time: 1.02(1.02)  Forward time: 2.11(2.11)  Batch time: 3.13(3.13)
2025-09-03 04:32:18,793   INFO  Train:    6/36 ( 17%) [   4/1759 (  0%)]  Loss: 4.135 (4.37)  LR: 1.027e-03  Grad: 2.5056  max=0.1456(module.backbone_3d.cls_conv.3.weight)  min: -1.3516(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8425, loss_cls=0.1976, loss_bbox=1.0063, matched_ious=0.4507, loss_iou=0.1027, loss_iou_reg=0.2599, d_time=0.01(0.21), f_time=1.25(1.47), b_time=1.26(1.68)  Time cost: 00:08/47:02 [3:12:46/24:21:18]  Acc_iter 8800        Data time: 0.01(0.21)  Forward time: 1.25(1.47)  Batch time: 1.26(1.68)
2025-09-03 04:33:23,918   INFO  Train:    6/36 ( 17%) [  54/1759 (  3%)]  Loss: 5.160 (4.37)  LR: 1.035e-03  Grad: 3.7985  max=1.5140(module.vfe.pfn_layers.0.linear.weight)  min: -2.4687(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8635, loss_cls=0.1922, loss_bbox=0.9927, matched_ious=0.4588, loss_iou=0.0977, loss_iou_reg=0.2517, d_time=0.01(0.03), f_time=1.25(1.31), b_time=1.26(1.34)  Time cost: 01:13/37:48 [3:13:51/20:07:51]  Acc_iter 8850        Data time: 0.01(0.03)  Forward time: 1.25(1.31)  Batch time: 1.26(1.34)
2025-09-03 04:34:29,749   INFO  Train:    6/36 ( 17%) [ 104/1759 (  6%)]  Loss: 4.230 (4.38)  LR: 1.042e-03  Grad: 9.7772  max=9.4818(module.vfe.pfn_layers.0.linear.weight)  min: -1.2141(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8884, loss_cls=0.1930, loss_bbox=1.0123, matched_ious=0.4598, loss_iou=0.1023, loss_iou_reg=0.2507, d_time=0.01(0.02), f_time=1.24(1.31), b_time=1.24(1.33)  Time cost: 02:18/36:30 [3:14:57/20:00:44]  Acc_iter 8900        Data time: 0.01(0.02)  Forward time: 1.24(1.31)  Batch time: 1.24(1.33)
2025-09-03 04:35:35,329   INFO  Train:    6/36 ( 17%) [ 154/1759 (  9%)]  Loss: 5.022 (4.38)  LR: 1.050e-03  Grad: 2.3969  max=1.4226(module.vfe.pfn_layers.0.linear.weight)  min: -0.2221(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8799, loss_cls=0.1885, loss_bbox=1.0168, matched_ious=0.4569, loss_iou=0.1015, loss_iou_reg=0.2522, d_time=0.01(0.01), f_time=1.32(1.31), b_time=1.33(1.32)  Time cost: 03:24/35:18 [3:16:02/19:56:07]  Acc_iter 8950        Data time: 0.01(0.01)  Forward time: 1.32(1.31)  Batch time: 1.33(1.32)
2025-09-03 04:36:40,467   INFO  Train:    6/36 ( 17%) [ 204/1759 ( 12%)]  Loss: 4.871 (4.38)  LR: 1.057e-03  Grad: 6.2018  max=1.0285(module.vfe.pfn_layers.0.linear.weight)  min: -5.5746(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8741, loss_cls=0.1861, loss_bbox=1.0725, matched_ious=0.4655, loss_iou=0.1038, loss_iou_reg=0.2461, d_time=0.01(0.01), f_time=1.27(1.30), b_time=1.28(1.32)  Time cost: 04:29/34:05 [3:17:07/19:51:14]  Acc_iter 9000        Data time: 0.01(0.01)  Forward time: 1.27(1.30)  Batch time: 1.28(1.32)
2025-09-03 04:37:45,867   INFO  Train:    6/36 ( 17%) [ 254/1759 ( 14%)]  Loss: 4.242 (4.35)  LR: 1.065e-03  Grad: 4.1534  max=2.7747(module.vfe.pfn_layers.0.linear.weight)  min: -1.7806(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8395, loss_cls=0.1905, loss_bbox=0.8993, matched_ious=0.4653, loss_iou=0.1004, loss_iou_reg=0.2493, d_time=0.01(0.01), f_time=1.25(1.30), b_time=1.26(1.32)  Time cost: 05:35/32:57 [3:18:13/19:48:47]  Acc_iter 9050        Data time: 0.01(0.01)  Forward time: 1.25(1.30)  Batch time: 1.26(1.32)
2025-09-03 04:38:50,328   INFO  Train:    6/36 ( 17%) [ 304/1759 ( 17%)]  Loss: 4.215 (4.35)  LR: 1.072e-03  Grad: 2.4823  max=1.0747(module.vfe.pfn_layers.0.linear.weight)  min: -0.2226(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.8818, loss_cls=0.1937, loss_bbox=1.0054, matched_ious=0.4539, loss_iou=0.1020, loss_iou_reg=0.2525, d_time=0.01(0.01), f_time=1.26(1.30), b_time=1.27(1.31)  Time cost: 06:39/31:46 [3:19:17/19:43:59]  Acc_iter 9100        Data time: 0.01(0.01)  Forward time: 1.26(1.30)  Batch time: 1.27(1.31)
2025-09-03 04:39:57,428   INFO  Train:    6/36 ( 17%) [ 354/1759 ( 20%)]  Loss: 5.044 (4.34)  LR: 1.080e-03  Grad: 2.4461  max=0.5410(module.vfe.pfn_layers.0.linear.weight)  min: -1.0967(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8350, loss_cls=0.1833, loss_bbox=0.9980, matched_ious=0.4710, loss_iou=0.1006, loss_iou_reg=0.2439, d_time=0.01(0.01), f_time=1.25(1.30), b_time=1.26(1.32)  Time cost: 07:46/30:46 [3:20:24/19:46:56]  Acc_iter 9150        Data time: 0.01(0.01)  Forward time: 1.25(1.30)  Batch time: 1.26(1.32)
2025-09-03 04:41:02,948   INFO  Train:    6/36 ( 17%) [ 404/1759 ( 23%)]  Loss: 3.357 (4.36)  LR: 1.087e-03  Grad: 3.0103  max=1.6378(module.vfe.pfn_layers.0.linear.weight)  min: -1.0872(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8964, loss_cls=0.1955, loss_bbox=1.0683, matched_ious=0.4549, loss_iou=0.1012, loss_iou_reg=0.2537, d_time=0.01(0.01), f_time=1.24(1.30), b_time=1.25(1.32)  Time cost: 08:52/29:40 [3:21:30/19:45:24]  Acc_iter 9200        Data time: 0.01(0.01)  Forward time: 1.24(1.30)  Batch time: 1.25(1.32)
2025-09-03 04:42:08,905   INFO  Train:    6/36 ( 17%) [ 454/1759 ( 26%)]  Loss: 4.372 (4.36)  LR: 1.095e-03  Grad: 2.3892  max=0.8975(module.vfe.pfn_layers.0.linear.weight)  min: -0.2641(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8705, loss_cls=0.1891, loss_bbox=1.0374, matched_ious=0.4638, loss_iou=0.1004, loss_iou_reg=0.2478, d_time=0.01(0.01), f_time=1.40(1.31), b_time=1.41(1.32)  Time cost: 09:58/28:35 [3:22:36/19:44:47]  Acc_iter 9250        Data time: 0.01(0.01)  Forward time: 1.40(1.31)  Batch time: 1.41(1.32)
2025-09-03 04:43:14,096   INFO  Train:    6/36 ( 17%) [ 504/1759 ( 29%)]  Loss: 4.244 (4.34)  LR: 1.103e-03  Grad: 2.9025  max=0.0851(module.vfe.pfn_layers.1.linear.weight)  min: -0.9932(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8362, loss_cls=0.1848, loss_bbox=0.9356, matched_ious=0.4725, loss_iou=0.1012, loss_iou_reg=0.2462, d_time=0.01(0.01), f_time=1.22(1.30), b_time=1.23(1.31)  Time cost: 11:03/27:28 [3:23:41/19:42:44]  Acc_iter 9300        Data time: 0.01(0.01)  Forward time: 1.22(1.30)  Batch time: 1.23(1.31)
2025-09-03 04:44:18,015   INFO  Train:    6/36 ( 17%) [ 554/1759 ( 31%)]  Loss: 4.532 (4.35)  LR: 1.110e-03  Grad: 2.9827  max=1.1897(module.vfe.pfn_layers.0.linear.weight)  min: -0.6766(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8790, loss_cls=0.1923, loss_bbox=1.0151, matched_ious=0.4539, loss_iou=0.1016, loss_iou_reg=0.2518, d_time=0.01(0.01), f_time=1.33(1.30), b_time=1.34(1.31)  Time cost: 12:07/26:19 [3:24:45/19:38:48]  Acc_iter 9350        Data time: 0.01(0.01)  Forward time: 1.33(1.30)  Batch time: 1.34(1.31)
2025-09-03 04:45:23,236   INFO  Train:    6/36 ( 17%) [ 604/1759 ( 34%)]  Loss: 5.143 (4.35)  LR: 1.118e-03  Grad: 5.9508  max=1.5155(module.vfe.pfn_layers.0.linear.weight)  min: -4.9513(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8748, loss_cls=0.1897, loss_bbox=1.0060, matched_ious=0.4581, loss_iou=0.1037, loss_iou_reg=0.2500, d_time=0.01(0.01), f_time=1.21(1.30), b_time=1.22(1.31)  Time cost: 13:12/25:12 [3:25:50/19:37:16]  Acc_iter 9400        Data time: 0.01(0.01)  Forward time: 1.21(1.30)  Batch time: 1.22(1.31)
2025-09-03 04:46:28,412   INFO  Train:    6/36 ( 17%) [ 654/1759 ( 37%)]  Loss: 3.928 (4.36)  LR: 1.126e-03  Grad: 3.5313  max=1.3399(module.vfe.pfn_layers.0.linear.weight)  min: -1.2357(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9051, loss_cls=0.1910, loss_bbox=1.0422, matched_ious=0.4576, loss_iou=0.1025, loss_iou_reg=0.2523, d_time=0.01(0.01), f_time=1.29(1.30), b_time=1.29(1.31)  Time cost: 14:17/24:06 [3:26:55/19:35:44]  Acc_iter 9450        Data time: 0.01(0.01)  Forward time: 1.29(1.30)  Batch time: 1.29(1.31)
2025-09-03 04:47:33,003   INFO  Train:    6/36 ( 17%) [ 704/1759 ( 40%)]  Loss: 3.336 (4.36)  LR: 1.133e-03  Grad: 4.5631  max=0.9682(module.vfe.pfn_layers.0.linear.weight)  min: -2.6433(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9029, loss_cls=0.1895, loss_bbox=1.0394, matched_ious=0.4482, loss_iou=0.1027, loss_iou_reg=0.2536, d_time=0.01(0.01), f_time=1.34(1.30), b_time=1.35(1.31)  Time cost: 15:22/23:00 [3:28:00/19:33:31]  Acc_iter 9500        Data time: 0.01(0.01)  Forward time: 1.34(1.30)  Batch time: 1.35(1.31)
2025-09-03 04:48:37,963   INFO  Train:    6/36 ( 17%) [ 754/1759 ( 43%)]  Loss: 4.485 (4.36)  LR: 1.141e-03  Grad: 3.9673  max=1.5942(module.vfe.pfn_layers.0.linear.weight)  min: -2.5457(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8658, loss_cls=0.1856, loss_bbox=1.0115, matched_ious=0.4629, loss_iou=0.0978, loss_iou_reg=0.2471, d_time=0.01(0.01), f_time=1.26(1.30), b_time=1.27(1.31)  Time cost: 16:27/21:54 [3:29:05/19:31:54]  Acc_iter 9550        Data time: 0.01(0.01)  Forward time: 1.26(1.30)  Batch time: 1.27(1.31)
2025-09-03 04:49:43,196   INFO  Train:    6/36 ( 17%) [ 804/1759 ( 46%)]  Loss: 4.063 (4.36)  LR: 1.149e-03  Grad: 3.0982  max=1.4407(module.vfe.pfn_layers.0.linear.weight)  min: -0.8423(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9012, loss_cls=0.1909, loss_bbox=1.0084, matched_ious=0.4720, loss_iou=0.1017, loss_iou_reg=0.2447, d_time=0.01(0.01), f_time=1.47(1.30), b_time=1.48(1.31)  Time cost: 17:32/20:48 [3:30:10/19:30:39]  Acc_iter 9600        Data time: 0.01(0.01)  Forward time: 1.47(1.30)  Batch time: 1.48(1.31)
2025-09-03 04:50:50,354   INFO  Train:    6/36 ( 17%) [ 854/1759 ( 49%)]  Loss: 4.217 (4.35)  LR: 1.157e-03  Grad: 5.5272  max=0.0999(module.backbone_3d.cls_conv.3.weight)  min: -4.3372(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8624, loss_cls=0.1841, loss_bbox=0.9570, matched_ious=0.4625, loss_iou=0.1017, loss_iou_reg=0.2505, d_time=0.01(0.01), f_time=1.29(1.30), b_time=1.30(1.31)  Time cost: 18:39/19:45 [3:31:17/19:31:26]  Acc_iter 9650        Data time: 0.01(0.01)  Forward time: 1.29(1.30)  Batch time: 1.30(1.31)
2025-09-03 04:51:54,168   INFO  Train:    6/36 ( 17%) [ 904/1759 ( 51%)]  Loss: 4.659 (4.35)  LR: 1.165e-03  Grad: 4.9315  max=3.6224(module.vfe.pfn_layers.0.linear.weight)  min: -0.1905(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.8578, loss_cls=0.1818, loss_bbox=0.9721, matched_ious=0.4704, loss_iou=0.1002, loss_iou_reg=0.2466, d_time=0.01(0.01), f_time=1.21(1.30), b_time=1.22(1.31)  Time cost: 19:43/18:38 [3:32:21/19:28:42]  Acc_iter 9700        Data time: 0.01(0.01)  Forward time: 1.21(1.30)  Batch time: 1.22(1.31)
2025-09-03 04:52:59,743   INFO  Train:    6/36 ( 17%) [ 954/1759 ( 54%)]  Loss: 3.760 (4.34)  LR: 1.172e-03  Grad: 3.2222  max=0.0900(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.7767(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8594, loss_cls=0.1810, loss_bbox=0.9481, matched_ious=0.4668, loss_iou=0.0998, loss_iou_reg=0.2476, d_time=0.01(0.01), f_time=1.30(1.30), b_time=1.30(1.31)  Time cost: 20:48/17:32 [3:33:27/19:27:47]  Acc_iter 9750        Data time: 0.01(0.01)  Forward time: 1.30(1.30)  Batch time: 1.30(1.31)
2025-09-03 04:54:05,196   INFO  Train:    6/36 ( 17%) [1004/1759 ( 57%)]  Loss: 4.133 (4.34)  LR: 1.180e-03  Grad: 3.7628  max=1.0304(module.vfe.pfn_layers.0.linear.weight)  min: -1.3285(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8633, loss_cls=0.1855, loss_bbox=0.9941, matched_ious=0.4527, loss_iou=0.1019, loss_iou_reg=0.2542, d_time=0.01(0.01), f_time=1.36(1.30), b_time=1.37(1.31)  Time cost: 21:54/16:27 [3:34:32/19:26:45]  Acc_iter 9800        Data time: 0.01(0.01)  Forward time: 1.36(1.30)  Batch time: 1.37(1.31)
2025-09-03 04:55:10,973   INFO  Train:    6/36 ( 17%) [1054/1759 ( 60%)]  Loss: 4.566 (4.34)  LR: 1.188e-03  Grad: 5.1477  max=0.0877(module.dense_head.prediction_head.height.1.weight)  min: -2.8708(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8567, loss_cls=0.1781, loss_bbox=0.9613, matched_ious=0.4641, loss_iou=0.1007, loss_iou_reg=0.2489, d_time=0.01(0.01), f_time=1.39(1.30), b_time=1.40(1.31)  Time cost: 23:00/15:22 [3:35:38/19:25:59]  Acc_iter 9850        Data time: 0.01(0.01)  Forward time: 1.39(1.30)  Batch time: 1.40(1.31)
2025-09-03 04:56:15,551   INFO  Train:    6/36 ( 17%) [1104/1759 ( 63%)]  Loss: 3.974 (4.33)  LR: 1.196e-03  Grad: 4.0841  max=0.2816(module.vfe.pfn_layers.0.linear.weight)  min: -2.1261(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8669, loss_cls=0.1854, loss_bbox=0.9762, matched_ious=0.4629, loss_iou=0.1003, loss_iou_reg=0.2472, d_time=0.01(0.01), f_time=1.34(1.30), b_time=1.35(1.31)  Time cost: 24:04/14:16 [3:36:42/19:24:13]  Acc_iter 9900        Data time: 0.01(0.01)  Forward time: 1.34(1.30)  Batch time: 1.35(1.31)
2025-09-03 04:57:20,959   INFO  Train:    6/36 ( 17%) [1154/1759 ( 66%)]  Loss: 3.849 (4.33)  LR: 1.204e-03  Grad: 5.4774  max=4.5907(module.vfe.pfn_layers.0.linear.weight)  min: -1.3660(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8353, loss_cls=0.1799, loss_bbox=0.9940, matched_ious=0.4641, loss_iou=0.0995, loss_iou_reg=0.2483, d_time=0.01(0.01), f_time=1.33(1.30), b_time=1.34(1.31)  Time cost: 25:10/13:11 [3:37:48/19:23:10]  Acc_iter 9950        Data time: 0.01(0.01)  Forward time: 1.33(1.30)  Batch time: 1.34(1.31)
2025-09-03 04:58:25,842   INFO  Train:    6/36 ( 17%) [1204/1759 ( 68%)]  Loss: 5.315 (4.32)  LR: 1.212e-03  Grad: 4.1990  max=1.9271(module.vfe.pfn_layers.0.linear.weight)  min: -0.2530(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.8637, loss_cls=0.1837, loss_bbox=0.9499, matched_ious=0.4716, loss_iou=0.0983, loss_iou_reg=0.2464, d_time=0.01(0.01), f_time=1.29(1.30), b_time=1.30(1.31)  Time cost: 26:15/12:05 [3:38:53/19:21:42]  Acc_iter 10000       Data time: 0.01(0.01)  Forward time: 1.29(1.30)  Batch time: 1.30(1.31)
2025-09-03 04:59:30,641   INFO  Train:    6/36 ( 17%) [1254/1759 ( 71%)]  Loss: 4.939 (4.32)  LR: 1.220e-03  Grad: 3.8809  max=1.2746(module.vfe.pfn_layers.0.linear.weight)  min: -1.8004(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8147, loss_cls=0.1782, loss_bbox=0.9587, matched_ious=0.4653, loss_iou=0.0989, loss_iou_reg=0.2500, d_time=0.01(0.01), f_time=1.16(1.30), b_time=1.17(1.31)  Time cost: 27:19/10:59 [3:39:57/19:20:13]  Acc_iter 10050       Data time: 0.01(0.01)  Forward time: 1.16(1.30)  Batch time: 1.17(1.31)
2025-09-03 05:00:35,486   INFO  Train:    6/36 ( 17%) [1304/1759 ( 74%)]  Loss: 4.087 (4.31)  LR: 1.228e-03  Grad: 5.1129  max=0.2263(module.dense_head.prediction_head.height.1.weight)  min: -2.7053(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8320, loss_cls=0.1783, loss_bbox=0.9723, matched_ious=0.4697, loss_iou=0.0996, loss_iou_reg=0.2462, d_time=0.01(0.01), f_time=1.35(1.30), b_time=1.36(1.31)  Time cost: 28:24/09:54 [3:41:02/19:18:48]  Acc_iter 10100       Data time: 0.01(0.01)  Forward time: 1.35(1.30)  Batch time: 1.36(1.31)
2025-09-03 05:01:43,175   INFO  Train:    6/36 ( 17%) [1354/1759 ( 77%)]  Loss: 4.908 (4.31)  LR: 1.236e-03  Grad: 3.6266  max=1.2926(module.vfe.pfn_layers.0.linear.weight)  min: -0.5970(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8512, loss_cls=0.1762, loss_bbox=1.0091, matched_ious=0.4612, loss_iou=0.0996, loss_iou_reg=0.2506, d_time=0.01(0.01), f_time=1.37(1.30), b_time=1.38(1.31)  Time cost: 29:32/08:49 [3:42:10/19:19:16]  Acc_iter 10150       Data time: 0.01(0.01)  Forward time: 1.37(1.30)  Batch time: 1.38(1.31)
2025-09-03 05:02:48,450   INFO  Train:    6/36 ( 17%) [1404/1759 ( 80%)]  Loss: 3.612 (4.31)  LR: 1.244e-03  Grad: 4.3417  max=1.0387(module.vfe.pfn_layers.0.linear.weight)  min: -2.2047(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8463, loss_cls=0.1832, loss_bbox=0.9278, matched_ious=0.4650, loss_iou=0.1003, loss_iou_reg=0.2482, d_time=0.01(0.01), f_time=1.32(1.30), b_time=1.32(1.31)  Time cost: 30:37/07:44 [3:43:15/19:18:05]  Acc_iter 10200       Data time: 0.01(0.01)  Forward time: 1.32(1.30)  Batch time: 1.32(1.31)
2025-09-03 05:03:52,898   INFO  Train:    6/36 ( 17%) [1454/1759 ( 83%)]  Loss: 4.727 (4.30)  LR: 1.252e-03  Grad: 4.8983  max=2.4762(module.vfe.pfn_layers.0.linear.weight)  min: -0.1341(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8257, loss_cls=0.1794, loss_bbox=0.9374, matched_ious=0.4678, loss_iou=0.1022, loss_iou_reg=0.2492, d_time=0.01(0.01), f_time=1.38(1.30), b_time=1.39(1.31)  Time cost: 31:42/06:38 [3:44:20/19:16:25]  Acc_iter 10250       Data time: 0.01(0.01)  Forward time: 1.38(1.30)  Batch time: 1.39(1.31)
2025-09-03 05:04:59,002   INFO  Train:    6/36 ( 17%) [1504/1759 ( 86%)]  Loss: 3.518 (4.30)  LR: 1.260e-03  Grad: 8.3486  max=4.0165(module.vfe.pfn_layers.0.linear.weight)  min: -6.2065(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8658, loss_cls=0.1845, loss_bbox=1.0029, matched_ious=0.4618, loss_iou=0.0993, loss_iou_reg=0.2516, d_time=0.01(0.01), f_time=1.28(1.30), b_time=1.28(1.31)  Time cost: 32:48/05:33 [3:45:26/19:15:46]  Acc_iter 10300       Data time: 0.01(0.01)  Forward time: 1.28(1.30)  Batch time: 1.28(1.31)
2025-09-03 05:06:03,573   INFO  Train:    6/36 ( 17%) [1554/1759 ( 88%)]  Loss: 4.502 (4.30)  LR: 1.268e-03  Grad: 3.8278  max=0.9356(module.vfe.pfn_layers.0.linear.weight)  min: -1.0401(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8369, loss_cls=0.1768, loss_bbox=0.9678, matched_ious=0.4708, loss_iou=0.0985, loss_iou_reg=0.2471, d_time=0.01(0.01), f_time=1.27(1.30), b_time=1.28(1.31)  Time cost: 33:52/04:27 [3:46:30/19:14:13]  Acc_iter 10350       Data time: 0.01(0.01)  Forward time: 1.27(1.30)  Batch time: 1.28(1.31)
2025-09-03 05:07:08,513   INFO  Train:    6/36 ( 17%) [1604/1759 ( 91%)]  Loss: 4.343 (4.30)  LR: 1.276e-03  Grad: 4.4525  max=1.8877(module.vfe.pfn_layers.0.linear.weight)  min: -0.4490(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.8686, loss_cls=0.1812, loss_bbox=1.0123, matched_ious=0.4697, loss_iou=0.1008, loss_iou_reg=0.2449, d_time=0.01(0.01), f_time=1.26(1.30), b_time=1.27(1.31)  Time cost: 34:57/03:22 [3:47:35/19:12:53]  Acc_iter 10400       Data time: 0.01(0.01)  Forward time: 1.26(1.30)  Batch time: 1.27(1.31)
2025-09-03 05:08:13,630   INFO  Train:    6/36 ( 17%) [1654/1759 ( 94%)]  Loss: 4.112 (4.30)  LR: 1.284e-03  Grad: 5.4552  max=0.6388(module.vfe.pfn_layers.0.linear.weight)  min: -3.3247(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8529, loss_cls=0.1851, loss_bbox=0.9393, matched_ious=0.4608, loss_iou=0.0989, loss_iou_reg=0.2525, d_time=0.01(0.01), f_time=1.32(1.30), b_time=1.33(1.31)  Time cost: 36:02/02:17 [3:48:40/19:11:41]  Acc_iter 10450       Data time: 0.01(0.01)  Forward time: 1.32(1.30)  Batch time: 1.33(1.31)
2025-09-03 05:09:18,642   INFO  Train:    6/36 ( 17%) [1704/1759 ( 97%)]  Loss: 5.085 (4.29)  LR: 1.292e-03  Grad: 4.1209  max=2.2825(module.vfe.pfn_layers.0.linear.weight)  min: -0.5028(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8468, loss_cls=0.1800, loss_bbox=0.9469, matched_ious=0.4645, loss_iou=0.0984, loss_iou_reg=0.2495, d_time=0.01(0.01), f_time=1.30(1.30), b_time=1.31(1.31)  Time cost: 37:07/01:11 [3:49:45/19:10:25]  Acc_iter 10500       Data time: 0.01(0.01)  Forward time: 1.30(1.30)  Batch time: 1.31(1.31)
2025-09-03 05:10:22,853   INFO  Train:    6/36 ( 17%) [1754/1759 (100%)]  Loss: 3.662 (4.29)  LR: 1.300e-03  Grad: 4.3335  max=1.5884(module.vfe.pfn_layers.0.linear.weight)  min: -2.1857(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8381, loss_cls=0.1782, loss_bbox=0.9392, matched_ious=0.4705, loss_iou=0.1016, loss_iou_reg=0.2466, d_time=0.01(0.01), f_time=1.23(1.30), b_time=1.24(1.31)  Time cost: 38:12/00:06 [3:50:50/19:08:46]  Acc_iter 10550       Data time: 0.01(0.01)  Forward time: 1.23(1.30)  Batch time: 1.24(1.31)
2025-09-03 05:10:27,813   INFO  Train:    6/36 ( 17%) [1758/1759 (100%)]  Loss: 4.190 (4.29)  LR: 1.300e-03  Grad: 9.3089  max=8.0713(module.vfe.pfn_layers.0.linear.weight)  min: -2.3154(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7733, loss_cls=0.1734, loss_bbox=0.9017, matched_ious=0.4423, loss_iou=0.1015, loss_iou_reg=0.2608, d_time=0.01(0.01), f_time=0.79(1.30), b_time=0.80(1.31)  Time cost: 38:17/00:01 [3:50:55/19:08:32]  Acc_iter 10554       Data time: 0.01(0.01)  Forward time: 0.79(1.30)  Batch time: 0.80(1.31)

                                               [Aepochs:  17%|█▋        | 6/36 [3:50:55<19:12:29, 2304.99s/it]epochs:  17%|█▋        | 6/36 [3:50:55<19:12:29, 2304.99s/it]epochs:  17%|█▋        | 6/36 [3:50:55<19:12:29, 2305.00s/it]epochs:  17%|█▋        | 6/36 [3:50:55<19:12:29, 2305.00s/it]epochs:  17%|█▋        | 6/36 [3:50:55<19:12:30, 2305.01s/it]epochs:  17%|█▋        | 6/36 [3:50:55<19:12:30, 2305.02s/it]epochs:  17%|█▋        | 6/36 [3:50:55<19:12:30, 2305.01s/it]epochs:  17%|█▋        | 6/36 [3:50:55<19:12:31, 2305.04s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 05:10:32,370   INFO  Train:    7/36 ( 19%) [   0/1759 (  0%)]  Loss: 4.093 (4.09)  LR: 1.301e-03  Grad: 4.0475  max=1.3836(module.vfe.pfn_layers.0.linear.weight)  min: -1.2680(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7826, loss_cls=0.1817, loss_bbox=0.8890, matched_ious=0.4409, loss_iou=0.0947, loss_iou_reg=0.2676, d_time=1.33(1.33), f_time=2.21(2.21), b_time=3.55(3.55)  Time cost: 00:03/1:32:05 [3:50:59/46:02:30]  Acc_iter 10555       Data time: 1.33(1.33)  Forward time: 2.21(2.21)  Batch time: 3.55(3.55)
2025-09-03 05:11:31,219   INFO  Train:    7/36 ( 19%) [  45/1759 (  3%)]  Loss: 3.996 (4.22)  LR: 1.308e-03  Grad: 3.7147  max=0.8099(module.vfe.pfn_layers.0.linear.weight)  min: -0.4219(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8419, loss_cls=0.1809, loss_bbox=0.9702, matched_ious=0.4620, loss_iou=0.1023, loss_iou_reg=0.2509, d_time=0.01(0.04), f_time=1.35(1.32), b_time=1.36(1.36)  Time cost: 01:01/38:29 [3:51:58/19:44:10]  Acc_iter 10600       Data time: 0.01(0.04)  Forward time: 1.35(1.32)  Batch time: 1.36(1.36)
2025-09-03 05:12:39,606   INFO  Train:    7/36 ( 19%) [  95/1759 (  5%)]  Loss: 3.810 (4.23)  LR: 1.316e-03  Grad: 4.5794  max=0.3304(module.dense_head.prediction_head.height.1.weight)  min: -2.2888(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8388, loss_cls=0.1797, loss_bbox=0.9632, matched_ious=0.4635, loss_iou=0.0979, loss_iou_reg=0.2502, d_time=0.01(0.03), f_time=1.34(1.33), b_time=1.35(1.36)  Time cost: 02:10/37:39 [3:53:06/19:52:12]  Acc_iter 10650       Data time: 0.01(0.03)  Forward time: 1.34(1.33)  Batch time: 1.35(1.36)
                                                             Traceback (most recent call last):
  File "/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/train.py", line 246, in <module>
    main()
  File "/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/train.py", line 188, in main
    train_model(
  File "/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/train_utils/train_utils.py", line 272, in train_model
    accumulated_iter = train_one_epoch(
  File "/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/train_utils/train_utils.py", line 71, in train_one_epoch
    loss, tb_dict, disp_dict = model_func(model, batch)
  File "/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/models/__init__.py", line 44, in model_func
    ret_dict, tb_dict, disp_dict = model(batch_dict)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/models/detectors/transfusion.py", line 12, in forward
    batch_dict = cur_module(batch_dict)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/models/dense_heads/transfusion_head.py", line 290, in forward
    res = self.predict(feats)
  File "/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/models/dense_heads/transfusion_head.py", line 267, in predict
    query_feat_T = self.decoder(
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/models/model_utils/transfusion_utils.py", line 91, in forward
    query2 = self.multihead_attn(query=self.with_pos_embed(query, query_pos_embed),
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/functional.py", line 5300, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/functional.py", line 4846, in _in_projection_packed
    return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 986.00 MiB. GPU 7 has a total capacty of 23.78 GiB of which 300.56 MiB is free. Process 3332667 has 23.48 GiB memory in use. Of the allocated memory 21.33 GiB is allocated by PyTorch, and 1.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2025-09-03 05:12:49,226] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 23 closing signal SIGTERM
[2025-09-03 05:12:49,226] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 24 closing signal SIGTERM
[2025-09-03 05:12:49,227] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 25 closing signal SIGTERM
[2025-09-03 05:12:49,227] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 26 closing signal SIGTERM
[2025-09-03 05:12:49,228] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 27 closing signal SIGTERM
[2025-09-03 05:12:49,228] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 28 closing signal SIGTERM
[2025-09-03 05:12:49,229] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 29 closing signal SIGTERM
[2025-09-03 05:12:49,446] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 7 (pid: 31) of binary: /root/miniconda3/envs/sparseformerv2/bin/python
Traceback (most recent call last):
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launch.py", line 196, in <module>
    main()
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launch.py", line 192, in main
    launch(args)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launch.py", line 177, in launch
    run(args)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-03_05:12:49
  host      : eflops66.aliyun.com
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 31)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
