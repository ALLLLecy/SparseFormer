/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
2025-09-04 09:21:48,003   INFO  **********************Start logging**********************
2025-09-04 09:21:48,003   INFO  CUDA_VISIBLE_DEVICES=ALL
2025-09-04 09:21:48,003   INFO  Training in distributed mode : total_batch_size: 16
2025-09-04 09:21:48,003   INFO  cfg_file         cfgs/sparse_models/sparse_former_base.yaml
2025-09-04 09:21:48,003   INFO  batch_size       2
2025-09-04 09:21:48,003   INFO  epochs           36
2025-09-04 09:21:48,003   INFO  workers          12
2025-09-04 09:21:48,003   INFO  extra_tag        default
2025-09-04 09:21:48,003   INFO  ckpt             None
2025-09-04 09:21:48,003   INFO  pretrained_model None
2025-09-04 09:21:48,003   INFO  launcher         pytorch
2025-09-04 09:21:48,004   INFO  tcp_port         18888
2025-09-04 09:21:48,004   INFO  sync_bn          True
2025-09-04 09:21:48,004   INFO  fix_random_seed  False
2025-09-04 09:21:48,004   INFO  ckpt_save_interval 1
2025-09-04 09:21:48,004   INFO  local_rank       0
2025-09-04 09:21:48,004   INFO  max_ckpt_save_num 30
2025-09-04 09:21:48,004   INFO  merge_all_iters_to_one_epoch False
2025-09-04 09:21:48,004   INFO  set_cfgs         None
2025-09-04 09:21:48,004   INFO  max_waiting_mins 0
2025-09-04 09:21:48,004   INFO  start_epoch      0
2025-09-04 09:21:48,004   INFO  num_epochs_to_eval 0
2025-09-04 09:21:48,004   INFO  save_to_file     False
2025-09-04 09:21:48,004   INFO  use_tqdm_to_record False
2025-09-04 09:21:48,004   INFO  logger_iter_interval 50
2025-09-04 09:21:48,004   INFO  ckpt_save_time_interval 300
2025-09-04 09:21:48,004   INFO  wo_gpu_stat      True
2025-09-04 09:21:48,004   INFO  use_amp          False
2025-09-04 09:21:48,004   INFO  eval_map         False
2025-09-04 09:21:48,004   INFO  dataset          nuscenes
2025-09-04 09:21:48,004   INFO  root_dir         /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2
2025-09-04 09:21:48,004   INFO  output_dir       /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_shuflle_.0diff_dense_hm_.3lr_.3wd
2025-09-04 09:21:48,004   INFO  cfg.ROOT_DIR: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2
2025-09-04 09:21:48,004   INFO  cfg.LOCAL_RANK: 0
2025-09-04 09:21:48,004   INFO  cfg.CLASS_NAMES: ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']
2025-09-04 09:21:48,004   INFO  ----------- DATA_CONFIG -----------
2025-09-04 09:21:48,005   INFO  cfg.DATA_CONFIG.DATASET: NuScenesDataset
2025-09-04 09:21:48,005   INFO  cfg.DATA_CONFIG.DATA_PATH: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/data/nuscenes
2025-09-04 09:21:48,005   INFO  cfg.DATA_CONFIG.VERSION: v1.0-trainval
2025-09-04 09:21:48,005   INFO  cfg.DATA_CONFIG.MAX_SWEEPS: 10
2025-09-04 09:21:48,005   INFO  cfg.DATA_CONFIG.PRED_VELOCITY: True
2025-09-04 09:21:48,005   INFO  cfg.DATA_CONFIG.SET_NAN_VELOCITY_TO_ZEROS: True
2025-09-04 09:21:48,005   INFO  cfg.DATA_CONFIG.FILTER_MIN_POINTS_IN_GT: 1
2025-09-04 09:21:48,005   INFO  ----------- DATA_SPLIT -----------
2025-09-04 09:21:48,005   INFO  cfg.DATA_CONFIG.DATA_SPLIT.train: train
2025-09-04 09:21:48,005   INFO  cfg.DATA_CONFIG.DATA_SPLIT.test: val
2025-09-04 09:21:48,005   INFO  ----------- INFO_PATH -----------
2025-09-04 09:21:48,005   INFO  cfg.DATA_CONFIG.INFO_PATH.train: ['nuscenes_infos_10sweeps_train.pkl']
2025-09-04 09:21:48,005   INFO  cfg.DATA_CONFIG.INFO_PATH.test: ['nuscenes_infos_10sweeps_val.pkl']
2025-09-04 09:21:48,005   INFO  cfg.DATA_CONFIG.POINT_CLOUD_RANGE: [-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]
2025-09-04 09:21:48,005   INFO  cfg.DATA_CONFIG.BALANCED_RESAMPLING: False
2025-09-04 09:21:48,005   INFO  ----------- DATA_AUGMENTOR -----------
2025-09-04 09:21:48,005   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.DISABLE_AUG_LIST: ['placeholder']
2025-09-04 09:21:48,005   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.AUG_CONFIG_LIST: [{'NAME': 'gt_sampling', 'DB_INFO_PATH': ['nuscenes_dbinfos_10sweeps_withvelo.pkl'], 'USE_SHARED_MEMORY': True, 'DB_DATA_PATH': ['nuscenes_10sweeps_withvelo_lidar.npy'], 'PREPARE': {'filter_by_min_points': ['car:5', 'truck:5', 'construction_vehicle:5', 'bus:5', 'trailer:5', 'barrier:5', 'motorcycle:5', 'bicycle:5', 'pedestrian:5', 'traffic_cone:5']}, 'SAMPLE_GROUPS': ['car:2', 'truck:3', 'construction_vehicle:7', 'bus:4', 'trailer:6', 'barrier:2', 'motorcycle:6', 'bicycle:6', 'pedestrian:2', 'traffic_cone:2'], 'NUM_POINT_FEATURES': 5, 'DATABASE_WITH_FAKELIDAR': False, 'REMOVE_EXTRA_WIDTH': [0.0, 0.0, 0.0], 'LIMIT_WHOLE_SCENE': True}, {'NAME': 'random_world_flip', 'ALONG_AXIS_LIST': ['x', 'y']}, {'NAME': 'random_world_rotation', 'WORLD_ROT_ANGLE': [-0.78539816, 0.78539816]}, {'NAME': 'random_world_scaling', 'WORLD_SCALE_RANGE': [0.9, 1.1]}, {'NAME': 'random_world_translation', 'NOISE_TRANSLATE_STD': [0.5, 0.5, 0.5]}]
2025-09-04 09:21:48,037   INFO  ----------- POINT_FEATURE_ENCODING -----------
2025-09-04 09:21:48,037   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.encoding_type: absolute_coordinates_encoding
2025-09-04 09:21:48,037   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.used_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-09-04 09:21:48,038   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.src_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-09-04 09:21:48,038   INFO  cfg.DATA_CONFIG.DATA_PROCESSOR: [{'NAME': 'mask_points_and_boxes_outside_range', 'REMOVE_OUTSIDE_BOXES': True}, {'NAME': 'shuffle_points', 'SHUFFLE_ENABLED': {'train': True, 'test': True}}, {'NAME': 'transform_points_to_voxels_placeholder', 'VOXEL_SIZE': [0.3, 0.3, 0.2]}]
2025-09-04 09:21:48,038   INFO  cfg.DATA_CONFIG._BASE_CONFIG_: cfgs/dataset_configs/nuscenes_dataset.yaml
2025-09-04 09:21:48,038   INFO  ----------- SAMPLED_INTERVAL -----------
2025-09-04 09:21:48,038   INFO  cfg.DATA_CONFIG.SAMPLED_INTERVAL.train: 1
2025-09-04 09:21:48,038   INFO  cfg.DATA_CONFIG.SAMPLED_INTERVAL.test: 1
2025-09-04 09:21:48,038   INFO  ----------- MODEL -----------
2025-09-04 09:21:48,038   INFO  cfg.MODEL.NAME: TransFusion
2025-09-04 09:21:48,038   INFO  ----------- VFE -----------
2025-09-04 09:21:48,039   INFO  cfg.MODEL.VFE.NAME: DynamicVoxelVFE
2025-09-04 09:21:48,039   INFO  cfg.MODEL.VFE.WITH_DISTANCE: False
2025-09-04 09:21:48,039   INFO  cfg.MODEL.VFE.USE_ABSLOTE_XYZ: True
2025-09-04 09:21:48,039   INFO  cfg.MODEL.VFE.USE_NORM: True
2025-09-04 09:21:48,039   INFO  cfg.MODEL.VFE.NUM_FILTERS: [64, 64]
2025-09-04 09:21:48,039   INFO  ----------- BACKBONE_3D -----------
2025-09-04 09:21:48,039   INFO  cfg.MODEL.BACKBONE_3D.NAME: FSHNet_nusc
2025-09-04 09:21:48,039   INFO  cfg.MODEL.BACKBONE_3D.FEATURE_DIM: 128
2025-09-04 09:21:48,039   INFO  ----------- SPENCODER -----------
2025-09-04 09:21:48,040   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.DIM: 3
2025-09-04 09:21:48,040   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.NUM_LAYERS: 3
2025-09-04 09:21:48,040   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SHUFFLE: True
2025-09-04 09:21:48,040   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.ORDERS: [['z', 'z-trans'], ['hilbert', 'hilbert-trans'], ['x', 'y']]
2025-09-04 09:21:48,040   INFO  ----------- SMSA -----------
2025-09-04 09:21:48,040   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.EMBED_DIM: 128
2025-09-04 09:21:48,040   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DEPTH: 8
2025-09-04 09:21:48,040   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.NUM_LEVELS: 3
2025-09-04 09:21:48,040   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.WINDOW_SHAPE: [9, 9, 5]
2025-09-04 09:21:48,041   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DROP_PATH: 0.2
2025-09-04 09:21:48,041   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.SPATIAL_ENHANCE: True
2025-09-04 09:21:48,041   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.FUSED_ADD_NORM: True
2025-09-04 09:21:48,041   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.RMS_NORM: True
2025-09-04 09:21:48,041   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.NORM_EPSILON: 1e-05
2025-09-04 09:21:48,041   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.RESIDUAL_IN_FP32: True
2025-09-04 09:21:48,041   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DIFF_COEF: 0.0
2025-09-04 09:21:48,041   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DIFF_KERNEL: [7, 5, 3]
2025-09-04 09:21:48,042   INFO  cfg.MODEL.BACKBONE_3D.AFD_FEATURE_DIM: 128
2025-09-04 09:21:48,042   INFO  cfg.MODEL.BACKBONE_3D.AFD_NUM_LAYERS: 4
2025-09-04 09:21:48,042   INFO  cfg.MODEL.BACKBONE_3D.AFD_NUM_SBB: [2, 1, 1]
2025-09-04 09:21:48,042   INFO  cfg.MODEL.BACKBONE_3D.AFD_DOWN_STRIDE: [1, 2, 2]
2025-09-04 09:21:48,042   INFO  cfg.MODEL.BACKBONE_3D.AFD_DOWN_KERNEL_SIZE: [3, 3, 3]
2025-09-04 09:21:48,042   INFO  cfg.MODEL.BACKBONE_3D.AFD: True
2025-09-04 09:21:48,042   INFO  cfg.MODEL.BACKBONE_3D.FG_THRESHOLD: 0.2
2025-09-04 09:21:48,042   INFO  cfg.MODEL.BACKBONE_3D.FEATMAP_STRIDE: 2
2025-09-04 09:21:48,042   INFO  cfg.MODEL.BACKBONE_3D.DETACH_FEATURE: True
2025-09-04 09:21:48,043   INFO  cfg.MODEL.BACKBONE_3D.GREOUP_POOLING_KERNEL_SIZE: [9, 15, 5, 5]
2025-09-04 09:21:48,043   INFO  cfg.MODEL.BACKBONE_3D.GROUP_CLASS_NAMES: [['car', 'truck', 'construction_vehicle'], ['bus', 'trailer'], ['barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']]
2025-09-04 09:21:48,043   INFO  ----------- DENSE_HEAD -----------
2025-09-04 09:21:48,043   INFO  cfg.MODEL.DENSE_HEAD.CLASS_AGNOSTIC: False
2025-09-04 09:21:48,043   INFO  cfg.MODEL.DENSE_HEAD.NAME: TransFusionHead
2025-09-04 09:21:48,043   INFO  cfg.MODEL.DENSE_HEAD.QUERY_RADIUS: 20
2025-09-04 09:21:48,043   INFO  cfg.MODEL.DENSE_HEAD.QUERY_LOCAL: True
2025-09-04 09:21:48,043   INFO  cfg.MODEL.DENSE_HEAD.USE_BIAS_BEFORE_NORM: True
2025-09-04 09:21:48,043   INFO  cfg.MODEL.DENSE_HEAD.INPUT_FEATURES: 128
2025-09-04 09:21:48,043   INFO  cfg.MODEL.DENSE_HEAD.NUM_PROPOSALS: 600
2025-09-04 09:21:48,043   INFO  cfg.MODEL.DENSE_HEAD.HIDDEN_CHANNEL: 128
2025-09-04 09:21:48,043   INFO  cfg.MODEL.DENSE_HEAD.NUM_CLASSES: 10
2025-09-04 09:21:48,043   INFO  cfg.MODEL.DENSE_HEAD.NUM_HEADS: 8
2025-09-04 09:21:48,043   INFO  cfg.MODEL.DENSE_HEAD.NMS_KERNEL_SIZE: 3
2025-09-04 09:21:48,043   INFO  cfg.MODEL.DENSE_HEAD.FFN_CHANNEL: 256
2025-09-04 09:21:48,044   INFO  cfg.MODEL.DENSE_HEAD.DROPOUT: 0.1
2025-09-04 09:21:48,044   INFO  cfg.MODEL.DENSE_HEAD.BN_MOMENTUM: 0.1
2025-09-04 09:21:48,044   INFO  cfg.MODEL.DENSE_HEAD.ACTIVATION: relu
2025-09-04 09:21:48,044   INFO  cfg.MODEL.DENSE_HEAD.NUM_HM_CONV: 2
2025-09-04 09:21:48,044   INFO  ----------- SEPARATE_HEAD_CFG -----------
2025-09-04 09:21:48,044   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_ORDER: ['center', 'height', 'dim', 'rot', 'vel', 'iou']
2025-09-04 09:21:48,044   INFO  ----------- HEAD_DICT -----------
2025-09-04 09:21:48,044   INFO  ----------- center -----------
2025-09-04 09:21:48,044   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.out_channels: 2
2025-09-04 09:21:48,044   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.num_conv: 2
2025-09-04 09:21:48,044   INFO  ----------- height -----------
2025-09-04 09:21:48,044   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.height.out_channels: 1
2025-09-04 09:21:48,044   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.height.num_conv: 2
2025-09-04 09:21:48,044   INFO  ----------- dim -----------
2025-09-04 09:21:48,044   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.out_channels: 3
2025-09-04 09:21:48,044   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.num_conv: 2
2025-09-04 09:21:48,044   INFO  ----------- rot -----------
2025-09-04 09:21:48,044   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.out_channels: 2
2025-09-04 09:21:48,045   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.num_conv: 2
2025-09-04 09:21:48,045   INFO  ----------- vel -----------
2025-09-04 09:21:48,045   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.out_channels: 2
2025-09-04 09:21:48,045   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.num_conv: 2
2025-09-04 09:21:48,045   INFO  ----------- iou -----------
2025-09-04 09:21:48,045   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.iou.out_channels: 1
2025-09-04 09:21:48,045   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.iou.num_conv: 2
2025-09-04 09:21:48,045   INFO  ----------- TARGET_ASSIGNER_CONFIG -----------
2025-09-04 09:21:48,045   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.FEATURE_MAP_STRIDE: 2
2025-09-04 09:21:48,045   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.DATASET: nuScenes
2025-09-04 09:21:48,045   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.GAUSSIAN_OVERLAP: 0.1
2025-09-04 09:21:48,045   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.MIN_RADIUS: 2
2025-09-04 09:21:48,045   INFO  ----------- HUNGARIAN_ASSIGNER -----------
2025-09-04 09:21:48,045   INFO  ----------- cls_cost -----------
2025-09-04 09:21:48,045   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.gamma: 2.0
2025-09-04 09:21:48,045   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.alpha: 0.25
2025-09-04 09:21:48,045   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.weight: 0.15
2025-09-04 09:21:48,045   INFO  ----------- reg_cost -----------
2025-09-04 09:21:48,046   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.reg_cost.weight: 0.25
2025-09-04 09:21:48,046   INFO  ----------- iou_cost -----------
2025-09-04 09:21:48,046   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.iou_cost.weight: 0.25
2025-09-04 09:21:48,046   INFO  ----------- LOSS_CONFIG -----------
2025-09-04 09:21:48,046   INFO  ----------- LOSS_WEIGHTS -----------
2025-09-04 09:21:48,046   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.cls_weight: 1.0
2025-09-04 09:21:48,046   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.bbox_weight: 0.25
2025-09-04 09:21:48,046   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.hm_weight: 1.0
2025-09-04 09:21:48,046   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.iou_weight: 0.5
2025-09-04 09:21:48,046   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.iou_reg_weight: 0.5
2025-09-04 09:21:48,046   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.code_weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2]
2025-09-04 09:21:48,046   INFO  ----------- LOSS_CLS -----------
2025-09-04 09:21:48,046   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.use_sigmoid: True
2025-09-04 09:21:48,046   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.gamma: 2.0
2025-09-04 09:21:48,046   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.alpha: 0.25
2025-09-04 09:21:48,046   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_IOU: True
2025-09-04 09:21:48,046   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_IOU_REG: True
2025-09-04 09:21:48,046   INFO  ----------- POST_PROCESSING -----------
2025-09-04 09:21:48,047   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.SCORE_THRESH: 0.0
2025-09-04 09:21:48,047   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.POST_CENTER_RANGE: [-61.2, -61.2, -10.0, 61.2, 61.2, 10.0]
2025-09-04 09:21:48,047   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.USE_IOU_TO_RECTIFY_SCORE: True
2025-09-04 09:21:48,047   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.IOU_RECTIFIER: [0.5]
2025-09-04 09:21:48,047   INFO  ----------- NMS_CONFIG -----------
2025-09-04 09:21:48,047   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_TYPE: nms_gpu
2025-09-04 09:21:48,047   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_THRESH: 0.2
2025-09-04 09:21:48,047   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_PRE_MAXSIZE: 1000
2025-09-04 09:21:48,047   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_POST_MAXSIZE: 100
2025-09-04 09:21:48,047   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.SCORE_THRES: 0.0
2025-09-04 09:21:48,047   INFO  ----------- POST_PROCESSING -----------
2025-09-04 09:21:48,047   INFO  cfg.MODEL.POST_PROCESSING.RECALL_THRESH_LIST: [0.3, 0.5, 0.7]
2025-09-04 09:21:48,047   INFO  cfg.MODEL.POST_PROCESSING.SCORE_THRESH: 0.1
2025-09-04 09:21:48,047   INFO  cfg.MODEL.POST_PROCESSING.OUTPUT_RAW_SCORE: False
2025-09-04 09:21:48,047   INFO  cfg.MODEL.POST_PROCESSING.EVAL_METRIC: kitti
2025-09-04 09:21:48,047   INFO  ----------- NMS_CONFIG -----------
2025-09-04 09:21:48,047   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.MULTI_CLASSES_NMS: True
2025-09-04 09:21:48,047   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_TYPE: nms_gpu
2025-09-04 09:21:48,048   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_THRESH: 0.2
2025-09-04 09:21:48,048   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_PRE_MAXSIZE: 1000
2025-09-04 09:21:48,048   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_POST_MAXSIZE: 83
2025-09-04 09:21:48,048   INFO  ----------- OPTIMIZATION -----------
2025-09-04 09:21:48,048   INFO  cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU: 2
2025-09-04 09:21:48,048   INFO  cfg.OPTIMIZATION.NUM_EPOCHS: 36
2025-09-04 09:21:48,048   INFO  cfg.OPTIMIZATION.OPTIMIZER: adam_onecycle
2025-09-04 09:21:48,048   INFO  cfg.OPTIMIZATION.LR: 0.003
2025-09-04 09:21:48,048   INFO  cfg.OPTIMIZATION.WEIGHT_DECAY: 0.03
2025-09-04 09:21:48,048   INFO  cfg.OPTIMIZATION.MOMENTUM: 0.9
2025-09-04 09:21:48,048   INFO  cfg.OPTIMIZATION.MOMS: [0.95, 0.85]
2025-09-04 09:21:48,048   INFO  cfg.OPTIMIZATION.PCT_START: 0.4
2025-09-04 09:21:48,048   INFO  cfg.OPTIMIZATION.DIV_FACTOR: 10
2025-09-04 09:21:48,048   INFO  cfg.OPTIMIZATION.DECAY_STEP_LIST: [35, 45]
2025-09-04 09:21:48,048   INFO  cfg.OPTIMIZATION.LR_DECAY: 0.1
2025-09-04 09:21:48,048   INFO  cfg.OPTIMIZATION.LR_CLIP: 1e-07
2025-09-04 09:21:48,048   INFO  cfg.OPTIMIZATION.LR_WARMUP: False
2025-09-04 09:21:48,048   INFO  cfg.OPTIMIZATION.WARMUP_EPOCH: 1
2025-09-04 09:21:48,049   INFO  cfg.OPTIMIZATION.GRAD_NORM_CLIP: 10
2025-09-04 09:21:48,049   INFO  cfg.OPTIMIZATION.LOSS_SCALE_FP16: 32
2025-09-04 09:21:48,049   INFO  ----------- HOOK -----------
2025-09-04 09:21:48,049   INFO  ----------- DisableAugmentationHook -----------
2025-09-04 09:21:48,049   INFO  cfg.HOOK.DisableAugmentationHook.DISABLE_AUG_LIST: ['gt_sampling']
2025-09-04 09:21:48,049   INFO  cfg.HOOK.DisableAugmentationHook.NUM_LAST_EPOCHS: 5
2025-09-04 09:21:48,049   INFO  cfg.TAG: sparse_former_base
2025-09-04 09:21:48,049   INFO  cfg.EXP_GROUP_PATH: sparse_models
2025-09-04 09:21:48,049   INFO  cfg.OUTPUT_DIR: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_shuflle_.0diff_dense_hm_.3lr_.3wd
2025-09-04 09:21:48,062   INFO  ----------- Create dataloader & network & optimizer -----------
2025-09-04 09:21:53,527   INFO  Database filter by min points car: 339949 => 294532
2025-09-04 09:21:53,538   INFO  Database filter by min points truck: 65262 => 60344
2025-09-04 09:21:53,540   INFO  Database filter by min points construction_vehicle: 11050 => 10589
2025-09-04 09:21:53,541   INFO  Database filter by min points bus: 12286 => 11619
2025-09-04 09:21:53,544   INFO  Database filter by min points trailer: 19202 => 17934
2025-09-04 09:21:53,556   INFO  Database filter by min points barrier: 107507 => 101993
2025-09-04 09:21:53,558   INFO  Database filter by min points motorcycle: 8846 => 8055
2025-09-04 09:21:53,559   INFO  Database filter by min points bicycle: 8185 => 7531
2025-09-04 09:21:53,576   INFO  Database filter by min points pedestrian: 161928 => 148520
2025-09-04 09:21:53,585   INFO  Database filter by min points traffic_cone: 62964 => 55504
2025-09-04 09:21:53,586   INFO  Loading GT database to shared memory
eflops102:23:23 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:23:23 [0] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:23:23 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:23:23 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:23:23 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.18.5+cuda11.8
eflops102:24:24 [1] NCCL INFO cudaDriverVersion 12050
eflops102:24:24 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:27:27 [4] NCCL INFO cudaDriverVersion 12050
eflops102:29:29 [6] NCCL INFO cudaDriverVersion 12050
eflops102:25:25 [2] NCCL INFO cudaDriverVersion 12050
eflops102:27:27 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:25:25 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:29:29 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:28:28 [5] NCCL INFO cudaDriverVersion 12050
eflops102:28:28 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:31:31 [7] NCCL INFO cudaDriverVersion 12050
eflops102:31:31 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:26:26 [3] NCCL INFO cudaDriverVersion 12050
eflops102:26:26 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:24:24 [1] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:29:29 [6] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:24:24 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:24:24 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:31:31 [7] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:26:26 [3] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:29:29 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:28:28 [5] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:29:29 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:27:27 [4] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:31:31 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:31:31 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:27:27 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:27:27 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:26:26 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:26:26 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:25:25 [2] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:28:28 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:28:28 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:25:25 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:25:25 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:23:99 [0] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:23:99 [0] NCCL INFO P2P plugin IBext
eflops102:23:99 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:23:99 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:23:99 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:23:99 [0] NCCL INFO NET/IB : No device found.
eflops102:23:99 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:23:99 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:23:99 [0] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:23:99 [0] NCCL INFO Using network Socket
eflops102:24:100 [1] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:24:100 [1] NCCL INFO P2P plugin IBext
eflops102:24:100 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:24:100 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:24:100 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:24:100 [1] NCCL INFO NET/IB : No device found.
eflops102:24:100 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:24:100 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:24:100 [1] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:24:100 [1] NCCL INFO Using network Socket
eflops102:26:102 [3] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:26:102 [3] NCCL INFO P2P plugin IBext
eflops102:26:102 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:26:102 [3] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:26:102 [3] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:26:102 [3] NCCL INFO NET/IB : No device found.
eflops102:26:102 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:26:102 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:26:102 [3] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:26:102 [3] NCCL INFO Using network Socket
eflops102:25:105 [2] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:25:105 [2] NCCL INFO P2P plugin IBext
eflops102:25:105 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:28:103 [5] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:28:103 [5] NCCL INFO P2P plugin IBext
eflops102:28:103 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:25:105 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:25:105 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:25:105 [2] NCCL INFO NET/IB : No device found.
eflops102:25:105 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:25:105 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:25:105 [2] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:25:105 [2] NCCL INFO Using network Socket

eflops102:28:103 [5] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:28:103 [5] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:28:103 [5] NCCL INFO NET/IB : No device found.
eflops102:28:103 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:28:103 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:28:103 [5] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:28:103 [5] NCCL INFO Using network Socket
eflops102:29:104 [6] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:29:104 [6] NCCL INFO P2P plugin IBext
eflops102:29:104 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:29:104 [6] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:29:104 [6] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:29:104 [6] NCCL INFO NET/IB : No device found.
eflops102:29:104 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:29:104 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:29:104 [6] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:29:104 [6] NCCL INFO Using network Socket
eflops102:31:106 [7] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:31:106 [7] NCCL INFO P2P plugin IBext
eflops102:31:106 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:27:101 [4] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:27:101 [4] NCCL INFO P2P plugin IBext
eflops102:27:101 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:31:106 [7] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:31:106 [7] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:31:106 [7] NCCL INFO NET/IB : No device found.
eflops102:31:106 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:31:106 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:27:101 [4] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:27:101 [4] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:27:101 [4] NCCL INFO NET/IB : No device found.
eflops102:27:101 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:27:101 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:31:106 [7] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:31:106 [7] NCCL INFO Using network Socket
eflops102:27:101 [4] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:27:101 [4] NCCL INFO Using network Socket
eflops102:26:102 [3] NCCL INFO comm 0x11c6e5c0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 57000 commId 0x5a3b90a45d7042a2 - Init START
eflops102:28:103 [5] NCCL INFO comm 0x121c7340 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 9d000 commId 0x5a3b90a45d7042a2 - Init START
eflops102:25:105 [2] NCCL INFO comm 0x10787a50 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 53000 commId 0x5a3b90a45d7042a2 - Init START
eflops102:24:100 [1] NCCL INFO comm 0x117c2650 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 50000 commId 0x5a3b90a45d7042a2 - Init START
eflops102:23:99 [0] NCCL INFO comm 0x12427050 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 4f000 commId 0x5a3b90a45d7042a2 - Init START
eflops102:27:101 [4] NCCL INFO comm 0x10de61f0 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9c000 commId 0x5a3b90a45d7042a2 - Init START
eflops102:31:106 [7] NCCL INFO comm 0x1190d5d0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a4000 commId 0x5a3b90a45d7042a2 - Init START
eflops102:29:104 [6] NCCL INFO comm 0x10c14240 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a0000 commId 0x5a3b90a45d7042a2 - Init START
eflops102:28:103 [5] NCCL INFO Setting affinity for GPU 5 to ffff,fff00000,00ffffff,f0000000
eflops102:26:102 [3] NCCL INFO Setting affinity for GPU 3 to 0fffff,ff000000,0fffffff
eflops102:23:99 [0] NCCL INFO Setting affinity for GPU 0 to 0fffff,ff000000,0fffffff
eflops102:27:101 [4] NCCL INFO Setting affinity for GPU 4 to ffff,fff00000,00ffffff,f0000000
eflops102:31:106 [7] NCCL INFO Setting affinity for GPU 7 to ffff,fff00000,00ffffff,f0000000
eflops102:29:104 [6] NCCL INFO Setting affinity for GPU 6 to ffff,fff00000,00ffffff,f0000000
eflops102:24:100 [1] NCCL INFO Setting affinity for GPU 1 to 0fffff,ff000000,0fffffff
eflops102:25:105 [2] NCCL INFO Setting affinity for GPU 2 to 0fffff,ff000000,0fffffff
eflops102:28:103 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
eflops102:29:104 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5
eflops102:25:105 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
eflops102:23:99 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
eflops102:26:102 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
eflops102:31:106 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
eflops102:24:100 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
eflops102:28:103 [5] NCCL INFO P2P Chunksize set to 131072
eflops102:29:104 [6] NCCL INFO P2P Chunksize set to 131072
eflops102:23:99 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
eflops102:25:105 [2] NCCL INFO P2P Chunksize set to 131072
eflops102:26:102 [3] NCCL INFO P2P Chunksize set to 131072
eflops102:27:101 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
eflops102:31:106 [7] NCCL INFO P2P Chunksize set to 131072
eflops102:24:100 [1] NCCL INFO P2P Chunksize set to 131072
eflops102:23:99 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
eflops102:27:101 [4] NCCL INFO P2P Chunksize set to 131072
eflops102:23:99 [0] NCCL INFO P2P Chunksize set to 131072
eflops102:24:100 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
eflops102:29:104 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC
eflops102:28:103 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC
eflops102:25:105 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
eflops102:31:106 [7] NCCL INFO Channel 00 : 7[7] -> 0[0] via SHM/direct/direct
eflops102:24:100 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
eflops102:31:106 [7] NCCL INFO Channel 01 : 7[7] -> 0[0] via SHM/direct/direct
eflops102:29:104 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC
eflops102:25:105 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
eflops102:28:103 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC
eflops102:26:102 [3] NCCL INFO Channel 00 : 3[3] -> 4[4] via SHM/direct/direct
eflops102:23:99 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
eflops102:27:101 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
eflops102:26:102 [3] NCCL INFO Channel 01 : 3[3] -> 4[4] via SHM/direct/direct
eflops102:23:99 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
eflops102:27:101 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
eflops102:29:104 [6] NCCL INFO Connected all rings
eflops102:25:105 [2] NCCL INFO Connected all rings
eflops102:29:104 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC
eflops102:29:104 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC
eflops102:23:99 [0] NCCL INFO Connected all rings
eflops102:24:100 [1] NCCL INFO Connected all rings
eflops102:25:105 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
eflops102:28:103 [5] NCCL INFO Connected all rings
eflops102:31:106 [7] NCCL INFO Connected all rings
eflops102:31:106 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC
eflops102:26:102 [3] NCCL INFO Connected all rings
eflops102:25:105 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
eflops102:31:106 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC
eflops102:27:101 [4] NCCL INFO Connected all rings
eflops102:28:103 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
eflops102:28:103 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
eflops102:31:106 [7] NCCL INFO Connected all trees
eflops102:31:106 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:31:106 [7] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:24:100 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
eflops102:24:100 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
eflops102:27:101 [4] NCCL INFO Channel 00 : 4[4] -> 3[3] via SHM/direct/direct
eflops102:29:104 [6] NCCL INFO Connected all trees
eflops102:29:104 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:29:104 [6] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:27:101 [4] NCCL INFO Channel 01 : 4[4] -> 3[3] via SHM/direct/direct
eflops102:23:99 [0] NCCL INFO Connected all trees
eflops102:23:99 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:23:99 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:24:100 [1] NCCL INFO Connected all trees
eflops102:24:100 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:24:100 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:28:103 [5] NCCL INFO Connected all trees
eflops102:28:103 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:28:103 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:26:102 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
eflops102:26:102 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
eflops102:25:105 [2] NCCL INFO Connected all trees
eflops102:25:105 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:25:105 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:26:102 [3] NCCL INFO Connected all trees
eflops102:26:102 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:26:102 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:27:101 [4] NCCL INFO Connected all trees
eflops102:27:101 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:27:101 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:28:103 [5] NCCL INFO comm 0x121c7340 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 9d000 commId 0x5a3b90a45d7042a2 - Init COMPLETE
eflops102:25:105 [2] NCCL INFO comm 0x10787a50 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 53000 commId 0x5a3b90a45d7042a2 - Init COMPLETE
eflops102:26:102 [3] NCCL INFO comm 0x11c6e5c0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 57000 commId 0x5a3b90a45d7042a2 - Init COMPLETE
eflops102:29:104 [6] NCCL INFO comm 0x10c14240 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a0000 commId 0x5a3b90a45d7042a2 - Init COMPLETE
eflops102:27:101 [4] NCCL INFO comm 0x10de61f0 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9c000 commId 0x5a3b90a45d7042a2 - Init COMPLETE
eflops102:23:99 [0] NCCL INFO comm 0x12427050 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 4f000 commId 0x5a3b90a45d7042a2 - Init COMPLETE
eflops102:31:106 [7] NCCL INFO comm 0x1190d5d0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a4000 commId 0x5a3b90a45d7042a2 - Init COMPLETE
eflops102:24:100 [1] NCCL INFO comm 0x117c2650 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 50000 commId 0x5a3b90a45d7042a2 - Init COMPLETE
2025-09-04 09:22:06,580   INFO  GT database has been saved to shared memory
2025-09-04 09:22:06,873   INFO  Loading NuScenes dataset
2025-09-04 09:22:08,769   INFO  Total samples for NuScenes dataset: 28130
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2025-09-04 09:22:09,151   INFO  ----------- Model TransFusion created, param count: 24443458 -----------
epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]2025-09-04 09:22:09,151   INFO  DistributedDataParallel(
  (module): TransFusion(
    (vfe): DynamicVoxelVFE(
      (pfn_layers): ModuleList(
        (0): PFNLayerV2(
          (linear): Linear(in_features=11, out_features=32, bias=False)
          (norm): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
        )
        (1): PFNLayerV2(
          (linear): Linear(in_features=64, out_features=64, bias=False)
          (norm): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
        )
      )
    )
    (backbone_3d): FSHNet_nusc(
      (cpe): SparseSequential(
        (0): SparseSequential(
          (0): SubMConv3d(64, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (2): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
      )
      (down1): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (down2): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (down3): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (stage): SPEncoder(
        (blocks): ModuleList(
          (0): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): ZOrderSerialization()
            )
          )
          (1): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): HilbertSerialization()
            )
          )
          (2): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): FlattenWindowsSerialization()
            )
          )
        )
        (embeddings): ModuleList(
          (0-2): 3 x MSSubConvEmbeddingLearned(
            (stem): ModuleList(
              (0-2): 3 x SparseBasicBlock(
                (conv1): SubMConv3d(3, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
              )
            )
          )
        )
      )
      (app): AttnPillarPool(
        (query_func): SparseMaxPool3d(kernel_size=[10, 1, 1], stride=[10, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], algo=ConvAlgo.MaskImplicitGemm)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (pos_embedding): Embedding(10, 128)
      )
      (cls_conv): SparseSequential(
        (0): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): SubMConv2d(128, 3, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
      )
      (afd_layers): ModuleList(
        (0-3): 4 x SEDLayer(
          (encoder): ModuleList(
            (0): SparseSequential(
              (0): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
              (1): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
            )
            (1-2): 2 x SparseSequential(
              (0): SparseSequential(
                (0): SparseConv2d(128, 128, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
            )
          )
          (decoder): ModuleList(
            (0-1): 2 x SparseSequential(
              (0): SparseInverseConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
              (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (decoder_norm): ModuleList(
            (0-1): 2 x SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (shared_conv): SparseSequential(
        (0): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (map_to_bev_module): None
    (pfe): None
    (backbone_2d): None
    (dense_head): TransFusionHead(
      (loss_cls): SigmoidFocalClassificationLoss()
      (loss_bbox): L1Loss()
      (loss_heatmap): GaussianFocalLoss()
      (shared_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (heatmap_head): Sequential(
        (0): BasicBlock2D(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): Conv2d(128, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (class_encoding): Conv1d(10, 128, kernel_size=(1,), stride=(1,))
      (decoder): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_posembed): PositionEmbeddingLearned(
          (position_embedding_head): Sequential(
            (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
        )
        (cross_posembed): PositionEmbeddingLearned(
          (position_embedding_head): Sequential(
            (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
        )
      )
      (prediction_head): SeparateHead_Transfusion(
        (center): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (height): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
        (dim): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 3, kernel_size=(1,), stride=(1,))
        )
        (rot): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (vel): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (iou): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
        (heatmap): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 10, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (point_head): None
    (roi_head): None
  )
)
epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]2025-09-04 09:22:09,157   INFO  **********************Start training sparse_models/sparse_former_base(default)**********************
epochs:   0%|          | 0/36 [00:00<?, ?it/s]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
2025-09-04 09:22:31,978   INFO  Train:    1/36 (  3%) [   0/1759 (  0%)]  Loss: 1751. (1.75e+03)  LR: 3.000e-04  Grad: 10.0000  max=0.2915(module.dense_head.heatmap_head.1.bias)  min: -0.1262(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1709.2856, loss_cls=24.4511, loss_bbox=8.5159, matched_ious=0.0080, loss_iou=0.3326, loss_iou_reg=0.4810, d_time=1.87(1.87), f_time=18.63(18.63), b_time=20.50(20.50)  Time cost: 00:20/9:47:42 [00:22/352:37:46]  Acc_iter 1           Data time: 1.87(1.87)  Forward time: 18.63(18.63)  Batch time: 20.50(20.50)
