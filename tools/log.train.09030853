/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
2025-09-03 08:53:35,451   INFO  **********************Start logging**********************
2025-09-03 08:53:35,451   INFO  CUDA_VISIBLE_DEVICES=ALL
2025-09-03 08:53:35,451   INFO  Training in distributed mode : total_batch_size: 16
2025-09-03 08:53:35,451   INFO  cfg_file         cfgs/sparse_models/sparse_former_base.yaml
2025-09-03 08:53:35,451   INFO  batch_size       2
2025-09-03 08:53:35,451   INFO  epochs           36
2025-09-03 08:53:35,451   INFO  workers          12
2025-09-03 08:53:35,451   INFO  extra_tag        default
2025-09-03 08:53:35,451   INFO  ckpt             None
2025-09-03 08:53:35,452   INFO  pretrained_model None
2025-09-03 08:53:35,452   INFO  launcher         pytorch
2025-09-03 08:53:35,452   INFO  tcp_port         18888
2025-09-03 08:53:35,452   INFO  sync_bn          True
2025-09-03 08:53:35,452   INFO  fix_random_seed  False
2025-09-03 08:53:35,452   INFO  ckpt_save_interval 1
2025-09-03 08:53:35,452   INFO  local_rank       0
2025-09-03 08:53:35,452   INFO  max_ckpt_save_num 30
2025-09-03 08:53:35,452   INFO  merge_all_iters_to_one_epoch False
2025-09-03 08:53:35,452   INFO  set_cfgs         None
2025-09-03 08:53:35,452   INFO  max_waiting_mins 0
2025-09-03 08:53:35,452   INFO  start_epoch      0
2025-09-03 08:53:35,452   INFO  num_epochs_to_eval 0
2025-09-03 08:53:35,452   INFO  save_to_file     False
2025-09-03 08:53:35,452   INFO  use_tqdm_to_record False
2025-09-03 08:53:35,452   INFO  logger_iter_interval 50
2025-09-03 08:53:35,452   INFO  ckpt_save_time_interval 300
2025-09-03 08:53:35,452   INFO  wo_gpu_stat      True
2025-09-03 08:53:35,452   INFO  use_amp          False
2025-09-03 08:53:35,452   INFO  eval_map         False
2025-09-03 08:53:35,452   INFO  dataset          nuscenes
2025-09-03 08:53:35,452   INFO  root_dir         /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2
2025-09-03 08:53:35,452   INFO  output_dir       /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_.0diff_dense_hm_.1lr_.1wd
2025-09-03 08:53:35,453   INFO  cfg.ROOT_DIR: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2
2025-09-03 08:53:35,453   INFO  cfg.LOCAL_RANK: 0
2025-09-03 08:53:35,453   INFO  cfg.CLASS_NAMES: ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']
2025-09-03 08:53:35,453   INFO  ----------- DATA_CONFIG -----------
2025-09-03 08:53:35,453   INFO  cfg.DATA_CONFIG.DATASET: NuScenesDataset
2025-09-03 08:53:35,453   INFO  cfg.DATA_CONFIG.DATA_PATH: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/data/nuscenes
2025-09-03 08:53:35,453   INFO  cfg.DATA_CONFIG.VERSION: v1.0-trainval
2025-09-03 08:53:35,453   INFO  cfg.DATA_CONFIG.MAX_SWEEPS: 10
2025-09-03 08:53:35,453   INFO  cfg.DATA_CONFIG.PRED_VELOCITY: True
2025-09-03 08:53:35,453   INFO  cfg.DATA_CONFIG.SET_NAN_VELOCITY_TO_ZEROS: True
2025-09-03 08:53:35,453   INFO  cfg.DATA_CONFIG.FILTER_MIN_POINTS_IN_GT: 1
2025-09-03 08:53:35,453   INFO  ----------- DATA_SPLIT -----------
2025-09-03 08:53:35,453   INFO  cfg.DATA_CONFIG.DATA_SPLIT.train: train
2025-09-03 08:53:35,453   INFO  cfg.DATA_CONFIG.DATA_SPLIT.test: val
2025-09-03 08:53:35,453   INFO  ----------- INFO_PATH -----------
2025-09-03 08:53:35,453   INFO  cfg.DATA_CONFIG.INFO_PATH.train: ['nuscenes_infos_10sweeps_train.pkl']
2025-09-03 08:53:35,453   INFO  cfg.DATA_CONFIG.INFO_PATH.test: ['nuscenes_infos_10sweeps_val.pkl']
2025-09-03 08:53:35,453   INFO  cfg.DATA_CONFIG.POINT_CLOUD_RANGE: [-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]
2025-09-03 08:53:35,453   INFO  cfg.DATA_CONFIG.BALANCED_RESAMPLING: False
2025-09-03 08:53:35,453   INFO  ----------- DATA_AUGMENTOR -----------
2025-09-03 08:53:35,453   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.DISABLE_AUG_LIST: ['placeholder']
2025-09-03 08:53:35,454   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.AUG_CONFIG_LIST: [{'NAME': 'gt_sampling', 'DB_INFO_PATH': ['nuscenes_dbinfos_10sweeps_withvelo.pkl'], 'USE_SHARED_MEMORY': True, 'DB_DATA_PATH': ['nuscenes_10sweeps_withvelo_lidar.npy'], 'PREPARE': {'filter_by_min_points': ['car:5', 'truck:5', 'construction_vehicle:5', 'bus:5', 'trailer:5', 'barrier:5', 'motorcycle:5', 'bicycle:5', 'pedestrian:5', 'traffic_cone:5']}, 'SAMPLE_GROUPS': ['car:2', 'truck:3', 'construction_vehicle:7', 'bus:4', 'trailer:6', 'barrier:2', 'motorcycle:6', 'bicycle:6', 'pedestrian:2', 'traffic_cone:2'], 'NUM_POINT_FEATURES': 5, 'DATABASE_WITH_FAKELIDAR': False, 'REMOVE_EXTRA_WIDTH': [0.0, 0.0, 0.0], 'LIMIT_WHOLE_SCENE': True}, {'NAME': 'random_world_flip', 'ALONG_AXIS_LIST': ['x', 'y']}, {'NAME': 'random_world_rotation', 'WORLD_ROT_ANGLE': [-0.78539816, 0.78539816]}, {'NAME': 'random_world_scaling', 'WORLD_SCALE_RANGE': [0.9, 1.1]}, {'NAME': 'random_world_translation', 'NOISE_TRANSLATE_STD': [0.5, 0.5, 0.5]}]
2025-09-03 08:53:35,455   INFO  ----------- POINT_FEATURE_ENCODING -----------
2025-09-03 08:53:35,455   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.encoding_type: absolute_coordinates_encoding
2025-09-03 08:53:35,455   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.used_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-09-03 08:53:35,455   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.src_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-09-03 08:53:35,455   INFO  cfg.DATA_CONFIG.DATA_PROCESSOR: [{'NAME': 'mask_points_and_boxes_outside_range', 'REMOVE_OUTSIDE_BOXES': True}, {'NAME': 'shuffle_points', 'SHUFFLE_ENABLED': {'train': True, 'test': True}}, {'NAME': 'transform_points_to_voxels_placeholder', 'VOXEL_SIZE': [0.3, 0.3, 0.2]}]
2025-09-03 08:53:35,455   INFO  cfg.DATA_CONFIG._BASE_CONFIG_: cfgs/dataset_configs/nuscenes_dataset.yaml
2025-09-03 08:53:35,455   INFO  ----------- SAMPLED_INTERVAL -----------
2025-09-03 08:53:35,455   INFO  cfg.DATA_CONFIG.SAMPLED_INTERVAL.train: 1
2025-09-03 08:53:35,455   INFO  cfg.DATA_CONFIG.SAMPLED_INTERVAL.test: 1
2025-09-03 08:53:35,455   INFO  ----------- MODEL -----------
2025-09-03 08:53:35,455   INFO  cfg.MODEL.NAME: TransFusion
2025-09-03 08:53:35,455   INFO  ----------- VFE -----------
2025-09-03 08:53:35,456   INFO  cfg.MODEL.VFE.NAME: DynamicVoxelVFE
2025-09-03 08:53:35,456   INFO  cfg.MODEL.VFE.WITH_DISTANCE: False
2025-09-03 08:53:35,456   INFO  cfg.MODEL.VFE.USE_ABSLOTE_XYZ: True
2025-09-03 08:53:35,456   INFO  cfg.MODEL.VFE.USE_NORM: True
2025-09-03 08:53:35,456   INFO  cfg.MODEL.VFE.NUM_FILTERS: [64, 64]
2025-09-03 08:53:35,456   INFO  ----------- BACKBONE_3D -----------
2025-09-03 08:53:35,456   INFO  cfg.MODEL.BACKBONE_3D.NAME: FSHNet_nusc
2025-09-03 08:53:35,456   INFO  cfg.MODEL.BACKBONE_3D.FEATURE_DIM: 128
2025-09-03 08:53:35,456   INFO  ----------- SPENCODER -----------
2025-09-03 08:53:35,456   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.DIM: 3
2025-09-03 08:53:35,456   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.NUM_LAYERS: 3
2025-09-03 08:53:35,456   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.ORDERS: [['z', 'z-trans'], ['hilbert', 'hilbert-trans'], ['x', 'y']]
2025-09-03 08:53:35,456   INFO  ----------- SMSA -----------
2025-09-03 08:53:35,456   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.EMBED_DIM: 128
2025-09-03 08:53:35,456   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DEPTH: 9
2025-09-03 08:53:35,456   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.NUM_LEVELS: 3
2025-09-03 08:53:35,457   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.WINDOW_SHAPE: [9, 9, 5]
2025-09-03 08:53:35,457   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DROP_PATH: 0.2
2025-09-03 08:53:35,457   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.SPATIAL_ENHANCE: True
2025-09-03 08:53:35,457   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.FUSED_ADD_NORM: True
2025-09-03 08:53:35,457   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.RMS_NORM: True
2025-09-03 08:53:35,457   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.NORM_EPSILON: 1e-05
2025-09-03 08:53:35,457   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.RESIDUAL_IN_FP32: True
2025-09-03 08:53:35,457   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DIFF_COEF: 0.0
2025-09-03 08:53:35,457   INFO  cfg.MODEL.BACKBONE_3D.SPENCODER.SMSA.DIFF_KERNEL: [7, 5, 3]
2025-09-03 08:53:35,457   INFO  cfg.MODEL.BACKBONE_3D.AFD_FEATURE_DIM: 128
2025-09-03 08:53:35,457   INFO  cfg.MODEL.BACKBONE_3D.AFD_NUM_LAYERS: 4
2025-09-03 08:53:35,457   INFO  cfg.MODEL.BACKBONE_3D.AFD_NUM_SBB: [2, 1, 1]
2025-09-03 08:53:35,457   INFO  cfg.MODEL.BACKBONE_3D.AFD_DOWN_STRIDE: [1, 2, 2]
2025-09-03 08:53:35,457   INFO  cfg.MODEL.BACKBONE_3D.AFD_DOWN_KERNEL_SIZE: [3, 3, 3]
2025-09-03 08:53:35,457   INFO  cfg.MODEL.BACKBONE_3D.AFD: True
2025-09-03 08:53:35,457   INFO  cfg.MODEL.BACKBONE_3D.FG_THRESHOLD: 0.2
2025-09-03 08:53:35,458   INFO  cfg.MODEL.BACKBONE_3D.FEATMAP_STRIDE: 2
2025-09-03 08:53:35,458   INFO  cfg.MODEL.BACKBONE_3D.DETACH_FEATURE: True
2025-09-03 08:53:35,458   INFO  cfg.MODEL.BACKBONE_3D.GREOUP_POOLING_KERNEL_SIZE: [9, 15, 5, 5]
2025-09-03 08:53:35,458   INFO  cfg.MODEL.BACKBONE_3D.GROUP_CLASS_NAMES: [['car', 'truck', 'construction_vehicle'], ['bus', 'trailer'], ['barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']]
2025-09-03 08:53:35,458   INFO  ----------- DENSE_HEAD -----------
2025-09-03 08:53:35,458   INFO  cfg.MODEL.DENSE_HEAD.CLASS_AGNOSTIC: False
2025-09-03 08:53:35,458   INFO  cfg.MODEL.DENSE_HEAD.NAME: TransFusionHead
2025-09-03 08:53:35,458   INFO  cfg.MODEL.DENSE_HEAD.QUERY_RADIUS: 20
2025-09-03 08:53:35,458   INFO  cfg.MODEL.DENSE_HEAD.QUERY_LOCAL: True
2025-09-03 08:53:35,458   INFO  cfg.MODEL.DENSE_HEAD.USE_BIAS_BEFORE_NORM: True
2025-09-03 08:53:35,458   INFO  cfg.MODEL.DENSE_HEAD.INPUT_FEATURES: 128
2025-09-03 08:53:35,458   INFO  cfg.MODEL.DENSE_HEAD.NUM_PROPOSALS: 600
2025-09-03 08:53:35,458   INFO  cfg.MODEL.DENSE_HEAD.HIDDEN_CHANNEL: 128
2025-09-03 08:53:35,458   INFO  cfg.MODEL.DENSE_HEAD.NUM_CLASSES: 10
2025-09-03 08:53:35,458   INFO  cfg.MODEL.DENSE_HEAD.NUM_HEADS: 8
2025-09-03 08:53:35,458   INFO  cfg.MODEL.DENSE_HEAD.NMS_KERNEL_SIZE: 3
2025-09-03 08:53:35,459   INFO  cfg.MODEL.DENSE_HEAD.FFN_CHANNEL: 256
2025-09-03 08:53:35,459   INFO  cfg.MODEL.DENSE_HEAD.DROPOUT: 0.1
2025-09-03 08:53:35,459   INFO  cfg.MODEL.DENSE_HEAD.BN_MOMENTUM: 0.1
2025-09-03 08:53:35,459   INFO  cfg.MODEL.DENSE_HEAD.ACTIVATION: relu
2025-09-03 08:53:35,459   INFO  cfg.MODEL.DENSE_HEAD.NUM_HM_CONV: 2
2025-09-03 08:53:35,459   INFO  ----------- SEPARATE_HEAD_CFG -----------
2025-09-03 08:53:35,459   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_ORDER: ['center', 'height', 'dim', 'rot', 'vel', 'iou']
2025-09-03 08:53:35,459   INFO  ----------- HEAD_DICT -----------
2025-09-03 08:53:35,459   INFO  ----------- center -----------
2025-09-03 08:53:35,459   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.out_channels: 2
2025-09-03 08:53:35,459   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.num_conv: 2
2025-09-03 08:53:35,459   INFO  ----------- height -----------
2025-09-03 08:53:35,459   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.height.out_channels: 1
2025-09-03 08:53:35,459   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.height.num_conv: 2
2025-09-03 08:53:35,459   INFO  ----------- dim -----------
2025-09-03 08:53:35,459   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.out_channels: 3
2025-09-03 08:53:35,460   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.num_conv: 2
2025-09-03 08:53:35,460   INFO  ----------- rot -----------
2025-09-03 08:53:35,460   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.out_channels: 2
2025-09-03 08:53:35,460   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.num_conv: 2
2025-09-03 08:53:35,460   INFO  ----------- vel -----------
2025-09-03 08:53:35,460   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.out_channels: 2
2025-09-03 08:53:35,460   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.num_conv: 2
2025-09-03 08:53:35,460   INFO  ----------- iou -----------
2025-09-03 08:53:35,460   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.iou.out_channels: 1
2025-09-03 08:53:35,460   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.iou.num_conv: 2
2025-09-03 08:53:35,460   INFO  ----------- TARGET_ASSIGNER_CONFIG -----------
2025-09-03 08:53:35,460   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.FEATURE_MAP_STRIDE: 2
2025-09-03 08:53:35,460   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.DATASET: nuScenes
2025-09-03 08:53:35,460   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.GAUSSIAN_OVERLAP: 0.1
2025-09-03 08:53:35,460   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.MIN_RADIUS: 2
2025-09-03 08:53:35,460   INFO  ----------- HUNGARIAN_ASSIGNER -----------
2025-09-03 08:53:35,461   INFO  ----------- cls_cost -----------
2025-09-03 08:53:35,461   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.gamma: 2.0
2025-09-03 08:53:35,461   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.alpha: 0.25
2025-09-03 08:53:35,461   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.cls_cost.weight: 0.15
2025-09-03 08:53:35,461   INFO  ----------- reg_cost -----------
2025-09-03 08:53:35,461   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.reg_cost.weight: 0.25
2025-09-03 08:53:35,461   INFO  ----------- iou_cost -----------
2025-09-03 08:53:35,461   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.HUNGARIAN_ASSIGNER.iou_cost.weight: 0.25
2025-09-03 08:53:35,461   INFO  ----------- LOSS_CONFIG -----------
2025-09-03 08:53:35,461   INFO  ----------- LOSS_WEIGHTS -----------
2025-09-03 08:53:35,461   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.cls_weight: 1.0
2025-09-03 08:53:35,461   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.bbox_weight: 0.25
2025-09-03 08:53:35,461   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.hm_weight: 1.0
2025-09-03 08:53:35,461   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.iou_weight: 0.5
2025-09-03 08:53:35,461   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.iou_reg_weight: 0.5
2025-09-03 08:53:35,461   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.code_weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2]
2025-09-03 08:53:35,462   INFO  ----------- LOSS_CLS -----------
2025-09-03 08:53:35,462   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.use_sigmoid: True
2025-09-03 08:53:35,462   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.gamma: 2.0
2025-09-03 08:53:35,462   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_CLS.alpha: 0.25
2025-09-03 08:53:35,462   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_IOU: True
2025-09-03 08:53:35,462   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_IOU_REG: True
2025-09-03 08:53:35,462   INFO  ----------- POST_PROCESSING -----------
2025-09-03 08:53:35,462   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.SCORE_THRESH: 0.0
2025-09-03 08:53:35,462   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.POST_CENTER_RANGE: [-61.2, -61.2, -10.0, 61.2, 61.2, 10.0]
2025-09-03 08:53:35,462   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.USE_IOU_TO_RECTIFY_SCORE: True
2025-09-03 08:53:35,462   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.IOU_RECTIFIER: [0.5]
2025-09-03 08:53:35,462   INFO  ----------- NMS_CONFIG -----------
2025-09-03 08:53:35,462   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_TYPE: nms_gpu
2025-09-03 08:53:35,462   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_THRESH: 0.2
2025-09-03 08:53:35,462   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_PRE_MAXSIZE: 1000
2025-09-03 08:53:35,463   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.NMS_POST_MAXSIZE: 100
2025-09-03 08:53:35,463   INFO  cfg.MODEL.DENSE_HEAD.NMS_CONFIG.SCORE_THRES: 0.0
2025-09-03 08:53:35,463   INFO  ----------- POST_PROCESSING -----------
2025-09-03 08:53:35,463   INFO  cfg.MODEL.POST_PROCESSING.RECALL_THRESH_LIST: [0.3, 0.5, 0.7]
2025-09-03 08:53:35,463   INFO  cfg.MODEL.POST_PROCESSING.SCORE_THRESH: 0.1
2025-09-03 08:53:35,463   INFO  cfg.MODEL.POST_PROCESSING.OUTPUT_RAW_SCORE: False
2025-09-03 08:53:35,463   INFO  cfg.MODEL.POST_PROCESSING.EVAL_METRIC: kitti
2025-09-03 08:53:35,463   INFO  ----------- NMS_CONFIG -----------
2025-09-03 08:53:35,463   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.MULTI_CLASSES_NMS: True
2025-09-03 08:53:35,463   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_TYPE: nms_gpu
2025-09-03 08:53:35,463   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_THRESH: 0.2
2025-09-03 08:53:35,463   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_PRE_MAXSIZE: 1000
2025-09-03 08:53:35,463   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_POST_MAXSIZE: 83
2025-09-03 08:53:35,463   INFO  ----------- OPTIMIZATION -----------
2025-09-03 08:53:35,463   INFO  cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU: 2
2025-09-03 08:53:35,463   INFO  cfg.OPTIMIZATION.NUM_EPOCHS: 36
2025-09-03 08:53:35,464   INFO  cfg.OPTIMIZATION.OPTIMIZER: adam_onecycle
2025-09-03 08:53:35,464   INFO  cfg.OPTIMIZATION.LR: 0.003
2025-09-03 08:53:35,464   INFO  cfg.OPTIMIZATION.WEIGHT_DECAY: 0.03
2025-09-03 08:53:35,464   INFO  cfg.OPTIMIZATION.MOMENTUM: 0.9
2025-09-03 08:53:35,464   INFO  cfg.OPTIMIZATION.MOMS: [0.95, 0.85]
2025-09-03 08:53:35,464   INFO  cfg.OPTIMIZATION.PCT_START: 0.4
2025-09-03 08:53:35,464   INFO  cfg.OPTIMIZATION.DIV_FACTOR: 10
2025-09-03 08:53:35,464   INFO  cfg.OPTIMIZATION.DECAY_STEP_LIST: [35, 45]
2025-09-03 08:53:35,464   INFO  cfg.OPTIMIZATION.LR_DECAY: 0.1
2025-09-03 08:53:35,464   INFO  cfg.OPTIMIZATION.LR_CLIP: 1e-07
2025-09-03 08:53:35,464   INFO  cfg.OPTIMIZATION.LR_WARMUP: False
2025-09-03 08:53:35,464   INFO  cfg.OPTIMIZATION.WARMUP_EPOCH: 1
2025-09-03 08:53:35,464   INFO  cfg.OPTIMIZATION.GRAD_NORM_CLIP: 10
2025-09-03 08:53:35,464   INFO  cfg.OPTIMIZATION.LOSS_SCALE_FP16: 32
2025-09-03 08:53:35,464   INFO  ----------- HOOK -----------
2025-09-03 08:53:35,464   INFO  ----------- DisableAugmentationHook -----------
2025-09-03 08:53:35,465   INFO  cfg.HOOK.DisableAugmentationHook.DISABLE_AUG_LIST: ['gt_sampling']
2025-09-03 08:53:35,465   INFO  cfg.HOOK.DisableAugmentationHook.NUM_LAST_EPOCHS: 5
2025-09-03 08:53:35,465   INFO  cfg.TAG: sparse_former_base
2025-09-03 08:53:35,465   INFO  cfg.EXP_GROUP_PATH: sparse_models
2025-09-03 08:53:35,465   INFO  cfg.OUTPUT_DIR: /mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/workdir/fshnet/encoder/fshnet_SPEncoder_3layer_1lc_zhx_.0diff_dense_hm_.1lr_.1wd
2025-09-03 08:53:35,477   INFO  ----------- Create dataloader & network & optimizer -----------
2025-09-03 08:53:40,990   INFO  Database filter by min points car: 339949 => 294532
2025-09-03 08:53:41,002   INFO  Database filter by min points truck: 65262 => 60344
2025-09-03 08:53:41,004   INFO  Database filter by min points construction_vehicle: 11050 => 10589
2025-09-03 08:53:41,005   INFO  Database filter by min points bus: 12286 => 11619
2025-09-03 08:53:41,008   INFO  Database filter by min points trailer: 19202 => 17934
2025-09-03 08:53:41,021   INFO  Database filter by min points barrier: 107507 => 101993
2025-09-03 08:53:41,023   INFO  Database filter by min points motorcycle: 8846 => 8055
2025-09-03 08:53:41,024   INFO  Database filter by min points bicycle: 8185 => 7531
2025-09-03 08:53:41,042   INFO  Database filter by min points pedestrian: 161928 => 148520
2025-09-03 08:53:41,051   INFO  Database filter by min points traffic_cone: 62964 => 55504
2025-09-03 08:53:41,052   INFO  Loading GT database to shared memory
eflops102:23:23 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:23:23 [0] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:23:23 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:23:23 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:23:23 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.18.5+cuda11.8
eflops102:29:29 [6] NCCL INFO cudaDriverVersion 12050
eflops102:28:28 [5] NCCL INFO cudaDriverVersion 12050
eflops102:30:30 [7] NCCL INFO cudaDriverVersion 12050
eflops102:29:29 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:28:28 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:30:30 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:24:24 [1] NCCL INFO cudaDriverVersion 12050
eflops102:26:26 [3] NCCL INFO cudaDriverVersion 12050
eflops102:24:24 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:26:26 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:27:27 [4] NCCL INFO cudaDriverVersion 12050
eflops102:25:25 [2] NCCL INFO cudaDriverVersion 12050
eflops102:27:27 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:25:25 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:28:28 [5] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:29:29 [6] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:30:30 [7] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:26:26 [3] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:29:29 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:29:29 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:28:28 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:28:28 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:25:25 [2] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:27:27 [4] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:30:30 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:30:30 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:24:24 [1] NCCL INFO Bootstrap : Using bond0:10.16.9.230<0>
eflops102:25:25 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:27:27 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:27:27 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:25:25 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:26:26 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:26:26 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:24:24 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
eflops102:24:24 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
eflops102:23:99 [0] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:23:99 [0] NCCL INFO P2P plugin IBext
eflops102:23:99 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:24:105 [1] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:24:105 [1] NCCL INFO P2P plugin IBext
eflops102:24:105 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:23:99 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:23:99 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:23:99 [0] NCCL INFO NET/IB : No device found.
eflops102:23:99 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:23:99 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:24:105 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:24:105 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:24:105 [1] NCCL INFO NET/IB : No device found.
eflops102:24:105 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:24:105 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:23:99 [0] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:23:99 [0] NCCL INFO Using network Socket
eflops102:24:105 [1] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:24:105 [1] NCCL INFO Using network Socket
eflops102:30:104 [7] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:30:104 [7] NCCL INFO P2P plugin IBext
eflops102:30:104 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:28:106 [5] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:28:106 [5] NCCL INFO P2P plugin IBext
eflops102:28:106 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:30:104 [7] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:30:104 [7] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:30:104 [7] NCCL INFO NET/IB : No device found.
eflops102:27:101 [4] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:27:101 [4] NCCL INFO P2P plugin IBext
eflops102:30:104 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:30:104 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:27:101 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:30:104 [7] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:30:104 [7] NCCL INFO Using network Socket

eflops102:28:106 [5] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:28:106 [5] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:28:106 [5] NCCL INFO NET/IB : No device found.
eflops102:28:106 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:28:106 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:27:101 [4] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:27:101 [4] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:27:101 [4] NCCL INFO NET/IB : No device found.
eflops102:27:101 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:27:101 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:28:106 [5] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:28:106 [5] NCCL INFO Using network Socket
eflops102:26:103 [3] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:26:103 [3] NCCL INFO P2P plugin IBext
eflops102:26:103 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:27:101 [4] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:27:101 [4] NCCL INFO Using network Socket
eflops102:25:102 [2] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:25:102 [2] NCCL INFO P2P plugin IBext
eflops102:25:102 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:26:103 [3] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:26:103 [3] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:26:103 [3] NCCL INFO NET/IB : No device found.
eflops102:26:103 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:26:103 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:26:103 [3] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:26:103 [3] NCCL INFO Using network Socket

eflops102:25:102 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:25:102 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:25:102 [2] NCCL INFO NET/IB : No device found.
eflops102:25:102 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:25:102 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:25:102 [2] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:25:102 [2] NCCL INFO Using network Socket
eflops102:29:100 [6] NCCL INFO Plugin Path : /usr/local/nccl-rdma-sharp-plugins/lib/libnccl-net.so
eflops102:29:100 [6] NCCL INFO P2P plugin IBext
eflops102:29:100 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond

eflops102:29:100 [6] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed

eflops102:29:100 [6] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_bond_0
eflops102:29:100 [6] NCCL INFO NET/IB : No device found.
eflops102:29:100 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
eflops102:29:100 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond
eflops102:29:100 [6] NCCL INFO NET/Socket : Using [0]bond0:10.16.9.230<0>
eflops102:29:100 [6] NCCL INFO Using network Socket
eflops102:24:105 [1] NCCL INFO comm 0x11b50670 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 50000 commId 0xccfe7c62eba2ebda - Init START
eflops102:23:99 [0] NCCL INFO comm 0x10c23a80 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 4f000 commId 0xccfe7c62eba2ebda - Init START
eflops102:30:104 [7] NCCL INFO comm 0x119179f0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a4000 commId 0xccfe7c62eba2ebda - Init START
eflops102:29:100 [6] NCCL INFO comm 0x11efc560 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a0000 commId 0xccfe7c62eba2ebda - Init START
eflops102:28:106 [5] NCCL INFO comm 0x115cef30 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 9d000 commId 0xccfe7c62eba2ebda - Init START
eflops102:27:101 [4] NCCL INFO comm 0x11e44d90 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9c000 commId 0xccfe7c62eba2ebda - Init START
eflops102:26:103 [3] NCCL INFO comm 0x118fc870 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 57000 commId 0xccfe7c62eba2ebda - Init START
eflops102:25:102 [2] NCCL INFO comm 0x121f21f0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 53000 commId 0xccfe7c62eba2ebda - Init START
eflops102:24:105 [1] NCCL INFO Setting affinity for GPU 1 to 0fffff,ff000000,0fffffff
eflops102:28:106 [5] NCCL INFO Setting affinity for GPU 5 to ffff,fff00000,00ffffff,f0000000
eflops102:29:100 [6] NCCL INFO Setting affinity for GPU 6 to ffff,fff00000,00ffffff,f0000000
eflops102:26:103 [3] NCCL INFO Setting affinity for GPU 3 to 0fffff,ff000000,0fffffff
eflops102:25:102 [2] NCCL INFO Setting affinity for GPU 2 to 0fffff,ff000000,0fffffff
eflops102:30:104 [7] NCCL INFO Setting affinity for GPU 7 to ffff,fff00000,00ffffff,f0000000
eflops102:23:99 [0] NCCL INFO Setting affinity for GPU 0 to 0fffff,ff000000,0fffffff
eflops102:27:101 [4] NCCL INFO Setting affinity for GPU 4 to ffff,fff00000,00ffffff,f0000000
eflops102:26:103 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
eflops102:26:103 [3] NCCL INFO P2P Chunksize set to 131072
eflops102:25:102 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
eflops102:25:102 [2] NCCL INFO P2P Chunksize set to 131072
eflops102:29:100 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5
eflops102:28:106 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
eflops102:24:105 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
eflops102:29:100 [6] NCCL INFO P2P Chunksize set to 131072
eflops102:28:106 [5] NCCL INFO P2P Chunksize set to 131072
eflops102:24:105 [1] NCCL INFO P2P Chunksize set to 131072
eflops102:23:99 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
eflops102:23:99 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
eflops102:27:101 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
eflops102:23:99 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
eflops102:30:104 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
eflops102:23:99 [0] NCCL INFO P2P Chunksize set to 131072
eflops102:27:101 [4] NCCL INFO P2P Chunksize set to 131072
eflops102:30:104 [7] NCCL INFO P2P Chunksize set to 131072
eflops102:24:105 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
eflops102:29:100 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC
eflops102:25:102 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
eflops102:28:106 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC
eflops102:26:103 [3] NCCL INFO Channel 00 : 3[3] -> 4[4] via SHM/direct/direct
eflops102:30:104 [7] NCCL INFO Channel 00 : 7[7] -> 0[0] via SHM/direct/direct
eflops102:24:105 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
eflops102:25:102 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
eflops102:28:106 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC
eflops102:26:103 [3] NCCL INFO Channel 01 : 3[3] -> 4[4] via SHM/direct/direct
eflops102:30:104 [7] NCCL INFO Channel 01 : 7[7] -> 0[0] via SHM/direct/direct
eflops102:27:101 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
eflops102:25:102 [2] NCCL INFO Connected all rings
eflops102:27:101 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
eflops102:25:102 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
eflops102:25:102 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
eflops102:23:99 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
eflops102:27:101 [4] NCCL INFO Connected all rings
eflops102:23:99 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
eflops102:26:103 [3] NCCL INFO Connected all rings
eflops102:27:101 [4] NCCL INFO Channel 00 : 4[4] -> 3[3] via SHM/direct/direct
eflops102:27:101 [4] NCCL INFO Channel 01 : 4[4] -> 3[3] via SHM/direct/direct
eflops102:24:105 [1] NCCL INFO Connected all rings
eflops102:23:99 [0] NCCL INFO Connected all rings
eflops102:24:105 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
eflops102:24:105 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
eflops102:29:100 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC
eflops102:26:103 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
eflops102:23:99 [0] NCCL INFO Connected all trees
eflops102:23:99 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:23:99 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:26:103 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
eflops102:24:105 [1] NCCL INFO Connected all trees
eflops102:24:105 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:24:105 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:25:102 [2] NCCL INFO Connected all trees
eflops102:25:102 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:25:102 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:26:103 [3] NCCL INFO Connected all trees
eflops102:26:103 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:26:103 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:28:106 [5] NCCL INFO Connected all rings
eflops102:29:100 [6] NCCL INFO Connected all rings
eflops102:30:104 [7] NCCL INFO Connected all rings
eflops102:30:104 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC
eflops102:30:104 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC
eflops102:29:100 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC
eflops102:28:106 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
eflops102:29:100 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC
eflops102:28:106 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
eflops102:30:104 [7] NCCL INFO Connected all trees
eflops102:30:104 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:30:104 [7] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:29:100 [6] NCCL INFO Connected all trees
eflops102:29:100 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:29:100 [6] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:28:106 [5] NCCL INFO Connected all trees
eflops102:28:106 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:28:106 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:27:101 [4] NCCL INFO Connected all trees
eflops102:27:101 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
eflops102:27:101 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
eflops102:24:105 [1] NCCL INFO comm 0x11b50670 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 50000 commId 0xccfe7c62eba2ebda - Init COMPLETE
eflops102:27:101 [4] NCCL INFO comm 0x11e44d90 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9c000 commId 0xccfe7c62eba2ebda - Init COMPLETE
eflops102:26:103 [3] NCCL INFO comm 0x118fc870 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 57000 commId 0xccfe7c62eba2ebda - Init COMPLETE
eflops102:30:104 [7] NCCL INFO comm 0x119179f0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a4000 commId 0xccfe7c62eba2ebda - Init COMPLETE
eflops102:23:99 [0] NCCL INFO comm 0x10c23a80 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 4f000 commId 0xccfe7c62eba2ebda - Init COMPLETE
eflops102:25:102 [2] NCCL INFO comm 0x121f21f0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 53000 commId 0xccfe7c62eba2ebda - Init COMPLETE
eflops102:29:100 [6] NCCL INFO comm 0x11efc560 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a0000 commId 0xccfe7c62eba2ebda - Init COMPLETE
eflops102:28:106 [5] NCCL INFO comm 0x115cef30 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 9d000 commId 0xccfe7c62eba2ebda - Init COMPLETE
2025-09-03 08:53:54,258   INFO  GT database has been saved to shared memory
2025-09-03 08:53:54,424   INFO  Loading NuScenes dataset
2025-09-03 08:53:56,457   INFO  Total samples for NuScenes dataset: 28130
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2025-09-03 08:53:57,407   INFO  ----------- Model TransFusion created, param count: 24443458 -----------
epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]epochs:   0%|          | 0/36 [00:00<?, ?it/s]2025-09-03 08:53:57,407   INFO  DistributedDataParallel(
  (module): TransFusion(
    (vfe): DynamicVoxelVFE(
      (pfn_layers): ModuleList(
        (0): PFNLayerV2(
          (linear): Linear(in_features=11, out_features=32, bias=False)
          (norm): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
        )
        (1): PFNLayerV2(
          (linear): Linear(in_features=64, out_features=64, bias=False)
          (norm): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
        )
      )
    )
    (backbone_3d): FSHNet_nusc(
      (cpe): SparseSequential(
        (0): SparseSequential(
          (0): SubMConv3d(64, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (2): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
      )
      (down1): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (down2): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (down3): SparseSequential(
        (0): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (stage): SPEncoder(
        (blocks): ModuleList(
          (0): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): ZOrderSerialization()
            )
          )
          (1): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): HilbertSerialization()
            )
          )
          (2): SMSA(
            (forward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (backward_blocks): ModuleList(
              (0-1): 2 x Block(
                (mixer): Mamba(
                  (in_proj): Linear(in_features=128, out_features=512, bias=False)
                  (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
                  (act): SiLU()
                  (x_proj): Linear(in_features=256, out_features=40, bias=False)
                  (dt_proj): Linear(in_features=8, out_features=256, bias=True)
                  (out_proj): Linear(in_features=256, out_features=128, bias=False)
                )
                (norm): RMSNorm()
              )
            )
            (forward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (backward_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (output_norms): ModuleList(
              (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (locals): ModuleList(
              (0-2): 3 x ResidualSparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (act): GELU(approximate='none')
              )
            )
            (input_layer): SerializationLayer(
              (serialization): FlattenWindowsSerialization()
            )
          )
        )
        (embeddings): ModuleList(
          (0-2): 3 x MSSubConvEmbeddingLearned(
            (stem): ModuleList(
              (0-2): 3 x SparseBasicBlock(
                (conv1): SubMConv3d(3, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU(inplace=True)
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
              )
            )
          )
        )
      )
      (app): AttnPillarPool(
        (query_func): SparseMaxPool3d(kernel_size=[10, 1, 1], stride=[10, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], algo=ConvAlgo.MaskImplicitGemm)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (pos_embedding): Embedding(10, 128)
      )
      (cls_conv): SparseSequential(
        (0): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): SubMConv2d(128, 3, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
      )
      (afd_layers): ModuleList(
        (0-3): 4 x SEDLayer(
          (encoder): ModuleList(
            (0): SparseSequential(
              (0): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
              (1): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
            )
            (1-2): 2 x SparseSequential(
              (0): SparseSequential(
                (0): SparseConv2d(128, 128, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock2D(
                (conv1): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (conv2): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
              )
            )
          )
          (decoder): ModuleList(
            (0-1): 2 x SparseSequential(
              (0): SparseInverseConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
              (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (decoder_norm): ModuleList(
            (0-1): 2 x SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (shared_conv): SparseSequential(
        (0): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (map_to_bev_module): None
    (pfe): None
    (backbone_2d): None
    (dense_head): TransFusionHead(
      (loss_cls): SigmoidFocalClassificationLoss()
      (loss_bbox): L1Loss()
      (loss_heatmap): GaussianFocalLoss()
      (shared_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (heatmap_head): Sequential(
        (0): BasicBlock2D(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): Conv2d(128, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (class_encoding): Conv1d(10, 128, kernel_size=(1,), stride=(1,))
      (decoder): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_posembed): PositionEmbeddingLearned(
          (position_embedding_head): Sequential(
            (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
        )
        (cross_posembed): PositionEmbeddingLearned(
          (position_embedding_head): Sequential(
            (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
        )
      )
      (prediction_head): SeparateHead_Transfusion(
        (center): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (height): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
        (dim): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 3, kernel_size=(1,), stride=(1,))
        )
        (rot): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (vel): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
        )
        (iou): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
        (heatmap): Sequential(
          (0): Sequential(
            (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv1d(64, 10, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (point_head): None
    (roi_head): None
  )
)
2025-09-03 08:53:57,413   INFO  **********************Start training sparse_models/sparse_former_base(default)**********************
epochs:   0%|          | 0/36 [00:00<?, ?it/s]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py:102: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overlaps_bev = torch.cuda.FloatTensor(torch.Size((boxes_a.shape[0], 1))).zero_()  # (N, M)
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 128, 1], strides() = [128, 1, 128]
bucket_view.sizes() = [128, 128, 1], strides() = [128, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/utils/commu_utils.py:66: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = torch.ByteStorage.from_buffer(buffer)
2025-09-03 08:54:14,323   INFO  Train:    1/36 (  3%) [   0/1759 (  0%)]  Loss: 1921. (1.92e+03)  LR: 3.000e-04  Grad: 10.0000  max=0.2784(module.dense_head.heatmap_head.1.bias)  min: -0.0842(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1880.8564, loss_cls=21.0691, loss_bbox=10.6782, matched_ious=0.0048, loss_iou=0.4795, loss_iou_reg=0.4729, d_time=1.97(1.97), f_time=13.07(13.07), b_time=15.05(15.05)  Time cost: 00:14/7:15:15 [00:16/261:09:05]  Acc_iter 1           Data time: 1.97(1.97)  Forward time: 13.07(13.07)  Batch time: 15.05(15.05)
2025-09-03 08:55:21,520   INFO  Train:    1/36 (  3%) [  49/1759 (  3%)]  Loss: 18.44 (164.)  LR: 3.000e-04  Grad: 10.0000  max=1.2391(module.dense_head.prediction_head.heatmap.1.bias)  min: -0.2910(module.dense_head.prediction_head.heatmap.0.1.bias)  NaN: False  loss_hm=102.2983, loss_cls=12.3934, loss_bbox=8.0636, matched_ious=0.0062, loss_iou=0.2979, loss_iou_reg=0.2788, d_time=0.00(0.10), f_time=1.24(1.55), b_time=1.24(1.64)  Time cost: 01:22/46:45 [01:24/28:50:28]  Acc_iter 50          Data time: 0.00(0.10)  Forward time: 1.24(1.55)  Batch time: 1.24(1.64)
2025-09-03 08:56:27,874   INFO  Train:    1/36 (  3%) [  99/1759 (  6%)]  Loss: 13.29 (90.8)  LR: 3.001e-04  Grad: 9.5399  max=0.9536(module.dense_head.prediction_head.heatmap.1.bias)  min: -0.3252(module.dense_head.prediction_head.heatmap.0.1.bias)  NaN: False  loss_hm=2.9844, loss_cls=4.5933, loss_bbox=4.9204, matched_ious=0.0406, loss_iou=0.1397, loss_iou_reg=0.4301, d_time=0.19(0.06), f_time=1.24(1.43), b_time=1.42(1.49)  Time cost: 02:28/41:03 [02:30/26:03:46]  Acc_iter 100         Data time: 0.19(0.06)  Forward time: 1.24(1.43)  Batch time: 1.42(1.49)
2025-09-03 08:57:30,668   INFO  Train:    1/36 (  3%) [ 149/1759 (  8%)]  Loss: 11.68 (64.4)  LR: 3.002e-04  Grad: 6.0101  max=0.3748(module.dense_head.prediction_head.heatmap.1.bias)  min: -0.1525(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=2.6133, loss_cls=1.6016, loss_bbox=3.1842, matched_ious=0.0840, loss_iou=0.1095, loss_iou_reg=0.4130, d_time=0.01(0.04), f_time=1.35(1.37), b_time=1.36(1.41)  Time cost: 03:31/37:46 [03:33/24:42:27]  Acc_iter 150         Data time: 0.01(0.04)  Forward time: 1.35(1.37)  Batch time: 1.36(1.41)
2025-09-03 08:58:34,037   INFO  Train:    1/36 (  3%) [ 199/1759 ( 11%)]  Loss: 9.716 (50.9)  LR: 3.004e-04  Grad: 6.0661  max=0.1469(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2116(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=2.4990, loss_cls=0.8774, loss_bbox=2.9730, matched_ious=0.1045, loss_iou=0.1057, loss_iou_reg=0.4013, d_time=0.00(0.03), f_time=1.16(1.34), b_time=1.16(1.37)  Time cost: 04:34/35:41 [04:36/24:04:17]  Acc_iter 200         Data time: 0.00(0.03)  Forward time: 1.16(1.34)  Batch time: 1.16(1.37)
2025-09-03 08:59:36,771   INFO  Train:    1/36 (  3%) [ 249/1759 ( 14%)]  Loss: 9.405 (42.6)  LR: 3.006e-04  Grad: 5.6271  max=0.1431(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.2277(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=2.3178, loss_cls=0.6907, loss_bbox=2.6012, matched_ious=0.1251, loss_iou=0.1069, loss_iou_reg=0.3969, d_time=0.00(0.03), f_time=1.22(1.32), b_time=1.23(1.35)  Time cost: 05:37/33:57 [05:39/23:38:20]  Acc_iter 250         Data time: 0.00(0.03)  Forward time: 1.22(1.32)  Batch time: 1.23(1.35)
2025-09-03 09:00:44,138   INFO  Train:    1/36 (  3%) [ 299/1759 ( 17%)]  Loss: 8.395 (37.0)  LR: 3.009e-04  Grad: 6.0444  max=0.2822(module.backbone_3d.cls_conv.3.weight)  min: -0.3756(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=2.1910, loss_cls=0.6027, loss_bbox=2.4673, matched_ious=0.1385, loss_iou=0.1077, loss_iou_reg=0.3916, d_time=0.01(0.03), f_time=1.35(1.32), b_time=1.36(1.35)  Time cost: 06:44/32:49 [06:46/23:36:53]  Acc_iter 300         Data time: 0.01(0.03)  Forward time: 1.35(1.32)  Batch time: 1.36(1.35)
2025-09-03 09:01:47,475   INFO  Train:    1/36 (  3%) [ 349/1759 ( 20%)]  Loss: 7.933 (33.0)  LR: 3.013e-04  Grad: 6.2388  max=0.1543(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.4090(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=2.1193, loss_cls=0.5762, loss_bbox=2.2858, matched_ious=0.1605, loss_iou=0.1088, loss_iou_reg=0.3810, d_time=0.00(0.02), f_time=1.17(1.31), b_time=1.17(1.34)  Time cost: 07:48/31:25 [07:49/23:23:26]  Acc_iter 350         Data time: 0.00(0.02)  Forward time: 1.17(1.31)  Batch time: 1.17(1.34)
2025-09-03 09:02:50,982   INFO  Train:    1/36 (  3%) [ 399/1759 ( 23%)]  Loss: 7.660 (29.9)  LR: 3.017e-04  Grad: 6.7792  max=0.1919(module.dense_head.heatmap_head.1.weight)  min: -0.2703(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=2.0111, loss_cls=0.5482, loss_bbox=2.0793, matched_ious=0.1830, loss_iou=0.1056, loss_iou_reg=0.3722, d_time=0.01(0.02), f_time=1.20(1.31), b_time=1.21(1.33)  Time cost: 08:51/30:07 [08:53/23:13:32]  Acc_iter 400         Data time: 0.01(0.02)  Forward time: 1.20(1.31)  Batch time: 1.21(1.33)
2025-09-03 09:03:53,541   INFO  Train:    1/36 (  3%) [ 449/1759 ( 26%)]  Loss: 7.823 (27.5)  LR: 3.021e-04  Grad: 7.4666  max=0.2221(module.vfe.pfn_layers.0.linear.weight)  min: -0.2429(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.9242, loss_cls=0.5399, loss_bbox=2.0074, matched_ious=0.2098, loss_iou=0.1112, loss_iou_reg=0.3556, d_time=0.00(0.02), f_time=1.23(1.30), b_time=1.24(1.32)  Time cost: 09:54/28:49 [09:56/23:03:24]  Acc_iter 450         Data time: 0.00(0.02)  Forward time: 1.23(1.30)  Batch time: 1.24(1.32)
2025-09-03 09:04:56,807   INFO  Train:    1/36 (  3%) [ 499/1759 ( 28%)]  Loss: 8.245 (25.5)  LR: 3.026e-04  Grad: 8.0539  max=0.2636(module.backbone_3d.cls_conv.3.bias)  min: -0.1992(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.8557, loss_cls=0.5138, loss_bbox=1.9296, matched_ious=0.2259, loss_iou=0.1109, loss_iou_reg=0.3468, d_time=0.00(0.02), f_time=1.28(1.30), b_time=1.28(1.32)  Time cost: 10:57/27:36 [10:59/22:56:33]  Acc_iter 500         Data time: 0.00(0.02)  Forward time: 1.28(1.30)  Batch time: 1.28(1.32)
2025-09-03 09:05:59,679   INFO  Train:    1/36 (  3%) [ 549/1759 ( 31%)]  Loss: 6.983 (23.8)  LR: 3.031e-04  Grad: 8.1939  max=0.2123(module.backbone_3d.cls_conv.3.weight)  min: -0.3180(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.7502, loss_cls=0.4885, loss_bbox=1.8110, matched_ious=0.2444, loss_iou=0.1122, loss_iou_reg=0.3388, d_time=0.00(0.02), f_time=1.18(1.29), b_time=1.19(1.31)  Time cost: 12:00/26:24 [12:02/22:50:01]  Acc_iter 550         Data time: 0.00(0.02)  Forward time: 1.18(1.29)  Batch time: 1.19(1.31)
2025-09-03 09:07:02,821   INFO  Train:    1/36 (  3%) [ 599/1759 ( 34%)]  Loss: 6.958 (22.5)  LR: 3.037e-04  Grad: 8.7881  max=0.2492(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.3212(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.7292, loss_cls=0.4659, loss_bbox=1.7803, matched_ious=0.2551, loss_iou=0.1121, loss_iou_reg=0.3308, d_time=0.00(0.02), f_time=1.22(1.29), b_time=1.22(1.31)  Time cost: 13:03/25:14 [13:05/22:44:52]  Acc_iter 600         Data time: 0.00(0.02)  Forward time: 1.22(1.29)  Batch time: 1.22(1.31)
2025-09-03 09:08:05,641   INFO  Train:    1/36 (  3%) [ 649/1759 ( 37%)]  Loss: 7.426 (21.3)  LR: 3.044e-04  Grad: 9.1716  max=0.2812(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.3821(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.6787, loss_cls=0.4407, loss_bbox=1.8145, matched_ious=0.2606, loss_iou=0.1121, loss_iou_reg=0.3277, d_time=0.00(0.02), f_time=1.30(1.29), b_time=1.31(1.30)  Time cost: 14:06/24:04 [14:08/22:39:47]  Acc_iter 650         Data time: 0.00(0.02)  Forward time: 1.30(1.29)  Batch time: 1.31(1.30)
2025-09-03 09:09:08,469   INFO  Train:    1/36 (  3%) [ 699/1759 ( 40%)]  Loss: 7.444 (20.3)  LR: 3.051e-04  Grad: 9.4322  max=0.3486(module.backbone_3d.cls_conv.3.weight)  min: -0.2576(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.6805, loss_cls=0.4356, loss_bbox=1.8290, matched_ious=0.2691, loss_iou=0.1121, loss_iou_reg=0.3278, d_time=0.00(0.02), f_time=1.24(1.28), b_time=1.24(1.30)  Time cost: 15:08/22:56 [15:10/22:35:22]  Acc_iter 700         Data time: 0.00(0.02)  Forward time: 1.24(1.28)  Batch time: 1.24(1.30)
2025-09-03 09:10:12,731   INFO  Train:    1/36 (  3%) [ 749/1759 ( 43%)]  Loss: 8.010 (19.4)  LR: 3.058e-04  Grad: 9.2210  max=0.2658(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2757(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.6252, loss_cls=0.4226, loss_bbox=1.8090, matched_ious=0.2834, loss_iou=0.1108, loss_iou_reg=0.3195, d_time=0.01(0.01), f_time=1.21(1.28), b_time=1.22(1.30)  Time cost: 16:13/21:50 [16:15/22:33:22]  Acc_iter 750         Data time: 0.01(0.01)  Forward time: 1.21(1.28)  Batch time: 1.22(1.30)
2025-09-03 09:11:16,983   INFO  Train:    1/36 (  3%) [ 799/1759 ( 45%)]  Loss: 5.848 (18.6)  LR: 3.066e-04  Grad: 9.9395  max=0.2883(module.dense_head.heatmap_head.1.weight)  min: -0.4195(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.5808, loss_cls=0.4029, loss_bbox=1.7804, matched_ious=0.2829, loss_iou=0.1120, loss_iou_reg=0.3179, d_time=0.00(0.01), f_time=1.28(1.28), b_time=1.28(1.30)  Time cost: 17:17/20:45 [17:19/22:31:27]  Acc_iter 800         Data time: 0.00(0.01)  Forward time: 1.28(1.28)  Batch time: 1.28(1.30)
2025-09-03 09:12:19,809   INFO  Train:    1/36 (  3%) [ 849/1759 ( 48%)]  Loss: 6.550 (17.9)  LR: 3.075e-04  Grad: 9.6900  max=0.7321(module.vfe.pfn_layers.0.linear.weight)  min: -0.6420(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.6082, loss_cls=0.4047, loss_bbox=1.7325, matched_ious=0.2877, loss_iou=0.1120, loss_iou_reg=0.3177, d_time=0.01(0.01), f_time=1.31(1.28), b_time=1.31(1.29)  Time cost: 18:20/19:38 [18:22/22:27:54]  Acc_iter 850         Data time: 0.01(0.01)  Forward time: 1.31(1.28)  Batch time: 1.31(1.29)
2025-09-03 09:13:23,161   INFO  Train:    1/36 (  3%) [ 899/1759 ( 51%)]  Loss: 6.738 (17.3)  LR: 3.084e-04  Grad: 9.7000  max=0.4175(module.vfe.pfn_layers.0.linear.weight)  min: -0.4940(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.5070, loss_cls=0.3852, loss_bbox=1.6369, matched_ious=0.3043, loss_iou=0.1108, loss_iou_reg=0.3095, d_time=0.01(0.01), f_time=1.22(1.28), b_time=1.23(1.29)  Time cost: 19:23/18:31 [19:25/22:25:14]  Acc_iter 900         Data time: 0.01(0.01)  Forward time: 1.22(1.28)  Batch time: 1.23(1.29)
2025-09-03 09:14:25,922   INFO  Train:    1/36 (  3%) [ 949/1759 ( 54%)]  Loss: 6.233 (16.8)  LR: 3.093e-04  Grad: 9.7257  max=0.4247(module.backbone_3d.cls_conv.3.weight)  min: -0.3427(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.5343, loss_cls=0.3866, loss_bbox=1.7107, matched_ious=0.3024, loss_iou=0.1112, loss_iou_reg=0.3101, d_time=0.00(0.01), f_time=1.20(1.28), b_time=1.20(1.29)  Time cost: 20:26/17:25 [20:28/22:22:06]  Acc_iter 950         Data time: 0.00(0.01)  Forward time: 1.20(1.28)  Batch time: 1.20(1.29)
2025-09-03 09:15:29,992   INFO  Train:    1/36 (  3%) [ 999/1759 ( 57%)]  Loss: 7.143 (16.3)  LR: 3.104e-04  Grad: 9.7554  max=0.4111(module.vfe.pfn_layers.0.linear.weight)  min: -0.3767(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.4923, loss_cls=0.3837, loss_bbox=1.7003, matched_ious=0.3109, loss_iou=0.1085, loss_iou_reg=0.3060, d_time=0.00(0.01), f_time=1.35(1.28), b_time=1.35(1.29)  Time cost: 21:30/16:20 [21:32/22:20:31]  Acc_iter 1000        Data time: 0.00(0.01)  Forward time: 1.35(1.28)  Batch time: 1.35(1.29)
2025-09-03 09:16:33,864   INFO  Train:    1/36 (  3%) [1049/1759 ( 60%)]  Loss: 6.688 (15.8)  LR: 3.114e-04  Grad: 9.8742  max=1.0558(module.vfe.pfn_layers.0.linear.weight)  min: -0.7963(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.4522, loss_cls=0.3644, loss_bbox=1.7124, matched_ious=0.3103, loss_iou=0.1110, loss_iou_reg=0.3014, d_time=0.00(0.01), f_time=1.21(1.28), b_time=1.21(1.29)  Time cost: 22:34/15:15 [22:36/22:18:48]  Acc_iter 1050        Data time: 0.00(0.01)  Forward time: 1.21(1.28)  Batch time: 1.21(1.29)
2025-09-03 09:17:37,146   INFO  Train:    1/36 (  3%) [1099/1759 ( 62%)]  Loss: 6.484 (15.4)  LR: 3.125e-04  Grad: 9.6278  max=0.2620(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.5290(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.4945, loss_cls=0.3669, loss_bbox=1.7410, matched_ious=0.3190, loss_iou=0.1089, loss_iou_reg=0.2982, d_time=0.00(0.01), f_time=1.21(1.28), b_time=1.21(1.29)  Time cost: 23:37/14:10 [23:39/22:16:35]  Acc_iter 1100        Data time: 0.00(0.01)  Forward time: 1.21(1.28)  Batch time: 1.21(1.29)
2025-09-03 09:18:40,157   INFO  Train:    1/36 (  3%) [1149/1759 ( 65%)]  Loss: 6.426 (15.0)  LR: 3.137e-04  Grad: 9.8738  max=1.0730(module.vfe.pfn_layers.0.linear.weight)  min: -0.8215(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.4294, loss_cls=0.3616, loss_bbox=1.6732, matched_ious=0.3209, loss_iou=0.1069, loss_iou_reg=0.2992, d_time=0.00(0.01), f_time=1.31(1.28), b_time=1.31(1.29)  Time cost: 24:40/13:05 [24:42/22:14:13]  Acc_iter 1150        Data time: 0.00(0.01)  Forward time: 1.31(1.28)  Batch time: 1.31(1.29)
2025-09-03 09:19:44,940   INFO  Train:    1/36 (  3%) [1199/1759 ( 68%)]  Loss: 6.988 (14.6)  LR: 3.149e-04  Grad: 9.7287  max=0.3234(module.vfe.pfn_layers.0.linear.weight)  min: -0.2735(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.4168, loss_cls=0.3591, loss_bbox=1.6567, matched_ious=0.3351, loss_iou=0.1110, loss_iou_reg=0.2895, d_time=0.01(0.01), f_time=1.24(1.28), b_time=1.25(1.29)  Time cost: 25:45/12:01 [25:47/22:13:30]  Acc_iter 1200        Data time: 0.01(0.01)  Forward time: 1.24(1.28)  Batch time: 1.25(1.29)
2025-09-03 09:20:48,409   INFO  Train:    1/36 (  3%) [1249/1759 ( 71%)]  Loss: 6.300 (14.3)  LR: 3.162e-04  Grad: 9.4614  max=0.2444(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2381(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=1.4065, loss_cls=0.3499, loss_bbox=1.6383, matched_ious=0.3330, loss_iou=0.1065, loss_iou_reg=0.2946, d_time=0.00(0.01), f_time=1.16(1.28), b_time=1.16(1.29)  Time cost: 26:48/10:56 [26:50/22:11:39]  Acc_iter 1250        Data time: 0.00(0.01)  Forward time: 1.16(1.28)  Batch time: 1.16(1.29)
2025-09-03 09:21:53,029   INFO  Train:    1/36 (  3%) [1299/1759 ( 74%)]  Loss: 6.588 (14.0)  LR: 3.175e-04  Grad: 9.4456  max=0.7382(module.vfe.pfn_layers.0.linear.weight)  min: -0.3643(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.3956, loss_cls=0.3518, loss_bbox=1.6590, matched_ious=0.3358, loss_iou=0.1088, loss_iou_reg=0.2918, d_time=0.01(0.01), f_time=1.29(1.28), b_time=1.30(1.29)  Time cost: 27:53/09:52 [27:55/22:10:47]  Acc_iter 1300        Data time: 0.01(0.01)  Forward time: 1.29(1.28)  Batch time: 1.30(1.29)
2025-09-03 09:22:55,557   INFO  Train:    1/36 (  3%) [1349/1759 ( 77%)]  Loss: 6.166 (13.7)  LR: 3.189e-04  Grad: 9.7072  max=1.5764(module.vfe.pfn_layers.0.linear.weight)  min: -1.6995(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.4052, loss_cls=0.3462, loss_bbox=1.7046, matched_ious=0.3293, loss_iou=0.1100, loss_iou_reg=0.2946, d_time=0.01(0.01), f_time=1.24(1.28), b_time=1.25(1.29)  Time cost: 28:56/08:47 [28:58/22:08:19]  Acc_iter 1350        Data time: 0.01(0.01)  Forward time: 1.24(1.28)  Batch time: 1.25(1.29)
2025-09-03 09:23:59,068   INFO  Train:    1/36 (  3%) [1399/1759 ( 80%)]  Loss: 5.174 (13.5)  LR: 3.203e-04  Grad: 7.9218  max=0.4390(module.vfe.pfn_layers.0.linear.weight)  min: -0.8150(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.3780, loss_cls=0.3467, loss_bbox=1.6503, matched_ious=0.3409, loss_iou=0.1073, loss_iou_reg=0.2921, d_time=0.00(0.01), f_time=1.30(1.28), b_time=1.30(1.29)  Time cost: 29:59/07:42 [30:01/22:06:39]  Acc_iter 1400        Data time: 0.00(0.01)  Forward time: 1.30(1.28)  Batch time: 1.30(1.29)
2025-09-03 09:25:02,720   INFO  Train:    1/36 (  3%) [1449/1759 ( 82%)]  Loss: 5.868 (13.2)  LR: 3.217e-04  Grad: 8.3119  max=0.7401(module.vfe.pfn_layers.0.linear.weight)  min: -1.9268(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.3607, loss_cls=0.3443, loss_bbox=1.6679, matched_ious=0.3426, loss_iou=0.1093, loss_iou_reg=0.2928, d_time=0.01(0.01), f_time=1.22(1.27), b_time=1.23(1.29)  Time cost: 31:03/06:38 [31:05/22:05:09]  Acc_iter 1450        Data time: 0.01(0.01)  Forward time: 1.22(1.27)  Batch time: 1.23(1.29)
2025-09-03 09:26:06,729   INFO  Train:    1/36 (  3%) [1499/1759 ( 85%)]  Loss: 6.284 (13.0)  LR: 3.233e-04  Grad: 8.5826  max=1.3725(module.vfe.pfn_layers.0.linear.weight)  min: -2.5270(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.3415, loss_cls=0.3348, loss_bbox=1.6199, matched_ious=0.3359, loss_iou=0.1094, loss_iou_reg=0.2927, d_time=0.00(0.01), f_time=1.39(1.27), b_time=1.39(1.28)  Time cost: 32:07/05:34 [32:09/22:03:55]  Acc_iter 1500        Data time: 0.00(0.01)  Forward time: 1.39(1.27)  Batch time: 1.39(1.28)
2025-09-03 09:27:08,991   INFO  Train:    1/36 (  3%) [1549/1759 ( 88%)]  Loss: 5.283 (12.8)  LR: 3.248e-04  Grad: 7.4500  max=1.5641(module.vfe.pfn_layers.0.linear.weight)  min: -1.4357(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.3979, loss_cls=0.3424, loss_bbox=1.7383, matched_ious=0.3312, loss_iou=0.1088, loss_iou_reg=0.2959, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.24(1.28)  Time cost: 33:09/04:29 [33:11/22:01:31]  Acc_iter 1550        Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.28)
2025-09-03 09:28:11,890   INFO  Train:    1/36 (  3%) [1599/1759 ( 91%)]  Loss: 6.191 (12.6)  LR: 3.265e-04  Grad: 8.7519  max=3.3150(module.vfe.pfn_layers.0.linear.weight)  min: -3.1325(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.3216, loss_cls=0.3406, loss_bbox=1.5708, matched_ious=0.3573, loss_iou=0.1069, loss_iou_reg=0.2825, d_time=0.01(0.01), f_time=1.24(1.27), b_time=1.25(1.28)  Time cost: 34:12/03:25 [34:14/21:59:38]  Acc_iter 1600        Data time: 0.01(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.28)
2025-09-03 09:29:16,255   INFO  Train:    1/36 (  3%) [1649/1759 ( 94%)]  Loss: 5.997 (12.4)  LR: 3.281e-04  Grad: 7.4126  max=0.2235(module.backbone_3d.cls_conv.3.weight)  min: -0.4458(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.3400, loss_cls=0.3411, loss_bbox=1.6175, matched_ious=0.3355, loss_iou=0.1061, loss_iou_reg=0.2951, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.30(1.28)  Time cost: 35:16/02:21 [35:18/21:58:42]  Acc_iter 1650        Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.30(1.28)
2025-09-03 09:30:19,449   INFO  Train:    1/36 (  3%) [1699/1759 ( 97%)]  Loss: 6.631 (12.2)  LR: 3.299e-04  Grad: 8.1257  max=2.8784(module.vfe.pfn_layers.0.linear.weight)  min: -0.6197(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.3094, loss_cls=0.3390, loss_bbox=1.5258, matched_ious=0.3573, loss_iou=0.1046, loss_iou_reg=0.2854, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.22(1.28)  Time cost: 36:19/01:16 [36:21/21:57:03]  Acc_iter 1700        Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.22(1.28)
2025-09-03 09:31:21,833   INFO  Train:    1/36 (  3%) [1749/1759 ( 99%)]  Loss: 7.516 (12.0)  LR: 3.316e-04  Grad: 7.9518  max=0.3935(module.vfe.pfn_layers.0.linear.weight)  min: -2.3708(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.3393, loss_cls=0.3370, loss_bbox=1.6298, matched_ious=0.3507, loss_iou=0.1076, loss_iou_reg=0.2866, d_time=0.01(0.01), f_time=1.21(1.27), b_time=1.21(1.28)  Time cost: 37:22/00:12 [37:24/21:54:59]  Acc_iter 1750        Data time: 0.01(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.28)
2025-09-03 09:31:32,610   INFO  Train:    1/36 (  3%) [1758/1759 (100%)]  Loss: 6.414 (12.0)  LR: 3.320e-04  Grad: 10.0000  max=5.4028(module.vfe.pfn_layers.0.linear.weight)  min: -2.0383(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.3487, loss_cls=0.3278, loss_bbox=1.6006, matched_ious=0.3505, loss_iou=0.1056, loss_iou_reg=0.2794, d_time=0.00(0.01), f_time=0.76(1.27), b_time=0.76(1.28)  Time cost: 37:33/00:01 [37:35/21:54:20]  Acc_iter 1759        Data time: 0.00(0.01)  Forward time: 0.76(1.27)  Batch time: 0.76(1.28)

                                               [Aepochs:   3%|▎         | 1/36 [37:35<21:55:42, 2255.50s/it]epochs:   3%|▎         | 1/36 [37:35<21:55:45, 2255.59s/it]epochs:   3%|▎         | 1/36 [37:35<21:55:45, 2255.59s/it]epochs:   3%|▎         | 1/36 [37:35<21:55:45, 2255.59s/it]epochs:   3%|▎         | 1/36 [37:35<21:55:45, 2255.59s/it]epochs:   3%|▎         | 1/36 [37:35<21:55:45, 2255.59s/it]epochs:   3%|▎         | 1/36 [37:35<21:55:45, 2255.59s/it]epochs:   3%|▎         | 1/36 [37:35<21:55:56, 2255.90s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 09:31:37,860   INFO  Train:    2/36 (  6%) [   0/1759 (  0%)]  Loss: 5.524 (5.52)  LR: 3.320e-04  Grad: 7.5506  max=2.4298(module.vfe.pfn_layers.0.linear.weight)  min: -0.1789(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1118, loss_cls=0.2996, loss_bbox=1.3260, matched_ious=0.3659, loss_iou=0.1047, loss_iou_reg=0.2686, d_time=1.73(1.73), f_time=2.32(2.32), b_time=4.06(4.06)  Time cost: 00:03/1:45:33 [37:40/61:34:45]  Acc_iter 1760        Data time: 1.73(1.73)  Forward time: 2.32(2.32)  Batch time: 4.06(4.06)
2025-09-03 09:32:31,072   INFO  Train:    2/36 (  6%) [  40/1759 (  2%)]  Loss: 6.051 (6.12)  LR: 3.335e-04  Grad: 7.5911  max=0.4285(module.vfe.pfn_layers.0.linear.weight)  min: -2.2907(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2812, loss_cls=0.3232, loss_bbox=1.6085, matched_ious=0.3518, loss_iou=0.1086, loss_iou_reg=0.2828, d_time=0.00(0.06), f_time=1.15(1.33), b_time=1.16(1.40)  Time cost: 00:56/39:42 [38:33/23:40:59]  Acc_iter 1800        Data time: 0.00(0.06)  Forward time: 1.15(1.33)  Batch time: 1.16(1.40)
2025-09-03 09:33:34,374   INFO  Train:    2/36 (  6%) [  90/1759 (  5%)]  Loss: 6.319 (6.16)  LR: 3.353e-04  Grad: 7.1264  max=0.2427(module.vfe.pfn_layers.0.linear.weight)  min: -1.3071(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.3122, loss_cls=0.3310, loss_bbox=1.6261, matched_ious=0.3558, loss_iou=0.1099, loss_iou_reg=0.2843, d_time=0.00(0.03), f_time=1.32(1.29), b_time=1.33(1.32)  Time cost: 02:00/36:43 [39:36/22:32:24]  Acc_iter 1850        Data time: 0.00(0.03)  Forward time: 1.32(1.29)  Batch time: 1.33(1.32)
2025-09-03 09:34:37,749   INFO  Train:    2/36 (  6%) [ 140/1759 (  8%)]  Loss: 6.594 (6.11)  LR: 3.373e-04  Grad: 8.3316  max=2.4034(module.vfe.pfn_layers.0.linear.weight)  min: -4.7479(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2575, loss_cls=0.3234, loss_bbox=1.5352, matched_ious=0.3578, loss_iou=0.1063, loss_iou_reg=0.2852, d_time=0.00(0.02), f_time=1.18(1.28), b_time=1.18(1.30)  Time cost: 03:03/35:06 [40:40/22:12:15]  Acc_iter 1900        Data time: 0.00(0.02)  Forward time: 1.18(1.28)  Batch time: 1.18(1.30)
2025-09-03 09:35:40,623   INFO  Train:    2/36 (  6%) [ 190/1759 ( 11%)]  Loss: 5.595 (6.07)  LR: 3.393e-04  Grad: 8.0924  max=6.4935(module.vfe.pfn_layers.0.linear.weight)  min: -1.6387(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2366, loss_cls=0.3215, loss_bbox=1.5098, matched_ious=0.3557, loss_iou=0.1092, loss_iou_reg=0.2863, d_time=0.00(0.02), f_time=1.21(1.27), b_time=1.21(1.29)  Time cost: 04:06/33:43 [41:43/21:59:26]  Acc_iter 1950        Data time: 0.00(0.02)  Forward time: 1.21(1.27)  Batch time: 1.21(1.29)
2025-09-03 09:36:44,413   INFO  Train:    2/36 (  6%) [ 240/1759 ( 14%)]  Loss: 5.714 (6.04)  LR: 3.413e-04  Grad: 8.5600  max=6.4213(module.vfe.pfn_layers.0.linear.weight)  min: -3.8682(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2289, loss_cls=0.3184, loss_bbox=1.5216, matched_ious=0.3579, loss_iou=0.1083, loss_iou_reg=0.2843, d_time=0.00(0.02), f_time=1.29(1.27), b_time=1.30(1.29)  Time cost: 05:10/32:34 [42:46/21:55:22]  Acc_iter 2000        Data time: 0.00(0.02)  Forward time: 1.29(1.27)  Batch time: 1.30(1.29)
2025-09-03 09:37:47,956   INFO  Train:    2/36 (  6%) [ 290/1759 ( 16%)]  Loss: 6.457 (6.04)  LR: 3.434e-04  Grad: 4.6698  max=2.7680(module.vfe.pfn_layers.0.linear.weight)  min: -0.3137(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2713, loss_cls=0.3233, loss_bbox=1.5675, matched_ious=0.3604, loss_iou=0.1085, loss_iou_reg=0.2829, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.29)  Time cost: 06:13/31:26 [43:50/21:51:28]  Acc_iter 2050        Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.29)
2025-09-03 09:38:50,624   INFO  Train:    2/36 (  6%) [ 340/1759 ( 19%)]  Loss: 6.027 (6.05)  LR: 3.455e-04  Grad: 5.3161  max=4.2121(module.vfe.pfn_layers.0.linear.weight)  min: -2.1547(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.3061, loss_cls=0.3228, loss_bbox=1.5846, matched_ious=0.3630, loss_iou=0.1078, loss_iou_reg=0.2847, d_time=0.00(0.01), f_time=1.33(1.27), b_time=1.34(1.28)  Time cost: 07:16/30:15 [44:53/21:45:47]  Acc_iter 2100        Data time: 0.00(0.01)  Forward time: 1.33(1.27)  Batch time: 1.34(1.28)
2025-09-03 09:39:54,949   INFO  Train:    2/36 (  6%) [ 390/1759 ( 22%)]  Loss: 6.527 (6.03)  LR: 3.477e-04  Grad: 3.9390  max=1.1834(module.vfe.pfn_layers.0.linear.weight)  min: -2.5865(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2475, loss_cls=0.3147, loss_bbox=1.5401, matched_ious=0.3599, loss_iou=0.1078, loss_iou_reg=0.2864, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.24(1.28)  Time cost: 08:20/29:13 [45:57/21:45:37]  Acc_iter 2150        Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.28)
2025-09-03 09:40:57,770   INFO  Train:    2/36 (  6%) [ 440/1759 ( 25%)]  Loss: 5.798 (6.01)  LR: 3.499e-04  Grad: 6.3749  max=5.7922(module.vfe.pfn_layers.0.linear.weight)  min: -0.1413(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2376, loss_cls=0.3155, loss_bbox=1.4896, matched_ious=0.3698, loss_iou=0.1071, loss_iou_reg=0.2794, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.21(1.28)  Time cost: 09:23/28:05 [47:00/21:41:45]  Acc_iter 2200        Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.28)
2025-09-03 09:42:00,903   INFO  Train:    2/36 (  6%) [ 490/1759 ( 28%)]  Loss: 6.969 (6.00)  LR: 3.522e-04  Grad: 7.3959  max=6.5836(module.vfe.pfn_layers.0.linear.weight)  min: -0.5626(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2292, loss_cls=0.3071, loss_bbox=1.5517, matched_ious=0.3587, loss_iou=0.1031, loss_iou_reg=0.2845, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.19(1.28)  Time cost: 10:26/26:59 [48:03/21:39:08]  Acc_iter 2250        Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.19(1.28)
2025-09-03 09:43:06,678   INFO  Train:    2/36 (  6%) [ 540/1759 ( 31%)]  Loss: 6.347 (5.98)  LR: 3.545e-04  Grad: 6.1822  max=0.1252(module.vfe.pfn_layers.0.linear.weight)  min: -5.5832(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2257, loss_cls=0.3134, loss_bbox=1.4674, matched_ious=0.3735, loss_iou=0.1034, loss_iou_reg=0.2765, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.28)  Time cost: 11:32/26:00 [49:09/21:41:45]  Acc_iter 2300        Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.28)
2025-09-03 09:44:09,514   INFO  Train:    2/36 (  6%) [ 590/1759 ( 34%)]  Loss: 5.599 (5.96)  LR: 3.569e-04  Grad: 10.0000  max=0.2237(module.backbone_3d.cls_conv.3.weight)  min: -8.9441(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2018, loss_cls=0.3112, loss_bbox=1.4550, matched_ious=0.3762, loss_iou=0.1051, loss_iou_reg=0.2762, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.28)  Time cost: 12:35/24:53 [50:12/21:38:41]  Acc_iter 2350        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.28)
2025-09-03 09:45:11,681   INFO  Train:    2/36 (  6%) [ 640/1759 ( 36%)]  Loss: 5.215 (5.95)  LR: 3.593e-04  Grad: 6.1112  max=1.5746(module.vfe.pfn_layers.0.linear.weight)  min: -1.7565(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1981, loss_cls=0.3073, loss_bbox=1.4505, matched_ious=0.3744, loss_iou=0.1056, loss_iou_reg=0.2772, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.20(1.28)  Time cost: 13:37/23:46 [51:14/21:34:53]  Acc_iter 2400        Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.28)
2025-09-03 09:46:15,164   INFO  Train:    2/36 (  6%) [ 690/1759 ( 39%)]  Loss: 5.439 (5.94)  LR: 3.618e-04  Grad: 10.0000  max=6.0382(module.vfe.pfn_layers.0.linear.weight)  min: -7.6933(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2135, loss_cls=0.3096, loss_bbox=1.5093, matched_ious=0.3780, loss_iou=0.1060, loss_iou_reg=0.2745, d_time=0.00(0.01), f_time=1.39(1.27), b_time=1.39(1.28)  Time cost: 14:40/22:42 [52:17/21:33:24]  Acc_iter 2450        Data time: 0.00(0.01)  Forward time: 1.39(1.27)  Batch time: 1.39(1.28)
2025-09-03 09:47:19,026   INFO  Train:    2/36 (  6%) [ 740/1759 ( 42%)]  Loss: 6.884 (5.93)  LR: 3.643e-04  Grad: 6.5969  max=0.2891(module.vfe.pfn_layers.0.linear.weight)  min: -5.0511(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2192, loss_cls=0.3063, loss_bbox=1.4840, matched_ious=0.3655, loss_iou=0.1051, loss_iou_reg=0.2824, d_time=0.00(0.01), f_time=1.34(1.27), b_time=1.35(1.28)  Time cost: 15:44/21:39 [53:21/21:32:31]  Acc_iter 2500        Data time: 0.00(0.01)  Forward time: 1.34(1.27)  Batch time: 1.35(1.28)
2025-09-03 09:48:22,362   INFO  Train:    2/36 (  6%) [ 790/1759 ( 45%)]  Loss: 4.620 (5.92)  LR: 3.669e-04  Grad: 7.4344  max=5.5649(module.vfe.pfn_layers.0.linear.weight)  min: -3.0667(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1815, loss_cls=0.3028, loss_bbox=1.4682, matched_ious=0.3893, loss_iou=0.1057, loss_iou_reg=0.2718, d_time=0.01(0.01), f_time=1.27(1.27), b_time=1.27(1.28)  Time cost: 16:48/20:34 [54:24/21:30:55]  Acc_iter 2550        Data time: 0.01(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.28)
2025-09-03 09:49:25,986   INFO  Train:    2/36 (  6%) [ 840/1759 ( 48%)]  Loss: 5.203 (5.91)  LR: 3.695e-04  Grad: 8.7532  max=7.8069(module.vfe.pfn_layers.0.linear.weight)  min: -2.3269(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2094, loss_cls=0.3056, loss_bbox=1.4521, matched_ious=0.3833, loss_iou=0.1038, loss_iou_reg=0.2744, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.27)  Time cost: 17:51/19:31 [55:28/21:29:45]  Acc_iter 2600        Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.27)
2025-09-03 09:50:29,145   INFO  Train:    2/36 (  6%) [ 890/1759 ( 51%)]  Loss: 4.802 (5.89)  LR: 3.722e-04  Grad: 8.0214  max=6.0943(module.vfe.pfn_layers.0.linear.weight)  min: -2.4127(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1743, loss_cls=0.3022, loss_bbox=1.4031, matched_ious=0.3836, loss_iou=0.1073, loss_iou_reg=0.2742, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.31(1.27)  Time cost: 18:54/18:26 [56:31/21:28:03]  Acc_iter 2650        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.31(1.27)
2025-09-03 09:51:32,015   INFO  Train:    2/36 (  6%) [ 940/1759 ( 53%)]  Loss: 5.185 (5.88)  LR: 3.749e-04  Grad: 7.2915  max=4.1505(module.vfe.pfn_layers.0.linear.weight)  min: -5.5673(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1540, loss_cls=0.2980, loss_bbox=1.4390, matched_ious=0.3834, loss_iou=0.1037, loss_iou_reg=0.2740, d_time=0.00(0.01), f_time=1.32(1.26), b_time=1.32(1.27)  Time cost: 19:57/17:22 [57:34/21:26:06]  Acc_iter 2700        Data time: 0.00(0.01)  Forward time: 1.32(1.26)  Batch time: 1.32(1.27)
2025-09-03 09:52:35,447   INFO  Train:    2/36 (  6%) [ 990/1759 ( 56%)]  Loss: 6.147 (5.87)  LR: 3.777e-04  Grad: 5.9416  max=2.7923(module.vfe.pfn_layers.0.linear.weight)  min: -4.5066(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.2005, loss_cls=0.2969, loss_bbox=1.4848, matched_ious=0.3806, loss_iou=0.1055, loss_iou_reg=0.2717, d_time=0.00(0.01), f_time=1.31(1.26), b_time=1.31(1.27)  Time cost: 21:01/16:18 [58:37/21:24:50]  Acc_iter 2750        Data time: 0.00(0.01)  Forward time: 1.31(1.26)  Batch time: 1.31(1.27)
2025-09-03 09:53:41,713   INFO  Train:    2/36 (  6%) [1040/1759 ( 59%)]  Loss: 5.300 (5.87)  LR: 3.805e-04  Grad: 10.0000  max=8.6110(module.vfe.pfn_layers.0.linear.weight)  min: -4.5336(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1949, loss_cls=0.2970, loss_bbox=1.4750, matched_ious=0.3798, loss_iou=0.1047, loss_iou_reg=0.2754, d_time=0.01(0.01), f_time=1.24(1.27), b_time=1.24(1.28)  Time cost: 22:07/15:16 [59:44/21:26:19]  Acc_iter 2800        Data time: 0.01(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.28)
2025-09-03 09:54:44,935   INFO  Train:    2/36 (  6%) [1090/1759 ( 62%)]  Loss: 5.192 (5.85)  LR: 3.834e-04  Grad: 4.5230  max=0.3061(module.vfe.pfn_layers.0.linear.weight)  min: -3.1832(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1621, loss_cls=0.3018, loss_bbox=1.3377, matched_ious=0.3904, loss_iou=0.1049, loss_iou_reg=0.2750, d_time=0.00(0.01), f_time=1.17(1.27), b_time=1.18(1.28)  Time cost: 23:10/14:12 [1:00:47/21:24:46]  Acc_iter 2850        Data time: 0.00(0.01)  Forward time: 1.17(1.27)  Batch time: 1.18(1.28)
2025-09-03 09:55:48,126   INFO  Train:    2/36 (  6%) [1140/1759 ( 65%)]  Loss: 6.689 (5.84)  LR: 3.863e-04  Grad: 6.2188  max=5.4392(module.vfe.pfn_layers.0.linear.weight)  min: -1.2506(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1562, loss_cls=0.2897, loss_bbox=1.4573, matched_ious=0.3887, loss_iou=0.1036, loss_iou_reg=0.2727, d_time=0.00(0.01), f_time=1.16(1.27), b_time=1.17(1.27)  Time cost: 24:13/13:08 [1:01:50/21:23:13]  Acc_iter 2900        Data time: 0.00(0.01)  Forward time: 1.16(1.27)  Batch time: 1.17(1.27)
2025-09-03 09:56:52,044   INFO  Train:    2/36 (  6%) [1190/1759 ( 68%)]  Loss: 4.453 (5.83)  LR: 3.893e-04  Grad: 7.3639  max=2.5346(module.vfe.pfn_layers.0.linear.weight)  min: -5.6486(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1277, loss_cls=0.2835, loss_bbox=1.4126, matched_ious=0.3992, loss_iou=0.1036, loss_iou_reg=0.2677, d_time=0.01(0.01), f_time=1.28(1.27), b_time=1.28(1.27)  Time cost: 25:17/12:05 [1:02:54/21:22:20]  Acc_iter 2950        Data time: 0.01(0.01)  Forward time: 1.28(1.27)  Batch time: 1.28(1.27)
2025-09-03 09:57:54,948   INFO  Train:    2/36 (  6%) [1240/1759 ( 70%)]  Loss: 5.048 (5.82)  LR: 3.923e-04  Grad: 5.8652  max=4.9137(module.vfe.pfn_layers.0.linear.weight)  min: -0.3315(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1252, loss_cls=0.2847, loss_bbox=1.3760, matched_ious=0.4046, loss_iou=0.1071, loss_iou_reg=0.2649, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.32(1.27)  Time cost: 26:20/11:01 [1:03:57/21:20:37]  Acc_iter 3000        Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.32(1.27)
2025-09-03 09:58:58,357   INFO  Train:    2/36 (  6%) [1290/1759 ( 73%)]  Loss: 5.321 (5.80)  LR: 3.954e-04  Grad: 7.6375  max=0.4493(module.vfe.pfn_layers.0.linear.weight)  min: -6.4663(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1016, loss_cls=0.2882, loss_bbox=1.3100, matched_ious=0.4010, loss_iou=0.1049, loss_iou_reg=0.2681, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.27(1.27)  Time cost: 27:24/09:57 [1:05:00/21:19:20]  Acc_iter 3050        Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.27)
2025-09-03 10:00:01,756   INFO  Train:    2/36 (  6%) [1340/1759 ( 76%)]  Loss: 6.310 (5.79)  LR: 3.985e-04  Grad: 6.6871  max=0.3662(module.backbone_3d.cls_conv.3.weight)  min: -5.5078(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1145, loss_cls=0.2839, loss_bbox=1.3295, matched_ious=0.4051, loss_iou=0.1042, loss_iou_reg=0.2705, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.31(1.27)  Time cost: 28:27/08:53 [1:06:04/21:18:04]  Acc_iter 3100        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.31(1.27)
2025-09-03 10:01:04,527   INFO  Train:    2/36 (  6%) [1390/1759 ( 79%)]  Loss: 5.061 (5.77)  LR: 4.017e-04  Grad: 8.8486  max=1.5922(module.vfe.pfn_layers.0.linear.weight)  min: -8.2679(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0686, loss_cls=0.2817, loss_bbox=1.2759, matched_ious=0.4124, loss_iou=0.1030, loss_iou_reg=0.2678, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.20(1.27)  Time cost: 29:30/07:49 [1:07:07/21:16:22]  Acc_iter 3150        Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.20(1.27)
2025-09-03 10:02:08,353   INFO  Train:    2/36 (  6%) [1440/1759 ( 82%)]  Loss: 5.246 (5.76)  LR: 4.049e-04  Grad: 10.0000  max=9.1864(module.vfe.pfn_layers.0.linear.weight)  min: -3.2952(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1292, loss_cls=0.2881, loss_bbox=1.3605, matched_ious=0.4025, loss_iou=0.1044, loss_iou_reg=0.2687, d_time=0.00(0.01), f_time=1.34(1.27), b_time=1.34(1.27)  Time cost: 30:34/06:46 [1:08:10/21:15:26]  Acc_iter 3200        Data time: 0.00(0.01)  Forward time: 1.34(1.27)  Batch time: 1.34(1.27)
2025-09-03 10:03:11,553   INFO  Train:    2/36 (  6%) [1490/1759 ( 85%)]  Loss: 6.588 (5.75)  LR: 4.081e-04  Grad: 9.4338  max=3.2678(module.vfe.pfn_layers.0.linear.weight)  min: -7.3016(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1238, loss_cls=0.2868, loss_bbox=1.3295, matched_ious=0.3944, loss_iou=0.1050, loss_iou_reg=0.2708, d_time=0.00(0.01), f_time=1.29(1.26), b_time=1.29(1.27)  Time cost: 31:37/05:42 [1:09:14/21:14:05]  Acc_iter 3250        Data time: 0.00(0.01)  Forward time: 1.29(1.26)  Batch time: 1.29(1.27)
2025-09-03 10:04:17,049   INFO  Train:    2/36 (  6%) [1540/1759 ( 88%)]  Loss: 5.077 (5.74)  LR: 4.114e-04  Grad: 6.9241  max=3.7905(module.vfe.pfn_layers.0.linear.weight)  min: -5.4296(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1312, loss_cls=0.2895, loss_bbox=1.3242, matched_ious=0.4021, loss_iou=0.1042, loss_iou_reg=0.2688, d_time=0.00(0.01), f_time=1.35(1.27), b_time=1.35(1.27)  Time cost: 32:42/04:38 [1:10:19/21:14:14]  Acc_iter 3300        Data time: 0.00(0.01)  Forward time: 1.35(1.27)  Batch time: 1.35(1.27)
2025-09-03 10:05:19,683   INFO  Train:    2/36 (  6%) [1590/1759 ( 90%)]  Loss: 4.570 (5.73)  LR: 4.148e-04  Grad: 4.8429  max=3.3175(module.vfe.pfn_layers.0.linear.weight)  min: -1.3390(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1059, loss_cls=0.2842, loss_bbox=1.3563, matched_ious=0.4040, loss_iou=0.1064, loss_iou_reg=0.2677, d_time=0.00(0.01), f_time=1.35(1.27), b_time=1.36(1.27)  Time cost: 33:45/03:35 [1:11:22/21:12:31]  Acc_iter 3350        Data time: 0.00(0.01)  Forward time: 1.35(1.27)  Batch time: 1.36(1.27)
2025-09-03 10:06:23,498   INFO  Train:    2/36 (  6%) [1640/1759 ( 93%)]  Loss: 5.023 (5.72)  LR: 4.182e-04  Grad: 10.0000  max=7.2478(module.vfe.pfn_layers.0.linear.weight)  min: -6.4544(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1228, loss_cls=0.2778, loss_bbox=1.3692, matched_ious=0.3955, loss_iou=0.1042, loss_iou_reg=0.2717, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.28(1.27)  Time cost: 34:49/02:31 [1:12:26/21:11:33]  Acc_iter 3400        Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.28(1.27)
2025-09-03 10:07:26,015   INFO  Train:    2/36 (  6%) [1690/1759 ( 96%)]  Loss: 5.622 (5.71)  LR: 4.217e-04  Grad: 10.0000  max=0.1819(module.vfe.pfn_layers.0.linear.weight)  min: -8.8889(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1231, loss_cls=0.2799, loss_bbox=1.4401, matched_ious=0.4032, loss_iou=0.1032, loss_iou_reg=0.2656, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.27)  Time cost: 35:51/01:27 [1:13:28/21:09:49]  Acc_iter 3450        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.27)
2025-09-03 10:08:29,679   INFO  Train:    2/36 (  6%) [1740/1759 ( 99%)]  Loss: 5.052 (5.70)  LR: 4.251e-04  Grad: 10.0000  max=9.3884(module.vfe.pfn_layers.0.linear.weight)  min: -3.2946(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0947, loss_cls=0.2720, loss_bbox=1.3549, matched_ious=0.4008, loss_iou=0.1029, loss_iou_reg=0.2700, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.24(1.27)  Time cost: 36:55/00:24 [1:14:32/21:08:47]  Acc_iter 3500        Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.24(1.27)
2025-09-03 10:08:51,237   INFO  Train:    2/36 (  6%) [1758/1759 (100%)]  Loss: 4.844 (5.70)  LR: 4.264e-04  Grad: 7.0827  max=2.9201(module.vfe.pfn_layers.0.linear.weight)  min: -4.7546(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0939, loss_cls=0.2759, loss_bbox=1.2548, matched_ious=0.4271, loss_iou=0.1064, loss_iou_reg=0.2521, d_time=0.00(0.01), f_time=0.69(1.26), b_time=0.69(1.27)  Time cost: 37:16/00:01 [1:14:53/21:07:38]  Acc_iter 3518        Data time: 0.00(0.01)  Forward time: 0.69(1.26)  Batch time: 0.69(1.27)

                                               [Aepochs:   6%|▌         | 2/36 [1:14:54<21:12:29, 2245.58s/it]epochs:   6%|▌         | 2/36 [1:14:54<21:12:30, 2245.60s/it]epochs:   6%|▌         | 2/36 [1:14:54<21:12:30, 2245.61s/it]epochs:   6%|▌         | 2/36 [1:14:54<21:12:30, 2245.61s/it]epochs:   6%|▌         | 2/36 [1:14:54<21:12:31, 2245.62s/it]epochs:   6%|▌         | 2/36 [1:14:54<21:12:31, 2245.62s/it]epochs:   6%|▌         | 2/36 [1:14:54<21:12:31, 2245.62s/it]epochs:   6%|▌         | 2/36 [1:14:54<21:12:34, 2245.73s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 10:08:56,837   INFO  Train:    3/36 (  8%) [   0/1759 (  0%)]  Loss: 4.732 (4.73)  LR: 4.265e-04  Grad: 5.3129  max=0.5021(module.vfe.pfn_layers.0.linear.weight)  min: -4.0926(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9773, loss_cls=0.2629, loss_bbox=1.0704, matched_ious=0.4600, loss_iou=0.1097, loss_iou_reg=0.2473, d_time=1.58(1.58), f_time=2.78(2.78), b_time=4.37(4.37)  Time cost: 00:03/1:50:55 [1:14:59/62:51:43]  Acc_iter 3519        Data time: 1.58(1.58)  Forward time: 2.78(2.78)  Batch time: 4.37(4.37)
2025-09-03 10:09:36,347   INFO  Train:    3/36 (  8%) [  31/1759 (  2%)]  Loss: 6.062 (5.51)  LR: 4.287e-04  Grad: 6.3194  max=0.6993(module.vfe.pfn_layers.0.linear.weight)  min: -5.0136(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.1591, loss_cls=0.2898, loss_bbox=1.4270, matched_ious=0.3922, loss_iou=0.1077, loss_iou_reg=0.2725, d_time=0.00(0.05), f_time=1.20(1.32), b_time=1.21(1.37)  Time cost: 00:43/38:57 [1:15:38/22:27:51]  Acc_iter 3550        Data time: 0.00(0.05)  Forward time: 1.20(1.32)  Batch time: 1.21(1.37)
2025-09-03 10:10:39,625   INFO  Train:    3/36 (  8%) [  81/1759 (  5%)]  Loss: 5.454 (5.43)  LR: 4.323e-04  Grad: 9.9121  max=3.0770(module.vfe.pfn_layers.0.linear.weight)  min: -9.2033(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0931, loss_cls=0.2732, loss_bbox=1.3162, matched_ious=0.4049, loss_iou=0.1021, loss_iou_reg=0.2703, d_time=0.02(0.02), f_time=1.23(1.28), b_time=1.25(1.31)  Time cost: 01:46/36:20 [1:16:42/21:33:41]  Acc_iter 3600        Data time: 0.02(0.02)  Forward time: 1.23(1.28)  Batch time: 1.25(1.31)
2025-09-03 10:11:42,301   INFO  Train:    3/36 (  8%) [ 131/1759 (  7%)]  Loss: 5.015 (5.40)  LR: 4.359e-04  Grad: 7.3426  max=1.5025(module.vfe.pfn_layers.0.linear.weight)  min: -6.7523(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0810, loss_cls=0.2714, loss_bbox=1.3013, matched_ious=0.4079, loss_iou=0.1067, loss_iou_reg=0.2655, d_time=0.01(0.02), f_time=1.23(1.27), b_time=1.24(1.29)  Time cost: 02:49/34:47 [1:17:44/21:15:15]  Acc_iter 3650        Data time: 0.01(0.02)  Forward time: 1.23(1.27)  Batch time: 1.24(1.29)
2025-09-03 10:12:45,700   INFO  Train:    3/36 (  8%) [ 181/1759 ( 10%)]  Loss: 5.233 (5.39)  LR: 4.396e-04  Grad: 3.2561  max=0.2361(module.vfe.pfn_layers.0.linear.weight)  min: -1.9394(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0973, loss_cls=0.2728, loss_bbox=1.3356, matched_ious=0.4034, loss_iou=0.1052, loss_iou_reg=0.2686, d_time=0.00(0.01), f_time=1.41(1.27), b_time=1.41(1.28)  Time cost: 03:52/33:37 [1:18:48/21:10:18]  Acc_iter 3700        Data time: 0.00(0.01)  Forward time: 1.41(1.27)  Batch time: 1.41(1.28)
2025-09-03 10:13:48,460   INFO  Train:    3/36 (  8%) [ 231/1759 ( 13%)]  Loss: 6.048 (5.38)  LR: 4.433e-04  Grad: 4.8501  max=0.7990(module.vfe.pfn_layers.0.linear.weight)  min: -3.4455(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0964, loss_cls=0.2789, loss_bbox=1.3166, matched_ious=0.4086, loss_iou=0.1027, loss_iou_reg=0.2668, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.28)  Time cost: 04:55/32:25 [1:19:50/21:04:17]  Acc_iter 3750        Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.28)
2025-09-03 10:14:54,573   INFO  Train:    3/36 (  8%) [ 281/1759 ( 16%)]  Loss: 4.836 (5.34)  LR: 4.471e-04  Grad: 8.1361  max=1.9631(module.vfe.pfn_layers.0.linear.weight)  min: -7.1154(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0616, loss_cls=0.2746, loss_bbox=1.2526, matched_ious=0.4164, loss_iou=0.1027, loss_iou_reg=0.2651, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.26(1.28)  Time cost: 06:01/31:34 [1:20:57/21:11:50]  Acc_iter 3800        Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.28)
2025-09-03 10:15:58,000   INFO  Train:    3/36 (  8%) [ 331/1759 ( 19%)]  Loss: 5.510 (5.33)  LR: 4.509e-04  Grad: 8.9818  max=7.8762(module.vfe.pfn_layers.0.linear.weight)  min: -0.1072(module.dense_head.heatmap_head.1.weight)  NaN: False  loss_hm=1.0819, loss_cls=0.2709, loss_bbox=1.3212, matched_ious=0.4103, loss_iou=0.1024, loss_iou_reg=0.2664, d_time=0.00(0.01), f_time=1.18(1.27), b_time=1.18(1.28)  Time cost: 07:04/30:27 [1:22:00/21:08:45]  Acc_iter 3850        Data time: 0.00(0.01)  Forward time: 1.18(1.27)  Batch time: 1.18(1.28)
2025-09-03 10:17:01,164   INFO  Train:    3/36 (  8%) [ 381/1759 ( 22%)]  Loss: 4.561 (5.30)  LR: 4.548e-04  Grad: 10.0000  max=6.9562(module.vfe.pfn_layers.0.linear.weight)  min: -0.1824(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.0430, loss_cls=0.2683, loss_bbox=1.2404, matched_ious=0.4125, loss_iou=0.1050, loss_iou_reg=0.2671, d_time=0.01(0.01), f_time=1.18(1.27), b_time=1.18(1.28)  Time cost: 08:08/29:20 [1:23:03/21:05:32]  Acc_iter 3900        Data time: 0.01(0.01)  Forward time: 1.18(1.27)  Batch time: 1.18(1.28)
2025-09-03 10:18:04,107   INFO  Train:    3/36 (  8%) [ 431/1759 ( 25%)]  Loss: 5.670 (5.28)  LR: 4.587e-04  Grad: 10.0000  max=2.7746(module.vfe.pfn_layers.0.linear.weight)  min: -9.2523(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0386, loss_cls=0.2633, loss_bbox=1.2420, matched_ious=0.4117, loss_iou=0.1068, loss_iou_reg=0.2678, d_time=0.00(0.01), f_time=1.35(1.27), b_time=1.36(1.28)  Time cost: 09:11/28:13 [1:24:06/21:02:18]  Acc_iter 3950        Data time: 0.00(0.01)  Forward time: 1.35(1.27)  Batch time: 1.36(1.28)
2025-09-03 10:19:08,698   INFO  Train:    3/36 (  8%) [ 481/1759 ( 27%)]  Loss: 5.643 (5.28)  LR: 4.627e-04  Grad: 3.7007  max=1.3493(module.vfe.pfn_layers.0.linear.weight)  min: -2.5474(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0788, loss_cls=0.2674, loss_bbox=1.3494, matched_ious=0.4131, loss_iou=0.1040, loss_iou_reg=0.2649, d_time=0.01(0.01), f_time=1.23(1.27), b_time=1.23(1.28)  Time cost: 10:15/27:12 [1:25:11/21:02:54]  Acc_iter 4000        Data time: 0.01(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.28)
2025-09-03 10:20:11,157   INFO  Train:    3/36 (  8%) [ 531/1759 ( 30%)]  Loss: 5.454 (5.27)  LR: 4.667e-04  Grad: 8.8025  max=5.4547(module.vfe.pfn_layers.0.linear.weight)  min: -0.2907(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=1.0491, loss_cls=0.2621, loss_bbox=1.2223, matched_ious=0.4049, loss_iou=0.1059, loss_iou_reg=0.2717, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.27(1.28)  Time cost: 11:18/26:05 [1:26:13/20:59:14]  Acc_iter 4050        Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.28)
2025-09-03 10:21:13,563   INFO  Train:    3/36 (  8%) [ 581/1759 ( 33%)]  Loss: 5.251 (5.26)  LR: 4.707e-04  Grad: 9.4751  max=6.3305(module.vfe.pfn_layers.0.linear.weight)  min: -6.5254(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0521, loss_cls=0.2642, loss_bbox=1.2850, matched_ious=0.4103, loss_iou=0.1063, loss_iou_reg=0.2676, d_time=0.03(0.01), f_time=1.17(1.26), b_time=1.20(1.27)  Time cost: 12:20/24:58 [1:27:16/20:55:55]  Acc_iter 4100        Data time: 0.03(0.01)  Forward time: 1.17(1.26)  Batch time: 1.20(1.27)
2025-09-03 10:22:16,795   INFO  Train:    3/36 (  8%) [ 631/1759 ( 36%)]  Loss: 4.368 (5.27)  LR: 4.748e-04  Grad: 4.4648  max=3.0842(module.vfe.pfn_layers.0.linear.weight)  min: -1.2867(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0888, loss_cls=0.2738, loss_bbox=1.3082, matched_ious=0.4046, loss_iou=0.1047, loss_iou_reg=0.2706, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.26(1.27)  Time cost: 13:23/23:54 [1:28:19/20:54:15]  Acc_iter 4150        Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.27)
2025-09-03 10:23:20,377   INFO  Train:    3/36 (  8%) [ 681/1759 ( 39%)]  Loss: 5.987 (5.27)  LR: 4.790e-04  Grad: 7.1580  max=3.8144(module.vfe.pfn_layers.0.linear.weight)  min: -5.3235(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0660, loss_cls=0.2643, loss_bbox=1.2784, matched_ious=0.4071, loss_iou=0.1054, loss_iou_reg=0.2692, d_time=0.00(0.01), f_time=1.35(1.26), b_time=1.35(1.27)  Time cost: 14:27/22:50 [1:29:22/20:53:11]  Acc_iter 4200        Data time: 0.00(0.01)  Forward time: 1.35(1.26)  Batch time: 1.35(1.27)
2025-09-03 10:24:23,892   INFO  Train:    3/36 (  8%) [ 731/1759 ( 42%)]  Loss: 5.467 (5.25)  LR: 4.832e-04  Grad: 10.0000  max=5.6655(module.vfe.pfn_layers.0.linear.weight)  min: -8.0053(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9951, loss_cls=0.2556, loss_bbox=1.1944, matched_ious=0.4214, loss_iou=0.1038, loss_iou_reg=0.2626, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.27(1.27)  Time cost: 15:30/21:47 [1:30:26/20:52:02]  Acc_iter 4250        Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.27(1.27)
2025-09-03 10:25:30,028   INFO  Train:    3/36 (  8%) [ 781/1759 ( 44%)]  Loss: 5.546 (5.25)  LR: 4.874e-04  Grad: 7.6440  max=0.4048(module.vfe.pfn_layers.0.linear.weight)  min: -7.1405(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0688, loss_cls=0.2595, loss_bbox=1.2885, matched_ious=0.4115, loss_iou=0.1066, loss_iou_reg=0.2662, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.28)  Time cost: 16:36/20:46 [1:31:32/20:54:11]  Acc_iter 4300        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.28)
2025-09-03 10:26:32,857   INFO  Train:    3/36 (  8%) [ 831/1759 ( 47%)]  Loss: 5.256 (5.24)  LR: 4.917e-04  Grad: 10.0000  max=0.1850(module.backbone_3d.cls_conv.3.weight)  min: -8.8517(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0265, loss_cls=0.2590, loss_bbox=1.2386, matched_ious=0.4122, loss_iou=0.1052, loss_iou_reg=0.2695, d_time=0.00(0.01), f_time=1.19(1.27), b_time=1.20(1.27)  Time cost: 17:39/19:42 [1:32:35/20:52:02]  Acc_iter 4350        Data time: 0.00(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.27)
2025-09-03 10:27:35,538   INFO  Train:    3/36 (  8%) [ 881/1759 ( 50%)]  Loss: 5.011 (5.23)  LR: 4.960e-04  Grad: 10.0000  max=8.8769(module.vfe.pfn_layers.0.linear.weight)  min: -3.9856(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0466, loss_cls=0.2623, loss_bbox=1.1972, matched_ious=0.4265, loss_iou=0.1059, loss_iou_reg=0.2613, d_time=0.00(0.01), f_time=1.14(1.27), b_time=1.15(1.27)  Time cost: 18:42/18:37 [1:33:38/20:49:51]  Acc_iter 4400        Data time: 0.00(0.01)  Forward time: 1.14(1.27)  Batch time: 1.15(1.27)
2025-09-03 10:28:39,609   INFO  Train:    3/36 (  8%) [ 931/1759 ( 53%)]  Loss: 5.588 (5.23)  LR: 5.004e-04  Grad: 5.3701  max=2.4984(module.vfe.pfn_layers.0.linear.weight)  min: -4.2239(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0454, loss_cls=0.2635, loss_bbox=1.2393, matched_ious=0.4204, loss_iou=0.1051, loss_iou_reg=0.2611, d_time=0.00(0.01), f_time=1.31(1.27), b_time=1.31(1.27)  Time cost: 19:46/17:34 [1:34:42/20:49:15]  Acc_iter 4450        Data time: 0.00(0.01)  Forward time: 1.31(1.27)  Batch time: 1.31(1.27)
2025-09-03 10:29:41,910   INFO  Train:    3/36 (  8%) [ 981/1759 ( 56%)]  Loss: 4.501 (5.21)  LR: 5.048e-04  Grad: 4.2405  max=2.1493(module.vfe.pfn_layers.0.linear.weight)  min: -2.9888(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0250, loss_cls=0.2564, loss_bbox=1.1979, matched_ious=0.4261, loss_iou=0.1025, loss_iou_reg=0.2620, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 20:48/16:29 [1:35:44/20:46:50]  Acc_iter 4500        Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 10:30:44,351   INFO  Train:    3/36 (  8%) [1031/1759 ( 59%)]  Loss: 5.578 (5.21)  LR: 5.092e-04  Grad: 8.5937  max=5.9957(module.vfe.pfn_layers.0.linear.weight)  min: -1.8243(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0524, loss_cls=0.2557, loss_bbox=1.2947, matched_ious=0.4171, loss_iou=0.1058, loss_iou_reg=0.2640, d_time=0.00(0.01), f_time=1.31(1.26), b_time=1.32(1.27)  Time cost: 21:51/15:25 [1:36:46/20:44:41]  Acc_iter 4550        Data time: 0.00(0.01)  Forward time: 1.31(1.26)  Batch time: 1.32(1.27)
2025-09-03 10:31:48,032   INFO  Train:    3/36 (  8%) [1081/1759 ( 61%)]  Loss: 5.002 (5.21)  LR: 5.137e-04  Grad: 4.6522  max=3.6221(module.vfe.pfn_layers.0.linear.weight)  min: -1.7594(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0260, loss_cls=0.2486, loss_bbox=1.2761, matched_ious=0.4202, loss_iou=0.1066, loss_iou_reg=0.2651, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 22:54/14:21 [1:37:50/20:43:46]  Acc_iter 4600        Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 10:32:51,407   INFO  Train:    3/36 (  8%) [1131/1759 ( 64%)]  Loss: 5.309 (5.20)  LR: 5.183e-04  Grad: 5.6334  max=1.4199(module.vfe.pfn_layers.0.linear.weight)  min: -5.1224(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0250, loss_cls=0.2479, loss_bbox=1.2692, matched_ious=0.4213, loss_iou=0.1040, loss_iou_reg=0.2574, d_time=0.00(0.01), f_time=1.33(1.26), b_time=1.34(1.27)  Time cost: 23:58/13:17 [1:38:53/20:42:34]  Acc_iter 4650        Data time: 0.00(0.01)  Forward time: 1.33(1.26)  Batch time: 1.34(1.27)
2025-09-03 10:33:55,401   INFO  Train:    3/36 (  8%) [1181/1759 ( 67%)]  Loss: 4.734 (5.20)  LR: 5.229e-04  Grad: 5.7699  max=4.9384(module.vfe.pfn_layers.0.linear.weight)  min: -2.1329(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0567, loss_cls=0.2573, loss_bbox=1.3008, matched_ious=0.4180, loss_iou=0.1050, loss_iou_reg=0.2638, d_time=0.04(0.01), f_time=1.48(1.26), b_time=1.51(1.27)  Time cost: 25:02/12:14 [1:39:57/20:41:53]  Acc_iter 4700        Data time: 0.04(0.01)  Forward time: 1.48(1.26)  Batch time: 1.51(1.27)
2025-09-03 10:34:58,330   INFO  Train:    3/36 (  8%) [1231/1759 ( 70%)]  Loss: 6.418 (5.20)  LR: 5.275e-04  Grad: 3.9821  max=1.2371(module.vfe.pfn_layers.0.linear.weight)  min: -1.9910(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0601, loss_cls=0.2548, loss_bbox=1.3224, matched_ious=0.4133, loss_iou=0.1070, loss_iou_reg=0.2656, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.21(1.27)  Time cost: 26:05/11:10 [1:41:00/20:40:20]  Acc_iter 4750        Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.27)
2025-09-03 10:36:04,733   INFO  Train:    3/36 (  8%) [1281/1759 ( 73%)]  Loss: 4.405 (5.20)  LR: 5.322e-04  Grad: 8.7937  max=8.5221(module.vfe.pfn_layers.0.linear.weight)  min: -0.6576(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0602, loss_cls=0.2554, loss_bbox=1.2884, matched_ious=0.4112, loss_iou=0.1057, loss_iou_reg=0.2668, d_time=0.00(0.01), f_time=1.42(1.27), b_time=1.43(1.27)  Time cost: 27:11/10:08 [1:42:07/20:41:28]  Acc_iter 4800        Data time: 0.00(0.01)  Forward time: 1.42(1.27)  Batch time: 1.43(1.27)
2025-09-03 10:37:07,976   INFO  Train:    3/36 (  8%) [1331/1759 ( 76%)]  Loss: 5.355 (5.20)  LR: 5.369e-04  Grad: 3.3112  max=1.6315(module.vfe.pfn_layers.0.linear.weight)  min: -0.3272(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=1.0279, loss_cls=0.2523, loss_bbox=1.2192, matched_ious=0.4267, loss_iou=0.1025, loss_iou_reg=0.2617, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.27(1.27)  Time cost: 28:14/09:04 [1:43:10/20:40:07]  Acc_iter 4850        Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.27(1.27)
2025-09-03 10:38:12,500   INFO  Train:    3/36 (  8%) [1381/1759 ( 79%)]  Loss: 5.130 (5.19)  LR: 5.416e-04  Grad: 5.3568  max=1.8093(module.vfe.pfn_layers.0.linear.weight)  min: -4.6247(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0322, loss_cls=0.2490, loss_bbox=1.2530, matched_ious=0.4274, loss_iou=0.1046, loss_iou_reg=0.2593, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.26(1.27)  Time cost: 29:19/08:01 [1:44:15/20:39:41]  Acc_iter 4900        Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.26(1.27)
2025-09-03 10:39:15,607   INFO  Train:    3/36 (  8%) [1431/1759 ( 81%)]  Loss: 4.920 (5.18)  LR: 5.464e-04  Grad: 9.0519  max=8.3812(module.vfe.pfn_layers.0.linear.weight)  min: -2.4860(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9854, loss_cls=0.2438, loss_bbox=1.1818, matched_ious=0.4205, loss_iou=0.1041, loss_iou_reg=0.2653, d_time=0.00(0.01), f_time=1.14(1.27), b_time=1.15(1.27)  Time cost: 30:22/06:57 [1:45:18/20:38:15]  Acc_iter 4950        Data time: 0.00(0.01)  Forward time: 1.14(1.27)  Batch time: 1.15(1.27)
2025-09-03 10:40:18,864   INFO  Train:    3/36 (  8%) [1481/1759 ( 84%)]  Loss: 4.690 (5.18)  LR: 5.513e-04  Grad: 7.4866  max=7.0339(module.vfe.pfn_layers.0.linear.weight)  min: -1.8679(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0422, loss_cls=0.2568, loss_bbox=1.2032, matched_ious=0.4250, loss_iou=0.1070, loss_iou_reg=0.2626, d_time=0.00(0.01), f_time=1.16(1.27), b_time=1.17(1.27)  Time cost: 31:25/05:53 [1:46:21/20:36:57]  Acc_iter 5000        Data time: 0.00(0.01)  Forward time: 1.16(1.27)  Batch time: 1.17(1.27)
2025-09-03 10:41:22,335   INFO  Train:    3/36 (  8%) [1531/1759 ( 87%)]  Loss: 4.937 (5.17)  LR: 5.562e-04  Grad: 7.3637  max=1.4507(module.vfe.pfn_layers.0.linear.weight)  min: -5.1057(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0160, loss_cls=0.2509, loss_bbox=1.1636, matched_ious=0.4340, loss_iou=0.1056, loss_iou_reg=0.2595, d_time=0.00(0.01), f_time=1.34(1.27), b_time=1.35(1.27)  Time cost: 32:29/04:50 [1:47:24/20:35:47]  Acc_iter 5050        Data time: 0.00(0.01)  Forward time: 1.34(1.27)  Batch time: 1.35(1.27)
2025-09-03 10:42:24,972   INFO  Train:    3/36 (  8%) [1581/1759 ( 90%)]  Loss: 4.812 (5.16)  LR: 5.611e-04  Grad: 6.3817  max=3.8126(module.vfe.pfn_layers.0.linear.weight)  min: -0.0797(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0203, loss_cls=0.2426, loss_bbox=1.2073, matched_ious=0.4202, loss_iou=0.1039, loss_iou_reg=0.2617, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.26(1.27)  Time cost: 33:31/03:46 [1:48:27/20:34:08]  Acc_iter 5100        Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.27)
2025-09-03 10:43:27,904   INFO  Train:    3/36 (  8%) [1631/1759 ( 93%)]  Loss: 4.620 (5.16)  LR: 5.661e-04  Grad: 4.0014  max=2.8000(module.vfe.pfn_layers.0.linear.weight)  min: -0.1907(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.9896, loss_cls=0.2448, loss_bbox=1.1431, matched_ious=0.4281, loss_iou=0.1050, loss_iou_reg=0.2610, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.19(1.27)  Time cost: 34:34/02:42 [1:49:30/20:32:41]  Acc_iter 5150        Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.27)
2025-09-03 10:44:31,326   INFO  Train:    3/36 (  8%) [1681/1759 ( 96%)]  Loss: 6.536 (5.15)  LR: 5.711e-04  Grad: 7.2338  max=6.0254(module.vfe.pfn_layers.0.linear.weight)  min: -2.0883(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0061, loss_cls=0.2430, loss_bbox=1.2178, matched_ious=0.4287, loss_iou=0.1031, loss_iou_reg=0.2580, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.28(1.27)  Time cost: 35:38/01:39 [1:50:33/20:31:32]  Acc_iter 5200        Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.27)
2025-09-03 10:45:34,665   INFO  Train:    3/36 (  8%) [1731/1759 ( 98%)]  Loss: 4.741 (5.14)  LR: 5.761e-04  Grad: 5.4381  max=4.6019(module.vfe.pfn_layers.0.linear.weight)  min: -1.7478(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9888, loss_cls=0.2437, loss_bbox=1.1587, matched_ious=0.4313, loss_iou=0.1035, loss_iou_reg=0.2607, d_time=0.00(0.01), f_time=1.14(1.26), b_time=1.14(1.27)  Time cost: 36:41/00:35 [1:51:37/20:30:21]  Acc_iter 5250        Data time: 0.00(0.01)  Forward time: 1.14(1.26)  Batch time: 1.14(1.27)
2025-09-03 10:46:10,330   INFO  Train:    3/36 (  8%) [1758/1759 (100%)]  Loss: 5.229 (5.14)  LR: 5.789e-04  Grad: 4.8700  max=3.5266(module.vfe.pfn_layers.0.linear.weight)  min: -1.8940(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0450, loss_cls=0.2537, loss_bbox=1.2300, matched_ious=0.4234, loss_iou=0.1034, loss_iou_reg=0.2672, d_time=0.00(0.01), f_time=0.70(1.27), b_time=0.71(1.27)  Time cost: 37:17/00:01 [1:52:12/20:30:31]  Acc_iter 5277        Data time: 0.00(0.01)  Forward time: 0.70(1.27)  Batch time: 0.71(1.27)

                                               [Aepochs:   8%|▊         | 3/36 [1:52:13<20:33:28, 2242.70s/it]epochs:   8%|▊         | 3/36 [1:52:13<20:33:29, 2242.71s/it]epochs:   8%|▊         | 3/36 [1:52:13<20:33:28, 2242.70s/it]epochs:   8%|▊         | 3/36 [1:52:13<20:33:28, 2242.70s/it]epochs:   8%|▊         | 3/36 [1:52:13<20:33:29, 2242.70s/it]epochs:   8%|▊         | 3/36 [1:52:13<20:33:29, 2242.70s/it]epochs:   8%|▊         | 3/36 [1:52:13<20:33:29, 2242.70s/it]epochs:   8%|▊         | 3/36 [1:52:13<20:33:31, 2242.77s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 10:46:15,997   INFO  Train:    4/36 ( 11%) [   0/1759 (  0%)]  Loss: 4.844 (4.84)  LR: 5.790e-04  Grad: 7.3853  max=7.0943(module.vfe.pfn_layers.0.linear.weight)  min: -0.6613(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0595, loss_cls=0.2568, loss_bbox=1.1478, matched_ious=0.4200, loss_iou=0.0984, loss_iou_reg=0.2581, d_time=1.62(1.62), f_time=2.63(2.63), b_time=4.25(4.25)  Time cost: 00:03/1:42:05 [1:52:18/56:09:12]  Acc_iter 5278        Data time: 1.62(1.62)  Forward time: 2.63(2.63)  Batch time: 4.25(4.25)
2025-09-03 10:46:44,254   INFO  Train:    4/36 ( 11%) [  22/1759 (  1%)]  Loss: 4.117 (4.79)  LR: 5.812e-04  Grad: 8.8758  max=0.1834(module.backbone_3d.cls_conv.3.weight)  min: -8.5454(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9858, loss_cls=0.2395, loss_bbox=1.1257, matched_ious=0.4394, loss_iou=0.1000, loss_iou_reg=0.2580, d_time=0.00(0.08), f_time=1.29(1.34), b_time=1.29(1.41)  Time cost: 00:31/39:57 [1:52:46/22:14:32]  Acc_iter 5300        Data time: 0.00(0.08)  Forward time: 1.29(1.34)  Batch time: 1.29(1.41)
2025-09-03 10:47:47,953   INFO  Train:    4/36 ( 11%) [  72/1759 (  4%)]  Loss: 4.007 (4.89)  LR: 5.864e-04  Grad: 6.1697  max=0.6101(module.vfe.pfn_layers.0.linear.weight)  min: -4.6628(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9970, loss_cls=0.2410, loss_bbox=1.1933, matched_ious=0.4224, loss_iou=0.1048, loss_iou_reg=0.2621, d_time=0.00(0.04), f_time=1.31(1.28), b_time=1.32(1.32)  Time cost: 01:35/36:45 [1:53:50/21:03:14]  Acc_iter 5350        Data time: 0.00(0.04)  Forward time: 1.31(1.28)  Batch time: 1.32(1.32)
2025-09-03 10:48:50,481   INFO  Train:    4/36 ( 11%) [ 122/1759 (  7%)]  Loss: 5.425 (4.94)  LR: 5.915e-04  Grad: 3.9445  max=3.1431(module.vfe.pfn_layers.0.linear.weight)  min: -0.1559(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.0343, loss_cls=0.2451, loss_bbox=1.1920, matched_ious=0.4311, loss_iou=0.1036, loss_iou_reg=0.2588, d_time=0.00(0.02), f_time=1.19(1.27), b_time=1.19(1.29)  Time cost: 02:37/35:02 [1:54:52/20:39:51]  Acc_iter 5400        Data time: 0.00(0.02)  Forward time: 1.19(1.27)  Batch time: 1.19(1.29)
2025-09-03 10:49:54,309   INFO  Train:    4/36 ( 11%) [ 172/1759 ( 10%)]  Loss: 5.334 (4.93)  LR: 5.968e-04  Grad: 6.6993  max=0.8384(module.vfe.pfn_layers.0.linear.weight)  min: -4.9674(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9828, loss_cls=0.2413, loss_bbox=1.1351, matched_ious=0.4285, loss_iou=0.1025, loss_iou_reg=0.2612, d_time=0.01(0.02), f_time=1.24(1.27), b_time=1.25(1.29)  Time cost: 03:41/33:54 [1:55:56/20:36:39]  Acc_iter 5450        Data time: 0.01(0.02)  Forward time: 1.24(1.27)  Batch time: 1.25(1.29)
2025-09-03 10:50:57,573   INFO  Train:    4/36 ( 11%) [ 222/1759 ( 13%)]  Loss: 5.558 (4.93)  LR: 6.020e-04  Grad: 4.2740  max=3.8677(module.vfe.pfn_layers.0.linear.weight)  min: -0.3366(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0084, loss_cls=0.2415, loss_bbox=1.1916, matched_ious=0.4269, loss_iou=0.1030, loss_iou_reg=0.2624, d_time=0.01(0.02), f_time=1.31(1.27), b_time=1.32(1.28)  Time cost: 04:45/32:44 [1:57:00/20:31:57]  Acc_iter 5500        Data time: 0.01(0.02)  Forward time: 1.31(1.27)  Batch time: 1.32(1.28)
2025-09-03 10:52:00,783   INFO  Train:    4/36 ( 11%) [ 272/1759 ( 15%)]  Loss: 4.798 (4.94)  LR: 6.073e-04  Grad: 3.2180  max=0.1070(module.dense_head.prediction_head.height.1.weight)  min: -2.7183(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0068, loss_cls=0.2422, loss_bbox=1.2258, matched_ious=0.4244, loss_iou=0.1068, loss_iou_reg=0.2627, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.26(1.28)  Time cost: 05:48/31:36 [1:58:03/20:28:24]  Acc_iter 5550        Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.28)
2025-09-03 10:53:03,627   INFO  Train:    4/36 ( 11%) [ 322/1759 ( 18%)]  Loss: 5.118 (4.95)  LR: 6.127e-04  Grad: 6.7407  max=0.2938(module.backbone_3d.cls_conv.3.weight)  min: -5.7190(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0106, loss_cls=0.2409, loss_bbox=1.1962, matched_ious=0.4282, loss_iou=0.1027, loss_iou_reg=0.2611, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.20(1.28)  Time cost: 06:51/30:29 [1:59:06/20:24:32]  Acc_iter 5600        Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.28)
2025-09-03 10:54:06,804   INFO  Train:    4/36 ( 11%) [ 372/1759 ( 21%)]  Loss: 4.945 (4.95)  LR: 6.180e-04  Grad: 2.9852  max=1.9224(module.vfe.pfn_layers.0.linear.weight)  min: -0.2385(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0048, loss_cls=0.2470, loss_bbox=1.1815, matched_ious=0.4282, loss_iou=0.1052, loss_iou_reg=0.2597, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.26(1.27)  Time cost: 07:54/29:23 [2:00:09/20:22:17]  Acc_iter 5650        Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.27)
2025-09-03 10:55:10,540   INFO  Train:    4/36 ( 11%) [ 422/1759 ( 24%)]  Loss: 5.341 (4.95)  LR: 6.234e-04  Grad: 5.3726  max=4.1906(module.vfe.pfn_layers.0.linear.weight)  min: -0.4079(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.0036, loss_cls=0.2410, loss_bbox=1.1935, matched_ious=0.4278, loss_iou=0.1040, loss_iou_reg=0.2610, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.22(1.27)  Time cost: 08:58/28:20 [2:01:13/20:21:35]  Acc_iter 5700        Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.27)
2025-09-03 10:56:12,814   INFO  Train:    4/36 ( 11%) [ 472/1759 ( 27%)]  Loss: 4.794 (4.94)  LR: 6.289e-04  Grad: 2.4737  max=1.5167(module.vfe.pfn_layers.0.linear.weight)  min: -0.1942(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.9688, loss_cls=0.2327, loss_bbox=1.1200, matched_ious=0.4389, loss_iou=0.1065, loss_iou_reg=0.2598, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.19(1.27)  Time cost: 10:00/27:13 [2:02:15/20:17:50]  Acc_iter 5750        Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.19(1.27)
2025-09-03 10:57:19,754   INFO  Train:    4/36 ( 11%) [ 522/1759 ( 30%)]  Loss: 4.515 (4.93)  LR: 6.344e-04  Grad: 4.3507  max=1.2982(module.vfe.pfn_layers.0.linear.weight)  min: -3.0538(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9921, loss_cls=0.2364, loss_bbox=1.1554, matched_ious=0.4362, loss_iou=0.1027, loss_iou_reg=0.2568, d_time=0.00(0.01), f_time=1.17(1.27), b_time=1.17(1.28)  Time cost: 11:07/26:18 [2:03:22/20:23:10]  Acc_iter 5800        Data time: 0.00(0.01)  Forward time: 1.17(1.27)  Batch time: 1.17(1.28)
2025-09-03 10:58:22,107   INFO  Train:    4/36 ( 11%) [ 572/1759 ( 33%)]  Loss: 4.928 (4.92)  LR: 6.399e-04  Grad: 5.2792  max=3.6848(module.vfe.pfn_layers.0.linear.weight)  min: -3.2569(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9925, loss_cls=0.2365, loss_bbox=1.1480, matched_ious=0.4226, loss_iou=0.1055, loss_iou_reg=0.2654, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.25(1.27)  Time cost: 12:09/25:11 [2:04:24/20:19:42]  Acc_iter 5850        Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.27)
2025-09-03 10:59:25,366   INFO  Train:    4/36 ( 11%) [ 622/1759 ( 35%)]  Loss: 4.845 (4.92)  LR: 6.455e-04  Grad: 3.9668  max=2.8578(module.vfe.pfn_layers.0.linear.weight)  min: -0.1818(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=1.0085, loss_cls=0.2442, loss_bbox=1.1782, matched_ious=0.4354, loss_iou=0.1011, loss_iou_reg=0.2595, d_time=0.01(0.01), f_time=1.23(1.26), b_time=1.24(1.27)  Time cost: 13:12/24:06 [2:05:27/20:18:01]  Acc_iter 5900        Data time: 0.01(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.27)
2025-09-03 11:00:29,384   INFO  Train:    4/36 ( 11%) [ 672/1759 ( 38%)]  Loss: 4.248 (4.92)  LR: 6.511e-04  Grad: 5.6610  max=4.3434(module.vfe.pfn_layers.0.linear.weight)  min: -0.4255(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=1.0006, loss_cls=0.2335, loss_bbox=1.1300, matched_ious=0.4372, loss_iou=0.1038, loss_iou_reg=0.2571, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 14:16/23:03 [2:06:31/20:17:30]  Acc_iter 5950        Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-03 11:01:32,791   INFO  Train:    4/36 ( 11%) [ 722/1759 ( 41%)]  Loss: 5.144 (4.91)  LR: 6.568e-04  Grad: 8.9873  max=5.6723(module.vfe.pfn_layers.0.linear.weight)  min: -0.1284(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9887, loss_cls=0.2361, loss_bbox=1.1700, matched_ious=0.4377, loss_iou=0.1045, loss_iou_reg=0.2555, d_time=0.01(0.01), f_time=1.27(1.26), b_time=1.28(1.27)  Time cost: 15:20/21:59 [2:07:35/20:16:06]  Acc_iter 6000        Data time: 0.01(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.27)
2025-09-03 11:02:36,468   INFO  Train:    4/36 ( 11%) [ 772/1759 ( 44%)]  Loss: 3.797 (4.91)  LR: 6.625e-04  Grad: 6.5642  max=6.2036(module.vfe.pfn_layers.0.linear.weight)  min: -0.3850(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=1.0080, loss_cls=0.2373, loss_bbox=1.1743, matched_ious=0.4351, loss_iou=0.1021, loss_iou_reg=0.2558, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.27(1.27)  Time cost: 16:23/20:56 [2:08:38/20:15:05]  Acc_iter 6050        Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.27)
2025-09-03 11:03:39,898   INFO  Train:    4/36 ( 11%) [ 822/1759 ( 47%)]  Loss: 4.244 (4.91)  LR: 6.682e-04  Grad: 3.8242  max=2.1465(module.vfe.pfn_layers.0.linear.weight)  min: -1.7091(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9982, loss_cls=0.2340, loss_bbox=1.1586, matched_ious=0.4367, loss_iou=0.1014, loss_iou_reg=0.2570, d_time=0.00(0.01), f_time=1.39(1.26), b_time=1.39(1.27)  Time cost: 17:27/19:52 [2:09:42/20:13:46]  Acc_iter 6100        Data time: 0.00(0.01)  Forward time: 1.39(1.26)  Batch time: 1.39(1.27)
2025-09-03 11:04:42,872   INFO  Train:    4/36 ( 11%) [ 872/1759 ( 50%)]  Loss: 4.603 (4.91)  LR: 6.740e-04  Grad: 6.9609  max=4.4539(module.vfe.pfn_layers.0.linear.weight)  min: -4.9815(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9856, loss_cls=0.2286, loss_bbox=1.2339, matched_ious=0.4303, loss_iou=0.1012, loss_iou_reg=0.2589, d_time=0.01(0.01), f_time=1.20(1.26), b_time=1.21(1.27)  Time cost: 18:30/18:48 [2:10:45/20:12:00]  Acc_iter 6150        Data time: 0.01(0.01)  Forward time: 1.20(1.26)  Batch time: 1.21(1.27)
2025-09-03 11:05:46,726   INFO  Train:    4/36 ( 11%) [ 922/1759 ( 52%)]  Loss: 5.678 (4.90)  LR: 6.798e-04  Grad: 5.0796  max=1.1880(module.vfe.pfn_layers.0.linear.weight)  min: -4.6016(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9524, loss_cls=0.2243, loss_bbox=1.1870, matched_ious=0.4385, loss_iou=0.1006, loss_iou_reg=0.2585, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.18(1.27)  Time cost: 19:34/17:44 [2:11:49/20:11:12]  Acc_iter 6200        Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.18(1.27)
2025-09-03 11:06:50,833   INFO  Train:    4/36 ( 11%) [ 972/1759 ( 55%)]  Loss: 4.882 (4.90)  LR: 6.856e-04  Grad: 8.3260  max=7.6257(module.vfe.pfn_layers.0.linear.weight)  min: -0.3136(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.9881, loss_cls=0.2251, loss_bbox=1.1880, matched_ious=0.4348, loss_iou=0.1039, loss_iou_reg=0.2583, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.19(1.27)  Time cost: 20:38/16:41 [2:12:53/20:10:38]  Acc_iter 6250        Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.27)
2025-09-03 11:07:56,496   INFO  Train:    4/36 ( 11%) [1022/1759 ( 58%)]  Loss: 5.040 (4.89)  LR: 6.915e-04  Grad: 2.3165  max=0.2675(module.vfe.pfn_layers.0.linear.weight)  min: -1.5638(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9638, loss_cls=0.2230, loss_bbox=1.0896, matched_ious=0.4332, loss_iou=0.1017, loss_iou_reg=0.2592, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.30(1.28)  Time cost: 21:43/15:39 [2:13:59/20:11:27]  Acc_iter 6300        Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.30(1.28)
2025-09-03 11:08:59,415   INFO  Train:    4/36 ( 11%) [1072/1759 ( 61%)]  Loss: 3.828 (4.89)  LR: 6.974e-04  Grad: 5.0367  max=4.3131(module.vfe.pfn_layers.0.linear.weight)  min: -1.7310(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9759, loss_cls=0.2287, loss_bbox=1.0768, matched_ious=0.4477, loss_iou=0.1049, loss_iou_reg=0.2535, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.24(1.27)  Time cost: 22:46/14:35 [2:15:01/20:09:40]  Acc_iter 6350        Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.27)
2025-09-03 11:10:02,091   INFO  Train:    4/36 ( 11%) [1122/1759 ( 64%)]  Loss: 4.318 (4.88)  LR: 7.033e-04  Grad: 5.4634  max=1.0181(module.vfe.pfn_layers.0.linear.weight)  min: -5.0030(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9523, loss_cls=0.2237, loss_bbox=1.1173, matched_ious=0.4471, loss_iou=0.1009, loss_iou_reg=0.2527, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.23(1.27)  Time cost: 23:49/13:30 [2:16:04/20:07:45]  Acc_iter 6400        Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.23(1.27)
2025-09-03 11:11:05,047   INFO  Train:    4/36 ( 11%) [1172/1759 ( 67%)]  Loss: 4.900 (4.87)  LR: 7.093e-04  Grad: 2.6196  max=2.0474(module.vfe.pfn_layers.0.linear.weight)  min: -0.1244(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.9444, loss_cls=0.2209, loss_bbox=1.0856, matched_ious=0.4412, loss_iou=0.1040, loss_iou_reg=0.2583, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.20(1.27)  Time cost: 24:52/12:26 [2:17:07/20:06:08]  Acc_iter 6450        Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.27)
2025-09-03 11:12:08,462   INFO  Train:    4/36 ( 11%) [1222/1759 ( 69%)]  Loss: 4.045 (4.86)  LR: 7.154e-04  Grad: 6.8186  max=3.3267(module.vfe.pfn_layers.0.linear.weight)  min: -5.5569(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9513, loss_cls=0.2182, loss_bbox=1.1337, matched_ious=0.4402, loss_iou=0.1036, loss_iou_reg=0.2551, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.20(1.27)  Time cost: 25:55/11:23 [2:18:10/20:04:55]  Acc_iter 6500        Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.20(1.27)
2025-09-03 11:13:11,242   INFO  Train:    4/36 ( 11%) [1272/1759 ( 72%)]  Loss: 4.987 (4.86)  LR: 7.214e-04  Grad: 4.3702  max=3.0426(module.vfe.pfn_layers.0.linear.weight)  min: -2.1594(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9644, loss_cls=0.2258, loss_bbox=1.0914, matched_ious=0.4413, loss_iou=0.1036, loss_iou_reg=0.2564, d_time=0.00(0.01), f_time=1.17(1.26), b_time=1.17(1.27)  Time cost: 26:58/10:19 [2:19:13/20:03:14]  Acc_iter 6550        Data time: 0.00(0.01)  Forward time: 1.17(1.26)  Batch time: 1.17(1.27)
2025-09-03 11:14:14,248   INFO  Train:    4/36 ( 11%) [1322/1759 ( 75%)]  Loss: 5.035 (4.85)  LR: 7.275e-04  Grad: 5.6366  max=0.5245(module.vfe.pfn_layers.0.linear.weight)  min: -5.1515(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9190, loss_cls=0.2190, loss_bbox=1.1128, matched_ious=0.4424, loss_iou=0.1037, loss_iou_reg=0.2598, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.23(1.27)  Time cost: 28:01/09:15 [2:20:16/20:01:46]  Acc_iter 6600        Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.27)
2025-09-03 11:15:17,104   INFO  Train:    4/36 ( 11%) [1372/1759 ( 78%)]  Loss: 4.497 (4.85)  LR: 7.336e-04  Grad: 9.2620  max=7.8258(module.vfe.pfn_layers.0.linear.weight)  min: -3.7763(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9969, loss_cls=0.2291, loss_bbox=1.1315, matched_ious=0.4369, loss_iou=0.1036, loss_iou_reg=0.2589, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.22(1.27)  Time cost: 29:04/08:11 [2:21:19/20:00:13]  Acc_iter 6650        Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.27)
2025-09-03 11:16:21,039   INFO  Train:    4/36 ( 11%) [1422/1759 ( 81%)]  Loss: 3.845 (4.84)  LR: 7.398e-04  Grad: 7.7359  max=4.0105(module.vfe.pfn_layers.0.linear.weight)  min: -6.3677(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9597, loss_cls=0.2213, loss_bbox=1.1047, matched_ious=0.4464, loss_iou=0.1041, loss_iou_reg=0.2529, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.19(1.27)  Time cost: 30:08/07:08 [2:22:23/19:59:26]  Acc_iter 6700        Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.27)
2025-09-03 11:17:23,642   INFO  Train:    4/36 ( 11%) [1472/1759 ( 84%)]  Loss: 4.228 (4.84)  LR: 7.460e-04  Grad: 5.1600  max=4.1298(module.vfe.pfn_layers.0.linear.weight)  min: -1.0112(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9423, loss_cls=0.2200, loss_bbox=1.0816, matched_ious=0.4433, loss_iou=0.1021, loss_iou_reg=0.2568, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 31:11/06:04 [2:23:26/19:57:46]  Acc_iter 6750        Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-03 11:18:28,538   INFO  Train:    4/36 ( 11%) [1522/1759 ( 87%)]  Loss: 4.699 (4.83)  LR: 7.522e-04  Grad: 10.0000  max=7.6315(module.vfe.pfn_layers.0.linear.weight)  min: -5.4106(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9408, loss_cls=0.2194, loss_bbox=1.0951, matched_ious=0.4407, loss_iou=0.1043, loss_iou_reg=0.2588, d_time=0.00(0.01), f_time=1.30(1.26), b_time=1.30(1.27)  Time cost: 32:16/05:01 [2:24:31/19:57:34]  Acc_iter 6800        Data time: 0.00(0.01)  Forward time: 1.30(1.26)  Batch time: 1.30(1.27)
2025-09-03 11:19:32,130   INFO  Train:    4/36 ( 11%) [1572/1759 ( 89%)]  Loss: 4.842 (4.83)  LR: 7.585e-04  Grad: 3.2107  max=1.9364(module.vfe.pfn_layers.0.linear.weight)  min: -1.2192(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9569, loss_cls=0.2206, loss_bbox=1.1516, matched_ious=0.4249, loss_iou=0.1050, loss_iou_reg=0.2641, d_time=0.01(0.01), f_time=1.17(1.26), b_time=1.18(1.27)  Time cost: 33:19/03:57 [2:25:34/19:56:31]  Acc_iter 6850        Data time: 0.01(0.01)  Forward time: 1.17(1.26)  Batch time: 1.18(1.27)
2025-09-03 11:20:35,376   INFO  Train:    4/36 ( 11%) [1622/1759 ( 92%)]  Loss: 5.320 (4.83)  LR: 7.648e-04  Grad: 1.9220  max=0.2267(module.backbone_3d.cls_conv.3.weight)  min: -0.6452(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9813, loss_cls=0.2242, loss_bbox=1.1241, matched_ious=0.4338, loss_iou=0.1024, loss_iou_reg=0.2610, d_time=0.00(0.01), f_time=1.31(1.26), b_time=1.32(1.27)  Time cost: 34:22/02:54 [2:26:37/19:55:17]  Acc_iter 6900        Data time: 0.00(0.01)  Forward time: 1.31(1.26)  Batch time: 1.32(1.27)
2025-09-03 11:21:39,123   INFO  Train:    4/36 ( 11%) [1672/1759 ( 95%)]  Loss: 4.711 (4.83)  LR: 7.711e-04  Grad: 3.2030  max=2.5600(module.vfe.pfn_layers.0.linear.weight)  min: -0.3632(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9873, loss_cls=0.2231, loss_bbox=1.1574, matched_ious=0.4364, loss_iou=0.1044, loss_iou_reg=0.2566, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 35:26/01:50 [2:27:41/19:54:20]  Acc_iter 6950        Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 11:22:41,449   INFO  Train:    4/36 ( 11%) [1722/1759 ( 98%)]  Loss: 4.961 (4.82)  LR: 7.775e-04  Grad: 4.9915  max=3.1933(module.vfe.pfn_layers.0.linear.weight)  min: -3.2425(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9526, loss_cls=0.2222, loss_bbox=1.0681, matched_ious=0.4457, loss_iou=0.1034, loss_iou_reg=0.2568, d_time=0.01(0.01), f_time=1.27(1.26), b_time=1.28(1.27)  Time cost: 36:28/00:47 [2:28:43/19:52:36]  Acc_iter 7000        Data time: 0.01(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.27)
2025-09-03 11:23:25,309   INFO  Train:    4/36 ( 11%) [1758/1759 (100%)]  Loss: 5.487 (4.82)  LR: 7.821e-04  Grad: 3.1981  max=0.2667(module.vfe.pfn_layers.0.linear.weight)  min: -1.4755(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9478, loss_cls=0.2167, loss_bbox=1.1111, matched_ious=0.4502, loss_iou=0.1020, loss_iou_reg=0.2497, d_time=0.00(0.01), f_time=0.75(1.26), b_time=0.76(1.27)  Time cost: 37:12/00:01 [2:29:27/19:50:50]  Acc_iter 7036        Data time: 0.00(0.01)  Forward time: 0.75(1.26)  Batch time: 0.76(1.27)

                                               [Aepochs:  11%|█         | 4/36 [2:29:28<19:54:25, 2239.56s/it]epochs:  11%|█         | 4/36 [2:29:28<19:54:26, 2239.58s/it]epochs:  11%|█         | 4/36 [2:29:28<19:54:28, 2239.63s/it]epochs:  11%|█         | 4/36 [2:29:28<19:54:28, 2239.63s/it]epochs:  11%|█         | 4/36 [2:29:28<19:54:28, 2239.63s/it]epochs:  11%|█         | 4/36 [2:29:28<19:54:28, 2239.63s/it]epochs:  11%|█         | 4/36 [2:29:28<19:54:28, 2239.64s/it]epochs:  11%|█         | 4/36 [2:29:28<19:54:29, 2239.67s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 11:23:30,882   INFO  Train:    5/36 ( 14%) [   0/1759 (  0%)]  Loss: 4.434 (4.43)  LR: 7.823e-04  Grad: 3.9776  max=0.3479(module.backbone_3d.cls_conv.3.weight)  min: -2.4525(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8656, loss_cls=0.1860, loss_bbox=0.9534, matched_ious=0.4428, loss_iou=0.1099, loss_iou_reg=0.2454, d_time=1.64(1.64), f_time=2.67(2.67), b_time=4.31(4.31)  Time cost: 00:03/1:53:03 [2:29:33/60:17:38]  Acc_iter 7037        Data time: 1.64(1.64)  Forward time: 2.67(2.67)  Batch time: 4.31(4.31)
2025-09-03 11:23:47,486   INFO  Train:    5/36 ( 14%) [  13/1759 (  1%)]  Loss: 4.509 (4.70)  LR: 7.839e-04  Grad: 4.3130  max=3.4037(module.vfe.pfn_layers.0.linear.weight)  min: -1.3627(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9697, loss_cls=0.2175, loss_bbox=1.1165, matched_ious=0.4255, loss_iou=0.1069, loss_iou_reg=0.2659, d_time=0.00(0.12), f_time=1.19(1.37), b_time=1.19(1.49)  Time cost: 00:20/42:31 [2:29:50/22:50:50]  Acc_iter 7050        Data time: 0.00(0.12)  Forward time: 1.19(1.37)  Batch time: 1.19(1.49)
2025-09-03 11:24:52,055   INFO  Train:    5/36 ( 14%) [  63/1759 (  4%)]  Loss: 4.213 (4.68)  LR: 7.904e-04  Grad: 3.2254  max=2.3038(module.vfe.pfn_layers.0.linear.weight)  min: -0.2383(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.9415, loss_cls=0.2159, loss_bbox=1.1185, matched_ious=0.4399, loss_iou=0.1043, loss_iou_reg=0.2562, d_time=0.01(0.03), f_time=1.39(1.30), b_time=1.40(1.34)  Time cost: 01:25/37:33 [2:30:54/20:45:01]  Acc_iter 7100        Data time: 0.01(0.03)  Forward time: 1.39(1.30)  Batch time: 1.40(1.34)
2025-09-03 11:25:54,696   INFO  Train:    5/36 ( 14%) [ 113/1759 (  6%)]  Loss: 4.470 (4.67)  LR: 7.968e-04  Grad: 4.4675  max=3.5433(module.vfe.pfn_layers.0.linear.weight)  min: -0.1925(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.9241, loss_cls=0.2119, loss_bbox=1.1022, matched_ious=0.4475, loss_iou=0.1008, loss_iou_reg=0.2530, d_time=0.00(0.02), f_time=1.25(1.28), b_time=1.26(1.30)  Time cost: 02:27/35:32 [2:31:57/20:12:47]  Acc_iter 7150        Data time: 0.00(0.02)  Forward time: 1.25(1.28)  Batch time: 1.26(1.30)
2025-09-03 11:26:58,369   INFO  Train:    5/36 ( 14%) [ 163/1759 (  9%)]  Loss: 4.171 (4.63)  LR: 8.033e-04  Grad: 2.9780  max=0.7240(module.vfe.pfn_layers.0.linear.weight)  min: -1.9805(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9249, loss_cls=0.2144, loss_bbox=1.0147, matched_ious=0.4511, loss_iou=0.1013, loss_iou_reg=0.2571, d_time=0.00(0.02), f_time=1.10(1.28), b_time=1.10(1.29)  Time cost: 03:31/34:16 [2:33:00/20:05:27]  Acc_iter 7200        Data time: 0.00(0.02)  Forward time: 1.10(1.28)  Batch time: 1.10(1.29)
2025-09-03 11:28:02,120   INFO  Train:    5/36 ( 14%) [ 213/1759 ( 12%)]  Loss: 5.110 (4.61)  LR: 8.099e-04  Grad: 4.0005  max=1.4768(module.vfe.pfn_layers.0.linear.weight)  min: -3.2161(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9416, loss_cls=0.2109, loss_bbox=1.0866, matched_ious=0.4475, loss_iou=0.1021, loss_iou_reg=0.2541, d_time=0.36(0.01), f_time=1.23(1.27), b_time=1.59(1.29)  Time cost: 04:35/33:07 [2:34:04/20:01:24]  Acc_iter 7250        Data time: 0.36(0.01)  Forward time: 1.23(1.27)  Batch time: 1.59(1.29)
2025-09-03 11:29:07,747   INFO  Train:    5/36 ( 14%) [ 263/1759 ( 15%)]  Loss: 6.454 (4.62)  LR: 8.164e-04  Grad: 5.1012  max=0.6496(module.vfe.pfn_layers.0.linear.weight)  min: -3.7336(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9218, loss_cls=0.2136, loss_bbox=1.0793, matched_ious=0.4403, loss_iou=0.1048, loss_iou_reg=0.2593, d_time=0.00(0.02), f_time=1.28(1.28), b_time=1.29(1.29)  Time cost: 05:40/32:10 [2:35:10/20:05:06]  Acc_iter 7300        Data time: 0.00(0.02)  Forward time: 1.28(1.28)  Batch time: 1.29(1.29)
2025-09-03 11:30:11,217   INFO  Train:    5/36 ( 14%) [ 313/1759 ( 18%)]  Loss: 4.266 (4.62)  LR: 8.231e-04  Grad: 5.2953  max=4.5195(module.vfe.pfn_layers.0.linear.weight)  min: -1.7209(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9346, loss_cls=0.2127, loss_bbox=1.1144, matched_ious=0.4511, loss_iou=0.1014, loss_iou_reg=0.2504, d_time=0.01(0.01), f_time=1.27(1.27), b_time=1.27(1.29)  Time cost: 06:44/31:01 [2:36:13/20:00:52]  Acc_iter 7350        Data time: 0.01(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.29)
2025-09-03 11:31:14,150   INFO  Train:    5/36 ( 14%) [ 363/1759 ( 21%)]  Loss: 4.725 (4.62)  LR: 8.297e-04  Grad: 1.8290  max=0.7979(module.vfe.pfn_layers.0.linear.weight)  min: -0.5760(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9287, loss_cls=0.2139, loss_bbox=1.0539, matched_ious=0.4460, loss_iou=0.1024, loss_iou_reg=0.2573, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.22(1.28)  Time cost: 07:47/29:51 [2:37:16/19:56:08]  Acc_iter 7400        Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.22(1.28)
2025-09-03 11:32:17,090   INFO  Train:    5/36 ( 14%) [ 413/1759 ( 23%)]  Loss: 4.398 (4.62)  LR: 8.363e-04  Grad: 3.1491  max=2.2153(module.vfe.pfn_layers.0.linear.weight)  min: -0.2771(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9318, loss_cls=0.2113, loss_bbox=1.0969, matched_ious=0.4463, loss_iou=0.1015, loss_iou_reg=0.2540, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.28)  Time cost: 08:50/28:43 [2:38:19/19:52:19]  Acc_iter 7450        Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.28)
2025-09-03 11:33:20,917   INFO  Train:    5/36 ( 14%) [ 463/1759 ( 26%)]  Loss: 4.150 (4.61)  LR: 8.430e-04  Grad: 5.1364  max=0.1391(module.vfe.pfn_layers.0.linear.weight)  min: -3.6543(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9281, loss_cls=0.2138, loss_bbox=1.0425, matched_ious=0.4412, loss_iou=0.1052, loss_iou_reg=0.2617, d_time=0.01(0.01), f_time=1.15(1.27), b_time=1.16(1.28)  Time cost: 09:53/27:38 [2:39:23/19:50:52]  Acc_iter 7500        Data time: 0.01(0.01)  Forward time: 1.15(1.27)  Batch time: 1.16(1.28)
2025-09-03 11:34:24,306   INFO  Train:    5/36 ( 14%) [ 513/1759 ( 29%)]  Loss: 5.120 (4.61)  LR: 8.498e-04  Grad: 6.5744  max=0.1726(module.backbone_3d.cls_conv.3.weight)  min: -3.8848(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9356, loss_cls=0.2144, loss_bbox=1.0347, matched_ious=0.4499, loss_iou=0.1015, loss_iou_reg=0.2547, d_time=0.00(0.01), f_time=1.33(1.27), b_time=1.34(1.28)  Time cost: 10:57/26:33 [2:40:26/19:48:42]  Acc_iter 7550        Data time: 0.00(0.01)  Forward time: 1.33(1.27)  Batch time: 1.34(1.28)
2025-09-03 11:35:26,817   INFO  Train:    5/36 ( 14%) [ 563/1759 ( 32%)]  Loss: 5.392 (4.61)  LR: 8.565e-04  Grad: 2.6889  max=1.2869(module.vfe.pfn_layers.0.linear.weight)  min: -0.1283(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.9065, loss_cls=0.2107, loss_bbox=1.0535, matched_ious=0.4456, loss_iou=0.1024, loss_iou_reg=0.2578, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.28)  Time cost: 11:59/25:26 [2:41:29/19:45:17]  Acc_iter 7600        Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.28)
2025-09-03 11:36:30,746   INFO  Train:    5/36 ( 14%) [ 613/1759 ( 35%)]  Loss: 4.134 (4.60)  LR: 8.633e-04  Grad: 2.7658  max=1.1661(module.vfe.pfn_layers.0.linear.weight)  min: -1.4504(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9196, loss_cls=0.2105, loss_bbox=1.0462, matched_ious=0.4539, loss_iou=0.1019, loss_iou_reg=0.2522, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.28)  Time cost: 13:03/24:22 [2:42:33/19:44:24]  Acc_iter 7650        Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.28)
2025-09-03 11:37:33,168   INFO  Train:    5/36 ( 14%) [ 663/1759 ( 38%)]  Loss: 4.755 (4.59)  LR: 8.701e-04  Grad: 2.8587  max=0.3062(module.backbone_3d.cls_conv.3.weight)  min: -1.9123(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9160, loss_cls=0.2095, loss_bbox=1.0470, matched_ious=0.4501, loss_iou=0.1039, loss_iou_reg=0.2559, d_time=0.01(0.01), f_time=1.28(1.26), b_time=1.29(1.27)  Time cost: 14:06/23:16 [2:43:35/19:41:23]  Acc_iter 7700        Data time: 0.01(0.01)  Forward time: 1.28(1.26)  Batch time: 1.29(1.27)
2025-09-03 11:38:38,526   INFO  Train:    5/36 ( 14%) [ 713/1759 ( 41%)]  Loss: 4.004 (4.60)  LR: 8.770e-04  Grad: 4.6915  max=2.5302(module.vfe.pfn_layers.0.linear.weight)  min: -2.7012(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9089, loss_cls=0.2078, loss_bbox=1.1027, matched_ious=0.4435, loss_iou=0.1011, loss_iou_reg=0.2564, d_time=0.00(0.01), f_time=2.14(1.27), b_time=2.14(1.28)  Time cost: 15:11/22:15 [2:44:41/19:42:27]  Acc_iter 7750        Data time: 0.00(0.01)  Forward time: 2.14(1.27)  Batch time: 2.14(1.28)
2025-09-03 11:39:42,443   INFO  Train:    5/36 ( 14%) [ 763/1759 ( 43%)]  Loss: 4.134 (4.59)  LR: 8.839e-04  Grad: 5.1761  max=4.1545(module.vfe.pfn_layers.0.linear.weight)  min: -0.5108(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9252, loss_cls=0.2069, loss_bbox=1.0644, matched_ious=0.4473, loss_iou=0.1013, loss_iou_reg=0.2554, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.28)  Time cost: 16:15/21:11 [2:45:44/19:41:30]  Acc_iter 7800        Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.28)
2025-09-03 11:40:45,095   INFO  Train:    5/36 ( 14%) [ 813/1759 ( 46%)]  Loss: 4.536 (4.59)  LR: 8.908e-04  Grad: 5.6684  max=4.8023(module.vfe.pfn_layers.0.linear.weight)  min: -0.3299(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.9139, loss_cls=0.2098, loss_bbox=1.0174, matched_ious=0.4440, loss_iou=0.1054, loss_iou_reg=0.2585, d_time=0.00(0.01), f_time=1.20(1.27), b_time=1.20(1.28)  Time cost: 17:18/20:06 [2:46:47/19:39:05]  Acc_iter 7850        Data time: 0.00(0.01)  Forward time: 1.20(1.27)  Batch time: 1.20(1.28)
2025-09-03 11:41:47,791   INFO  Train:    5/36 ( 14%) [ 863/1759 ( 49%)]  Loss: 3.971 (4.58)  LR: 8.977e-04  Grad: 3.2996  max=2.1830(module.vfe.pfn_layers.0.linear.weight)  min: -0.5994(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8863, loss_cls=0.2011, loss_bbox=1.0314, matched_ious=0.4506, loss_iou=0.1022, loss_iou_reg=0.2549, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.27)  Time cost: 18:20/19:01 [2:47:50/19:36:53]  Acc_iter 7900        Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.27)
2025-09-03 11:42:50,885   INFO  Train:    5/36 ( 14%) [ 913/1759 ( 52%)]  Loss: 4.531 (4.58)  LR: 9.047e-04  Grad: 3.4506  max=1.9658(module.vfe.pfn_layers.0.linear.weight)  min: -2.0488(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8947, loss_cls=0.2026, loss_bbox=1.0729, matched_ious=0.4512, loss_iou=0.1013, loss_iou_reg=0.2555, d_time=0.00(0.01), f_time=1.31(1.26), b_time=1.31(1.27)  Time cost: 19:23/17:57 [2:48:53/19:35:12]  Acc_iter 7950        Data time: 0.00(0.01)  Forward time: 1.31(1.26)  Batch time: 1.31(1.27)
2025-09-03 11:43:54,480   INFO  Train:    5/36 ( 14%) [ 963/1759 ( 55%)]  Loss: 4.115 (4.58)  LR: 9.117e-04  Grad: 2.5771  max=1.0355(module.vfe.pfn_layers.0.linear.weight)  min: -1.1237(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9286, loss_cls=0.2048, loss_bbox=1.0707, matched_ious=0.4469, loss_iou=0.1020, loss_iou_reg=0.2539, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 20:27/16:53 [2:49:56/19:34:04]  Acc_iter 8000        Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-03 11:44:57,442   INFO  Train:    5/36 ( 14%) [1013/1759 ( 58%)]  Loss: 4.063 (4.58)  LR: 9.187e-04  Grad: 5.4920  max=3.8595(module.vfe.pfn_layers.0.linear.weight)  min: -0.2330(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.9362, loss_cls=0.2059, loss_bbox=1.1367, matched_ious=0.4332, loss_iou=0.1041, loss_iou_reg=0.2610, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.27(1.27)  Time cost: 21:30/15:49 [2:50:59/19:32:22]  Acc_iter 8050        Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.27)
2025-09-03 11:46:00,998   INFO  Train:    5/36 ( 14%) [1063/1759 ( 60%)]  Loss: 4.226 (4.57)  LR: 9.257e-04  Grad: 3.9264  max=0.1374(module.vfe.pfn_layers.0.linear.weight)  min: -2.5440(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8984, loss_cls=0.1977, loss_bbox=1.0146, matched_ious=0.4671, loss_iou=0.1013, loss_iou_reg=0.2453, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.20(1.27)  Time cost: 22:33/14:45 [2:52:03/19:31:15]  Acc_iter 8100        Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.20(1.27)
2025-09-03 11:47:04,357   INFO  Train:    5/36 ( 14%) [1113/1759 ( 63%)]  Loss: 4.310 (4.57)  LR: 9.328e-04  Grad: 4.8851  max=2.9782(module.vfe.pfn_layers.0.linear.weight)  min: -2.4401(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9197, loss_cls=0.2002, loss_bbox=1.0879, matched_ious=0.4545, loss_iou=0.1007, loss_iou_reg=0.2520, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 23:37/13:41 [2:53:06/19:29:58]  Acc_iter 8150        Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-03 11:48:07,230   INFO  Train:    5/36 ( 14%) [1163/1759 ( 66%)]  Loss: 4.247 (4.57)  LR: 9.399e-04  Grad: 4.6723  max=0.3231(module.backbone_3d.cls_conv.3.weight)  min: -3.0391(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8845, loss_cls=0.1938, loss_bbox=1.0700, matched_ious=0.4496, loss_iou=0.1008, loss_iou_reg=0.2535, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.28(1.27)  Time cost: 24:40/12:37 [2:54:09/19:28:19]  Acc_iter 8200        Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.27)
2025-09-03 11:49:12,674   INFO  Train:    5/36 ( 14%) [1213/1759 ( 69%)]  Loss: 4.540 (4.56)  LR: 9.471e-04  Grad: 6.9980  max=5.1129(module.vfe.pfn_layers.0.linear.weight)  min: -0.0919(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9149, loss_cls=0.2002, loss_bbox=1.0491, matched_ious=0.4553, loss_iou=0.1004, loss_iou_reg=0.2501, d_time=0.00(0.01), f_time=2.10(1.27), b_time=2.11(1.27)  Time cost: 25:45/11:35 [2:55:15/19:28:40]  Acc_iter 8250        Data time: 0.00(0.01)  Forward time: 2.10(1.27)  Batch time: 2.11(1.27)
2025-09-03 11:50:15,512   INFO  Train:    5/36 ( 14%) [1263/1759 ( 72%)]  Loss: 4.104 (4.56)  LR: 9.542e-04  Grad: 8.3022  max=0.4707(module.backbone_3d.cls_conv.3.weight)  min: -7.8280(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8751, loss_cls=0.1951, loss_bbox=1.0377, matched_ious=0.4532, loss_iou=0.1012, loss_iou_reg=0.2528, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 26:48/10:31 [2:56:18/19:27:01]  Acc_iter 8300        Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-03 11:51:18,125   INFO  Train:    5/36 ( 14%) [1313/1759 ( 75%)]  Loss: 4.376 (4.56)  LR: 9.614e-04  Grad: 3.9627  max=2.2845(module.vfe.pfn_layers.0.linear.weight)  min: -0.1348(module.dense_head.prediction_head.dim.1.weight)  NaN: False  loss_hm=0.9123, loss_cls=0.1976, loss_bbox=1.0564, matched_ious=0.4492, loss_iou=0.1018, loss_iou_reg=0.2543, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.28(1.27)  Time cost: 27:51/09:27 [2:57:20/19:25:15]  Acc_iter 8350        Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.27)
2025-09-03 11:52:21,491   INFO  Train:    5/36 ( 14%) [1363/1759 ( 77%)]  Loss: 4.624 (4.56)  LR: 9.686e-04  Grad: 3.6987  max=1.7884(module.vfe.pfn_layers.0.linear.weight)  min: -0.4719(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.9007, loss_cls=0.1958, loss_bbox=1.0888, matched_ious=0.4503, loss_iou=0.1013, loss_iou_reg=0.2532, d_time=0.00(0.01), f_time=1.29(1.26), b_time=1.29(1.27)  Time cost: 28:54/08:23 [2:58:24/19:24:02]  Acc_iter 8400        Data time: 0.00(0.01)  Forward time: 1.29(1.26)  Batch time: 1.29(1.27)
2025-09-03 11:53:24,716   INFO  Train:    5/36 ( 14%) [1413/1759 ( 80%)]  Loss: 4.527 (4.56)  LR: 9.759e-04  Grad: 3.7593  max=0.2778(module.vfe.pfn_layers.0.linear.weight)  min: -1.7531(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8909, loss_cls=0.1939, loss_bbox=1.0285, matched_ious=0.4505, loss_iou=0.1031, loss_iou_reg=0.2561, d_time=0.00(0.01), f_time=1.29(1.26), b_time=1.30(1.27)  Time cost: 29:57/07:19 [2:59:27/19:22:45]  Acc_iter 8450        Data time: 0.00(0.01)  Forward time: 1.29(1.26)  Batch time: 1.30(1.27)
2025-09-03 11:54:27,759   INFO  Train:    5/36 ( 14%) [1463/1759 ( 83%)]  Loss: 4.288 (4.56)  LR: 9.831e-04  Grad: 5.0647  max=3.4707(module.vfe.pfn_layers.0.linear.weight)  min: -1.7650(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9086, loss_cls=0.2005, loss_bbox=1.0628, matched_ious=0.4557, loss_iou=0.0997, loss_iou_reg=0.2539, d_time=0.00(0.01), f_time=1.15(1.26), b_time=1.16(1.27)  Time cost: 31:00/06:16 [3:00:30/19:21:22]  Acc_iter 8500        Data time: 0.00(0.01)  Forward time: 1.15(1.26)  Batch time: 1.16(1.27)
2025-09-03 11:55:31,394   INFO  Train:    5/36 ( 14%) [1513/1759 ( 86%)]  Loss: 5.679 (4.56)  LR: 9.904e-04  Grad: 4.6406  max=0.1401(module.dense_head.prediction_head.height.1.weight)  min: -2.8045(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9120, loss_cls=0.1977, loss_bbox=1.0861, matched_ious=0.4355, loss_iou=0.1050, loss_iou_reg=0.2601, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.25(1.27)  Time cost: 32:04/05:12 [3:01:33/19:20:21]  Acc_iter 8550        Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.27)
2025-09-03 11:56:34,818   INFO  Train:    5/36 ( 14%) [1563/1759 ( 89%)]  Loss: 4.319 (4.56)  LR: 9.977e-04  Grad: 3.5479  max=2.4078(module.vfe.pfn_layers.0.linear.weight)  min: -0.1072(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.9244, loss_cls=0.2012, loss_bbox=1.1421, matched_ious=0.4392, loss_iou=0.1012, loss_iou_reg=0.2569, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.19(1.27)  Time cost: 33:07/04:09 [3:02:37/19:19:13]  Acc_iter 8600        Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.27)
2025-09-03 11:57:37,036   INFO  Train:    5/36 ( 14%) [1613/1759 ( 92%)]  Loss: 4.479 (4.56)  LR: 1.005e-03  Grad: 2.7288  max=0.2943(module.dense_head.prediction_head.height.1.weight)  min: -1.0730(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8956, loss_cls=0.1956, loss_bbox=1.0431, matched_ious=0.4537, loss_iou=0.1021, loss_iou_reg=0.2505, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.26(1.27)  Time cost: 34:10/03:05 [3:03:39/19:17:25]  Acc_iter 8650        Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.27)
2025-09-03 11:58:39,551   INFO  Train:    5/36 ( 14%) [1663/1759 ( 95%)]  Loss: 3.846 (4.55)  LR: 1.012e-03  Grad: 5.3785  max=4.4839(module.vfe.pfn_layers.0.linear.weight)  min: -1.3231(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8681, loss_cls=0.1896, loss_bbox=0.9995, matched_ious=0.4553, loss_iou=0.1002, loss_iou_reg=0.2512, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.24(1.27)  Time cost: 35:12/02:01 [3:04:42/19:15:49]  Acc_iter 8700        Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.27)
2025-09-03 11:59:44,974   INFO  Train:    5/36 ( 14%) [1713/1759 ( 97%)]  Loss: 4.621 (4.55)  LR: 1.020e-03  Grad: 3.7820  max=2.2112(module.vfe.pfn_layers.0.linear.weight)  min: -1.2951(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9051, loss_cls=0.1942, loss_bbox=1.1043, matched_ious=0.4620, loss_iou=0.0983, loss_iou_reg=0.2472, d_time=0.00(0.01), f_time=1.31(1.26), b_time=1.31(1.27)  Time cost: 36:17/00:58 [3:05:47/19:15:47]  Acc_iter 8750        Data time: 0.00(0.01)  Forward time: 1.31(1.26)  Batch time: 1.31(1.27)
2025-09-03 12:00:40,864   INFO  Train:    5/36 ( 14%) [1758/1759 (100%)]  Loss: 7.182 (4.55)  LR: 1.027e-03  Grad: 5.5371  max=0.2321(module.dense_head.prediction_head.height.1.weight)  min: -3.5524(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8825, loss_cls=0.1917, loss_bbox=1.0310, matched_ious=0.4504, loss_iou=0.1017, loss_iou_reg=0.2533, d_time=0.00(0.01), f_time=0.72(1.26), b_time=0.72(1.27)  Time cost: 37:13/00:01 [3:06:43/19:14:10]  Acc_iter 8795        Data time: 0.00(0.01)  Forward time: 0.72(1.26)  Batch time: 0.72(1.27)

                                               [Aepochs:  14%|█▍        | 5/36 [3:06:43<19:16:21, 2238.12s/it]epochs:  14%|█▍        | 5/36 [3:06:43<19:16:22, 2238.16s/it]epochs:  14%|█▍        | 5/36 [3:06:43<19:16:22, 2238.16s/it]epochs:  14%|█▍        | 5/36 [3:06:43<19:16:22, 2238.16s/it]epochs:  14%|█▍        | 5/36 [3:06:43<19:16:23, 2238.18s/it]epochs:  14%|█▍        | 5/36 [3:06:43<19:16:22, 2238.16s/it]epochs:  14%|█▍        | 5/36 [3:06:43<19:16:23, 2238.16s/it]epochs:  14%|█▍        | 5/36 [3:06:44<19:16:23, 2238.17s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 12:00:46,438   INFO  Train:    6/36 ( 17%) [   0/1759 (  0%)]  Loss: 3.999 (4.00)  LR: 1.027e-03  Grad: 3.3078  max=0.9746(module.vfe.pfn_layers.0.linear.weight)  min: -0.1817(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8011, loss_cls=0.1875, loss_bbox=0.8715, matched_ious=0.4912, loss_iou=0.0936, loss_iou_reg=0.2363, d_time=1.38(1.38), f_time=2.90(2.90), b_time=4.28(4.28)  Time cost: 00:03/1:56:27 [3:06:48/60:10:03]  Acc_iter 8796        Data time: 1.38(1.38)  Forward time: 2.90(2.90)  Batch time: 4.28(4.28)
2025-09-03 12:00:51,465   INFO  Train:    6/36 ( 17%) [   4/1759 (  0%)]  Loss: 4.891 (4.68)  LR: 1.027e-03  Grad: 4.2843  max=2.8049(module.vfe.pfn_layers.0.linear.weight)  min: -0.2268(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.9529, loss_cls=0.1954, loss_bbox=1.3633, matched_ious=0.4203, loss_iou=0.0999, loss_iou_reg=0.2675, d_time=0.01(0.28), f_time=1.21(1.58), b_time=1.22(1.86)  Time cost: 00:09/52:39 [3:06:53/27:16:07]  Acc_iter 8800        Data time: 0.01(0.28)  Forward time: 1.21(1.58)  Batch time: 1.22(1.86)
2025-09-03 12:01:54,825   INFO  Train:    6/36 ( 17%) [  54/1759 (  3%)]  Loss: 4.970 (4.46)  LR: 1.035e-03  Grad: 4.6590  max=3.1803(module.vfe.pfn_layers.0.linear.weight)  min: -0.4228(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9019, loss_cls=0.1945, loss_bbox=1.0029, matched_ious=0.4513, loss_iou=0.1026, loss_iou_reg=0.2548, d_time=0.01(0.03), f_time=1.21(1.29), b_time=1.22(1.32)  Time cost: 01:12/37:23 [3:07:57/19:54:31]  Acc_iter 8850        Data time: 0.01(0.03)  Forward time: 1.21(1.29)  Batch time: 1.22(1.32)
2025-09-03 12:02:58,163   INFO  Train:    6/36 ( 17%) [ 104/1759 (  6%)]  Loss: 4.403 (4.46)  LR: 1.042e-03  Grad: 3.1339  max=0.7801(module.vfe.pfn_layers.0.linear.weight)  min: -0.7782(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9017, loss_cls=0.1932, loss_bbox=1.0319, matched_ious=0.4554, loss_iou=0.1022, loss_iou_reg=0.2520, d_time=0.00(0.02), f_time=1.21(1.28), b_time=1.22(1.30)  Time cost: 02:15/35:38 [3:09:00/19:32:18]  Acc_iter 8900        Data time: 0.00(0.02)  Forward time: 1.21(1.28)  Batch time: 1.22(1.30)
2025-09-03 12:04:01,722   INFO  Train:    6/36 ( 17%) [ 154/1759 (  9%)]  Loss: 4.245 (4.43)  LR: 1.050e-03  Grad: 5.6892  max=2.1282(module.vfe.pfn_layers.0.linear.weight)  min: -4.3012(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8659, loss_cls=0.1867, loss_bbox=1.0168, matched_ious=0.4541, loss_iou=0.0987, loss_iou_reg=0.2503, d_time=0.00(0.01), f_time=1.29(1.27), b_time=1.29(1.29)  Time cost: 03:19/34:22 [3:10:04/19:24:50]  Acc_iter 8950        Data time: 0.00(0.01)  Forward time: 1.29(1.27)  Batch time: 1.29(1.29)
2025-09-03 12:05:04,900   INFO  Train:    6/36 ( 17%) [ 204/1759 ( 12%)]  Loss: 4.838 (4.41)  LR: 1.057e-03  Grad: 2.6144  max=0.3840(module.vfe.pfn_layers.0.linear.weight)  min: -0.6336(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8659, loss_cls=0.1855, loss_bbox=1.0377, matched_ious=0.4643, loss_iou=0.0995, loss_iou_reg=0.2464, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.28)  Time cost: 04:22/33:10 [3:11:07/19:19:06]  Acc_iter 9000        Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.28)
2025-09-03 12:06:09,061   INFO  Train:    6/36 ( 17%) [ 254/1759 ( 14%)]  Loss: 4.213 (4.39)  LR: 1.065e-03  Grad: 3.7995  max=2.3516(module.vfe.pfn_layers.0.linear.weight)  min: -0.8481(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8526, loss_cls=0.1834, loss_bbox=1.0009, matched_ious=0.4596, loss_iou=0.0987, loss_iou_reg=0.2501, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.27(1.28)  Time cost: 05:26/32:07 [3:12:11/19:18:34]  Acc_iter 9050        Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.28)
2025-09-03 12:07:11,792   INFO  Train:    6/36 ( 17%) [ 304/1759 ( 17%)]  Loss: 5.192 (4.40)  LR: 1.072e-03  Grad: 5.8019  max=1.5320(module.vfe.pfn_layers.0.linear.weight)  min: -4.9972(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8853, loss_cls=0.1876, loss_bbox=1.0415, matched_ious=0.4569, loss_iou=0.1000, loss_iou_reg=0.2511, d_time=0.00(0.01), f_time=1.22(1.27), b_time=1.22(1.28)  Time cost: 06:29/30:57 [3:13:14/19:13:37]  Acc_iter 9100        Data time: 0.00(0.01)  Forward time: 1.22(1.27)  Batch time: 1.22(1.28)
2025-09-03 12:08:14,385   INFO  Train:    6/36 ( 17%) [ 354/1759 ( 20%)]  Loss: 5.345 (4.42)  LR: 1.080e-03  Grad: 5.1123  max=1.2409(module.vfe.pfn_layers.0.linear.weight)  min: -4.7284(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8789, loss_cls=0.1851, loss_bbox=1.0650, matched_ious=0.4494, loss_iou=0.0992, loss_iou_reg=0.2532, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.26(1.27)  Time cost: 07:31/29:48 [3:14:16/19:09:25]  Acc_iter 9150        Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.27)
2025-09-03 12:09:18,333   INFO  Train:    6/36 ( 17%) [ 404/1759 ( 23%)]  Loss: 3.320 (4.41)  LR: 1.087e-03  Grad: 2.5150  max=1.4414(module.vfe.pfn_layers.0.linear.weight)  min: -0.5713(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8746, loss_cls=0.1931, loss_bbox=1.0018, matched_ious=0.4608, loss_iou=0.1004, loss_iou_reg=0.2517, d_time=0.01(0.01), f_time=1.19(1.27), b_time=1.20(1.27)  Time cost: 08:35/28:45 [3:15:20/19:09:02]  Acc_iter 9200        Data time: 0.01(0.01)  Forward time: 1.19(1.27)  Batch time: 1.20(1.27)
2025-09-03 12:10:24,758   INFO  Train:    6/36 ( 17%) [ 454/1759 ( 26%)]  Loss: 4.345 (4.41)  LR: 1.095e-03  Grad: 4.7667  max=4.2202(module.vfe.pfn_layers.0.linear.weight)  min: -1.0687(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8807, loss_cls=0.1916, loss_bbox=1.0181, matched_ious=0.4549, loss_iou=0.1033, loss_iou_reg=0.2542, d_time=0.00(0.01), f_time=1.34(1.27), b_time=1.35(1.28)  Time cost: 09:42/27:50 [3:16:27/19:13:23]  Acc_iter 9250        Data time: 0.00(0.01)  Forward time: 1.34(1.27)  Batch time: 1.35(1.28)
2025-09-03 12:11:28,094   INFO  Train:    6/36 ( 17%) [ 504/1759 ( 29%)]  Loss: 4.120 (4.40)  LR: 1.103e-03  Grad: 3.1196  max=2.3295(module.vfe.pfn_layers.0.linear.weight)  min: -0.9204(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8760, loss_cls=0.1863, loss_bbox=1.0088, matched_ious=0.4624, loss_iou=0.0992, loss_iou_reg=0.2488, d_time=0.01(0.01), f_time=1.23(1.27), b_time=1.24(1.28)  Time cost: 10:45/26:44 [3:17:30/19:11:09]  Acc_iter 9300        Data time: 0.01(0.01)  Forward time: 1.23(1.27)  Batch time: 1.24(1.28)
2025-09-03 12:12:30,265   INFO  Train:    6/36 ( 17%) [ 554/1759 ( 31%)]  Loss: 4.783 (4.41)  LR: 1.110e-03  Grad: 4.8052  max=0.1767(module.dense_head.prediction_head.height.1.bias)  min: -3.3050(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8843, loss_cls=0.1815, loss_bbox=1.1178, matched_ious=0.4491, loss_iou=0.1015, loss_iou_reg=0.2549, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.28(1.28)  Time cost: 11:47/25:36 [3:18:32/19:07:15]  Acc_iter 9350        Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.28(1.28)
2025-09-03 12:13:33,044   INFO  Train:    6/36 ( 17%) [ 604/1759 ( 34%)]  Loss: 4.499 (4.41)  LR: 1.118e-03  Grad: 3.3005  max=2.1992(module.vfe.pfn_layers.0.linear.weight)  min: -1.2806(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8971, loss_cls=0.1910, loss_bbox=1.0379, matched_ious=0.4568, loss_iou=0.0997, loss_iou_reg=0.2527, d_time=0.00(0.01), f_time=1.16(1.27), b_time=1.16(1.27)  Time cost: 12:50/24:31 [3:19:35/19:04:43]  Acc_iter 9400        Data time: 0.00(0.01)  Forward time: 1.16(1.27)  Batch time: 1.16(1.27)
2025-09-03 12:14:36,055   INFO  Train:    6/36 ( 17%) [ 654/1759 ( 37%)]  Loss: 3.878 (4.42)  LR: 1.126e-03  Grad: 3.8188  max=2.4370(module.vfe.pfn_layers.0.linear.weight)  min: -1.7803(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8987, loss_cls=0.1871, loss_bbox=1.0321, matched_ious=0.4580, loss_iou=0.0983, loss_iou_reg=0.2522, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 13:53/23:26 [3:20:38/19:02:44]  Acc_iter 9450        Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 12:15:39,702   INFO  Train:    6/36 ( 17%) [ 704/1759 ( 40%)]  Loss: 3.548 (4.42)  LR: 1.133e-03  Grad: 3.4341  max=0.8558(module.vfe.pfn_layers.0.linear.weight)  min: -2.1397(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8940, loss_cls=0.1926, loss_bbox=1.0217, matched_ious=0.4520, loss_iou=0.1002, loss_iou_reg=0.2534, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.26(1.27)  Time cost: 14:57/22:22 [3:21:42/19:01:42]  Acc_iter 9500        Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.27)
2025-09-03 12:16:43,300   INFO  Train:    6/36 ( 17%) [ 754/1759 ( 43%)]  Loss: 4.549 (4.42)  LR: 1.141e-03  Grad: 4.2292  max=0.2559(module.vfe.pfn_layers.0.linear.weight)  min: -3.2626(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8840, loss_cls=0.1813, loss_bbox=1.0798, matched_ious=0.4574, loss_iou=0.1013, loss_iou_reg=0.2465, d_time=0.01(0.01), f_time=1.22(1.27), b_time=1.23(1.27)  Time cost: 16:00/21:18 [3:22:45/19:00:35]  Acc_iter 9550        Data time: 0.01(0.01)  Forward time: 1.22(1.27)  Batch time: 1.23(1.27)
2025-09-03 12:17:46,610   INFO  Train:    6/36 ( 17%) [ 804/1759 ( 46%)]  Loss: 3.718 (4.41)  LR: 1.149e-03  Grad: 3.9143  max=2.0065(module.vfe.pfn_layers.0.linear.weight)  min: -0.1907(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8998, loss_cls=0.1857, loss_bbox=1.0317, matched_ious=0.4615, loss_iou=0.0981, loss_iou_reg=0.2490, d_time=0.00(0.01), f_time=1.29(1.26), b_time=1.30(1.27)  Time cost: 17:04/20:14 [3:23:49/18:59:10]  Acc_iter 9600        Data time: 0.00(0.01)  Forward time: 1.29(1.26)  Batch time: 1.30(1.27)
2025-09-03 12:18:49,226   INFO  Train:    6/36 ( 17%) [ 854/1759 ( 49%)]  Loss: 3.996 (4.41)  LR: 1.157e-03  Grad: 6.6154  max=5.8954(module.vfe.pfn_layers.0.linear.weight)  min: -0.7779(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8606, loss_cls=0.1804, loss_bbox=1.0101, matched_ious=0.4603, loss_iou=0.1013, loss_iou_reg=0.2499, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 18:06/19:10 [3:24:51/18:57:04]  Acc_iter 9650        Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 12:19:51,298   INFO  Train:    6/36 ( 17%) [ 904/1759 ( 51%)]  Loss: 4.119 (4.40)  LR: 1.165e-03  Grad: 4.5928  max=2.3641(module.vfe.pfn_layers.0.linear.weight)  min: -2.0330(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8687, loss_cls=0.1805, loss_bbox=0.9626, matched_ious=0.4639, loss_iou=0.1009, loss_iou_reg=0.2486, d_time=0.00(0.01), f_time=1.16(1.26), b_time=1.16(1.27)  Time cost: 19:08/18:05 [3:25:53/18:54:33]  Acc_iter 9700        Data time: 0.00(0.01)  Forward time: 1.16(1.26)  Batch time: 1.16(1.27)
2025-09-03 12:20:56,845   INFO  Train:    6/36 ( 17%) [ 954/1759 ( 54%)]  Loss: 4.378 (4.40)  LR: 1.172e-03  Grad: 3.8450  max=0.9868(module.vfe.pfn_layers.0.linear.weight)  min: -1.5959(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8753, loss_cls=0.1816, loss_bbox=1.0134, matched_ious=0.4527, loss_iou=0.0982, loss_iou_reg=0.2555, d_time=0.00(0.01), f_time=1.31(1.26), b_time=1.31(1.27)  Time cost: 20:14/17:03 [3:26:59/18:55:26]  Acc_iter 9750        Data time: 0.00(0.01)  Forward time: 1.31(1.26)  Batch time: 1.31(1.27)
2025-09-03 12:22:00,093   INFO  Train:    6/36 ( 17%) [1004/1759 ( 57%)]  Loss: 4.060 (4.39)  LR: 1.180e-03  Grad: 4.3515  max=3.1038(module.vfe.pfn_layers.0.linear.weight)  min: -0.1895(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.8494, loss_cls=0.1820, loss_bbox=0.9288, matched_ious=0.4628, loss_iou=0.0988, loss_iou_reg=0.2513, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.26(1.27)  Time cost: 21:17/15:59 [3:28:02/18:54:04]  Acc_iter 9800        Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.27)
2025-09-03 12:23:03,351   INFO  Train:    6/36 ( 17%) [1054/1759 ( 60%)]  Loss: 4.191 (4.38)  LR: 1.188e-03  Grad: 4.4616  max=0.1438(module.vfe.pfn_layers.0.linear.weight)  min: -2.7630(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8411, loss_cls=0.1818, loss_bbox=0.9663, matched_ious=0.4680, loss_iou=0.0997, loss_iou_reg=0.2450, d_time=0.00(0.01), f_time=1.33(1.26), b_time=1.33(1.27)  Time cost: 22:20/14:56 [3:29:05/18:52:45]  Acc_iter 9850        Data time: 0.00(0.01)  Forward time: 1.33(1.26)  Batch time: 1.33(1.27)
2025-09-03 12:24:06,683   INFO  Train:    6/36 ( 17%) [1104/1759 ( 63%)]  Loss: 4.532 (4.38)  LR: 1.196e-03  Grad: 4.9582  max=0.4673(module.vfe.pfn_layers.0.linear.weight)  min: -4.4108(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8925, loss_cls=0.1847, loss_bbox=0.9716, matched_ious=0.4607, loss_iou=0.1026, loss_iou_reg=0.2513, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.28(1.27)  Time cost: 23:24/13:52 [3:30:09/18:51:31]  Acc_iter 9900        Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.27)
2025-09-03 12:25:10,676   INFO  Train:    6/36 ( 17%) [1154/1759 ( 66%)]  Loss: 5.801 (4.38)  LR: 1.204e-03  Grad: 3.8450  max=2.8456(module.vfe.pfn_layers.0.linear.weight)  min: -1.3195(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8516, loss_cls=0.1806, loss_bbox=0.9811, matched_ious=0.4550, loss_iou=0.1002, loss_iou_reg=0.2541, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.28(1.27)  Time cost: 24:28/12:49 [3:31:13/18:50:49]  Acc_iter 9950        Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.27)
2025-09-03 12:26:13,581   INFO  Train:    6/36 ( 17%) [1204/1759 ( 68%)]  Loss: 4.802 (4.37)  LR: 1.212e-03  Grad: 4.5696  max=3.5534(module.vfe.pfn_layers.0.linear.weight)  min: -1.6317(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8667, loss_cls=0.1817, loss_bbox=0.9698, matched_ious=0.4627, loss_iou=0.1012, loss_iou_reg=0.2495, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.24(1.27)  Time cost: 25:31/11:45 [3:32:16/18:49:16]  Acc_iter 10000       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.27)
2025-09-03 12:27:16,674   INFO  Train:    6/36 ( 17%) [1254/1759 ( 71%)]  Loss: 4.453 (4.37)  LR: 1.220e-03  Grad: 3.2715  max=0.1780(module.dense_head.prediction_head.dim.1.bias)  min: -1.4577(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8416, loss_cls=0.1815, loss_bbox=0.8981, matched_ious=0.4586, loss_iou=0.0983, loss_iou_reg=0.2543, d_time=0.00(0.01), f_time=1.16(1.26), b_time=1.17(1.27)  Time cost: 26:34/10:41 [3:33:19/18:47:54]  Acc_iter 10050       Data time: 0.00(0.01)  Forward time: 1.16(1.26)  Batch time: 1.17(1.27)
2025-09-03 12:28:20,115   INFO  Train:    6/36 ( 17%) [1304/1759 ( 74%)]  Loss: 4.291 (4.36)  LR: 1.228e-03  Grad: 3.2776  max=1.7929(module.vfe.pfn_layers.0.linear.weight)  min: -0.3604(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8349, loss_cls=0.1734, loss_bbox=0.9732, matched_ious=0.4609, loss_iou=0.1008, loss_iou_reg=0.2500, d_time=0.00(0.01), f_time=1.36(1.26), b_time=1.36(1.27)  Time cost: 27:37/09:37 [3:34:22/18:46:48]  Acc_iter 10100       Data time: 0.00(0.01)  Forward time: 1.36(1.26)  Batch time: 1.36(1.27)
2025-09-03 12:29:22,917   INFO  Train:    6/36 ( 17%) [1354/1759 ( 77%)]  Loss: 4.050 (4.36)  LR: 1.236e-03  Grad: 4.3471  max=3.5940(module.vfe.pfn_layers.0.linear.weight)  min: -0.1441(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.8681, loss_cls=0.1821, loss_bbox=1.0292, matched_ious=0.4514, loss_iou=0.1025, loss_iou_reg=0.2546, d_time=0.00(0.01), f_time=1.29(1.26), b_time=1.29(1.27)  Time cost: 28:40/08:34 [3:35:25/18:45:16]  Acc_iter 10150       Data time: 0.00(0.01)  Forward time: 1.29(1.26)  Batch time: 1.29(1.27)
2025-09-03 12:30:25,693   INFO  Train:    6/36 ( 17%) [1404/1759 ( 80%)]  Loss: 4.252 (4.36)  LR: 1.244e-03  Grad: 2.7132  max=0.4643(module.vfe.pfn_layers.0.linear.weight)  min: -1.7999(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8663, loss_cls=0.1774, loss_bbox=1.0030, matched_ious=0.4652, loss_iou=0.1002, loss_iou_reg=0.2462, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.26(1.27)  Time cost: 29:43/07:30 [3:36:28/18:43:46]  Acc_iter 10200       Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.27)
2025-09-03 12:31:30,273   INFO  Train:    6/36 ( 17%) [1454/1759 ( 83%)]  Loss: 4.722 (4.36)  LR: 1.252e-03  Grad: 2.2326  max=0.4630(module.vfe.pfn_layers.0.linear.weight)  min: -0.9823(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8615, loss_cls=0.1801, loss_bbox=0.9717, matched_ious=0.4658, loss_iou=0.0990, loss_iou_reg=0.2480, d_time=0.00(0.01), f_time=1.30(1.26), b_time=1.30(1.27)  Time cost: 30:47/06:27 [3:37:32/18:43:23]  Acc_iter 10250       Data time: 0.00(0.01)  Forward time: 1.30(1.26)  Batch time: 1.30(1.27)
2025-09-03 12:32:33,378   INFO  Train:    6/36 ( 17%) [1504/1759 ( 86%)]  Loss: 3.877 (4.36)  LR: 1.260e-03  Grad: 8.6488  max=6.8020(module.vfe.pfn_layers.0.linear.weight)  min: -4.6442(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8783, loss_cls=0.1814, loss_bbox=1.0106, matched_ious=0.4575, loss_iou=0.0972, loss_iou_reg=0.2517, d_time=0.00(0.01), f_time=1.31(1.26), b_time=1.31(1.27)  Time cost: 31:50/05:23 [3:38:35/18:42:06]  Acc_iter 10300       Data time: 0.00(0.01)  Forward time: 1.31(1.26)  Batch time: 1.31(1.27)
2025-09-03 12:33:36,274   INFO  Train:    6/36 ( 17%) [1554/1759 ( 88%)]  Loss: 4.988 (4.35)  LR: 1.268e-03  Grad: 3.6818  max=1.5939(module.vfe.pfn_layers.0.linear.weight)  min: -2.3619(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8271, loss_cls=0.1751, loss_bbox=0.9368, matched_ious=0.4656, loss_iou=0.1003, loss_iou_reg=0.2475, d_time=0.02(0.01), f_time=1.29(1.26), b_time=1.31(1.27)  Time cost: 32:53/04:20 [3:39:38/18:40:42]  Acc_iter 10350       Data time: 0.02(0.01)  Forward time: 1.29(1.26)  Batch time: 1.31(1.27)
2025-09-03 12:34:39,545   INFO  Train:    6/36 ( 17%) [1604/1759 ( 91%)]  Loss: 4.367 (4.35)  LR: 1.276e-03  Grad: 3.6227  max=2.7057(module.vfe.pfn_layers.0.linear.weight)  min: -0.4401(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8728, loss_cls=0.1828, loss_bbox=1.0121, matched_ious=0.4655, loss_iou=0.0994, loss_iou_reg=0.2477, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 33:57/03:16 [3:40:42/18:39:32]  Acc_iter 10400       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-03 12:35:42,578   INFO  Train:    6/36 ( 17%) [1654/1759 ( 94%)]  Loss: 4.728 (4.35)  LR: 1.284e-03  Grad: 3.7669  max=1.0899(module.vfe.pfn_layers.0.linear.weight)  min: -2.5645(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8686, loss_cls=0.1823, loss_bbox=1.0158, matched_ious=0.4592, loss_iou=0.0997, loss_iou_reg=0.2503, d_time=0.00(0.01), f_time=1.29(1.26), b_time=1.29(1.27)  Time cost: 35:00/02:13 [3:41:45/18:38:15]  Acc_iter 10450       Data time: 0.00(0.01)  Forward time: 1.29(1.26)  Batch time: 1.29(1.27)
2025-09-03 12:36:46,318   INFO  Train:    6/36 ( 17%) [1704/1759 ( 97%)]  Loss: 4.727 (4.35)  LR: 1.292e-03  Grad: 4.6953  max=2.7919(module.vfe.pfn_layers.0.linear.weight)  min: -3.0627(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8761, loss_cls=0.1796, loss_bbox=1.0409, matched_ious=0.4614, loss_iou=0.1004, loss_iou_reg=0.2454, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.28(1.27)  Time cost: 36:03/01:09 [3:42:48/18:37:21]  Acc_iter 10500       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.27)
2025-09-03 12:37:48,277   INFO  Train:    6/36 ( 17%) [1754/1759 (100%)]  Loss: 4.358 (4.35)  LR: 1.300e-03  Grad: 2.0327  max=0.5434(module.vfe.pfn_layers.0.linear.weight)  min: -0.7023(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8423, loss_cls=0.1764, loss_bbox=0.9288, matched_ious=0.4689, loss_iou=0.0982, loss_iou_reg=0.2475, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.22(1.27)  Time cost: 37:05/00:06 [3:43:50/18:35:33]  Acc_iter 10550       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.27)
2025-09-03 12:37:53,005   INFO  Train:    6/36 ( 17%) [1758/1759 (100%)]  Loss: 3.974 (4.35)  LR: 1.300e-03  Grad: 6.7701  max=4.9054(module.vfe.pfn_layers.0.linear.weight)  min: -3.8635(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7842, loss_cls=0.1710, loss_bbox=0.8519, matched_ious=0.4689, loss_iou=0.0998, loss_iou_reg=0.2478, d_time=0.00(0.01), f_time=0.78(1.26), b_time=0.79(1.27)  Time cost: 37:10/00:01 [3:43:55/18:35:17]  Acc_iter 10554       Data time: 0.00(0.01)  Forward time: 0.78(1.26)  Batch time: 0.79(1.27)

                                               [Aepochs:  17%|█▋        | 6/36 [3:43:55<18:38:02, 2236.08s/it]epochs:  17%|█▋        | 6/36 [3:43:56<18:38:03, 2236.13s/it]epochs:  17%|█▋        | 6/36 [3:43:56<18:38:03, 2236.12s/it]epochs:  17%|█▋        | 6/36 [3:43:56<18:38:03, 2236.11s/it]epochs:  17%|█▋        | 6/36 [3:43:56<18:38:03, 2236.10s/it]epochs:  17%|█▋        | 6/36 [3:43:56<18:38:03, 2236.11s/it]epochs:  17%|█▋        | 6/36 [3:43:56<18:38:03, 2236.11s/it]epochs:  17%|█▋        | 6/36 [3:43:56<18:38:03, 2236.12s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 12:37:58,536   INFO  Train:    7/36 ( 19%) [   0/1759 (  0%)]  Loss: 4.447 (4.45)  LR: 1.301e-03  Grad: 1.9690  max=0.4031(module.vfe.pfn_layers.0.linear.weight)  min: -0.2034(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8616, loss_cls=0.2027, loss_bbox=0.9537, matched_ious=0.4320, loss_iou=0.1078, loss_iou_reg=0.2651, d_time=1.89(1.89), f_time=2.36(2.36), b_time=4.25(4.25)  Time cost: 00:03/1:55:05 [3:44:01/57:32:37]  Acc_iter 10555       Data time: 1.89(1.89)  Forward time: 2.36(2.36)  Batch time: 4.25(4.25)
2025-09-03 12:38:55,583   INFO  Train:    7/36 ( 19%) [  45/1759 (  3%)]  Loss: 4.341 (4.20)  LR: 1.308e-03  Grad: 3.5493  max=1.3616(module.vfe.pfn_layers.0.linear.weight)  min: -2.1261(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8391, loss_cls=0.1810, loss_bbox=0.9543, matched_ious=0.4617, loss_iou=0.1017, loss_iou_reg=0.2501, d_time=0.00(0.05), f_time=1.31(1.29), b_time=1.31(1.33)  Time cost: 01:00/37:51 [3:44:58/19:24:27]  Acc_iter 10600       Data time: 0.00(0.05)  Forward time: 1.31(1.29)  Batch time: 1.31(1.33)
2025-09-03 12:39:58,907   INFO  Train:    7/36 ( 19%) [  95/1759 (  5%)]  Loss: 4.141 (4.28)  LR: 1.316e-03  Grad: 2.4270  max=1.2490(module.vfe.pfn_layers.0.linear.weight)  min: -0.6593(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8563, loss_cls=0.1774, loss_bbox=1.0212, matched_ious=0.4557, loss_iou=0.1001, loss_iou_reg=0.2507, d_time=0.00(0.03), f_time=1.20(1.27), b_time=1.21(1.30)  Time cost: 02:04/35:54 [3:46:01/18:56:41]  Acc_iter 10650       Data time: 0.00(0.03)  Forward time: 1.20(1.27)  Batch time: 1.21(1.30)
2025-09-03 12:41:01,934   INFO  Train:    7/36 ( 19%) [ 145/1759 (  8%)]  Loss: 4.639 (4.25)  LR: 1.324e-03  Grad: 2.5668  max=1.3504(module.vfe.pfn_layers.0.linear.weight)  min: -0.5289(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8243, loss_cls=0.1702, loss_bbox=0.9963, matched_ious=0.4650, loss_iou=0.1003, loss_iou_reg=0.2479, d_time=0.01(0.02), f_time=1.31(1.27), b_time=1.32(1.29)  Time cost: 03:07/34:30 [3:47:04/18:45:19]  Acc_iter 10700       Data time: 0.01(0.02)  Forward time: 1.31(1.27)  Batch time: 1.32(1.29)
2025-09-03 12:42:07,199   INFO  Train:    7/36 ( 19%) [ 195/1759 ( 11%)]  Loss: 3.946 (4.27)  LR: 1.332e-03  Grad: 3.0201  max=2.0316(module.vfe.pfn_layers.0.linear.weight)  min: -0.1462(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.8640, loss_cls=0.1742, loss_bbox=1.0146, matched_ious=0.4637, loss_iou=0.0993, loss_iou_reg=0.2504, d_time=0.00(0.02), f_time=1.25(1.27), b_time=1.25(1.29)  Time cost: 04:12/33:35 [3:48:09/18:49:14]  Acc_iter 10750       Data time: 0.00(0.02)  Forward time: 1.25(1.27)  Batch time: 1.25(1.29)
2025-09-03 12:43:10,308   INFO  Train:    7/36 ( 19%) [ 245/1759 ( 14%)]  Loss: 4.274 (4.26)  LR: 1.340e-03  Grad: 2.6117  max=1.3618(module.vfe.pfn_layers.0.linear.weight)  min: -0.2120(module.dense_head.prediction_head.dim.1.bias)  NaN: False  loss_hm=0.8271, loss_cls=0.1731, loss_bbox=0.9979, matched_ious=0.4574, loss_iou=0.0997, loss_iou_reg=0.2526, d_time=0.00(0.02), f_time=1.26(1.27), b_time=1.27(1.28)  Time cost: 05:15/32:22 [3:49:12/18:43:26]  Acc_iter 10800       Data time: 0.00(0.02)  Forward time: 1.26(1.27)  Batch time: 1.27(1.28)
2025-09-03 12:44:14,600   INFO  Train:    7/36 ( 19%) [ 295/1759 ( 17%)]  Loss: 4.024 (4.24)  LR: 1.349e-03  Grad: 2.5510  max=0.3655(module.vfe.pfn_layers.0.linear.weight)  min: -0.9951(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8099, loss_cls=0.1687, loss_bbox=0.9462, matched_ious=0.4655, loss_iou=0.0994, loss_iou_reg=0.2486, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.28)  Time cost: 06:19/31:19 [3:50:17/18:42:44]  Acc_iter 10850       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.28)
2025-09-03 12:45:17,833   INFO  Train:    7/36 ( 19%) [ 345/1759 ( 20%)]  Loss: 4.534 (4.25)  LR: 1.357e-03  Grad: 5.5692  max=0.1988(module.backbone_3d.cls_conv.3.weight)  min: -4.0533(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8583, loss_cls=0.1763, loss_bbox=0.9818, matched_ious=0.4601, loss_iou=0.0988, loss_iou_reg=0.2503, d_time=0.00(0.01), f_time=1.25(1.27), b_time=1.25(1.28)  Time cost: 07:23/30:11 [3:51:20/18:39:15]  Acc_iter 10900       Data time: 0.00(0.01)  Forward time: 1.25(1.27)  Batch time: 1.25(1.28)
2025-09-03 12:46:20,463   INFO  Train:    7/36 ( 19%) [ 395/1759 ( 22%)]  Loss: 3.803 (4.26)  LR: 1.365e-03  Grad: 2.8060  max=1.6530(module.vfe.pfn_layers.0.linear.weight)  min: -0.1713(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.8594, loss_cls=0.1756, loss_bbox=1.0314, matched_ious=0.4651, loss_iou=0.0973, loss_iou_reg=0.2452, d_time=0.00(0.01), f_time=1.27(1.27), b_time=1.27(1.28)  Time cost: 08:25/29:02 [3:52:22/18:35:04]  Acc_iter 10950       Data time: 0.00(0.01)  Forward time: 1.27(1.27)  Batch time: 1.27(1.28)
2025-09-03 12:47:22,798   INFO  Train:    7/36 ( 19%) [ 445/1759 ( 25%)]  Loss: 4.160 (4.26)  LR: 1.373e-03  Grad: 2.7827  max=1.5856(module.vfe.pfn_layers.0.linear.weight)  min: -0.2373(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8718, loss_cls=0.1832, loss_bbox=0.9717, matched_ious=0.4684, loss_iou=0.1003, loss_iou_reg=0.2469, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.24(1.27)  Time cost: 09:28/27:53 [3:53:25/18:31:00]  Acc_iter 11000       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.27)
2025-09-03 12:48:26,070   INFO  Train:    7/36 ( 19%) [ 495/1759 ( 28%)]  Loss: 5.257 (4.27)  LR: 1.381e-03  Grad: 3.5497  max=1.5831(module.vfe.pfn_layers.0.linear.weight)  min: -1.6384(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8648, loss_cls=0.1784, loss_bbox=1.0256, matched_ious=0.4597, loss_iou=0.1016, loss_iou_reg=0.2498, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.27(1.27)  Time cost: 10:31/26:49 [3:54:28/18:29:11]  Acc_iter 11050       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.27)
2025-09-03 12:49:28,875   INFO  Train:    7/36 ( 19%) [ 545/1759 ( 31%)]  Loss: 4.461 (4.26)  LR: 1.390e-03  Grad: 4.2331  max=0.5058(module.backbone_3d.cls_conv.3.weight)  min: -2.6080(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8196, loss_cls=0.1742, loss_bbox=0.8883, matched_ious=0.4786, loss_iou=0.0968, loss_iou_reg=0.2432, d_time=0.00(0.01), f_time=1.30(1.26), b_time=1.30(1.27)  Time cost: 11:34/25:43 [3:55:31/18:26:46]  Acc_iter 11100       Data time: 0.00(0.01)  Forward time: 1.30(1.26)  Batch time: 1.30(1.27)
2025-09-03 12:50:31,571   INFO  Train:    7/36 ( 19%) [ 595/1759 ( 34%)]  Loss: 4.014 (4.25)  LR: 1.398e-03  Grad: 4.2985  max=0.1632(module.backbone_3d.cls_conv.3.weight)  min: -3.5733(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8275, loss_cls=0.1692, loss_bbox=0.9560, matched_ious=0.4651, loss_iou=0.0983, loss_iou_reg=0.2499, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.20(1.27)  Time cost: 12:36/24:38 [3:56:34/18:24:25]  Acc_iter 11150       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.20(1.27)
2025-09-03 12:51:34,676   INFO  Train:    7/36 ( 19%) [ 645/1759 ( 37%)]  Loss: 3.888 (4.25)  LR: 1.406e-03  Grad: 3.6053  max=1.5328(module.vfe.pfn_layers.0.linear.weight)  min: -2.5522(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8443, loss_cls=0.1729, loss_bbox=0.9716, matched_ious=0.4669, loss_iou=0.0996, loss_iou_reg=0.2474, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.29(1.27)  Time cost: 13:40/23:34 [3:57:37/18:22:50]  Acc_iter 11200       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.29(1.27)
2025-09-03 12:52:39,354   INFO  Train:    7/36 ( 19%) [ 695/1759 ( 40%)]  Loss: 4.642 (4.25)  LR: 1.414e-03  Grad: 2.1973  max=0.5085(module.vfe.pfn_layers.0.linear.weight)  min: -0.3210(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.8490, loss_cls=0.1707, loss_bbox=0.9995, matched_ious=0.4640, loss_iou=0.0998, loss_iou_reg=0.2479, d_time=0.00(0.01), f_time=1.31(1.26), b_time=1.32(1.27)  Time cost: 14:44/22:32 [3:58:41/18:23:16]  Acc_iter 11250       Data time: 0.00(0.01)  Forward time: 1.31(1.26)  Batch time: 1.32(1.27)
2025-09-03 12:53:42,469   INFO  Train:    7/36 ( 19%) [ 745/1759 ( 42%)]  Loss: 3.850 (4.25)  LR: 1.422e-03  Grad: 2.2551  max=0.6439(module.vfe.pfn_layers.0.linear.weight)  min: -0.8833(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8500, loss_cls=0.1728, loss_bbox=1.0425, matched_ious=0.4622, loss_iou=0.1002, loss_iou_reg=0.2471, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.25(1.27)  Time cost: 15:47/21:28 [3:59:44/18:21:42]  Acc_iter 11300       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.27)
2025-09-03 12:54:47,026   INFO  Train:    7/36 ( 19%) [ 795/1759 ( 45%)]  Loss: 3.610 (4.25)  LR: 1.431e-03  Grad: 2.4673  max=0.9272(module.vfe.pfn_layers.0.linear.weight)  min: -0.8171(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8394, loss_cls=0.1670, loss_bbox=0.9884, matched_ious=0.4712, loss_iou=0.0999, loss_iou_reg=0.2434, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.27(1.27)  Time cost: 16:52/20:26 [4:00:49/18:21:45]  Acc_iter 11350       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.27)
2025-09-03 12:55:49,658   INFO  Train:    7/36 ( 19%) [ 845/1759 ( 48%)]  Loss: 4.655 (4.26)  LR: 1.439e-03  Grad: 3.2140  max=1.9683(module.vfe.pfn_layers.0.linear.weight)  min: -0.0701(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=0.8734, loss_cls=0.1703, loss_bbox=1.0394, matched_ious=0.4527, loss_iou=0.1015, loss_iou_reg=0.2539, d_time=0.00(0.01), f_time=1.30(1.26), b_time=1.31(1.27)  Time cost: 17:55/19:21 [4:01:52/18:19:43]  Acc_iter 11400       Data time: 0.00(0.01)  Forward time: 1.30(1.26)  Batch time: 1.31(1.27)
2025-09-03 12:56:52,327   INFO  Train:    7/36 ( 19%) [ 895/1759 ( 51%)]  Loss: 4.512 (4.26)  LR: 1.447e-03  Grad: 3.6615  max=0.9052(module.vfe.pfn_layers.0.linear.weight)  min: -2.4738(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8421, loss_cls=0.1721, loss_bbox=0.9620, matched_ious=0.4697, loss_iou=0.1006, loss_iou_reg=0.2450, d_time=0.02(0.01), f_time=1.31(1.26), b_time=1.32(1.27)  Time cost: 18:57/18:17 [4:02:54/18:17:49]  Acc_iter 11450       Data time: 0.02(0.01)  Forward time: 1.31(1.26)  Batch time: 1.32(1.27)
2025-09-03 12:57:54,507   INFO  Train:    7/36 ( 19%) [ 945/1759 ( 54%)]  Loss: 4.014 (4.26)  LR: 1.456e-03  Grad: 3.8154  max=2.5284(module.vfe.pfn_layers.0.linear.weight)  min: -1.2055(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8464, loss_cls=0.1667, loss_bbox=0.9989, matched_ious=0.4623, loss_iou=0.0990, loss_iou_reg=0.2501, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.23(1.27)  Time cost: 19:59/17:12 [4:03:57/18:15:34]  Acc_iter 11500       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.27)
2025-09-03 12:58:56,784   INFO  Train:    7/36 ( 19%) [ 995/1759 ( 57%)]  Loss: 3.491 (4.25)  LR: 1.464e-03  Grad: 2.8891  max=0.7734(module.vfe.pfn_layers.0.linear.weight)  min: -1.0594(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8464, loss_cls=0.1746, loss_bbox=0.9672, matched_ious=0.4624, loss_iou=0.1004, loss_iou_reg=0.2506, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.20(1.27)  Time cost: 21:02/16:08 [4:04:59/18:13:31]  Acc_iter 11550       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.20(1.27)
2025-09-03 12:59:59,491   INFO  Train:    7/36 ( 19%) [1045/1759 ( 59%)]  Loss: 4.904 (4.25)  LR: 1.472e-03  Grad: 1.6789  max=0.7712(module.vfe.pfn_layers.0.linear.weight)  min: -0.6292(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8592, loss_cls=0.1723, loss_bbox=1.0095, matched_ious=0.4640, loss_iou=0.0981, loss_iou_reg=0.2490, d_time=0.00(0.01), f_time=1.43(1.26), b_time=1.44(1.27)  Time cost: 22:04/15:04 [4:06:02/18:11:55]  Acc_iter 11600       Data time: 0.00(0.01)  Forward time: 1.43(1.26)  Batch time: 1.44(1.27)
2025-09-03 13:01:01,915   INFO  Train:    7/36 ( 19%) [1095/1759 ( 62%)]  Loss: 4.472 (4.26)  LR: 1.480e-03  Grad: 2.8144  max=2.2760(module.vfe.pfn_layers.0.linear.weight)  min: -0.1598(module.dense_head.prediction_head.dim.1.weight)  NaN: False  loss_hm=0.8553, loss_cls=0.1758, loss_bbox=0.9885, matched_ious=0.4690, loss_iou=0.0963, loss_iou_reg=0.2460, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 23:07/14:00 [4:07:04/18:10:09]  Acc_iter 11650       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-03 13:02:04,810   INFO  Train:    7/36 ( 19%) [1145/1759 ( 65%)]  Loss: 4.183 (4.25)  LR: 1.489e-03  Grad: 3.0807  max=2.1938(module.vfe.pfn_layers.0.linear.weight)  min: -0.9257(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8123, loss_cls=0.1678, loss_bbox=0.9586, matched_ious=0.4602, loss_iou=0.0996, loss_iou_reg=0.2501, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.21(1.27)  Time cost: 24:10/12:56 [4:08:07/18:08:48]  Acc_iter 11700       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.27)
2025-09-03 13:03:09,461   INFO  Train:    7/36 ( 19%) [1195/1759 ( 68%)]  Loss: 4.241 (4.25)  LR: 1.497e-03  Grad: 2.3708  max=1.2107(module.vfe.pfn_layers.0.linear.weight)  min: -0.1669(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.8200, loss_cls=0.1663, loss_bbox=0.9365, matched_ious=0.4698, loss_iou=0.1000, loss_iou_reg=0.2460, d_time=0.00(0.01), f_time=1.38(1.26), b_time=1.39(1.27)  Time cost: 25:14/11:54 [4:09:11/18:08:44]  Acc_iter 11750       Data time: 0.00(0.01)  Forward time: 1.38(1.26)  Batch time: 1.39(1.27)
2025-09-03 13:04:13,008   INFO  Train:    7/36 ( 19%) [1245/1759 ( 71%)]  Loss: 5.018 (4.24)  LR: 1.505e-03  Grad: 2.2204  max=0.5778(module.vfe.pfn_layers.0.linear.weight)  min: -0.9246(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7966, loss_cls=0.1606, loss_bbox=0.9247, matched_ious=0.4711, loss_iou=0.0986, loss_iou_reg=0.2484, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.23(1.27)  Time cost: 26:18/10:51 [4:10:15/18:07:50]  Acc_iter 11800       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.27)
2025-09-03 13:05:15,529   INFO  Train:    7/36 ( 19%) [1295/1759 ( 74%)]  Loss: 3.943 (4.24)  LR: 1.514e-03  Grad: 2.6696  max=0.8949(module.vfe.pfn_layers.0.linear.weight)  min: -0.6006(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8487, loss_cls=0.1692, loss_bbox=0.9456, matched_ious=0.4619, loss_iou=0.0976, loss_iou_reg=0.2504, d_time=0.01(0.01), f_time=1.23(1.26), b_time=1.24(1.27)  Time cost: 27:20/09:47 [4:11:18/18:06:14]  Acc_iter 11850       Data time: 0.01(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.27)
2025-09-03 13:06:18,786   INFO  Train:    7/36 ( 19%) [1345/1759 ( 76%)]  Loss: 4.287 (4.24)  LR: 1.522e-03  Grad: 3.9695  max=2.9369(module.vfe.pfn_layers.0.linear.weight)  min: -1.4415(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8160, loss_cls=0.1682, loss_bbox=0.9460, matched_ious=0.4642, loss_iou=0.1009, loss_iou_reg=0.2468, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.24(1.27)  Time cost: 28:24/08:44 [4:12:21/18:05:09]  Acc_iter 11900       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.27)
2025-09-03 13:07:21,727   INFO  Train:    7/36 ( 19%) [1395/1759 ( 79%)]  Loss: 3.968 (4.24)  LR: 1.530e-03  Grad: 2.8302  max=1.6101(module.vfe.pfn_layers.0.linear.weight)  min: -1.3133(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8760, loss_cls=0.1759, loss_bbox=1.0200, matched_ious=0.4711, loss_iou=0.0979, loss_iou_reg=0.2424, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.23(1.27)  Time cost: 29:27/07:40 [4:13:24/18:03:52]  Acc_iter 11950       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.27)
2025-09-03 13:08:24,105   INFO  Train:    7/36 ( 19%) [1445/1759 ( 82%)]  Loss: 3.576 (4.24)  LR: 1.539e-03  Grad: 3.4707  max=2.8453(module.vfe.pfn_layers.0.linear.weight)  min: -0.7857(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8329, loss_cls=0.1701, loss_bbox=0.9668, matched_ious=0.4714, loss_iou=0.0973, loss_iou_reg=0.2431, d_time=0.00(0.01), f_time=1.17(1.26), b_time=1.18(1.27)  Time cost: 30:29/06:37 [4:14:26/18:02:16]  Acc_iter 12000       Data time: 0.00(0.01)  Forward time: 1.17(1.26)  Batch time: 1.18(1.27)
2025-09-03 13:09:26,855   INFO  Train:    7/36 ( 19%) [1495/1759 ( 85%)]  Loss: 3.850 (4.23)  LR: 1.547e-03  Grad: 3.0616  max=0.5947(module.vfe.pfn_layers.0.linear.weight)  min: -1.7506(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8292, loss_cls=0.1690, loss_bbox=0.9357, matched_ious=0.4735, loss_iou=0.0986, loss_iou_reg=0.2452, d_time=0.00(0.01), f_time=1.35(1.26), b_time=1.36(1.27)  Time cost: 31:32/05:33 [4:15:29/18:00:56]  Acc_iter 12050       Data time: 0.00(0.01)  Forward time: 1.35(1.26)  Batch time: 1.36(1.27)
2025-09-03 13:10:29,479   INFO  Train:    7/36 ( 19%) [1545/1759 ( 88%)]  Loss: 4.308 (4.23)  LR: 1.555e-03  Grad: 3.1332  max=0.2205(module.dense_head.prediction_head.height.1.weight)  min: -1.6631(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8286, loss_cls=0.1669, loss_bbox=0.9936, matched_ious=0.4612, loss_iou=0.0996, loss_iou_reg=0.2523, d_time=0.00(0.01), f_time=1.32(1.26), b_time=1.32(1.26)  Time cost: 32:34/04:30 [4:16:32/17:59:32]  Acc_iter 12100       Data time: 0.00(0.01)  Forward time: 1.32(1.26)  Batch time: 1.32(1.26)
2025-09-03 13:11:32,403   INFO  Train:    7/36 ( 19%) [1595/1759 ( 91%)]  Loss: 4.119 (4.23)  LR: 1.564e-03  Grad: 3.6017  max=1.1122(module.vfe.pfn_layers.0.linear.weight)  min: -2.2401(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8113, loss_cls=0.1589, loss_bbox=0.9629, matched_ious=0.4709, loss_iou=0.0982, loss_iou_reg=0.2457, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.26)  Time cost: 33:37/03:27 [4:17:34/17:58:19]  Acc_iter 12150       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.26)
2025-09-03 13:12:35,667   INFO  Train:    7/36 ( 19%) [1645/1759 ( 94%)]  Loss: 4.237 (4.23)  LR: 1.572e-03  Grad: 3.7768  max=0.2788(module.dense_head.prediction_head.height.1.weight)  min: -1.9451(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8355, loss_cls=0.1671, loss_bbox=0.9375, matched_ious=0.4717, loss_iou=0.0986, loss_iou_reg=0.2461, d_time=0.00(0.01), f_time=1.35(1.26), b_time=1.35(1.26)  Time cost: 34:41/02:24 [4:18:38/17:57:17]  Acc_iter 12200       Data time: 0.00(0.01)  Forward time: 1.35(1.26)  Batch time: 1.35(1.26)
2025-09-03 13:13:41,855   INFO  Train:    7/36 ( 19%) [1695/1759 ( 96%)]  Loss: 4.376 (4.23)  LR: 1.580e-03  Grad: 2.5421  max=0.8290(module.vfe.pfn_layers.0.linear.weight)  min: -1.1972(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8329, loss_cls=0.1652, loss_bbox=0.9745, matched_ious=0.4676, loss_iou=0.0990, loss_iou_reg=0.2473, d_time=0.00(0.01), f_time=1.34(1.26), b_time=1.35(1.27)  Time cost: 35:47/01:21 [4:19:44/17:57:44]  Acc_iter 12250       Data time: 0.00(0.01)  Forward time: 1.34(1.26)  Batch time: 1.35(1.27)
2025-09-03 13:14:45,181   INFO  Train:    7/36 ( 19%) [1745/1759 ( 99%)]  Loss: 4.028 (4.23)  LR: 1.589e-03  Grad: 3.4167  max=2.0614(module.vfe.pfn_layers.0.linear.weight)  min: -1.1031(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8201, loss_cls=0.1634, loss_bbox=0.9277, matched_ious=0.4722, loss_iou=0.1007, loss_iou_reg=0.2480, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 36:50/00:17 [4:20:47/17:56:41]  Acc_iter 12300       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 13:15:00,723   INFO  Train:    7/36 ( 19%) [1758/1759 (100%)]  Loss: 4.463 (4.23)  LR: 1.591e-03  Grad: 3.7917  max=0.2214(module.dense_head.prediction_head.height.1.bias)  min: -2.1080(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8327, loss_cls=0.1746, loss_bbox=0.9448, matched_ious=0.4613, loss_iou=0.0989, loss_iou_reg=0.2434, d_time=0.01(0.01), f_time=0.73(1.26), b_time=0.74(1.27)  Time cost: 37:06/00:01 [4:21:03/17:55:58]  Acc_iter 12313       Data time: 0.01(0.01)  Forward time: 0.73(1.26)  Batch time: 0.74(1.27)

                                               [Aepochs:  19%|█▉        | 7/36 [4:21:03<17:59:25, 2233.30s/it]epochs:  19%|█▉        | 7/36 [4:21:03<17:59:25, 2233.31s/it]epochs:  19%|█▉        | 7/36 [4:21:03<17:59:28, 2233.40s/it]epochs:  19%|█▉        | 7/36 [4:21:03<17:59:28, 2233.39s/it]epochs:  19%|█▉        | 7/36 [4:21:03<17:59:28, 2233.38s/it]epochs:  19%|█▉        | 7/36 [4:21:03<17:59:28, 2233.38s/it]epochs:  19%|█▉        | 7/36 [4:21:03<17:59:28, 2233.38s/it]epochs:  19%|█▉        | 7/36 [4:21:04<17:59:28, 2233.39s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 13:15:06,269   INFO  Train:    8/36 ( 22%) [   0/1759 (  0%)]  Loss: 3.989 (3.99)  LR: 1.591e-03  Grad: 3.1592  max=1.8973(module.vfe.pfn_layers.0.linear.weight)  min: -0.1618(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.7747, loss_cls=0.1822, loss_bbox=0.6819, matched_ious=0.4938, loss_iou=0.0914, loss_iou_reg=0.2367, d_time=1.56(1.56), f_time=2.65(2.65), b_time=4.21(4.21)  Time cost: 00:03/1:52:13 [4:21:08/54:14:17]  Acc_iter 12314       Data time: 1.56(1.56)  Forward time: 2.65(2.65)  Batch time: 4.21(4.21)
2025-09-03 13:15:51,684   INFO  Train:    8/36 ( 22%) [  36/1759 (  2%)]  Loss: 3.947 (4.25)  LR: 1.597e-03  Grad: 3.6284  max=0.7548(module.vfe.pfn_layers.0.linear.weight)  min: -2.4557(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8171, loss_cls=0.1646, loss_bbox=1.0114, matched_ious=0.4669, loss_iou=0.0979, loss_iou_reg=0.2441, d_time=0.01(0.05), f_time=1.35(1.29), b_time=1.35(1.34)  Time cost: 00:49/38:14 [4:21:54/18:51:09]  Acc_iter 12350       Data time: 0.01(0.05)  Forward time: 1.35(1.29)  Batch time: 1.35(1.34)
2025-09-03 13:16:55,001   INFO  Train:    8/36 ( 22%) [  86/1759 (  5%)]  Loss: 3.587 (4.20)  LR: 1.606e-03  Grad: 3.0622  max=0.6688(module.vfe.pfn_layers.0.linear.weight)  min: -0.6812(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8259, loss_cls=0.1657, loss_bbox=0.9337, matched_ious=0.4713, loss_iou=0.0991, loss_iou_reg=0.2455, d_time=0.00(0.02), f_time=1.23(1.28), b_time=1.24(1.30)  Time cost: 01:52/36:04 [4:22:57/18:18:17]  Acc_iter 12400       Data time: 0.00(0.02)  Forward time: 1.23(1.28)  Batch time: 1.24(1.30)
2025-09-03 13:17:58,342   INFO  Train:    8/36 ( 22%) [ 136/1759 (  8%)]  Loss: 3.995 (4.21)  LR: 1.614e-03  Grad: 3.5821  max=1.6519(module.vfe.pfn_layers.0.linear.weight)  min: -0.9186(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8302, loss_cls=0.1643, loss_bbox=0.9900, matched_ious=0.4659, loss_iou=0.0978, loss_iou_reg=0.2488, d_time=0.01(0.02), f_time=1.24(1.27), b_time=1.25(1.29)  Time cost: 02:55/34:44 [4:24:00/18:08:47]  Acc_iter 12450       Data time: 0.01(0.02)  Forward time: 1.24(1.27)  Batch time: 1.25(1.29)
2025-09-03 13:19:00,768   INFO  Train:    8/36 ( 22%) [ 186/1759 ( 11%)]  Loss: 4.668 (4.18)  LR: 1.622e-03  Grad: 3.4765  max=1.2728(module.vfe.pfn_layers.0.linear.weight)  min: -0.0897(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=0.8142, loss_cls=0.1706, loss_bbox=0.9354, matched_ious=0.4695, loss_iou=0.1010, loss_iou_reg=0.2451, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.28)  Time cost: 03:58/33:24 [4:25:03/17:59:40]  Acc_iter 12500       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.28)
2025-09-03 13:20:03,327   INFO  Train:    8/36 ( 22%) [ 236/1759 ( 13%)]  Loss: 4.599 (4.19)  LR: 1.631e-03  Grad: 5.3754  max=1.2455(module.vfe.pfn_layers.0.linear.weight)  min: -3.0617(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8357, loss_cls=0.1672, loss_bbox=0.9991, matched_ious=0.4600, loss_iou=0.0993, loss_iou_reg=0.2491, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.26(1.27)  Time cost: 05:00/32:13 [4:26:05/17:54:26]  Acc_iter 12550       Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.27)
2025-09-03 13:21:07,338   INFO  Train:    8/36 ( 22%) [ 286/1759 ( 16%)]  Loss: 4.427 (4.19)  LR: 1.639e-03  Grad: 2.8592  max=0.2956(module.dense_head.prediction_head.height.1.weight)  min: -0.9570(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8203, loss_cls=0.1625, loss_bbox=0.9519, matched_ious=0.4703, loss_iou=0.0994, loss_iou_reg=0.2445, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 06:04/31:12 [4:27:09/17:54:56]  Acc_iter 12600       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 13:22:10,196   INFO  Train:    8/36 ( 22%) [ 336/1759 ( 19%)]  Loss: 4.759 (4.17)  LR: 1.647e-03  Grad: 7.7244  max=3.8160(module.vfe.pfn_layers.0.linear.weight)  min: -6.1301(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7969, loss_cls=0.1611, loss_bbox=0.9455, matched_ious=0.4698, loss_iou=0.0987, loss_iou_reg=0.2456, d_time=0.00(0.01), f_time=1.14(1.26), b_time=1.14(1.27)  Time cost: 07:07/30:06 [4:28:12/17:52:04]  Acc_iter 12650       Data time: 0.00(0.01)  Forward time: 1.14(1.26)  Batch time: 1.14(1.27)
2025-09-03 13:23:14,532   INFO  Train:    8/36 ( 22%) [ 386/1759 ( 22%)]  Loss: 3.900 (4.18)  LR: 1.656e-03  Grad: 2.6695  max=0.1161(module.backbone_3d.cls_conv.3.weight)  min: -0.4754(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8331, loss_cls=0.1692, loss_bbox=0.9532, matched_ious=0.4607, loss_iou=0.0985, loss_iou_reg=0.2496, d_time=0.00(0.01), f_time=1.33(1.26), b_time=1.33(1.27)  Time cost: 08:12/29:05 [4:29:17/17:52:54]  Acc_iter 12700       Data time: 0.00(0.01)  Forward time: 1.33(1.26)  Batch time: 1.33(1.27)
2025-09-03 13:24:18,516   INFO  Train:    8/36 ( 22%) [ 436/1759 ( 25%)]  Loss: 3.208 (4.18)  LR: 1.664e-03  Grad: 3.4992  max=1.4300(module.vfe.pfn_layers.0.linear.weight)  min: -0.2103(module.dense_head.prediction_head.dim.1.weight)  NaN: False  loss_hm=0.8273, loss_cls=0.1640, loss_bbox=0.9562, matched_ious=0.4688, loss_iou=0.0984, loss_iou_reg=0.2457, d_time=0.01(0.01), f_time=1.17(1.26), b_time=1.18(1.27)  Time cost: 09:16/28:03 [4:30:21/17:52:37]  Acc_iter 12750       Data time: 0.01(0.01)  Forward time: 1.17(1.26)  Batch time: 1.18(1.27)
2025-09-03 13:25:21,307   INFO  Train:    8/36 ( 22%) [ 486/1759 ( 28%)]  Loss: 4.795 (4.17)  LR: 1.673e-03  Grad: 2.0881  max=0.6934(module.vfe.pfn_layers.0.linear.weight)  min: -0.1689(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.7953, loss_cls=0.1654, loss_bbox=0.8925, matched_ious=0.4684, loss_iou=0.0984, loss_iou_reg=0.2484, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.27(1.27)  Time cost: 10:18/26:57 [4:31:23/17:50:07]  Acc_iter 12800       Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.27(1.27)
2025-09-03 13:26:25,050   INFO  Train:    8/36 ( 22%) [ 536/1759 ( 30%)]  Loss: 3.894 (4.16)  LR: 1.681e-03  Grad: 2.4537  max=1.0037(module.vfe.pfn_layers.0.linear.weight)  min: -0.1028(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8184, loss_cls=0.1633, loss_bbox=0.9326, matched_ious=0.4710, loss_iou=0.0984, loss_iou_reg=0.2463, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.26(1.27)  Time cost: 11:22/25:54 [4:32:27/17:49:23]  Acc_iter 12850       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.27)
2025-09-03 13:27:28,275   INFO  Train:    8/36 ( 22%) [ 586/1759 ( 33%)]  Loss: 4.392 (4.16)  LR: 1.689e-03  Grad: 4.7227  max=0.1098(module.dense_head.prediction_head.dim.1.weight)  min: -4.0516(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8152, loss_cls=0.1648, loss_bbox=0.9313, matched_ious=0.4682, loss_iou=0.1006, loss_iou_reg=0.2481, d_time=0.02(0.01), f_time=1.19(1.26), b_time=1.21(1.27)  Time cost: 12:25/24:50 [4:33:30/17:47:51]  Acc_iter 12900       Data time: 0.02(0.01)  Forward time: 1.19(1.26)  Batch time: 1.21(1.27)
2025-09-03 13:28:31,684   INFO  Train:    8/36 ( 22%) [ 636/1759 ( 36%)]  Loss: 3.762 (4.15)  LR: 1.698e-03  Grad: 3.2929  max=0.3184(module.vfe.pfn_layers.0.linear.weight)  min: -2.1410(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7959, loss_cls=0.1606, loss_bbox=0.9305, matched_ious=0.4736, loss_iou=0.0998, loss_iou_reg=0.2445, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.23(1.27)  Time cost: 13:29/23:46 [4:34:34/17:46:37]  Acc_iter 12950       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.27)
2025-09-03 13:29:34,296   INFO  Train:    8/36 ( 22%) [ 686/1759 ( 39%)]  Loss: 3.991 (4.15)  LR: 1.706e-03  Grad: 2.6629  max=0.4919(module.vfe.pfn_layers.0.linear.weight)  min: -0.5833(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8103, loss_cls=0.1569, loss_bbox=0.9752, matched_ious=0.4607, loss_iou=0.1002, loss_iou_reg=0.2497, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.22(1.27)  Time cost: 14:31/22:41 [4:35:36/17:44:27]  Acc_iter 13000       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.27)
2025-09-03 13:30:36,990   INFO  Train:    8/36 ( 22%) [ 736/1759 ( 42%)]  Loss: 3.961 (4.16)  LR: 1.714e-03  Grad: 2.3849  max=0.6647(module.vfe.pfn_layers.0.linear.weight)  min: -0.3150(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8414, loss_cls=0.1658, loss_bbox=0.9527, matched_ious=0.4784, loss_iou=0.0984, loss_iou_reg=0.2433, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 15:34/21:37 [4:36:39/17:42:32]  Acc_iter 13050       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-03 13:31:40,035   INFO  Train:    8/36 ( 22%) [ 786/1759 ( 45%)]  Loss: 3.602 (4.15)  LR: 1.723e-03  Grad: 2.8689  max=0.6678(module.vfe.pfn_layers.0.linear.weight)  min: -1.2621(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7896, loss_cls=0.1606, loss_bbox=0.9076, matched_ious=0.4780, loss_iou=0.0980, loss_iou_reg=0.2463, d_time=0.00(0.01), f_time=1.32(1.26), b_time=1.32(1.27)  Time cost: 16:37/20:33 [4:37:42/17:41:06]  Acc_iter 13100       Data time: 0.00(0.01)  Forward time: 1.32(1.26)  Batch time: 1.32(1.27)
2025-09-03 13:32:43,790   INFO  Train:    8/36 ( 22%) [ 836/1759 ( 48%)]  Loss: 4.742 (4.15)  LR: 1.731e-03  Grad: 3.0769  max=0.7364(module.vfe.pfn_layers.0.linear.weight)  min: -1.3504(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8106, loss_cls=0.1621, loss_bbox=0.9453, matched_ious=0.4714, loss_iou=0.0978, loss_iou_reg=0.2438, d_time=0.02(0.01), f_time=1.28(1.26), b_time=1.30(1.27)  Time cost: 17:41/19:30 [4:38:46/17:40:25]  Acc_iter 13150       Data time: 0.02(0.01)  Forward time: 1.28(1.26)  Batch time: 1.30(1.27)
2025-09-03 13:33:46,924   INFO  Train:    8/36 ( 22%) [ 886/1759 ( 50%)]  Loss: 4.112 (4.14)  LR: 1.739e-03  Grad: 2.5242  max=0.2819(module.backbone_3d.cls_conv.3.weight)  min: -1.3612(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8037, loss_cls=0.1597, loss_bbox=0.9637, matched_ious=0.4758, loss_iou=0.1002, loss_iou_reg=0.2426, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 18:44/18:26 [4:39:49/17:39:06]  Acc_iter 13200       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 13:34:50,720   INFO  Train:    8/36 ( 22%) [ 936/1759 ( 53%)]  Loss: 4.226 (4.15)  LR: 1.748e-03  Grad: 2.4309  max=0.7970(module.vfe.pfn_layers.0.linear.weight)  min: -0.4424(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8116, loss_cls=0.1591, loss_bbox=0.9734, matched_ious=0.4759, loss_iou=0.0951, loss_iou_reg=0.2438, d_time=0.01(0.01), f_time=1.18(1.26), b_time=1.19(1.27)  Time cost: 19:48/17:23 [4:40:53/17:38:24]  Acc_iter 13250       Data time: 0.01(0.01)  Forward time: 1.18(1.26)  Batch time: 1.19(1.27)
2025-09-03 13:35:53,592   INFO  Train:    8/36 ( 22%) [ 986/1759 ( 56%)]  Loss: 4.281 (4.15)  LR: 1.756e-03  Grad: 4.5111  max=2.3152(module.vfe.pfn_layers.0.linear.weight)  min: -3.2039(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8535, loss_cls=0.1644, loss_bbox=0.9987, matched_ious=0.4567, loss_iou=0.1009, loss_iou_reg=0.2508, d_time=0.00(0.01), f_time=1.17(1.26), b_time=1.18(1.27)  Time cost: 20:51/16:19 [4:41:56/17:36:54]  Acc_iter 13300       Data time: 0.00(0.01)  Forward time: 1.17(1.26)  Batch time: 1.18(1.27)
2025-09-03 13:36:56,321   INFO  Train:    8/36 ( 22%) [1036/1759 ( 59%)]  Loss: 3.848 (4.15)  LR: 1.764e-03  Grad: 2.3214  max=1.0000(module.vfe.pfn_layers.0.linear.weight)  min: -0.1272(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8238, loss_cls=0.1617, loss_bbox=0.9535, matched_ious=0.4686, loss_iou=0.0997, loss_iou_reg=0.2452, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 21:53/15:16 [4:42:58/17:35:19]  Acc_iter 13350       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 13:38:00,022   INFO  Train:    8/36 ( 22%) [1086/1759 ( 62%)]  Loss: 3.594 (4.15)  LR: 1.773e-03  Grad: 2.9030  max=0.9195(module.vfe.pfn_layers.0.linear.weight)  min: -1.7382(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7973, loss_cls=0.1612, loss_bbox=0.9111, matched_ious=0.4779, loss_iou=0.0972, loss_iou_reg=0.2433, d_time=0.01(0.01), f_time=1.28(1.26), b_time=1.29(1.27)  Time cost: 22:57/14:12 [4:44:02/17:34:32]  Acc_iter 13400       Data time: 0.01(0.01)  Forward time: 1.28(1.26)  Batch time: 1.29(1.27)
2025-09-03 13:39:03,197   INFO  Train:    8/36 ( 22%) [1136/1759 ( 65%)]  Loss: 3.453 (4.14)  LR: 1.781e-03  Grad: 2.8517  max=0.9205(module.vfe.pfn_layers.0.linear.weight)  min: -0.3658(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7953, loss_cls=0.1560, loss_bbox=0.9195, matched_ious=0.4749, loss_iou=0.1006, loss_iou_reg=0.2460, d_time=0.00(0.01), f_time=1.29(1.26), b_time=1.30(1.27)  Time cost: 24:00/13:09 [4:45:05/17:33:20]  Acc_iter 13450       Data time: 0.00(0.01)  Forward time: 1.29(1.26)  Batch time: 1.30(1.27)
2025-09-03 13:40:06,524   INFO  Train:    8/36 ( 22%) [1186/1759 ( 67%)]  Loss: 3.864 (4.13)  LR: 1.789e-03  Grad: 3.5041  max=0.1415(module.backbone_3d.cls_conv.3.weight)  min: -2.1757(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7853, loss_cls=0.1535, loss_bbox=0.8946, matched_ious=0.4858, loss_iou=0.0974, loss_iou_reg=0.2377, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 25:04/12:06 [4:46:09/17:32:15]  Acc_iter 13500       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-03 13:41:09,361   INFO  Train:    8/36 ( 22%) [1236/1759 ( 70%)]  Loss: 3.784 (4.14)  LR: 1.798e-03  Grad: 2.9974  max=0.8147(module.vfe.pfn_layers.0.linear.weight)  min: -0.1299(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8308, loss_cls=0.1624, loss_bbox=0.9843, matched_ious=0.4704, loss_iou=0.0974, loss_iou_reg=0.2460, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.29(1.27)  Time cost: 26:06/11:02 [4:47:11/17:30:51]  Acc_iter 13550       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.29(1.27)
2025-09-03 13:42:12,820   INFO  Train:    8/36 ( 22%) [1286/1759 ( 73%)]  Loss: 3.467 (4.13)  LR: 1.806e-03  Grad: 3.3614  max=0.1435(module.dense_head.prediction_head.height.1.weight)  min: -1.1519(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7824, loss_cls=0.1579, loss_bbox=0.9139, matched_ious=0.4831, loss_iou=0.0960, loss_iou_reg=0.2404, d_time=0.00(0.01), f_time=1.37(1.26), b_time=1.37(1.27)  Time cost: 27:10/09:59 [4:48:15/17:29:52]  Acc_iter 13600       Data time: 0.00(0.01)  Forward time: 1.37(1.26)  Batch time: 1.37(1.27)
2025-09-03 13:43:16,273   INFO  Train:    8/36 ( 22%) [1336/1759 ( 76%)]  Loss: 3.748 (4.13)  LR: 1.814e-03  Grad: 3.2227  max=0.5990(module.vfe.pfn_layers.0.linear.weight)  min: -0.1636(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.8215, loss_cls=0.1579, loss_bbox=0.9936, matched_ious=0.4733, loss_iou=0.0974, loss_iou_reg=0.2424, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.27(1.27)  Time cost: 28:13/08:55 [4:49:18/17:28:53]  Acc_iter 13650       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.27)
2025-09-03 13:44:19,221   INFO  Train:    8/36 ( 22%) [1386/1759 ( 79%)]  Loss: 3.647 (4.13)  LR: 1.823e-03  Grad: 3.6063  max=1.0474(module.vfe.pfn_layers.0.linear.weight)  min: -0.5742(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8331, loss_cls=0.1673, loss_bbox=0.8830, matched_ious=0.4742, loss_iou=0.0981, loss_iou_reg=0.2446, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 29:16/07:52 [4:50:21/17:27:35]  Acc_iter 13700       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-03 13:45:23,009   INFO  Train:    8/36 ( 22%) [1436/1759 ( 82%)]  Loss: 4.058 (4.13)  LR: 1.831e-03  Grad: 3.3654  max=0.3682(module.vfe.pfn_layers.0.linear.weight)  min: -0.4878(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7958, loss_cls=0.1597, loss_bbox=0.8956, matched_ious=0.4776, loss_iou=0.0989, loss_iou_reg=0.2440, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.20(1.27)  Time cost: 30:20/06:49 [4:51:25/17:26:48]  Acc_iter 13750       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.27)
2025-09-03 13:46:25,447   INFO  Train:    8/36 ( 22%) [1486/1759 ( 84%)]  Loss: 3.740 (4.12)  LR: 1.839e-03  Grad: 3.7404  max=0.5831(module.vfe.pfn_layers.0.linear.weight)  min: -0.6858(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8153, loss_cls=0.1619, loss_bbox=0.8979, matched_ious=0.4779, loss_iou=0.0977, loss_iou_reg=0.2420, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.24(1.27)  Time cost: 31:23/05:45 [4:52:27/17:25:14]  Acc_iter 13800       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.27)
2025-09-03 13:47:27,816   INFO  Train:    8/36 ( 22%) [1536/1759 ( 87%)]  Loss: 3.953 (4.12)  LR: 1.848e-03  Grad: 3.8737  max=0.7598(module.vfe.pfn_layers.0.linear.weight)  min: -0.1114(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=0.7965, loss_cls=0.1632, loss_bbox=0.9274, matched_ious=0.4725, loss_iou=0.0969, loss_iou_reg=0.2456, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 32:25/04:42 [4:53:30/17:23:40]  Acc_iter 13850       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-03 13:48:30,789   INFO  Train:    8/36 ( 22%) [1586/1759 ( 90%)]  Loss: 4.613 (4.12)  LR: 1.856e-03  Grad: 4.7712  max=0.5950(module.vfe.pfn_layers.0.linear.weight)  min: -2.0313(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8373, loss_cls=0.1636, loss_bbox=0.9347, matched_ious=0.4802, loss_iou=0.0990, loss_iou_reg=0.2411, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.18(1.27)  Time cost: 33:28/03:38 [4:54:33/17:22:27]  Acc_iter 13900       Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.18(1.27)
2025-09-03 13:49:34,035   INFO  Train:    8/36 ( 22%) [1636/1759 ( 93%)]  Loss: 4.086 (4.12)  LR: 1.864e-03  Grad: 2.4325  max=0.5231(module.vfe.pfn_layers.0.linear.weight)  min: -0.1070(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.7907, loss_cls=0.1574, loss_bbox=0.8745, matched_ious=0.4826, loss_iou=0.0959, loss_iou_reg=0.2414, d_time=0.00(0.01), f_time=1.34(1.26), b_time=1.35(1.27)  Time cost: 34:31/02:35 [4:55:36/17:21:23]  Acc_iter 13950       Data time: 0.00(0.01)  Forward time: 1.34(1.26)  Batch time: 1.35(1.27)
2025-09-03 13:50:37,023   INFO  Train:    8/36 ( 22%) [1686/1759 ( 96%)]  Loss: 3.989 (4.12)  LR: 1.872e-03  Grad: 2.8561  max=1.1365(module.vfe.pfn_layers.0.linear.weight)  min: -0.2559(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8356, loss_cls=0.1638, loss_bbox=0.8994, matched_ious=0.4716, loss_iou=0.0973, loss_iou_reg=0.2453, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 35:34/01:32 [4:56:39/17:20:12]  Acc_iter 14000       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-03 13:51:40,321   INFO  Train:    8/36 ( 22%) [1736/1759 ( 99%)]  Loss: 3.675 (4.12)  LR: 1.881e-03  Grad: 5.7068  max=0.2567(module.vfe.pfn_layers.0.linear.weight)  min: -4.0744(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7981, loss_cls=0.1574, loss_bbox=0.9082, matched_ious=0.4713, loss_iou=0.0991, loss_iou_reg=0.2469, d_time=0.02(0.01), f_time=1.24(1.26), b_time=1.26(1.27)  Time cost: 36:37/00:29 [4:57:42/17:19:09]  Acc_iter 14050       Data time: 0.02(0.01)  Forward time: 1.24(1.26)  Batch time: 1.26(1.27)
2025-09-03 13:52:07,990   INFO  Train:    8/36 ( 22%) [1758/1759 (100%)]  Loss: 3.379 (4.12)  LR: 1.884e-03  Grad: 3.1369  max=0.3748(module.vfe.pfn_layers.0.linear.weight)  min: -0.1972(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7750, loss_cls=0.1541, loss_bbox=0.8498, matched_ious=0.4905, loss_iou=0.0950, loss_iou_reg=0.2394, d_time=0.00(0.01), f_time=0.74(1.26), b_time=0.74(1.27)  Time cost: 37:05/00:01 [4:58:10/17:18:37]  Acc_iter 14072       Data time: 0.00(0.01)  Forward time: 0.74(1.26)  Batch time: 0.74(1.27)

                                               [Aepochs:  22%|██▏       | 8/36 [4:58:10<17:21:19, 2231.40s/it]epochs:  22%|██▏       | 8/36 [4:58:11<17:21:20, 2231.44s/it]epochs:  22%|██▏       | 8/36 [4:58:11<17:21:20, 2231.45s/it]epochs:  22%|██▏       | 8/36 [4:58:11<17:21:20, 2231.43s/it]epochs:  22%|██▏       | 8/36 [4:58:11<17:21:19, 2231.42s/it]epochs:  22%|██▏       | 8/36 [4:58:11<17:21:19, 2231.42s/it]epochs:  22%|██▏       | 8/36 [4:58:11<17:21:19, 2231.43s/it]epochs:  22%|██▏       | 8/36 [4:58:11<17:21:20, 2231.43s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 13:52:13,372   INFO  Train:    9/36 ( 25%) [   0/1759 (  0%)]  Loss: 3.790 (3.79)  LR: 1.884e-03  Grad: 3.7988  max=2.1262(module.vfe.pfn_layers.0.linear.weight)  min: -0.2038(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.6987, loss_cls=0.1303, loss_bbox=1.0095, matched_ious=0.4430, loss_iou=0.0833, loss_iou_reg=0.2572, d_time=1.11(1.11), f_time=2.95(2.95), b_time=4.06(4.06)  Time cost: 00:03/1:47:09 [4:58:15/50:00:26]  Acc_iter 14073       Data time: 1.11(1.11)  Forward time: 2.95(2.95)  Batch time: 4.06(4.06)
2025-09-03 13:52:47,136   INFO  Train:    9/36 ( 25%) [  27/1759 (  2%)]  Loss: 4.317 (4.10)  LR: 1.889e-03  Grad: 9.7096  max=7.7888(module.vfe.pfn_layers.0.linear.weight)  min: -0.3128(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7922, loss_cls=0.1591, loss_bbox=0.9619, matched_ious=0.4858, loss_iou=0.0974, loss_iou_reg=0.2375, d_time=0.00(0.04), f_time=1.33(1.31), b_time=1.34(1.35)  Time cost: 00:37/38:34 [4:58:49/18:16:22]  Acc_iter 14100       Data time: 0.00(0.04)  Forward time: 1.33(1.31)  Batch time: 1.34(1.35)
2025-09-03 13:53:50,341   INFO  Train:    9/36 ( 25%) [  77/1759 (  4%)]  Loss: 3.392 (4.05)  LR: 1.897e-03  Grad: 3.1634  max=0.5716(module.vfe.pfn_layers.0.linear.weight)  min: -1.9603(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7972, loss_cls=0.1581, loss_bbox=0.9110, matched_ious=0.4744, loss_iou=0.1004, loss_iou_reg=0.2452, d_time=0.00(0.02), f_time=1.26(1.28), b_time=1.26(1.30)  Time cost: 01:40/36:09 [4:59:52/17:37:17]  Acc_iter 14150       Data time: 0.00(0.02)  Forward time: 1.26(1.28)  Batch time: 1.26(1.30)
2025-09-03 13:54:53,226   INFO  Train:    9/36 ( 25%) [ 127/1759 (  7%)]  Loss: 4.405 (4.04)  LR: 1.905e-03  Grad: 2.5624  max=0.4557(module.vfe.pfn_layers.0.linear.weight)  min: -0.1581(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.7929, loss_cls=0.1559, loss_bbox=0.9199, matched_ious=0.4730, loss_iou=0.1000, loss_iou_reg=0.2438, d_time=0.00(0.01), f_time=1.26(1.27), b_time=1.26(1.28)  Time cost: 02:43/34:44 [5:00:55/17:25:52]  Acc_iter 14200       Data time: 0.00(0.01)  Forward time: 1.26(1.27)  Batch time: 1.26(1.28)
2025-09-03 13:55:57,391   INFO  Train:    9/36 ( 25%) [ 177/1759 ( 10%)]  Loss: 4.186 (4.01)  LR: 1.914e-03  Grad: 3.3061  max=1.4063(module.vfe.pfn_layers.0.linear.weight)  min: -0.8386(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7804, loss_cls=0.1522, loss_bbox=0.8492, matched_ious=0.4877, loss_iou=0.0978, loss_iou_reg=0.2372, d_time=0.00(0.01), f_time=1.39(1.27), b_time=1.40(1.28)  Time cost: 03:47/33:43 [5:01:59/17:26:09]  Acc_iter 14250       Data time: 0.00(0.01)  Forward time: 1.39(1.27)  Batch time: 1.40(1.28)
2025-09-03 13:57:00,248   INFO  Train:    9/36 ( 25%) [ 227/1759 ( 13%)]  Loss: 4.441 (4.02)  LR: 1.922e-03  Grad: 2.8051  max=0.7569(module.vfe.pfn_layers.0.linear.weight)  min: -0.2281(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8037, loss_cls=0.1576, loss_bbox=0.9220, matched_ious=0.4780, loss_iou=0.0980, loss_iou_reg=0.2434, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.23(1.28)  Time cost: 04:50/32:32 [5:03:02/17:21:10]  Acc_iter 14300       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.28)
2025-09-03 13:58:03,128   INFO  Train:    9/36 ( 25%) [ 277/1759 ( 16%)]  Loss: 4.127 (4.04)  LR: 1.930e-03  Grad: 4.3580  max=2.9349(module.vfe.pfn_layers.0.linear.weight)  min: -1.9159(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7944, loss_cls=0.1548, loss_bbox=0.9563, matched_ious=0.4808, loss_iou=0.0972, loss_iou_reg=0.2421, d_time=0.01(0.01), f_time=1.34(1.26), b_time=1.35(1.27)  Time cost: 05:53/31:24 [5:04:05/17:17:40]  Acc_iter 14350       Data time: 0.01(0.01)  Forward time: 1.34(1.26)  Batch time: 1.35(1.27)
2025-09-03 13:59:05,653   INFO  Train:    9/36 ( 25%) [ 327/1759 ( 19%)]  Loss: 3.976 (4.03)  LR: 1.938e-03  Grad: 2.2724  max=1.1296(module.vfe.pfn_layers.0.linear.weight)  min: -0.1650(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.7721, loss_cls=0.1520, loss_bbox=0.9199, matched_ious=0.4738, loss_iou=0.0993, loss_iou_reg=0.2455, d_time=0.02(0.01), f_time=1.18(1.26), b_time=1.19(1.27)  Time cost: 06:55/30:15 [5:05:08/17:14:01]  Acc_iter 14400       Data time: 0.02(0.01)  Forward time: 1.18(1.26)  Batch time: 1.19(1.27)
2025-09-03 14:00:08,503   INFO  Train:    9/36 ( 25%) [ 377/1759 ( 21%)]  Loss: 3.827 (4.05)  LR: 1.946e-03  Grad: 2.1012  max=0.8221(module.vfe.pfn_layers.0.linear.weight)  min: -0.1446(module.dense_head.prediction_head.dim.1.bias)  NaN: False  loss_hm=0.8319, loss_cls=0.1592, loss_bbox=0.9963, matched_ious=0.4756, loss_iou=0.0982, loss_iou_reg=0.2408, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.21(1.27)  Time cost: 07:58/29:10 [5:06:11/17:11:45]  Acc_iter 14450       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.27)
2025-09-03 14:01:11,328   INFO  Train:    9/36 ( 25%) [ 427/1759 ( 24%)]  Loss: 4.033 (4.06)  LR: 1.955e-03  Grad: 3.4100  max=0.0509(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -2.6253(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8165, loss_cls=0.1580, loss_bbox=1.0095, matched_ious=0.4786, loss_iou=0.0979, loss_iou_reg=0.2412, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.18(1.27)  Time cost: 09:01/28:05 [5:07:13/17:09:45]  Acc_iter 14500       Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.18(1.27)
2025-09-03 14:02:14,982   INFO  Train:    9/36 ( 25%) [ 477/1759 ( 27%)]  Loss: 4.606 (4.05)  LR: 1.963e-03  Grad: 1.9032  max=0.2095(module.backbone_3d.cls_conv.3.bias)  min: -1.0118(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7936, loss_cls=0.1589, loss_bbox=0.8774, matched_ious=0.4718, loss_iou=0.0996, loss_iou_reg=0.2463, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.20(1.27)  Time cost: 10:05/27:03 [5:08:17/17:09:21]  Acc_iter 14550       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.27)
2025-09-03 14:03:18,074   INFO  Train:    9/36 ( 25%) [ 527/1759 ( 30%)]  Loss: 3.368 (4.05)  LR: 1.971e-03  Grad: 1.6725  max=0.1874(module.backbone_3d.cls_conv.3.weight)  min: -0.3444(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.8168, loss_cls=0.1522, loss_bbox=0.9517, matched_ious=0.4763, loss_iou=0.1010, loss_iou_reg=0.2412, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 11:08/25:59 [5:09:20/17:07:57]  Acc_iter 14600       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 14:04:20,610   INFO  Train:    9/36 ( 25%) [ 577/1759 ( 33%)]  Loss: 4.024 (4.05)  LR: 1.979e-03  Grad: 1.8593  max=0.0750(module.dense_head.prediction_head.dim.1.bias)  min: -1.1547(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8034, loss_cls=0.1564, loss_bbox=0.9621, matched_ious=0.4710, loss_iou=0.0982, loss_iou_reg=0.2458, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.21(1.27)  Time cost: 12:10/24:54 [5:10:23/17:05:50]  Acc_iter 14650       Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.27)
2025-09-03 14:05:22,782   INFO  Train:    9/36 ( 25%) [ 627/1759 ( 36%)]  Loss: 3.734 (4.05)  LR: 1.987e-03  Grad: 1.9293  max=0.9277(module.vfe.pfn_layers.0.linear.weight)  min: -0.2013(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.7919, loss_cls=0.1567, loss_bbox=0.9212, matched_ious=0.4785, loss_iou=0.0962, loss_iou_reg=0.2421, d_time=0.00(0.01), f_time=1.15(1.25), b_time=1.15(1.26)  Time cost: 13:13/23:49 [5:11:25/17:03:25]  Acc_iter 14700       Data time: 0.00(0.01)  Forward time: 1.15(1.25)  Batch time: 1.15(1.26)
2025-09-03 14:06:26,833   INFO  Train:    9/36 ( 25%) [ 677/1759 ( 38%)]  Loss: 4.585 (4.05)  LR: 1.995e-03  Grad: 2.5165  max=1.5589(module.vfe.pfn_layers.0.linear.weight)  min: -0.2073(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7740, loss_cls=0.1505, loss_bbox=0.9222, matched_ious=0.4779, loss_iou=0.1008, loss_iou_reg=0.2431, d_time=0.00(0.01), f_time=1.16(1.26), b_time=1.16(1.26)  Time cost: 14:17/22:47 [5:12:29/17:03:27]  Acc_iter 14750       Data time: 0.00(0.01)  Forward time: 1.16(1.26)  Batch time: 1.16(1.26)
2025-09-03 14:07:30,375   INFO  Train:    9/36 ( 25%) [ 727/1759 ( 41%)]  Loss: 3.186 (4.05)  LR: 2.003e-03  Grad: 2.5897  max=0.1314(module.dense_head.prediction_head.height.1.weight)  min: -1.9083(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8127, loss_cls=0.1549, loss_bbox=0.9096, matched_ious=0.4767, loss_iou=0.0988, loss_iou_reg=0.2445, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.21(1.27)  Time cost: 15:20/21:45 [5:13:32/17:02:46]  Acc_iter 14800       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.27)
2025-09-03 14:08:32,970   INFO  Train:    9/36 ( 25%) [ 777/1759 ( 44%)]  Loss: 3.250 (4.05)  LR: 2.011e-03  Grad: 1.3752  max=0.2873(module.dense_head.prediction_head.height.1.weight)  min: -0.3824(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8182, loss_cls=0.1570, loss_bbox=0.9348, matched_ious=0.4759, loss_iou=0.1001, loss_iou_reg=0.2441, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.23(1.26)  Time cost: 16:23/20:41 [5:14:35/17:01:03]  Acc_iter 14850       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.26)
2025-09-03 14:09:35,743   INFO  Train:    9/36 ( 25%) [ 827/1759 ( 47%)]  Loss: 4.031 (4.05)  LR: 2.019e-03  Grad: 3.5045  max=2.5747(module.vfe.pfn_layers.0.linear.weight)  min: -1.5967(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8131, loss_cls=0.1631, loss_bbox=0.8852, matched_ious=0.4728, loss_iou=0.1011, loss_iou_reg=0.2438, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.20(1.26)  Time cost: 17:26/19:37 [5:15:38/16:59:36]  Acc_iter 14900       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.26)
2025-09-03 14:10:38,633   INFO  Train:    9/36 ( 25%) [ 877/1759 ( 50%)]  Loss: 3.887 (4.05)  LR: 2.027e-03  Grad: 2.4669  max=0.1680(module.vfe.pfn_layers.0.linear.weight)  min: -1.6268(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8021, loss_cls=0.1514, loss_bbox=0.9227, matched_ious=0.4829, loss_iou=0.0940, loss_iou_reg=0.2388, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.21(1.26)  Time cost: 18:28/18:33 [5:16:41/16:58:17]  Acc_iter 14950       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.26)
2025-09-03 14:11:42,194   INFO  Train:    9/36 ( 25%) [ 927/1759 ( 53%)]  Loss: 3.038 (4.05)  LR: 2.035e-03  Grad: 2.4021  max=1.8139(module.vfe.pfn_layers.0.linear.weight)  min: -0.3770(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8016, loss_cls=0.1554, loss_bbox=0.8905, matched_ious=0.4845, loss_iou=0.0974, loss_iou_reg=0.2392, d_time=0.01(0.01), f_time=1.23(1.26), b_time=1.23(1.26)  Time cost: 19:32/17:31 [5:17:44/16:57:35]  Acc_iter 15000       Data time: 0.01(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.26)
2025-09-03 14:12:45,896   INFO  Train:    9/36 ( 25%) [ 977/1759 ( 56%)]  Loss: 4.022 (4.05)  LR: 2.043e-03  Grad: 1.9429  max=0.4594(module.vfe.pfn_layers.0.linear.weight)  min: -0.3373(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7839, loss_cls=0.1520, loss_bbox=0.8807, matched_ious=0.4865, loss_iou=0.0962, loss_iou_reg=0.2390, d_time=0.00(0.01), f_time=1.35(1.26), b_time=1.36(1.26)  Time cost: 20:36/16:28 [5:18:48/16:56:58]  Acc_iter 15050       Data time: 0.00(0.01)  Forward time: 1.35(1.26)  Batch time: 1.36(1.26)
2025-09-03 14:13:48,381   INFO  Train:    9/36 ( 25%) [1027/1759 ( 58%)]  Loss: 3.348 (4.05)  LR: 2.051e-03  Grad: 1.2188  max=0.0861(module.vfe.pfn_layers.0.linear.weight)  min: -0.5518(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8241, loss_cls=0.1559, loss_bbox=0.9502, matched_ious=0.4746, loss_iou=0.0989, loss_iou_reg=0.2445, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.18(1.26)  Time cost: 21:38/15:24 [5:19:50/16:55:22]  Acc_iter 15100       Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.18(1.26)
2025-09-03 14:14:51,435   INFO  Train:    9/36 ( 25%) [1077/1759 ( 61%)]  Loss: 4.200 (4.05)  LR: 2.059e-03  Grad: 1.6808  max=0.4564(module.vfe.pfn_layers.0.linear.weight)  min: -0.7057(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7895, loss_cls=0.1560, loss_bbox=0.9084, matched_ious=0.4743, loss_iou=0.0973, loss_iou_reg=0.2449, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.22(1.26)  Time cost: 22:41/14:21 [5:20:53/16:54:14]  Acc_iter 15150       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.26)
2025-09-03 14:15:54,458   INFO  Train:    9/36 ( 25%) [1127/1759 ( 64%)]  Loss: 3.753 (4.05)  LR: 2.067e-03  Grad: 1.3414  max=0.3761(module.vfe.pfn_layers.0.linear.weight)  min: -0.2050(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.8156, loss_cls=0.1579, loss_bbox=0.9147, matched_ious=0.4873, loss_iou=0.0969, loss_iou_reg=0.2393, d_time=0.01(0.01), f_time=1.23(1.26), b_time=1.24(1.26)  Time cost: 23:44/13:18 [5:21:56/16:53:05]  Acc_iter 15200       Data time: 0.01(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.26)
2025-09-03 14:16:58,867   INFO  Train:    9/36 ( 25%) [1177/1759 ( 67%)]  Loss: 3.431 (4.05)  LR: 2.075e-03  Grad: 3.8467  max=3.2991(module.vfe.pfn_layers.0.linear.weight)  min: -1.3078(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8017, loss_cls=0.1508, loss_bbox=0.9474, matched_ious=0.4740, loss_iou=0.0979, loss_iou_reg=0.2418, d_time=0.00(0.01), f_time=1.30(1.26), b_time=1.31(1.26)  Time cost: 24:49/12:15 [5:23:01/16:52:53]  Acc_iter 15250       Data time: 0.00(0.01)  Forward time: 1.30(1.26)  Batch time: 1.31(1.26)
2025-09-03 14:18:01,894   INFO  Train:    9/36 ( 25%) [1227/1759 ( 70%)]  Loss: 3.573 (4.05)  LR: 2.083e-03  Grad: 3.3704  max=2.7964(module.vfe.pfn_layers.0.linear.weight)  min: -0.9601(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7844, loss_cls=0.1537, loss_bbox=0.9287, matched_ious=0.4733, loss_iou=0.0974, loss_iou_reg=0.2469, d_time=0.01(0.01), f_time=1.23(1.26), b_time=1.24(1.26)  Time cost: 25:52/11:12 [5:24:04/16:51:43]  Acc_iter 15300       Data time: 0.01(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.26)
2025-09-03 14:19:05,155   INFO  Train:    9/36 ( 25%) [1277/1759 ( 73%)]  Loss: 4.450 (4.05)  LR: 2.091e-03  Grad: 2.6154  max=1.6251(module.vfe.pfn_layers.0.linear.weight)  min: -0.2929(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7801, loss_cls=0.1527, loss_bbox=0.8778, matched_ious=0.4804, loss_iou=0.0966, loss_iou_reg=0.2417, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.25(1.26)  Time cost: 26:55/10:09 [5:25:07/16:50:42]  Acc_iter 15350       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.26)
2025-09-03 14:20:08,820   INFO  Train:    9/36 ( 25%) [1327/1759 ( 75%)]  Loss: 4.674 (4.05)  LR: 2.099e-03  Grad: 3.4839  max=2.7258(module.vfe.pfn_layers.0.linear.weight)  min: -0.1474(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.7980, loss_cls=0.1535, loss_bbox=0.9161, matched_ious=0.4806, loss_iou=0.0974, loss_iou_reg=0.2413, d_time=0.00(0.01), f_time=1.34(1.26), b_time=1.34(1.26)  Time cost: 27:59/09:06 [5:26:11/16:49:55]  Acc_iter 15400       Data time: 0.00(0.01)  Forward time: 1.34(1.26)  Batch time: 1.34(1.26)
2025-09-03 14:21:12,653   INFO  Train:    9/36 ( 25%) [1377/1759 ( 78%)]  Loss: 3.551 (4.05)  LR: 2.107e-03  Grad: 2.4658  max=0.8067(module.vfe.pfn_layers.0.linear.weight)  min: -0.4352(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8023, loss_cls=0.1553, loss_bbox=0.9152, matched_ious=0.4762, loss_iou=0.0979, loss_iou_reg=0.2428, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.28(1.27)  Time cost: 29:02/08:03 [5:27:15/16:49:13]  Acc_iter 15450       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.27)
2025-09-03 14:22:16,074   INFO  Train:    9/36 ( 25%) [1427/1759 ( 81%)]  Loss: 3.890 (4.05)  LR: 2.115e-03  Grad: 2.9248  max=0.1508(module.backbone_3d.cls_conv.3.weight)  min: -1.6084(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8376, loss_cls=0.1576, loss_bbox=0.9883, matched_ious=0.4716, loss_iou=0.0988, loss_iou_reg=0.2425, d_time=0.00(0.01), f_time=1.33(1.26), b_time=1.34(1.27)  Time cost: 30:06/06:59 [5:28:18/16:48:16]  Acc_iter 15500       Data time: 0.00(0.01)  Forward time: 1.33(1.26)  Batch time: 1.34(1.27)
2025-09-03 14:23:18,544   INFO  Train:    9/36 ( 25%) [1477/1759 ( 84%)]  Loss: 4.244 (4.05)  LR: 2.123e-03  Grad: 1.7772  max=0.1030(module.backbone_3d.cls_conv.3.weight)  min: -0.7222(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7761, loss_cls=0.1507, loss_bbox=0.9154, matched_ious=0.4763, loss_iou=0.0962, loss_iou_reg=0.2423, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.18(1.26)  Time cost: 31:08/05:56 [5:29:21/16:46:48]  Acc_iter 15550       Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.18(1.26)
2025-09-03 14:24:21,438   INFO  Train:    9/36 ( 25%) [1527/1759 ( 87%)]  Loss: 3.556 (4.05)  LR: 2.131e-03  Grad: 2.0584  max=0.7784(module.vfe.pfn_layers.0.linear.weight)  min: -0.1129(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7708, loss_cls=0.1515, loss_bbox=0.8894, matched_ious=0.4849, loss_iou=0.0972, loss_iou_reg=0.2417, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.25(1.26)  Time cost: 32:11/04:53 [5:30:23/16:45:34]  Acc_iter 15600       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.26)
2025-09-03 14:25:23,903   INFO  Train:    9/36 ( 25%) [1577/1759 ( 90%)]  Loss: 3.884 (4.05)  LR: 2.138e-03  Grad: 2.8055  max=2.0732(module.vfe.pfn_layers.0.linear.weight)  min: -0.0978(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.7886, loss_cls=0.1564, loss_bbox=0.9512, matched_ious=0.4762, loss_iou=0.0972, loss_iou_reg=0.2419, d_time=0.00(0.01), f_time=1.29(1.26), b_time=1.30(1.26)  Time cost: 33:14/03:50 [5:31:26/16:44:08]  Acc_iter 15650       Data time: 0.00(0.01)  Forward time: 1.29(1.26)  Batch time: 1.30(1.26)
2025-09-03 14:26:26,702   INFO  Train:    9/36 ( 25%) [1627/1759 ( 92%)]  Loss: 3.316 (4.04)  LR: 2.146e-03  Grad: 2.2566  max=0.7306(module.vfe.pfn_layers.0.linear.weight)  min: -1.1843(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7618, loss_cls=0.1537, loss_bbox=0.8717, matched_ious=0.4859, loss_iou=0.0943, loss_iou_reg=0.2421, d_time=0.01(0.01), f_time=1.19(1.26), b_time=1.19(1.26)  Time cost: 34:16/02:46 [5:32:29/16:42:54]  Acc_iter 15700       Data time: 0.01(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.26)
2025-09-03 14:27:31,208   INFO  Train:    9/36 ( 25%) [1677/1759 ( 95%)]  Loss: 3.443 (4.04)  LR: 2.154e-03  Grad: 2.5073  max=0.9273(module.vfe.pfn_layers.0.linear.weight)  min: -1.1827(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7884, loss_cls=0.1523, loss_bbox=0.8905, matched_ious=0.4856, loss_iou=0.0966, loss_iou_reg=0.2390, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.23(1.26)  Time cost: 35:21/01:43 [5:33:33/16:42:28]  Acc_iter 15750       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.26)
2025-09-03 14:28:33,700   INFO  Train:    9/36 ( 25%) [1727/1759 ( 98%)]  Loss: 3.221 (4.04)  LR: 2.162e-03  Grad: 2.4312  max=0.1364(module.backbone_3d.cls_conv.3.weight)  min: -0.9469(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7825, loss_cls=0.1548, loss_bbox=0.8988, matched_ious=0.4817, loss_iou=0.0973, loss_iou_reg=0.2404, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.29(1.26)  Time cost: 36:23/00:40 [5:34:36/16:41:05]  Acc_iter 15800       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.29(1.26)
2025-09-03 14:29:11,879   INFO  Train:    9/36 ( 25%) [1758/1759 (100%)]  Loss: 3.089 (4.04)  LR: 2.167e-03  Grad: 4.8458  max=0.5208(module.backbone_3d.cls_conv.3.bias)  min: -3.9162(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7298, loss_cls=0.1470, loss_bbox=0.8022, matched_ious=0.4980, loss_iou=0.1004, loss_iou_reg=0.2360, d_time=0.00(0.01), f_time=0.77(1.26), b_time=0.77(1.26)  Time cost: 37:02/00:01 [5:35:14/16:39:59]  Acc_iter 15831       Data time: 0.00(0.01)  Forward time: 0.77(1.26)  Batch time: 0.77(1.26)

                                               [Aepochs:  25%|██▌       | 9/36 [5:35:14<16:43:04, 2229.04s/it]epochs:  25%|██▌       | 9/36 [5:35:14<16:43:03, 2229.03s/it]epochs:  25%|██▌       | 9/36 [5:35:14<16:43:05, 2229.08s/it]epochs:  25%|██▌       | 9/36 [5:35:14<16:43:05, 2229.09s/it]epochs:  25%|██▌       | 9/36 [5:35:14<16:43:04, 2229.07s/it]epochs:  25%|██▌       | 9/36 [5:35:14<16:43:04, 2229.07s/it]epochs:  25%|██▌       | 9/36 [5:35:14<16:43:04, 2229.07s/it]epochs:  25%|██▌       | 9/36 [5:35:15<16:43:05, 2229.08s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 14:29:17,588   INFO  Train:   10/36 ( 28%) [   0/1759 (  0%)]  Loss: 4.008 (4.01)  LR: 2.167e-03  Grad: 2.8575  max=0.5248(module.vfe.pfn_layers.0.linear.weight)  min: -0.9075(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8395, loss_cls=0.1963, loss_bbox=0.6448, matched_ious=0.4826, loss_iou=0.0941, loss_iou_reg=0.2428, d_time=1.68(1.68), f_time=2.70(2.70), b_time=4.38(4.38)  Time cost: 00:03/1:55:14 [5:35:20/51:51:33]  Acc_iter 15832       Data time: 1.68(1.68)  Forward time: 2.70(2.70)  Batch time: 4.38(4.38)
2025-09-03 14:29:40,237   INFO  Train:   10/36 ( 28%) [  18/1759 (  1%)]  Loss: 3.845 (4.17)  LR: 2.169e-03  Grad: 2.6400  max=0.6753(module.vfe.pfn_layers.0.linear.weight)  min: -0.1792(module.dense_head.prediction_head.dim.1.weight)  NaN: False  loss_hm=0.8313, loss_cls=0.1566, loss_bbox=0.9947, matched_ious=0.4626, loss_iou=0.1016, loss_iou_reg=0.2513, d_time=0.00(0.09), f_time=1.28(1.33), b_time=1.28(1.42)  Time cost: 00:26/40:35 [5:35:42/18:26:57]  Acc_iter 15850       Data time: 0.00(0.09)  Forward time: 1.28(1.33)  Batch time: 1.28(1.42)
2025-09-03 14:30:43,747   INFO  Train:   10/36 ( 28%) [  68/1759 (  4%)]  Loss: 4.765 (4.03)  LR: 2.177e-03  Grad: 2.0426  max=1.1388(module.vfe.pfn_layers.0.linear.weight)  min: -0.1986(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.7792, loss_cls=0.1519, loss_bbox=0.8822, matched_ious=0.4831, loss_iou=0.0982, loss_iou_reg=0.2399, d_time=0.00(0.03), f_time=1.25(1.28), b_time=1.25(1.31)  Time cost: 01:30/36:47 [5:36:46/17:12:02]  Acc_iter 15900       Data time: 0.00(0.03)  Forward time: 1.25(1.28)  Batch time: 1.25(1.31)
2025-09-03 14:31:46,149   INFO  Train:   10/36 ( 28%) [ 118/1759 (  7%)]  Loss: 4.466 (4.00)  LR: 2.185e-03  Grad: 1.9118  max=0.2215(module.dense_head.prediction_head.height.1.bias)  min: -0.2487(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7922, loss_cls=0.1513, loss_bbox=0.9147, matched_ious=0.4865, loss_iou=0.0984, loss_iou_reg=0.2374, d_time=0.00(0.02), f_time=1.18(1.27), b_time=1.18(1.29)  Time cost: 02:32/35:02 [5:37:48/16:51:49]  Acc_iter 15950       Data time: 0.00(0.02)  Forward time: 1.18(1.27)  Batch time: 1.18(1.29)
2025-09-03 14:32:48,685   INFO  Train:   10/36 ( 28%) [ 168/1759 ( 10%)]  Loss: 4.446 (4.01)  LR: 2.193e-03  Grad: 2.2829  max=0.9789(module.vfe.pfn_layers.0.linear.weight)  min: -0.1936(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7828, loss_cls=0.1536, loss_bbox=0.9417, matched_ious=0.4832, loss_iou=0.0968, loss_iou_reg=0.2408, d_time=0.00(0.02), f_time=1.23(1.26), b_time=1.24(1.28)  Time cost: 03:35/33:44 [5:38:51/16:43:34]  Acc_iter 16000       Data time: 0.00(0.02)  Forward time: 1.23(1.26)  Batch time: 1.24(1.28)
2025-09-03 14:33:51,908   INFO  Train:   10/36 ( 28%) [ 218/1759 ( 12%)]  Loss: 4.853 (4.02)  LR: 2.200e-03  Grad: 2.5225  max=0.7169(module.vfe.pfn_layers.0.linear.weight)  min: -0.1528(module.dense_head.prediction_head.height.1.weight)  NaN: False  loss_hm=0.8026, loss_cls=0.1522, loss_bbox=0.9181, matched_ious=0.4780, loss_iou=0.0961, loss_iou_reg=0.2398, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 04:38/32:37 [5:39:54/16:41:02]  Acc_iter 16050       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 14:34:55,581   INFO  Train:   10/36 ( 28%) [ 268/1759 ( 15%)]  Loss: 3.677 (4.01)  LR: 2.208e-03  Grad: 1.8811  max=1.1199(module.vfe.pfn_layers.0.linear.weight)  min: -0.0813(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.7834, loss_cls=0.1537, loss_bbox=0.8897, matched_ious=0.4818, loss_iou=0.0979, loss_iou_reg=0.2431, d_time=0.01(0.01), f_time=1.18(1.26), b_time=1.19(1.27)  Time cost: 05:41/31:35 [5:40:58/16:40:27]  Acc_iter 16100       Data time: 0.01(0.01)  Forward time: 1.18(1.26)  Batch time: 1.19(1.27)
2025-09-03 14:35:58,361   INFO  Train:   10/36 ( 28%) [ 318/1759 ( 18%)]  Loss: 4.283 (4.00)  LR: 2.215e-03  Grad: 2.2555  max=1.3082(module.vfe.pfn_layers.0.linear.weight)  min: -0.1899(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7766, loss_cls=0.1504, loss_bbox=0.8844, matched_ious=0.4787, loss_iou=0.0967, loss_iou_reg=0.2410, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.19(1.27)  Time cost: 06:44/30:28 [5:42:00/16:37:29]  Acc_iter 16150       Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.19(1.27)
2025-09-03 14:37:01,014   INFO  Train:   10/36 ( 28%) [ 368/1759 ( 21%)]  Loss: 3.369 (4.00)  LR: 2.223e-03  Grad: 2.2682  max=0.9187(module.vfe.pfn_layers.0.linear.weight)  min: -0.1217(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.7940, loss_cls=0.1532, loss_bbox=0.9302, matched_ious=0.4812, loss_iou=0.0957, loss_iou_reg=0.2396, d_time=0.00(0.01), f_time=1.16(1.26), b_time=1.16(1.27)  Time cost: 07:47/29:21 [5:43:03/16:34:46]  Acc_iter 16200       Data time: 0.00(0.01)  Forward time: 1.16(1.26)  Batch time: 1.16(1.27)
2025-09-03 14:38:04,072   INFO  Train:   10/36 ( 28%) [ 418/1759 ( 24%)]  Loss: 3.681 (3.99)  LR: 2.231e-03  Grad: 2.7948  max=1.4643(module.vfe.pfn_layers.0.linear.weight)  min: -0.4230(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7719, loss_cls=0.1509, loss_bbox=0.8739, matched_ious=0.4818, loss_iou=0.0969, loss_iou_reg=0.2434, d_time=0.00(0.01), f_time=1.38(1.26), b_time=1.38(1.27)  Time cost: 08:50/28:17 [5:44:06/16:33:12]  Acc_iter 16250       Data time: 0.00(0.01)  Forward time: 1.38(1.26)  Batch time: 1.38(1.27)
2025-09-03 14:39:06,592   INFO  Train:   10/36 ( 28%) [ 468/1759 ( 27%)]  Loss: 4.055 (3.99)  LR: 2.238e-03  Grad: 2.3407  max=0.7539(module.vfe.pfn_layers.0.linear.weight)  min: -0.4337(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7739, loss_cls=0.1485, loss_bbox=0.8852, matched_ious=0.4836, loss_iou=0.0970, loss_iou_reg=0.2387, d_time=0.00(0.01), f_time=1.16(1.26), b_time=1.16(1.27)  Time cost: 09:52/27:12 [5:45:09/16:30:50]  Acc_iter 16300       Data time: 0.00(0.01)  Forward time: 1.16(1.26)  Batch time: 1.16(1.27)
2025-09-03 14:40:10,152   INFO  Train:   10/36 ( 28%) [ 518/1759 ( 29%)]  Loss: 4.321 (3.99)  LR: 2.246e-03  Grad: 2.7733  max=1.0022(module.vfe.pfn_layers.0.linear.weight)  min: -1.1802(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7904, loss_cls=0.1496, loss_bbox=0.9091, matched_ious=0.4893, loss_iou=0.0954, loss_iou_reg=0.2368, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.26(1.27)  Time cost: 10:56/26:09 [5:46:12/16:30:19]  Acc_iter 16350       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.27)
2025-09-03 14:41:13,088   INFO  Train:   10/36 ( 28%) [ 568/1759 ( 32%)]  Loss: 3.943 (4.00)  LR: 2.253e-03  Grad: 3.2462  max=1.4968(module.vfe.pfn_layers.0.linear.weight)  min: -1.5301(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8104, loss_cls=0.1580, loss_bbox=0.9825, matched_ious=0.4693, loss_iou=0.0988, loss_iou_reg=0.2465, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 11:59/25:05 [5:47:15/16:28:51]  Acc_iter 16400       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-03 14:42:15,543   INFO  Train:   10/36 ( 28%) [ 618/1759 ( 35%)]  Loss: 3.774 (3.99)  LR: 2.261e-03  Grad: 3.0933  max=1.5441(module.vfe.pfn_layers.0.linear.weight)  min: -0.1705(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.7620, loss_cls=0.1442, loss_bbox=0.9194, matched_ious=0.4822, loss_iou=0.0964, loss_iou_reg=0.2400, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.23(1.26)  Time cost: 13:01/24:01 [5:48:18/16:26:49]  Acc_iter 16450       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.26)
2025-09-03 14:43:17,437   INFO  Train:   10/36 ( 28%) [ 668/1759 ( 38%)]  Loss: 4.658 (3.99)  LR: 2.268e-03  Grad: 4.5634  max=1.0483(module.vfe.pfn_layers.0.linear.weight)  min: -3.3532(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7809, loss_cls=0.1534, loss_bbox=0.8345, matched_ious=0.4860, loss_iou=0.0977, loss_iou_reg=0.2410, d_time=0.00(0.01), f_time=1.33(1.25), b_time=1.33(1.26)  Time cost: 14:03/22:56 [5:49:19/16:24:18]  Acc_iter 16500       Data time: 0.00(0.01)  Forward time: 1.33(1.25)  Batch time: 1.33(1.26)
2025-09-03 14:44:20,395   INFO  Train:   10/36 ( 28%) [ 718/1759 ( 41%)]  Loss: 3.971 (3.98)  LR: 2.276e-03  Grad: 3.1699  max=0.9922(module.vfe.pfn_layers.0.linear.weight)  min: -0.2972(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7504, loss_cls=0.1392, loss_bbox=0.8725, matched_ious=0.4910, loss_iou=0.0967, loss_iou_reg=0.2385, d_time=0.01(0.01), f_time=1.25(1.25), b_time=1.26(1.26)  Time cost: 15:06/21:52 [5:50:22/16:23:08]  Acc_iter 16550       Data time: 0.01(0.01)  Forward time: 1.25(1.25)  Batch time: 1.26(1.26)
2025-09-03 14:45:23,416   INFO  Train:   10/36 ( 28%) [ 768/1759 ( 44%)]  Loss: 4.025 (3.98)  LR: 2.283e-03  Grad: 3.4123  max=0.3619(module.vfe.pfn_layers.0.linear.weight)  min: -0.8365(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7859, loss_cls=0.1504, loss_bbox=0.8863, matched_ious=0.4904, loss_iou=0.0976, loss_iou_reg=0.2374, d_time=0.00(0.01), f_time=1.21(1.25), b_time=1.21(1.26)  Time cost: 16:09/20:49 [5:51:25/16:22:03]  Acc_iter 16600       Data time: 0.00(0.01)  Forward time: 1.21(1.25)  Batch time: 1.21(1.26)
2025-09-03 14:46:26,128   INFO  Train:   10/36 ( 28%) [ 818/1759 ( 47%)]  Loss: 4.730 (3.98)  LR: 2.290e-03  Grad: 3.8301  max=0.9655(module.vfe.pfn_layers.0.linear.weight)  min: -0.3443(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7501, loss_cls=0.1477, loss_bbox=0.8493, matched_ious=0.4917, loss_iou=0.0954, loss_iou_reg=0.2398, d_time=0.00(0.01), f_time=1.46(1.25), b_time=1.47(1.26)  Time cost: 17:12/19:46 [5:52:28/16:20:40]  Acc_iter 16650       Data time: 0.00(0.01)  Forward time: 1.46(1.25)  Batch time: 1.47(1.26)
2025-09-03 14:47:29,122   INFO  Train:   10/36 ( 28%) [ 868/1759 ( 49%)]  Loss: 4.143 (3.97)  LR: 2.298e-03  Grad: 4.0081  max=0.3078(module.vfe.pfn_layers.0.linear.weight)  min: -1.3130(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7804, loss_cls=0.1485, loss_bbox=0.8649, matched_ious=0.4836, loss_iou=0.0976, loss_iou_reg=0.2395, d_time=0.00(0.01), f_time=1.24(1.25), b_time=1.25(1.26)  Time cost: 18:15/18:43 [5:53:31/16:19:35]  Acc_iter 16700       Data time: 0.00(0.01)  Forward time: 1.24(1.25)  Batch time: 1.25(1.26)
2025-09-03 14:48:33,391   INFO  Train:   10/36 ( 28%) [ 918/1759 ( 52%)]  Loss: 4.180 (3.97)  LR: 2.305e-03  Grad: 4.0475  max=0.3128(module.vfe.pfn_layers.0.linear.weight)  min: -0.1227(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=0.7844, loss_cls=0.1472, loss_bbox=0.8895, matched_ious=0.4899, loss_iou=0.0970, loss_iou_reg=0.2358, d_time=0.01(0.01), f_time=1.27(1.26), b_time=1.28(1.26)  Time cost: 19:19/17:41 [5:54:35/16:19:35]  Acc_iter 16750       Data time: 0.01(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.26)
2025-09-03 14:49:36,827   INFO  Train:   10/36 ( 28%) [ 968/1759 ( 55%)]  Loss: 4.497 (3.97)  LR: 2.312e-03  Grad: 10.0000  max=8.8294(module.vfe.pfn_layers.0.linear.weight)  min: -2.3815(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7640, loss_cls=0.1456, loss_bbox=0.8653, matched_ious=0.4878, loss_iou=0.0968, loss_iou_reg=0.2367, d_time=0.00(0.01), f_time=1.15(1.26), b_time=1.15(1.26)  Time cost: 20:23/16:38 [5:55:39/16:18:48]  Acc_iter 16800       Data time: 0.00(0.01)  Forward time: 1.15(1.26)  Batch time: 1.15(1.26)
2025-09-03 14:50:40,484   INFO  Train:   10/36 ( 28%) [1018/1759 ( 58%)]  Loss: 3.586 (3.96)  LR: 2.320e-03  Grad: 4.9227  max=2.5148(module.vfe.pfn_layers.0.linear.weight)  min: -1.1521(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7814, loss_cls=0.1507, loss_bbox=0.8879, matched_ious=0.4872, loss_iou=0.0947, loss_iou_reg=0.2376, d_time=0.00(0.01), f_time=1.30(1.26), b_time=1.30(1.26)  Time cost: 21:26/15:35 [5:56:43/16:18:10]  Acc_iter 16850       Data time: 0.00(0.01)  Forward time: 1.30(1.26)  Batch time: 1.30(1.26)
2025-09-03 14:51:43,349   INFO  Train:   10/36 ( 28%) [1068/1759 ( 61%)]  Loss: 4.320 (3.96)  LR: 2.327e-03  Grad: 4.3855  max=1.2226(module.vfe.pfn_layers.0.linear.weight)  min: -0.3788(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7743, loss_cls=0.1487, loss_bbox=0.8796, matched_ious=0.4868, loss_iou=0.0976, loss_iou_reg=0.2389, d_time=0.01(0.01), f_time=1.30(1.26), b_time=1.30(1.26)  Time cost: 22:29/14:32 [5:57:45/16:16:55]  Acc_iter 16900       Data time: 0.01(0.01)  Forward time: 1.30(1.26)  Batch time: 1.30(1.26)
2025-09-03 14:52:46,672   INFO  Train:   10/36 ( 28%) [1118/1759 ( 64%)]  Loss: 4.099 (3.96)  LR: 2.334e-03  Grad: 4.4225  max=0.5118(module.vfe.pfn_layers.0.linear.weight)  min: -0.3085(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7611, loss_cls=0.1444, loss_bbox=0.8467, matched_ious=0.4976, loss_iou=0.0945, loss_iou_reg=0.2326, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.20(1.26)  Time cost: 23:33/13:29 [5:58:49/16:16:00]  Acc_iter 16950       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.26)
2025-09-03 14:53:49,523   INFO  Train:   10/36 ( 28%) [1168/1759 ( 66%)]  Loss: 3.478 (3.96)  LR: 2.341e-03  Grad: 3.4321  max=0.1409(module.dense_head.prediction_head.height.1.bias)  min: -0.7221(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7641, loss_cls=0.1458, loss_bbox=0.8935, matched_ious=0.4864, loss_iou=0.0967, loss_iou_reg=0.2387, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.25(1.26)  Time cost: 24:35/12:26 [5:59:52/16:14:45]  Acc_iter 17000       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.26)
2025-09-03 14:54:52,601   INFO  Train:   10/36 ( 28%) [1218/1759 ( 69%)]  Loss: 4.066 (3.96)  LR: 2.348e-03  Grad: 3.8399  max=0.5811(module.vfe.pfn_layers.0.linear.weight)  min: -1.1159(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7867, loss_cls=0.1532, loss_bbox=0.9106, matched_ious=0.4856, loss_iou=0.0942, loss_iou_reg=0.2370, d_time=0.00(0.01), f_time=1.43(1.26), b_time=1.43(1.26)  Time cost: 25:38/11:22 [6:00:55/16:13:40]  Acc_iter 17050       Data time: 0.00(0.01)  Forward time: 1.43(1.26)  Batch time: 1.43(1.26)
2025-09-03 14:55:55,059   INFO  Train:   10/36 ( 28%) [1268/1759 ( 72%)]  Loss: 3.868 (3.95)  LR: 2.356e-03  Grad: 3.8762  max=0.4156(module.vfe.pfn_layers.0.linear.weight)  min: -0.3242(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7414, loss_cls=0.1444, loss_bbox=0.8991, matched_ious=0.4823, loss_iou=0.0970, loss_iou_reg=0.2410, d_time=0.00(0.01), f_time=1.27(1.25), b_time=1.27(1.26)  Time cost: 26:41/10:19 [6:01:57/16:12:13]  Acc_iter 17100       Data time: 0.00(0.01)  Forward time: 1.27(1.25)  Batch time: 1.27(1.26)
2025-09-03 14:56:58,257   INFO  Train:   10/36 ( 28%) [1318/1759 ( 75%)]  Loss: 3.877 (3.96)  LR: 2.363e-03  Grad: 4.1305  max=0.3135(module.vfe.pfn_layers.0.linear.weight)  min: -0.3174(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.7794, loss_cls=0.1498, loss_bbox=0.8954, matched_ious=0.4816, loss_iou=0.0988, loss_iou_reg=0.2397, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.27(1.26)  Time cost: 27:44/09:16 [6:03:00/16:11:13]  Acc_iter 17150       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.26)
2025-09-03 14:58:00,717   INFO  Train:   10/36 ( 28%) [1368/1759 ( 78%)]  Loss: 4.770 (3.96)  LR: 2.370e-03  Grad: 4.4698  max=0.2277(module.dense_head.prediction_head.height.1.bias)  min: -0.9881(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7623, loss_cls=0.1455, loss_bbox=0.8875, matched_ious=0.4876, loss_iou=0.0954, loss_iou_reg=0.2379, d_time=0.00(0.01), f_time=1.29(1.25), b_time=1.30(1.26)  Time cost: 28:47/08:13 [6:04:03/16:09:47]  Acc_iter 17200       Data time: 0.00(0.01)  Forward time: 1.29(1.25)  Batch time: 1.30(1.26)
2025-09-03 14:59:05,434   INFO  Train:   10/36 ( 28%) [1418/1759 ( 81%)]  Loss: 3.735 (3.95)  LR: 2.377e-03  Grad: 3.1744  max=0.8095(module.vfe.pfn_layers.0.linear.weight)  min: -1.0039(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7510, loss_cls=0.1439, loss_bbox=0.8842, matched_ious=0.4867, loss_iou=0.0956, loss_iou_reg=0.2386, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.25(1.26)  Time cost: 29:51/07:10 [6:05:07/16:09:39]  Acc_iter 17250       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.26)
2025-09-03 15:00:10,077   INFO  Train:   10/36 ( 28%) [1468/1759 ( 83%)]  Loss: 5.165 (3.96)  LR: 2.384e-03  Grad: 2.9364  max=1.4467(module.vfe.pfn_layers.0.linear.weight)  min: -0.2847(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7990, loss_cls=0.1532, loss_bbox=0.9574, matched_ious=0.4814, loss_iou=0.0956, loss_iou_reg=0.2400, d_time=0.00(0.01), f_time=1.37(1.26), b_time=1.37(1.26)  Time cost: 30:56/06:07 [6:06:12/16:09:23]  Acc_iter 17300       Data time: 0.00(0.01)  Forward time: 1.37(1.26)  Batch time: 1.37(1.26)
2025-09-03 15:01:12,833   INFO  Train:   10/36 ( 28%) [1518/1759 ( 86%)]  Loss: 3.512 (3.96)  LR: 2.391e-03  Grad: 2.6555  max=0.5933(module.vfe.pfn_layers.0.linear.weight)  min: -0.3110(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.7643, loss_cls=0.1483, loss_bbox=0.8927, matched_ious=0.4771, loss_iou=0.0986, loss_iou_reg=0.2428, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.20(1.26)  Time cost: 31:59/05:04 [6:07:15/16:08:07]  Acc_iter 17350       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.20(1.26)
2025-09-03 15:02:15,470   INFO  Train:   10/36 ( 28%) [1568/1759 ( 89%)]  Loss: 4.100 (3.96)  LR: 2.398e-03  Grad: 2.9459  max=0.3999(module.vfe.pfn_layers.0.linear.weight)  min: -0.6973(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.8070, loss_cls=0.1562, loss_bbox=0.9278, matched_ious=0.4742, loss_iou=0.1003, loss_iou_reg=0.2429, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.18(1.26)  Time cost: 33:01/04:01 [6:08:17/16:06:48]  Acc_iter 17400       Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.18(1.26)
2025-09-03 15:03:17,717   INFO  Train:   10/36 ( 28%) [1618/1759 ( 92%)]  Loss: 3.321 (3.96)  LR: 2.405e-03  Grad: 3.1226  max=0.3413(module.vfe.pfn_layers.0.linear.weight)  min: -0.5644(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7636, loss_cls=0.1474, loss_bbox=0.8706, matched_ious=0.4920, loss_iou=0.0959, loss_iou_reg=0.2352, d_time=0.00(0.01), f_time=1.13(1.26), b_time=1.14(1.26)  Time cost: 34:04/02:58 [6:09:20/16:05:19]  Acc_iter 17450       Data time: 0.00(0.01)  Forward time: 1.13(1.26)  Batch time: 1.14(1.26)
2025-09-03 15:04:21,133   INFO  Train:   10/36 ( 28%) [1668/1759 ( 95%)]  Loss: 3.678 (3.96)  LR: 2.412e-03  Grad: 3.5162  max=1.3764(module.vfe.pfn_layers.0.linear.weight)  min: -0.2701(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7776, loss_cls=0.1487, loss_bbox=0.9014, matched_ious=0.4812, loss_iou=0.0982, loss_iou_reg=0.2404, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.20(1.26)  Time cost: 35:07/01:54 [6:10:23/16:04:24]  Acc_iter 17500       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.26)
2025-09-03 15:05:24,171   INFO  Train:   10/36 ( 28%) [1718/1759 ( 98%)]  Loss: 4.256 (3.96)  LR: 2.419e-03  Grad: 3.5207  max=0.3423(module.vfe.pfn_layers.0.linear.weight)  min: -0.7686(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7746, loss_cls=0.1474, loss_bbox=0.8329, matched_ious=0.4904, loss_iou=0.0975, loss_iou_reg=0.2387, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.28(1.26)  Time cost: 36:10/00:51 [6:11:26/16:03:18]  Acc_iter 17550       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.26)
2025-09-03 15:06:14,347   INFO  Train:   10/36 ( 28%) [1758/1759 (100%)]  Loss: 3.690 (3.95)  LR: 2.424e-03  Grad: 2.8597  max=1.2172(module.vfe.pfn_layers.0.linear.weight)  min: -0.3162(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7421, loss_cls=0.1446, loss_bbox=0.8671, matched_ious=0.4968, loss_iou=0.0954, loss_iou_reg=0.2331, d_time=0.00(0.01), f_time=0.74(1.26), b_time=0.74(1.26)  Time cost: 37:00/00:01 [6:12:16/16:02:18]  Acc_iter 17590       Data time: 0.00(0.01)  Forward time: 0.74(1.26)  Batch time: 0.74(1.26)

                                               [Aepochs:  28%|██▊       | 10/36 [6:12:17<16:05:02, 2227.02s/it]epochs:  28%|██▊       | 10/36 [6:12:17<16:05:02, 2227.03s/it]epochs:  28%|██▊       | 10/36 [6:12:17<16:05:02, 2227.02s/it]epochs:  28%|██▊       | 10/36 [6:12:17<16:05:03, 2227.04s/it]epochs:  28%|██▊       | 10/36 [6:12:17<16:05:02, 2227.03s/it]epochs:  28%|██▊       | 10/36 [6:12:17<16:05:02, 2227.03s/it]epochs:  28%|██▊       | 10/36 [6:12:17<16:05:03, 2227.05s/it]epochs:  28%|██▊       | 10/36 [6:12:17<16:05:02, 2227.03s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 15:06:20,014   INFO  Train:   11/36 ( 31%) [   0/1759 (  0%)]  Loss: 4.665 (4.67)  LR: 2.424e-03  Grad: 3.0220  max=1.6770(module.vfe.pfn_layers.0.linear.weight)  min: -0.7389(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.9908, loss_cls=0.1471, loss_bbox=1.0739, matched_ious=0.4569, loss_iou=0.0992, loss_iou_reg=0.2410, d_time=1.67(1.67), f_time=2.68(2.68), b_time=4.35(4.35)  Time cost: 00:04/1:58:32 [6:12:22/51:21:53]  Acc_iter 17591       Data time: 1.67(1.67)  Forward time: 2.68(2.68)  Batch time: 4.35(4.35)
2025-09-03 15:06:31,988   INFO  Train:   11/36 ( 31%) [   9/1759 (  1%)]  Loss: 4.301 (3.95)  LR: 2.426e-03  Grad: 3.1984  max=1.7801(module.vfe.pfn_layers.0.linear.weight)  min: -0.1324(module.dense_head.heatmap_head.0.bn.bias)  NaN: False  loss_hm=0.7693, loss_cls=0.1492, loss_bbox=0.8565, matched_ious=0.4944, loss_iou=0.0972, loss_iou_reg=0.2337, d_time=0.00(0.17), f_time=1.37(1.46), b_time=1.37(1.63)  Time cost: 00:16/46:42 [6:12:34/20:20:30]  Acc_iter 17600       Data time: 0.00(0.17)  Forward time: 1.37(1.46)  Batch time: 1.37(1.63)
2025-09-03 15:07:35,384   INFO  Train:   11/36 ( 31%) [  59/1759 (  3%)]  Loss: 3.745 (3.91)  LR: 2.432e-03  Grad: 3.4832  max=1.7834(module.vfe.pfn_layers.0.linear.weight)  min: -0.3534(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7523, loss_cls=0.1432, loss_bbox=0.9039, matched_ious=0.4869, loss_iou=0.0957, loss_iou_reg=0.2390, d_time=0.01(0.03), f_time=1.29(1.29), b_time=1.29(1.33)  Time cost: 01:19/37:29 [6:13:37/16:47:32]  Acc_iter 17650       Data time: 0.01(0.03)  Forward time: 1.29(1.29)  Batch time: 1.29(1.33)
2025-09-03 15:08:38,788   INFO  Train:   11/36 ( 31%) [ 109/1759 (  6%)]  Loss: 3.747 (3.92)  LR: 2.439e-03  Grad: 5.6949  max=1.5736(module.vfe.pfn_layers.0.linear.weight)  min: -4.6423(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7874, loss_cls=0.1482, loss_bbox=0.8879, matched_ious=0.4860, loss_iou=0.0980, loss_iou_reg=0.2380, d_time=0.00(0.02), f_time=1.24(1.28), b_time=1.24(1.30)  Time cost: 02:22/35:42 [6:14:41/16:27:16]  Acc_iter 17700       Data time: 0.00(0.02)  Forward time: 1.24(1.28)  Batch time: 1.24(1.30)
2025-09-03 15:09:41,720   INFO  Train:   11/36 ( 31%) [ 159/1759 (  9%)]  Loss: 4.197 (3.90)  LR: 2.446e-03  Grad: 2.9079  max=0.2480(module.dense_head.prediction_head.height.1.weight)  min: -0.5083(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7363, loss_cls=0.1416, loss_bbox=0.8617, matched_ious=0.4894, loss_iou=0.0951, loss_iou_reg=0.2373, d_time=0.00(0.02), f_time=1.25(1.27), b_time=1.25(1.29)  Time cost: 03:25/34:17 [6:15:44/16:16:46]  Acc_iter 17750       Data time: 0.00(0.02)  Forward time: 1.25(1.27)  Batch time: 1.25(1.29)
2025-09-03 15:10:44,133   INFO  Train:   11/36 ( 31%) [ 209/1759 ( 12%)]  Loss: 3.493 (3.90)  LR: 2.453e-03  Grad: 3.2268  max=0.4344(module.vfe.pfn_layers.0.linear.weight)  min: -0.6476(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7550, loss_cls=0.1473, loss_bbox=0.8862, matched_ious=0.4900, loss_iou=0.0975, loss_iou_reg=0.2393, d_time=0.01(0.02), f_time=1.24(1.26), b_time=1.25(1.28)  Time cost: 04:28/32:59 [6:16:46/16:08:53]  Acc_iter 17800       Data time: 0.01(0.02)  Forward time: 1.24(1.26)  Batch time: 1.25(1.28)
2025-09-03 15:11:47,715   INFO  Train:   11/36 ( 31%) [ 259/1759 ( 15%)]  Loss: 3.586 (3.89)  LR: 2.460e-03  Grad: 3.1757  max=0.8630(module.vfe.pfn_layers.0.linear.weight)  min: -0.1712(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7483, loss_cls=0.1439, loss_bbox=0.8658, matched_ious=0.4920, loss_iou=0.0972, loss_iou_reg=0.2366, d_time=0.00(0.02), f_time=1.36(1.26), b_time=1.37(1.28)  Time cost: 05:31/31:53 [6:17:50/16:07:03]  Acc_iter 17850       Data time: 0.00(0.02)  Forward time: 1.36(1.26)  Batch time: 1.37(1.28)
2025-09-03 15:12:50,612   INFO  Train:   11/36 ( 31%) [ 309/1759 ( 18%)]  Loss: 3.780 (3.88)  LR: 2.466e-03  Grad: 3.4933  max=0.7051(module.vfe.pfn_layers.0.linear.weight)  min: -0.1209(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7538, loss_cls=0.1461, loss_bbox=0.8569, matched_ious=0.4901, loss_iou=0.0958, loss_iou_reg=0.2371, d_time=0.00(0.01), f_time=1.36(1.26), b_time=1.37(1.27)  Time cost: 06:34/30:45 [6:18:53/16:03:47]  Acc_iter 17900       Data time: 0.00(0.01)  Forward time: 1.36(1.26)  Batch time: 1.37(1.27)
2025-09-03 15:13:53,400   INFO  Train:   11/36 ( 31%) [ 359/1759 ( 20%)]  Loss: 4.148 (3.88)  LR: 2.473e-03  Grad: 3.8296  max=1.3795(module.vfe.pfn_layers.0.linear.weight)  min: -0.3748(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7602, loss_cls=0.1434, loss_bbox=0.8516, matched_ious=0.4951, loss_iou=0.0959, loss_iou_reg=0.2343, d_time=0.01(0.01), f_time=1.35(1.26), b_time=1.36(1.27)  Time cost: 07:37/29:38 [6:19:55/16:00:55]  Acc_iter 17950       Data time: 0.01(0.01)  Forward time: 1.35(1.26)  Batch time: 1.36(1.27)
2025-09-03 15:14:55,857   INFO  Train:   11/36 ( 31%) [ 409/1759 ( 23%)]  Loss: 3.337 (3.89)  LR: 2.479e-03  Grad: 5.0375  max=0.1293(module.vfe.pfn_layers.0.linear.weight)  min: -2.3219(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7480, loss_cls=0.1388, loss_bbox=0.9203, matched_ious=0.4937, loss_iou=0.0975, loss_iou_reg=0.2370, d_time=0.01(0.01), f_time=1.17(1.26), b_time=1.17(1.27)  Time cost: 08:39/28:31 [6:20:58/15:57:52]  Acc_iter 18000       Data time: 0.01(0.01)  Forward time: 1.17(1.26)  Batch time: 1.17(1.27)
2025-09-03 15:15:58,895   INFO  Train:   11/36 ( 31%) [ 459/1759 ( 26%)]  Loss: 3.328 (3.88)  LR: 2.486e-03  Grad: 4.6006  max=0.9521(module.vfe.pfn_layers.0.linear.weight)  min: -1.9383(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7288, loss_cls=0.1415, loss_bbox=0.8204, matched_ious=0.4910, loss_iou=0.0950, loss_iou_reg=0.2368, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.21(1.27)  Time cost: 09:42/27:27 [6:22:01/15:56:13]  Acc_iter 18050       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.21(1.27)
2025-09-03 15:17:02,022   INFO  Train:   11/36 ( 31%) [ 509/1759 ( 29%)]  Loss: 3.365 (3.88)  LR: 2.493e-03  Grad: 4.3479  max=0.1795(module.vfe.pfn_layers.0.linear.weight)  min: -1.0662(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7783, loss_cls=0.1464, loss_bbox=0.8646, matched_ious=0.4891, loss_iou=0.0962, loss_iou_reg=0.2413, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 10:46/26:23 [6:23:04/15:54:49]  Acc_iter 18100       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 15:18:04,302   INFO  Train:   11/36 ( 31%) [ 559/1759 ( 32%)]  Loss: 3.832 (3.89)  LR: 2.499e-03  Grad: 3.8186  max=0.4959(module.vfe.pfn_layers.0.linear.weight)  min: -0.5958(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7838, loss_cls=0.1528, loss_bbox=0.8866, matched_ious=0.4917, loss_iou=0.0968, loss_iou_reg=0.2347, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.25(1.27)  Time cost: 11:48/25:17 [6:24:06/15:52:20]  Acc_iter 18150       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.27)
2025-09-03 15:19:08,563   INFO  Train:   11/36 ( 31%) [ 609/1759 ( 35%)]  Loss: 3.533 (3.88)  LR: 2.506e-03  Grad: 4.3128  max=1.3167(module.vfe.pfn_layers.0.linear.weight)  min: -0.4505(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7662, loss_cls=0.1431, loss_bbox=0.8861, matched_ious=0.4932, loss_iou=0.0955, loss_iou_reg=0.2357, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.21(1.27)  Time cost: 12:52/24:16 [6:25:11/15:52:32]  Acc_iter 18200       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.21(1.27)
2025-09-03 15:20:11,453   INFO  Train:   11/36 ( 31%) [ 659/1759 ( 37%)]  Loss: 3.743 (3.89)  LR: 2.512e-03  Grad: 4.3274  max=0.7483(module.vfe.pfn_layers.0.linear.weight)  min: -0.2562(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7658, loss_cls=0.1496, loss_bbox=0.8671, matched_ious=0.4942, loss_iou=0.0938, loss_iou_reg=0.2347, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 13:55/23:12 [6:26:13/15:50:59]  Acc_iter 18250       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 15:21:13,365   INFO  Train:   11/36 ( 31%) [ 709/1759 ( 40%)]  Loss: 4.151 (3.88)  LR: 2.519e-03  Grad: 6.3560  max=4.4893(module.vfe.pfn_layers.0.linear.weight)  min: -2.8550(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7585, loss_cls=0.1487, loss_bbox=0.8600, matched_ious=0.4981, loss_iou=0.0945, loss_iou_reg=0.2324, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.19(1.26)  Time cost: 14:57/22:07 [6:27:15/15:48:28]  Acc_iter 18300       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.26)
2025-09-03 15:22:16,560   INFO  Train:   11/36 ( 31%) [ 759/1759 ( 43%)]  Loss: 4.427 (3.88)  LR: 2.525e-03  Grad: 3.5639  max=0.5024(module.vfe.pfn_layers.0.linear.weight)  min: -0.2478(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7478, loss_cls=0.1458, loss_bbox=0.8350, matched_ious=0.4943, loss_iou=0.0971, loss_iou_reg=0.2357, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.25(1.26)  Time cost: 16:00/21:03 [6:28:19/15:47:25]  Acc_iter 18350       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.26)
2025-09-03 15:23:19,565   INFO  Train:   11/36 ( 31%) [ 809/1759 ( 46%)]  Loss: 3.698 (3.88)  LR: 2.531e-03  Grad: 3.8448  max=0.6725(module.vfe.pfn_layers.0.linear.weight)  min: -0.4249(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7455, loss_cls=0.1443, loss_bbox=0.8515, matched_ious=0.4874, loss_iou=0.0985, loss_iou_reg=0.2370, d_time=0.02(0.01), f_time=1.22(1.26), b_time=1.24(1.26)  Time cost: 17:03/20:00 [6:29:22/15:46:11]  Acc_iter 18400       Data time: 0.02(0.01)  Forward time: 1.22(1.26)  Batch time: 1.24(1.26)
2025-09-03 15:24:21,629   INFO  Train:   11/36 ( 31%) [ 859/1759 ( 49%)]  Loss: 4.360 (3.87)  LR: 2.538e-03  Grad: 3.9257  max=0.1957(module.dense_head.prediction_head.height.1.bias)  min: -0.5650(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7473, loss_cls=0.1484, loss_bbox=0.8123, matched_ious=0.4973, loss_iou=0.0973, loss_iou_reg=0.2339, d_time=0.00(0.01), f_time=1.32(1.25), b_time=1.32(1.26)  Time cost: 18:05/18:56 [6:30:24/15:44:09]  Acc_iter 18450       Data time: 0.00(0.01)  Forward time: 1.32(1.25)  Batch time: 1.32(1.26)
2025-09-03 15:25:24,219   INFO  Train:   11/36 ( 31%) [ 909/1759 ( 52%)]  Loss: 3.333 (3.87)  LR: 2.544e-03  Grad: 6.8480  max=4.4896(module.vfe.pfn_layers.0.linear.weight)  min: -2.2446(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7516, loss_cls=0.1432, loss_bbox=0.8152, matched_ious=0.4978, loss_iou=0.0969, loss_iou_reg=0.2363, d_time=0.00(0.01), f_time=1.16(1.25), b_time=1.16(1.26)  Time cost: 19:08/17:52 [6:31:26/15:42:40]  Acc_iter 18500       Data time: 0.00(0.01)  Forward time: 1.16(1.25)  Batch time: 1.16(1.26)
2025-09-03 15:26:26,609   INFO  Train:   11/36 ( 31%) [ 959/1759 ( 55%)]  Loss: 3.393 (3.86)  LR: 2.550e-03  Grad: 4.4526  max=0.3619(module.vfe.pfn_layers.0.linear.weight)  min: -0.5488(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7426, loss_cls=0.1419, loss_bbox=0.8185, matched_ious=0.4932, loss_iou=0.0971, loss_iou_reg=0.2369, d_time=0.00(0.01), f_time=1.35(1.25), b_time=1.36(1.26)  Time cost: 20:10/16:48 [6:32:29/15:41:04]  Acc_iter 18550       Data time: 0.00(0.01)  Forward time: 1.35(1.25)  Batch time: 1.36(1.26)
2025-09-03 15:27:29,759   INFO  Train:   11/36 ( 31%) [1009/1759 ( 57%)]  Loss: 4.003 (3.86)  LR: 2.556e-03  Grad: 4.6814  max=0.1577(module.backbone_3d.cls_conv.3.weight)  min: -0.3846(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7693, loss_cls=0.1485, loss_bbox=0.8282, matched_ious=0.4907, loss_iou=0.0958, loss_iou_reg=0.2380, d_time=0.00(0.01), f_time=1.25(1.25), b_time=1.25(1.26)  Time cost: 21:13/15:45 [6:33:32/15:40:06]  Acc_iter 18600       Data time: 0.00(0.01)  Forward time: 1.25(1.25)  Batch time: 1.25(1.26)
2025-09-03 15:28:34,263   INFO  Train:   11/36 ( 31%) [1059/1759 ( 60%)]  Loss: 4.624 (3.87)  LR: 2.563e-03  Grad: 4.9078  max=0.3690(module.vfe.pfn_layers.0.linear.weight)  min: -0.5984(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7861, loss_cls=0.1479, loss_bbox=0.9178, matched_ious=0.4901, loss_iou=0.0954, loss_iou_reg=0.2373, d_time=0.00(0.01), f_time=1.24(1.25), b_time=1.25(1.26)  Time cost: 22:18/14:43 [6:34:36/15:40:03]  Acc_iter 18650       Data time: 0.00(0.01)  Forward time: 1.24(1.25)  Batch time: 1.25(1.26)
2025-09-03 15:29:38,633   INFO  Train:   11/36 ( 31%) [1109/1759 ( 63%)]  Loss: 3.731 (3.86)  LR: 2.569e-03  Grad: 10.0000  max=1.8641(module.vfe.pfn_layers.0.linear.weight)  min: -8.6141(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7421, loss_cls=0.1444, loss_bbox=0.8149, matched_ious=0.5048, loss_iou=0.0939, loss_iou_reg=0.2324, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.26(1.26)  Time cost: 23:22/13:41 [6:35:41/15:39:50]  Acc_iter 18700       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.26)
2025-09-03 15:30:41,916   INFO  Train:   11/36 ( 31%) [1159/1759 ( 66%)]  Loss: 5.304 (3.86)  LR: 2.575e-03  Grad: 4.7551  max=0.3162(module.vfe.pfn_layers.0.linear.weight)  min: -0.5274(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7681, loss_cls=0.1432, loss_bbox=0.8637, matched_ious=0.4929, loss_iou=0.0973, loss_iou_reg=0.2367, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.27(1.26)  Time cost: 24:25/12:38 [6:36:44/15:38:51]  Acc_iter 18750       Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.27(1.26)
2025-09-03 15:31:44,888   INFO  Train:   11/36 ( 31%) [1209/1759 ( 69%)]  Loss: 3.516 (3.86)  LR: 2.581e-03  Grad: 5.1504  max=1.2586(module.vfe.pfn_layers.0.linear.weight)  min: -0.6007(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7574, loss_cls=0.1460, loss_bbox=0.8512, matched_ious=0.4962, loss_iou=0.0955, loss_iou_reg=0.2344, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.22(1.26)  Time cost: 25:28/11:34 [6:37:47/15:37:40]  Acc_iter 18800       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.26)
2025-09-03 15:32:47,488   INFO  Train:   11/36 ( 31%) [1259/1759 ( 72%)]  Loss: 3.839 (3.86)  LR: 2.587e-03  Grad: 3.3427  max=0.5803(module.vfe.pfn_layers.0.linear.weight)  min: -1.0520(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7356, loss_cls=0.1417, loss_bbox=0.8756, matched_ious=0.4868, loss_iou=0.0943, loss_iou_reg=0.2397, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.20(1.26)  Time cost: 26:31/10:31 [6:38:50/15:36:16]  Acc_iter 18850       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.20(1.26)
2025-09-03 15:33:49,587   INFO  Train:   11/36 ( 31%) [1309/1759 ( 74%)]  Loss: 3.820 (3.86)  LR: 2.593e-03  Grad: 3.2319  max=1.4684(module.vfe.pfn_layers.0.linear.weight)  min: -1.0902(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7561, loss_cls=0.1450, loss_bbox=0.8794, matched_ious=0.4912, loss_iou=0.0967, loss_iou_reg=0.2345, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.22(1.26)  Time cost: 27:33/09:28 [6:39:52/15:34:37]  Acc_iter 18900       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.26)
2025-09-03 15:34:52,361   INFO  Train:   11/36 ( 31%) [1359/1759 ( 77%)]  Loss: 3.300 (3.86)  LR: 2.599e-03  Grad: 2.7072  max=0.1824(module.vfe.pfn_layers.0.linear.weight)  min: -0.3273(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7237, loss_cls=0.1396, loss_bbox=0.8373, matched_ious=0.4962, loss_iou=0.0986, loss_iou_reg=0.2362, d_time=0.00(0.01), f_time=1.25(1.25), b_time=1.26(1.26)  Time cost: 28:36/08:24 [6:40:54/15:33:23]  Acc_iter 18950       Data time: 0.00(0.01)  Forward time: 1.25(1.25)  Batch time: 1.26(1.26)
2025-09-03 15:35:55,086   INFO  Train:   11/36 ( 31%) [1409/1759 ( 80%)]  Loss: 3.394 (3.86)  LR: 2.605e-03  Grad: 2.5062  max=0.4022(module.vfe.pfn_layers.0.linear.weight)  min: -0.0754(module.backbone_3d.cls_conv.1.bias)  NaN: False  loss_hm=0.7513, loss_cls=0.1395, loss_bbox=0.8723, matched_ious=0.4899, loss_iou=0.0993, loss_iou_reg=0.2362, d_time=0.00(0.01), f_time=1.16(1.25), b_time=1.17(1.26)  Time cost: 29:39/07:21 [6:41:57/15:32:07]  Acc_iter 19000       Data time: 0.00(0.01)  Forward time: 1.16(1.25)  Batch time: 1.17(1.26)
2025-09-03 15:36:58,425   INFO  Train:   11/36 ( 31%) [1459/1759 ( 83%)]  Loss: 4.360 (3.86)  LR: 2.611e-03  Grad: 2.8297  max=1.7703(module.vfe.pfn_layers.0.linear.weight)  min: -0.2490(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.7246, loss_cls=0.1386, loss_bbox=0.8586, matched_ious=0.4937, loss_iou=0.0950, loss_iou_reg=0.2380, d_time=0.00(0.01), f_time=1.39(1.25), b_time=1.40(1.26)  Time cost: 30:42/06:18 [6:43:00/15:31:12]  Acc_iter 19050       Data time: 0.00(0.01)  Forward time: 1.39(1.25)  Batch time: 1.40(1.26)
2025-09-03 15:38:02,173   INFO  Train:   11/36 ( 31%) [1509/1759 ( 86%)]  Loss: 4.672 (3.85)  LR: 2.617e-03  Grad: 2.4803  max=0.3786(module.vfe.pfn_layers.0.linear.weight)  min: -0.0852(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7373, loss_cls=0.1418, loss_bbox=0.7952, matched_ious=0.4948, loss_iou=0.0967, loss_iou_reg=0.2349, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.29(1.26)  Time cost: 31:46/05:15 [6:44:04/15:30:28]  Acc_iter 19100       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.29(1.26)
2025-09-03 15:39:04,739   INFO  Train:   11/36 ( 31%) [1559/1759 ( 89%)]  Loss: 3.837 (3.85)  LR: 2.622e-03  Grad: 2.8720  max=0.2978(module.vfe.pfn_layers.0.linear.weight)  min: -1.0420(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7524, loss_cls=0.1430, loss_bbox=0.8167, matched_ious=0.4960, loss_iou=0.0961, loss_iou_reg=0.2342, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.29(1.26)  Time cost: 32:48/04:12 [6:45:07/15:29:10]  Acc_iter 19150       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.29(1.26)
2025-09-03 15:40:08,988   INFO  Train:   11/36 ( 31%) [1609/1759 ( 91%)]  Loss: 4.359 (3.85)  LR: 2.628e-03  Grad: 3.5627  max=1.2679(module.vfe.pfn_layers.0.linear.weight)  min: -1.3527(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7671, loss_cls=0.1435, loss_bbox=0.8602, matched_ious=0.4890, loss_iou=0.0986, loss_iou_reg=0.2380, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.21(1.26)  Time cost: 33:53/03:09 [6:46:11/15:28:38]  Acc_iter 19200       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.21(1.26)
2025-09-03 15:41:11,585   INFO  Train:   11/36 ( 31%) [1659/1759 ( 94%)]  Loss: 4.141 (3.85)  LR: 2.634e-03  Grad: 3.3309  max=0.7193(module.vfe.pfn_layers.0.linear.weight)  min: -0.1387(module.backbone_3d.cls_conv.3.weight)  NaN: False  loss_hm=0.7445, loss_cls=0.1412, loss_bbox=0.8826, matched_ious=0.4923, loss_iou=0.0936, loss_iou_reg=0.2335, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.29(1.26)  Time cost: 34:55/02:06 [6:47:14/15:27:21]  Acc_iter 19250       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.29(1.26)
2025-09-03 15:42:14,168   INFO  Train:   11/36 ( 31%) [1709/1759 ( 97%)]  Loss: 3.361 (3.85)  LR: 2.640e-03  Grad: 3.4383  max=0.5137(module.vfe.pfn_layers.0.linear.weight)  min: -0.3975(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7334, loss_cls=0.1386, loss_bbox=0.8221, matched_ious=0.5046, loss_iou=0.0945, loss_iou_reg=0.2299, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.26(1.26)  Time cost: 35:58/01:03 [6:48:16/15:26:04]  Acc_iter 19300       Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.26)
2025-09-03 15:43:15,161   INFO  Train:   11/36 ( 31%) [1758/1759 (100%)]  Loss: 5.451 (3.85)  LR: 2.645e-03  Grad: 4.6394  max=0.5067(module.vfe.pfn_layers.0.linear.weight)  min: -3.7511(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7259, loss_cls=0.1398, loss_bbox=0.8296, matched_ious=0.5004, loss_iou=0.0960, loss_iou_reg=0.2311, d_time=0.01(0.01), f_time=0.80(1.25), b_time=0.81(1.26)  Time cost: 36:59/00:01 [6:49:17/15:24:40]  Acc_iter 19349       Data time: 0.01(0.01)  Forward time: 0.80(1.25)  Batch time: 0.81(1.26)

                                               [Aepochs:  31%|███       | 11/36 [6:49:18<15:27:07, 2225.11s/it]epochs:  31%|███       | 11/36 [6:49:18<15:27:07, 2225.11s/it]epochs:  31%|███       | 11/36 [6:49:18<15:27:08, 2225.12s/it]epochs:  31%|███       | 11/36 [6:49:18<15:27:07, 2225.11s/it]epochs:  31%|███       | 11/36 [6:49:18<15:27:08, 2225.14s/it]epochs:  31%|███       | 11/36 [6:49:18<15:27:08, 2225.13s/it]epochs:  31%|███       | 11/36 [6:49:18<15:27:08, 2225.13s/it]epochs:  31%|███       | 11/36 [6:49:18<15:27:07, 2225.11s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 15:43:20,508   INFO  Train:   12/36 ( 33%) [   0/1759 (  0%)]  Loss: 3.844 (3.84)  LR: 2.645e-03  Grad: 2.5403  max=0.4301(module.vfe.pfn_layers.0.linear.weight)  min: -0.3798(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6727, loss_cls=0.1531, loss_bbox=0.9273, matched_ious=0.4648, loss_iou=0.1186, loss_iou_reg=0.2595, d_time=1.27(1.27), f_time=2.78(2.78), b_time=4.05(4.05)  Time cost: 00:03/1:49:33 [6:49:23/45:39:04]  Acc_iter 19350       Data time: 1.27(1.27)  Forward time: 2.78(2.78)  Batch time: 4.05(4.05)
2025-09-03 15:44:23,914   INFO  Train:   12/36 ( 33%) [  50/1759 (  3%)]  Loss: 3.032 (3.81)  LR: 2.651e-03  Grad: 2.8266  max=0.6476(module.vfe.pfn_layers.0.linear.weight)  min: -0.5047(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7256, loss_cls=0.1422, loss_bbox=0.8481, matched_ious=0.4877, loss_iou=0.0972, loss_iou_reg=0.2390, d_time=0.00(0.03), f_time=1.16(1.29), b_time=1.17(1.32)  Time cost: 01:07/37:29 [6:50:26/16:03:49]  Acc_iter 19400       Data time: 0.00(0.03)  Forward time: 1.16(1.29)  Batch time: 1.17(1.32)
2025-09-03 15:45:26,789   INFO  Train:   12/36 ( 33%) [ 100/1759 (  6%)]  Loss: 3.340 (3.76)  LR: 2.657e-03  Grad: 2.1769  max=1.1499(module.vfe.pfn_layers.0.linear.weight)  min: -0.2939(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7234, loss_cls=0.1402, loss_bbox=0.8064, matched_ious=0.5078, loss_iou=0.0951, loss_iou_reg=0.2272, d_time=0.00(0.02), f_time=1.24(1.27), b_time=1.24(1.29)  Time cost: 02:10/35:35 [6:51:29/15:41:21]  Acc_iter 19450       Data time: 0.00(0.02)  Forward time: 1.24(1.27)  Batch time: 1.24(1.29)
2025-09-03 15:46:29,570   INFO  Train:   12/36 ( 33%) [ 150/1759 (  9%)]  Loss: 3.410 (3.81)  LR: 2.662e-03  Grad: 2.3816  max=0.4283(module.vfe.pfn_layers.0.linear.weight)  min: -0.7159(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7706, loss_cls=0.1471, loss_bbox=0.8868, matched_ious=0.4896, loss_iou=0.0981, loss_iou_reg=0.2389, d_time=0.01(0.01), f_time=1.27(1.26), b_time=1.28(1.28)  Time cost: 03:12/34:14 [6:52:32/15:32:37]  Acc_iter 19500       Data time: 0.01(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.28)
2025-09-03 15:47:32,429   INFO  Train:   12/36 ( 33%) [ 200/1759 ( 11%)]  Loss: 2.975 (3.79)  LR: 2.668e-03  Grad: 2.4270  max=0.4467(module.vfe.pfn_layers.0.linear.weight)  min: -0.1941(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.7236, loss_cls=0.1419, loss_bbox=0.7848, matched_ious=0.4997, loss_iou=0.0949, loss_iou_reg=0.2353, d_time=0.01(0.01), f_time=1.32(1.26), b_time=1.34(1.27)  Time cost: 04:15/33:02 [6:53:34/15:27:59]  Acc_iter 19550       Data time: 0.01(0.01)  Forward time: 1.32(1.26)  Batch time: 1.34(1.27)
2025-09-03 15:48:35,759   INFO  Train:   12/36 ( 33%) [ 250/1759 ( 14%)]  Loss: 4.809 (3.80)  LR: 2.673e-03  Grad: 2.7458  max=0.5073(module.vfe.pfn_layers.0.linear.weight)  min: -0.2939(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7439, loss_cls=0.1439, loss_bbox=0.8426, matched_ious=0.4833, loss_iou=0.0973, loss_iou_reg=0.2415, d_time=0.00(0.01), f_time=1.29(1.26), b_time=1.29(1.27)  Time cost: 05:18/31:57 [6:54:38/15:26:09]  Acc_iter 19600       Data time: 0.00(0.01)  Forward time: 1.29(1.26)  Batch time: 1.29(1.27)
2025-09-03 15:49:38,515   INFO  Train:   12/36 ( 33%) [ 300/1759 ( 17%)]  Loss: 3.753 (3.81)  LR: 2.679e-03  Grad: 2.9780  max=0.1363(module.backbone_3d.cls_conv.3.bias)  min: -0.7438(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7285, loss_cls=0.1405, loss_bbox=0.8371, matched_ious=0.4787, loss_iou=0.0978, loss_iou_reg=0.2432, d_time=0.00(0.01), f_time=1.32(1.26), b_time=1.32(1.27)  Time cost: 06:21/30:50 [6:55:41/15:23:11]  Acc_iter 19650       Data time: 0.00(0.01)  Forward time: 1.32(1.26)  Batch time: 1.32(1.27)
2025-09-03 15:50:42,041   INFO  Train:   12/36 ( 33%) [ 350/1759 ( 20%)]  Loss: 3.798 (3.79)  LR: 2.684e-03  Grad: 2.9741  max=1.3816(module.vfe.pfn_layers.0.linear.weight)  min: -0.4893(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7305, loss_cls=0.1405, loss_bbox=0.8133, matched_ious=0.4892, loss_iou=0.0968, loss_iou_reg=0.2385, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.27(1.27)  Time cost: 07:25/29:47 [6:56:44/15:22:21]  Acc_iter 19700       Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.27(1.27)
2025-09-03 15:51:44,998   INFO  Train:   12/36 ( 33%) [ 400/1759 ( 23%)]  Loss: 3.580 (3.80)  LR: 2.689e-03  Grad: 2.9457  max=0.9007(module.vfe.pfn_layers.0.linear.weight)  min: -0.8727(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7514, loss_cls=0.1437, loss_bbox=0.8509, matched_ious=0.4990, loss_iou=0.0957, loss_iou_reg=0.2335, d_time=0.01(0.01), f_time=1.30(1.26), b_time=1.31(1.27)  Time cost: 08:28/28:42 [6:57:47/15:20:27]  Acc_iter 19750       Data time: 0.01(0.01)  Forward time: 1.30(1.26)  Batch time: 1.31(1.27)
2025-09-03 15:52:47,522   INFO  Train:   12/36 ( 33%) [ 450/1759 ( 26%)]  Loss: 3.715 (3.79)  LR: 2.695e-03  Grad: 2.7297  max=0.7047(module.vfe.pfn_layers.0.linear.weight)  min: -0.7888(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7188, loss_cls=0.1397, loss_bbox=0.7746, matched_ious=0.4983, loss_iou=0.0961, loss_iou_reg=0.2357, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 09:30/27:36 [6:58:50/15:18:02]  Acc_iter 19800       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 15:53:50,270   INFO  Train:   12/36 ( 33%) [ 500/1759 ( 28%)]  Loss: 3.565 (3.79)  LR: 2.700e-03  Grad: 3.3562  max=1.5962(module.vfe.pfn_layers.0.linear.weight)  min: -0.4203(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7495, loss_cls=0.1432, loss_bbox=0.8664, matched_ious=0.4942, loss_iou=0.0946, loss_iou_reg=0.2338, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.18(1.27)  Time cost: 10:33/26:31 [6:59:52/15:16:13]  Acc_iter 19850       Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.18(1.27)
2025-09-03 15:54:53,429   INFO  Train:   12/36 ( 33%) [ 550/1759 ( 31%)]  Loss: 3.565 (3.79)  LR: 2.705e-03  Grad: 3.0836  max=0.5958(module.vfe.pfn_layers.0.linear.weight)  min: -0.6577(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7261, loss_cls=0.1429, loss_bbox=0.8201, matched_ious=0.4908, loss_iou=0.0967, loss_iou_reg=0.2370, d_time=0.01(0.01), f_time=1.31(1.26), b_time=1.31(1.26)  Time cost: 11:36/25:28 [7:00:55/15:15:04]  Acc_iter 19900       Data time: 0.01(0.01)  Forward time: 1.31(1.26)  Batch time: 1.31(1.26)
2025-09-03 15:55:56,431   INFO  Train:   12/36 ( 33%) [ 600/1759 ( 34%)]  Loss: 3.501 (3.80)  LR: 2.710e-03  Grad: 3.8859  max=1.6624(module.vfe.pfn_layers.0.linear.weight)  min: -1.2876(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7484, loss_cls=0.1414, loss_bbox=0.8832, matched_ious=0.4954, loss_iou=0.0960, loss_iou_reg=0.2323, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.22(1.26)  Time cost: 12:39/24:24 [7:01:58/15:13:45]  Acc_iter 19950       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.26)
2025-09-03 15:56:58,943   INFO  Train:   12/36 ( 33%) [ 650/1759 ( 37%)]  Loss: 3.063 (3.79)  LR: 2.716e-03  Grad: 3.7996  max=1.0518(module.vfe.pfn_layers.0.linear.weight)  min: -0.2940(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7172, loss_cls=0.1387, loss_bbox=0.8067, matched_ious=0.4900, loss_iou=0.0947, loss_iou_reg=0.2374, d_time=0.00(0.01), f_time=1.30(1.25), b_time=1.30(1.26)  Time cost: 13:42/23:20 [7:03:01/15:11:56]  Acc_iter 20000       Data time: 0.00(0.01)  Forward time: 1.30(1.25)  Batch time: 1.30(1.26)
2025-09-03 15:58:01,793   INFO  Train:   12/36 ( 33%) [ 700/1759 ( 40%)]  Loss: 3.668 (3.79)  LR: 2.721e-03  Grad: 3.1118  max=0.8211(module.vfe.pfn_layers.0.linear.weight)  min: -0.4885(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7698, loss_cls=0.1500, loss_bbox=0.8444, matched_ious=0.4913, loss_iou=0.0966, loss_iou_reg=0.2394, d_time=0.00(0.01), f_time=1.25(1.25), b_time=1.26(1.26)  Time cost: 14:45/22:17 [7:04:04/15:10:35]  Acc_iter 20050       Data time: 0.00(0.01)  Forward time: 1.25(1.25)  Batch time: 1.26(1.26)
2025-09-03 15:59:04,340   INFO  Train:   12/36 ( 33%) [ 750/1759 ( 43%)]  Loss: 4.071 (3.79)  LR: 2.726e-03  Grad: 3.0191  max=1.5507(module.vfe.pfn_layers.0.linear.weight)  min: -0.1042(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.7125, loss_cls=0.1360, loss_bbox=0.7884, matched_ious=0.4947, loss_iou=0.0981, loss_iou_reg=0.2377, d_time=0.00(0.01), f_time=1.20(1.25), b_time=1.20(1.26)  Time cost: 15:47/21:13 [7:05:06/15:08:59]  Acc_iter 20100       Data time: 0.00(0.01)  Forward time: 1.20(1.25)  Batch time: 1.20(1.26)
2025-09-03 16:00:08,157   INFO  Train:   12/36 ( 33%) [ 800/1759 ( 45%)]  Loss: 3.046 (3.79)  LR: 2.731e-03  Grad: 2.9233  max=1.3169(module.vfe.pfn_layers.0.linear.weight)  min: -0.7790(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7438, loss_cls=0.1453, loss_bbox=0.8100, matched_ious=0.4996, loss_iou=0.0975, loss_iou_reg=0.2331, d_time=0.00(0.01), f_time=1.17(1.26), b_time=1.18(1.26)  Time cost: 16:51/20:10 [7:06:10/15:08:35]  Acc_iter 20150       Data time: 0.00(0.01)  Forward time: 1.17(1.26)  Batch time: 1.18(1.26)
2025-09-03 16:01:11,294   INFO  Train:   12/36 ( 33%) [ 850/1759 ( 48%)]  Loss: 3.472 (3.78)  LR: 2.736e-03  Grad: 2.8404  max=0.6339(module.vfe.pfn_layers.0.linear.weight)  min: -0.3751(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7257, loss_cls=0.1392, loss_bbox=0.8128, matched_ious=0.4973, loss_iou=0.0944, loss_iou_reg=0.2343, d_time=0.01(0.01), f_time=1.16(1.26), b_time=1.16(1.26)  Time cost: 17:54/19:07 [7:07:13/15:07:32]  Acc_iter 20200       Data time: 0.01(0.01)  Forward time: 1.16(1.26)  Batch time: 1.16(1.26)
2025-09-03 16:02:14,361   INFO  Train:   12/36 ( 33%) [ 900/1759 ( 51%)]  Loss: 3.992 (3.79)  LR: 2.741e-03  Grad: 3.1666  max=0.5977(module.vfe.pfn_layers.0.linear.weight)  min: -0.3863(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7367, loss_cls=0.1410, loss_bbox=0.8819, matched_ious=0.4951, loss_iou=0.0965, loss_iou_reg=0.2355, d_time=0.00(0.01), f_time=1.33(1.26), b_time=1.33(1.26)  Time cost: 18:57/18:04 [7:08:16/15:06:25]  Acc_iter 20250       Data time: 0.00(0.01)  Forward time: 1.33(1.26)  Batch time: 1.33(1.26)
2025-09-03 16:03:16,673   INFO  Train:   12/36 ( 33%) [ 950/1759 ( 54%)]  Loss: 4.664 (3.78)  LR: 2.746e-03  Grad: 2.5561  max=0.7186(module.vfe.pfn_layers.0.linear.weight)  min: -0.2324(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7341, loss_cls=0.1401, loss_bbox=0.8561, matched_ious=0.4964, loss_iou=0.0967, loss_iou_reg=0.2335, d_time=0.00(0.01), f_time=1.39(1.25), b_time=1.39(1.26)  Time cost: 19:59/17:00 [7:09:19/15:04:45]  Acc_iter 20300       Data time: 0.00(0.01)  Forward time: 1.39(1.25)  Batch time: 1.39(1.26)
2025-09-03 16:04:19,918   INFO  Train:   12/36 ( 33%) [1000/1759 ( 57%)]  Loss: 5.029 (3.79)  LR: 2.751e-03  Grad: 2.8310  max=0.4026(module.vfe.pfn_layers.0.linear.weight)  min: -0.5451(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7443, loss_cls=0.1431, loss_bbox=0.8782, matched_ious=0.5000, loss_iou=0.0953, loss_iou_reg=0.2321, d_time=0.00(0.01), f_time=1.25(1.25), b_time=1.26(1.26)  Time cost: 21:03/15:57 [7:10:22/15:03:49]  Acc_iter 20350       Data time: 0.00(0.01)  Forward time: 1.25(1.25)  Batch time: 1.26(1.26)
2025-09-03 16:05:23,638   INFO  Train:   12/36 ( 33%) [1050/1759 ( 60%)]  Loss: 4.197 (3.79)  LR: 2.755e-03  Grad: 2.9848  max=0.3329(module.vfe.pfn_layers.0.linear.weight)  min: -0.3056(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7331, loss_cls=0.1405, loss_bbox=0.8368, matched_ious=0.5047, loss_iou=0.0969, loss_iou_reg=0.2318, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.25(1.26)  Time cost: 22:06/14:55 [7:11:26/15:03:12]  Acc_iter 20400       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.26)
2025-09-03 16:06:26,397   INFO  Train:   12/36 ( 33%) [1100/1759 ( 63%)]  Loss: 4.076 (3.78)  LR: 2.760e-03  Grad: 3.3545  max=0.1074(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0761(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7092, loss_cls=0.1358, loss_bbox=0.8188, matched_ious=0.4994, loss_iou=0.0958, loss_iou_reg=0.2341, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.19(1.26)  Time cost: 23:09/13:51 [7:12:28/15:01:54]  Acc_iter 20450       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.26)
2025-09-03 16:07:29,830   INFO  Train:   12/36 ( 33%) [1150/1759 ( 65%)]  Loss: 2.942 (3.79)  LR: 2.765e-03  Grad: 3.7097  max=0.8458(module.vfe.pfn_layers.0.linear.weight)  min: -0.4827(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7623, loss_cls=0.1459, loss_bbox=0.8538, matched_ious=0.4973, loss_iou=0.0942, loss_iou_reg=0.2339, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.29(1.26)  Time cost: 24:13/12:48 [7:13:32/15:01:03]  Acc_iter 20500       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.29(1.26)
2025-09-03 16:08:32,322   INFO  Train:   12/36 ( 33%) [1200/1759 ( 68%)]  Loss: 4.082 (3.78)  LR: 2.770e-03  Grad: 3.7171  max=0.2598(module.dense_head.prediction_head.height.1.weight)  min: -0.3861(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7268, loss_cls=0.1371, loss_bbox=0.8396, matched_ious=0.4912, loss_iou=0.0969, loss_iou_reg=0.2379, d_time=0.00(0.01), f_time=1.21(1.25), b_time=1.22(1.26)  Time cost: 25:15/11:45 [7:14:34/14:59:38]  Acc_iter 20550       Data time: 0.00(0.01)  Forward time: 1.21(1.25)  Batch time: 1.22(1.26)
2025-09-03 16:09:35,156   INFO  Train:   12/36 ( 33%) [1250/1759 ( 71%)]  Loss: 3.541 (3.79)  LR: 2.774e-03  Grad: 4.0951  max=0.1379(module.dense_head.decoder.self_attn.in_proj_weight)  min: -1.0661(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7454, loss_cls=0.1421, loss_bbox=0.8332, matched_ious=0.4910, loss_iou=0.0955, loss_iou_reg=0.2382, d_time=0.00(0.01), f_time=1.16(1.25), b_time=1.16(1.26)  Time cost: 26:18/10:42 [7:15:37/14:58:26]  Acc_iter 20600       Data time: 0.00(0.01)  Forward time: 1.16(1.25)  Batch time: 1.16(1.26)
2025-09-03 16:10:38,631   INFO  Train:   12/36 ( 33%) [1300/1759 ( 74%)]  Loss: 2.837 (3.78)  LR: 2.779e-03  Grad: 3.7687  max=0.9122(module.vfe.pfn_layers.0.linear.weight)  min: -0.6000(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7373, loss_cls=0.1423, loss_bbox=0.8271, matched_ious=0.4989, loss_iou=0.0963, loss_iou_reg=0.2345, d_time=0.00(0.01), f_time=1.30(1.26), b_time=1.30(1.26)  Time cost: 27:21/09:39 [7:16:41/14:57:35]  Acc_iter 20650       Data time: 0.00(0.01)  Forward time: 1.30(1.26)  Batch time: 1.30(1.26)
2025-09-03 16:11:42,452   INFO  Train:   12/36 ( 33%) [1350/1759 ( 77%)]  Loss: 3.291 (3.78)  LR: 2.783e-03  Grad: 3.8199  max=0.2682(module.vfe.pfn_layers.0.linear.weight)  min: -0.6193(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7281, loss_cls=0.1386, loss_bbox=0.8692, matched_ious=0.4935, loss_iou=0.0955, loss_iou_reg=0.2352, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.25(1.26)  Time cost: 28:25/08:36 [7:17:44/14:56:55]  Acc_iter 20700       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.26)
2025-09-03 16:12:45,004   INFO  Train:   12/36 ( 33%) [1400/1759 ( 80%)]  Loss: 4.017 (3.79)  LR: 2.788e-03  Grad: 5.0576  max=0.2891(module.vfe.pfn_layers.0.linear.weight)  min: -2.6522(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7504, loss_cls=0.1424, loss_bbox=0.8621, matched_ious=0.4907, loss_iou=0.0955, loss_iou_reg=0.2369, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.19(1.26)  Time cost: 29:28/07:33 [7:18:47/14:55:34]  Acc_iter 20750       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.26)
2025-09-03 16:13:48,684   INFO  Train:   12/36 ( 33%) [1450/1759 ( 82%)]  Loss: 3.187 (3.79)  LR: 2.792e-03  Grad: 3.4296  max=0.1989(module.vfe.pfn_layers.0.linear.weight)  min: -0.2181(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7369, loss_cls=0.1413, loss_bbox=0.7982, matched_ious=0.4924, loss_iou=0.0962, loss_iou_reg=0.2376, d_time=0.00(0.01), f_time=1.29(1.26), b_time=1.29(1.26)  Time cost: 30:31/06:30 [7:19:51/14:54:48]  Acc_iter 20800       Data time: 0.00(0.01)  Forward time: 1.29(1.26)  Batch time: 1.29(1.26)
2025-09-03 16:14:51,450   INFO  Train:   12/36 ( 33%) [1500/1759 ( 85%)]  Loss: 4.013 (3.79)  LR: 2.797e-03  Grad: 1.9619  max=0.2088(module.vfe.pfn_layers.0.linear.weight)  min: -0.8807(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7419, loss_cls=0.1429, loss_bbox=0.8741, matched_ious=0.4938, loss_iou=0.0960, loss_iou_reg=0.2369, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.27(1.26)  Time cost: 31:34/05:26 [7:20:53/14:53:35]  Acc_iter 20850       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.26)
2025-09-03 16:15:54,709   INFO  Train:   12/36 ( 33%) [1550/1759 ( 88%)]  Loss: 3.575 (3.79)  LR: 2.801e-03  Grad: 2.0819  max=0.3884(module.vfe.pfn_layers.0.linear.weight)  min: -0.8290(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7142, loss_cls=0.1362, loss_bbox=0.8428, matched_ious=0.5004, loss_iou=0.0942, loss_iou_reg=0.2324, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.23(1.26)  Time cost: 32:37/04:23 [7:21:57/14:52:36]  Acc_iter 20900       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.26)
2025-09-03 16:16:58,239   INFO  Train:   12/36 ( 33%) [1600/1759 ( 91%)]  Loss: 3.770 (3.79)  LR: 2.806e-03  Grad: 4.4938  max=3.8731(module.vfe.pfn_layers.0.linear.weight)  min: -0.5947(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7251, loss_cls=0.1368, loss_bbox=0.8258, matched_ious=0.4944, loss_iou=0.0953, loss_iou_reg=0.2361, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.24(1.26)  Time cost: 33:41/03:20 [7:23:00/14:51:43]  Acc_iter 20950       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.26)
2025-09-03 16:18:01,449   INFO  Train:   12/36 ( 33%) [1650/1759 ( 94%)]  Loss: 3.438 (3.79)  LR: 2.810e-03  Grad: 2.1909  max=0.5819(module.vfe.pfn_layers.0.linear.weight)  min: -0.5309(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7274, loss_cls=0.1363, loss_bbox=0.8345, matched_ious=0.4993, loss_iou=0.0962, loss_iou_reg=0.2336, d_time=0.00(0.01), f_time=1.31(1.26), b_time=1.31(1.26)  Time cost: 34:44/02:17 [7:24:03/14:50:42]  Acc_iter 21000       Data time: 0.00(0.01)  Forward time: 1.31(1.26)  Batch time: 1.31(1.26)
2025-09-03 16:19:04,863   INFO  Train:   12/36 ( 33%) [1700/1759 ( 97%)]  Loss: 3.941 (3.78)  LR: 2.814e-03  Grad: 2.5560  max=0.3635(module.vfe.pfn_layers.0.linear.weight)  min: -0.8187(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7390, loss_cls=0.1401, loss_bbox=0.8266, matched_ious=0.5051, loss_iou=0.0960, loss_iou_reg=0.2280, d_time=0.02(0.01), f_time=1.17(1.26), b_time=1.19(1.26)  Time cost: 35:48/01:14 [7:25:07/14:49:46]  Acc_iter 21050       Data time: 0.02(0.01)  Forward time: 1.17(1.26)  Batch time: 1.19(1.26)
2025-09-03 16:20:07,864   INFO  Train:   12/36 ( 33%) [1750/1759 ( 99%)]  Loss: 4.482 (3.79)  LR: 2.818e-03  Grad: 2.7135  max=0.8059(module.vfe.pfn_layers.0.linear.weight)  min: -0.1438(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7590, loss_cls=0.1425, loss_bbox=0.8687, matched_ious=0.4945, loss_iou=0.0955, loss_iou_reg=0.2321, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.28(1.26)  Time cost: 36:51/00:11 [7:26:10/14:48:40]  Acc_iter 21100       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.26)
2025-09-03 16:20:17,262   INFO  Train:   12/36 ( 33%) [1758/1759 (100%)]  Loss: 4.491 (3.79)  LR: 2.819e-03  Grad: 3.9158  max=0.8767(module.vfe.pfn_layers.0.linear.weight)  min: -2.4741(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7223, loss_cls=0.1416, loss_bbox=0.9161, matched_ious=0.4842, loss_iou=0.1032, loss_iou_reg=0.2421, d_time=0.00(0.01), f_time=0.67(1.26), b_time=0.67(1.26)  Time cost: 37:00/00:01 [7:26:19/14:48:12]  Acc_iter 21108       Data time: 0.00(0.01)  Forward time: 0.67(1.26)  Batch time: 0.67(1.26)

                                               [Aepochs:  33%|███▎      | 12/36 [7:26:20<14:49:40, 2224.19s/it]epochs:  33%|███▎      | 12/36 [7:26:20<14:49:40, 2224.20s/it]epochs:  33%|███▎      | 12/36 [7:26:20<14:49:40, 2224.20s/it]epochs:  33%|███▎      | 12/36 [7:26:20<14:49:40, 2224.20s/it]epochs:  33%|███▎      | 12/36 [7:26:20<14:49:41, 2224.22s/it]epochs:  33%|███▎      | 12/36 [7:26:20<14:49:40, 2224.20s/it]epochs:  33%|███▎      | 12/36 [7:26:20<14:49:40, 2224.20s/it]epochs:  33%|███▎      | 12/36 [7:26:20<14:49:40, 2224.20s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 16:20:22,685   INFO  Train:   13/36 ( 36%) [   0/1759 (  0%)]  Loss: 3.852 (3.85)  LR: 2.819e-03  Grad: 3.2571  max=1.5352(module.vfe.pfn_layers.0.linear.weight)  min: -0.6500(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6534, loss_cls=0.1528, loss_bbox=0.7538, matched_ious=0.4601, loss_iou=0.0906, loss_iou_reg=0.2726, d_time=1.66(1.66), f_time=2.44(2.44), b_time=4.10(4.10)  Time cost: 00:03/1:46:44 [7:26:25/42:41:38]  Acc_iter 21109       Data time: 1.66(1.66)  Forward time: 2.44(2.44)  Batch time: 4.10(4.10)
2025-09-03 16:21:14,102   INFO  Train:   13/36 ( 36%) [  41/1759 (  2%)]  Loss: 4.156 (3.67)  LR: 2.823e-03  Grad: 2.8922  max=0.2528(module.dense_head.prediction_head.height.1.bias)  min: -0.3476(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7094, loss_cls=0.1399, loss_bbox=0.7788, matched_ious=0.4938, loss_iou=0.0946, loss_iou_reg=0.2396, d_time=0.00(0.04), f_time=1.19(1.28), b_time=1.19(1.32)  Time cost: 00:55/37:32 [7:27:16/15:21:29]  Acc_iter 21150       Data time: 0.00(0.04)  Forward time: 1.19(1.28)  Batch time: 1.19(1.32)
2025-09-03 16:22:18,933   INFO  Train:   13/36 ( 36%) [  91/1759 (  5%)]  Loss: 4.441 (3.74)  LR: 2.827e-03  Grad: 3.0551  max=0.6786(module.vfe.pfn_layers.0.linear.weight)  min: -0.2625(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7405, loss_cls=0.1402, loss_bbox=0.8036, matched_ious=0.4976, loss_iou=0.0952, loss_iou_reg=0.2352, d_time=0.00(0.03), f_time=1.23(1.28), b_time=1.24(1.31)  Time cost: 01:59/36:13 [7:28:21/15:14:55]  Acc_iter 21200       Data time: 0.00(0.03)  Forward time: 1.23(1.28)  Batch time: 1.24(1.31)
2025-09-03 16:23:22,135   INFO  Train:   13/36 ( 36%) [ 141/1759 (  8%)]  Loss: 3.736 (3.74)  LR: 2.831e-03  Grad: 2.8373  max=0.4962(module.vfe.pfn_layers.0.linear.weight)  min: -0.6021(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7237, loss_cls=0.1433, loss_bbox=0.7850, matched_ious=0.4941, loss_iou=0.0980, loss_iou_reg=0.2390, d_time=0.00(0.02), f_time=1.34(1.27), b_time=1.35(1.29)  Time cost: 03:03/34:46 [7:29:24/15:04:10]  Acc_iter 21250       Data time: 0.00(0.02)  Forward time: 1.34(1.27)  Batch time: 1.35(1.29)
2025-09-03 16:24:25,097   INFO  Train:   13/36 ( 36%) [ 191/1759 ( 11%)]  Loss: 3.479 (3.73)  LR: 2.835e-03  Grad: 3.0574  max=0.2130(module.vfe.pfn_layers.0.linear.weight)  min: -0.5021(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7114, loss_cls=0.1343, loss_bbox=0.8170, matched_ious=0.4958, loss_iou=0.0974, loss_iou_reg=0.2386, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.28)  Time cost: 04:06/33:29 [7:30:27/14:57:36]  Acc_iter 21300       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.28)
2025-09-03 16:25:28,562   INFO  Train:   13/36 ( 36%) [ 241/1759 ( 14%)]  Loss: 3.752 (3.76)  LR: 2.839e-03  Grad: 3.4471  max=1.0731(module.vfe.pfn_layers.0.linear.weight)  min: -0.4768(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7657, loss_cls=0.1431, loss_bbox=0.8657, matched_ious=0.4923, loss_iou=0.0970, loss_iou_reg=0.2361, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.21(1.28)  Time cost: 05:09/32:21 [7:31:31/14:54:46]  Acc_iter 21350       Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.28)
2025-09-03 16:26:32,285   INFO  Train:   13/36 ( 36%) [ 291/1759 ( 17%)]  Loss: 3.840 (3.76)  LR: 2.843e-03  Grad: 3.5728  max=0.4362(module.vfe.pfn_layers.0.linear.weight)  min: -0.8048(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7336, loss_cls=0.1386, loss_bbox=0.8639, matched_ious=0.4926, loss_iou=0.0940, loss_iou_reg=0.2355, d_time=0.00(0.01), f_time=1.30(1.27), b_time=1.31(1.28)  Time cost: 06:13/31:16 [7:32:34/14:53:09]  Acc_iter 21400       Data time: 0.00(0.01)  Forward time: 1.30(1.27)  Batch time: 1.31(1.28)
2025-09-03 16:27:35,363   INFO  Train:   13/36 ( 36%) [ 341/1759 ( 19%)]  Loss: 3.902 (3.75)  LR: 2.847e-03  Grad: 2.6413  max=0.2584(module.dense_head.prediction_head.height.1.bias)  min: -0.8565(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7007, loss_cls=0.1333, loss_bbox=0.8107, matched_ious=0.5032, loss_iou=0.0954, loss_iou_reg=0.2334, d_time=0.01(0.01), f_time=1.18(1.26), b_time=1.20(1.28)  Time cost: 07:16/30:09 [7:33:37/14:50:23]  Acc_iter 21450       Data time: 0.01(0.01)  Forward time: 1.18(1.26)  Batch time: 1.20(1.28)
2025-09-03 16:28:38,421   INFO  Train:   13/36 ( 36%) [ 391/1759 ( 22%)]  Loss: 3.372 (3.75)  LR: 2.851e-03  Grad: 3.4381  max=1.9149(module.vfe.pfn_layers.0.linear.weight)  min: -0.8396(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7294, loss_cls=0.1361, loss_bbox=0.8176, matched_ious=0.4995, loss_iou=0.0941, loss_iou_reg=0.2325, d_time=0.02(0.01), f_time=1.22(1.26), b_time=1.24(1.28)  Time cost: 08:19/29:02 [7:34:40/14:48:02]  Acc_iter 21500       Data time: 0.02(0.01)  Forward time: 1.22(1.26)  Batch time: 1.24(1.28)
2025-09-03 16:29:40,535   INFO  Train:   13/36 ( 36%) [ 441/1759 ( 25%)]  Loss: 3.399 (3.75)  LR: 2.854e-03  Grad: 3.0865  max=0.3527(module.vfe.pfn_layers.0.linear.weight)  min: -0.9164(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7161, loss_cls=0.1328, loss_bbox=0.8305, matched_ious=0.4992, loss_iou=0.0937, loss_iou_reg=0.2332, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 09:21/27:54 [7:35:43/14:44:28]  Acc_iter 21550       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-03 16:30:43,476   INFO  Train:   13/36 ( 36%) [ 491/1759 ( 28%)]  Loss: 3.675 (3.75)  LR: 2.858e-03  Grad: 3.4219  max=0.7989(module.vfe.pfn_layers.0.linear.weight)  min: -1.0911(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7399, loss_cls=0.1399, loss_bbox=0.8028, matched_ious=0.4976, loss_iou=0.0968, loss_iou_reg=0.2333, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.22(1.27)  Time cost: 10:24/26:49 [7:36:45/14:42:36]  Acc_iter 21600       Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.27)
2025-09-03 16:31:46,138   INFO  Train:   13/36 ( 36%) [ 541/1759 ( 31%)]  Loss: 3.064 (3.76)  LR: 2.862e-03  Grad: 3.3199  max=0.2962(module.vfe.pfn_layers.0.linear.weight)  min: -1.0518(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7329, loss_cls=0.1385, loss_bbox=0.8474, matched_ious=0.4942, loss_iou=0.0953, loss_iou_reg=0.2353, d_time=0.00(0.01), f_time=1.13(1.26), b_time=1.13(1.27)  Time cost: 11:27/25:44 [7:37:48/14:40:31]  Acc_iter 21650       Data time: 0.00(0.01)  Forward time: 1.13(1.26)  Batch time: 1.13(1.27)
2025-09-03 16:32:50,597   INFO  Train:   13/36 ( 36%) [ 591/1759 ( 34%)]  Loss: 3.497 (3.75)  LR: 2.865e-03  Grad: 3.4795  max=0.6898(module.vfe.pfn_layers.0.linear.weight)  min: -0.1726(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7262, loss_cls=0.1394, loss_bbox=0.8161, matched_ious=0.4990, loss_iou=0.0946, loss_iou_reg=0.2333, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.21(1.27)  Time cost: 12:31/24:42 [7:38:53/14:40:43]  Acc_iter 21700       Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.27)
2025-09-03 16:33:53,121   INFO  Train:   13/36 ( 36%) [ 641/1759 ( 36%)]  Loss: 3.400 (3.75)  LR: 2.869e-03  Grad: 3.0778  max=0.3486(module.vfe.pfn_layers.0.linear.weight)  min: -0.5923(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7107, loss_cls=0.1351, loss_bbox=0.7693, matched_ious=0.5047, loss_iou=0.0953, loss_iou_reg=0.2331, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.19(1.27)  Time cost: 13:34/23:37 [7:39:55/14:38:38]  Acc_iter 21750       Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.19(1.27)
2025-09-03 16:34:55,841   INFO  Train:   13/36 ( 36%) [ 691/1759 ( 39%)]  Loss: 3.376 (3.75)  LR: 2.873e-03  Grad: 3.3562  max=0.2914(module.vfe.pfn_layers.0.linear.weight)  min: -0.9214(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7277, loss_cls=0.1365, loss_bbox=0.8229, matched_ious=0.5024, loss_iou=0.0978, loss_iou_reg=0.2320, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.23(1.27)  Time cost: 14:36/22:33 [7:40:58/14:36:54]  Acc_iter 21800       Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.23(1.27)
2025-09-03 16:35:59,945   INFO  Train:   13/36 ( 36%) [ 741/1759 ( 42%)]  Loss: 3.583 (3.74)  LR: 2.876e-03  Grad: 3.9961  max=0.5525(module.vfe.pfn_layers.0.linear.weight)  min: -1.8745(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7059, loss_cls=0.1330, loss_bbox=0.7894, matched_ious=0.5094, loss_iou=0.0952, loss_iou_reg=0.2298, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.23(1.27)  Time cost: 15:40/21:30 [7:42:02/14:36:32]  Acc_iter 21850       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.27)
2025-09-03 16:37:02,697   INFO  Train:   13/36 ( 36%) [ 791/1759 ( 45%)]  Loss: 3.538 (3.74)  LR: 2.880e-03  Grad: 2.3278  max=1.0278(module.vfe.pfn_layers.0.linear.weight)  min: -0.4512(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7149, loss_cls=0.1334, loss_bbox=0.8361, matched_ious=0.4989, loss_iou=0.0939, loss_iou_reg=0.2359, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 16:43/20:26 [7:43:05/14:34:54]  Acc_iter 21900       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 16:38:04,819   INFO  Train:   13/36 ( 36%) [ 841/1759 ( 48%)]  Loss: 3.701 (3.74)  LR: 2.883e-03  Grad: 2.5974  max=0.8421(module.vfe.pfn_layers.0.linear.weight)  min: -0.5878(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7317, loss_cls=0.1392, loss_bbox=0.7852, matched_ious=0.5135, loss_iou=0.0941, loss_iou_reg=0.2281, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.27(1.27)  Time cost: 17:45/19:21 [7:44:07/14:32:50]  Acc_iter 21950       Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.27(1.27)
2025-09-03 16:39:06,961   INFO  Train:   13/36 ( 36%) [ 891/1759 ( 51%)]  Loss: 3.472 (3.74)  LR: 2.886e-03  Grad: 2.7461  max=0.7388(module.vfe.pfn_layers.0.linear.weight)  min: -0.5879(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7409, loss_cls=0.1416, loss_bbox=0.8585, matched_ious=0.5069, loss_iou=0.0943, loss_iou_reg=0.2300, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.19(1.27)  Time cost: 18:47/18:17 [7:45:09/14:30:54]  Acc_iter 22000       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.27)
2025-09-03 16:40:09,439   INFO  Train:   13/36 ( 36%) [ 941/1759 ( 53%)]  Loss: 4.154 (3.74)  LR: 2.890e-03  Grad: 2.8457  max=0.2024(module.vfe.pfn_layers.0.linear.weight)  min: -0.4313(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6949, loss_cls=0.1340, loss_bbox=0.7805, matched_ious=0.4955, loss_iou=0.0926, loss_iou_reg=0.2369, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.26(1.26)  Time cost: 19:50/17:13 [7:46:11/14:29:18]  Acc_iter 22050       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.26)
2025-09-03 16:41:11,973   INFO  Train:   13/36 ( 36%) [ 991/1759 ( 56%)]  Loss: 3.820 (3.74)  LR: 2.893e-03  Grad: 3.1102  max=0.4750(module.vfe.pfn_layers.0.linear.weight)  min: -0.2411(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7769, loss_cls=0.1460, loss_bbox=0.8822, matched_ious=0.4838, loss_iou=0.1003, loss_iou_reg=0.2426, d_time=0.00(0.01), f_time=1.30(1.26), b_time=1.30(1.26)  Time cost: 20:52/16:10 [7:47:14/14:27:48]  Acc_iter 22100       Data time: 0.00(0.01)  Forward time: 1.30(1.26)  Batch time: 1.30(1.26)
2025-09-03 16:42:14,620   INFO  Train:   13/36 ( 36%) [1041/1759 ( 59%)]  Loss: 4.474 (3.74)  LR: 2.896e-03  Grad: 3.6649  max=1.3282(module.vfe.pfn_layers.0.linear.weight)  min: -0.3603(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6829, loss_cls=0.1251, loss_bbox=0.8168, matched_ious=0.4977, loss_iou=0.0950, loss_iou_reg=0.2358, d_time=0.00(0.01), f_time=1.33(1.25), b_time=1.33(1.26)  Time cost: 21:55/15:06 [7:48:17/14:26:25]  Acc_iter 22150       Data time: 0.00(0.01)  Forward time: 1.33(1.25)  Batch time: 1.33(1.26)
2025-09-03 16:43:18,399   INFO  Train:   13/36 ( 36%) [1091/1759 ( 62%)]  Loss: 4.082 (3.73)  LR: 2.900e-03  Grad: 3.7561  max=0.3822(module.vfe.pfn_layers.0.linear.weight)  min: -0.6814(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7010, loss_cls=0.1314, loss_bbox=0.7991, matched_ious=0.5061, loss_iou=0.0943, loss_iou_reg=0.2284, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.25(1.26)  Time cost: 22:59/14:03 [7:49:20/14:25:46]  Acc_iter 22200       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.26)
2025-09-03 16:44:21,795   INFO  Train:   13/36 ( 36%) [1141/1759 ( 65%)]  Loss: 3.473 (3.73)  LR: 2.903e-03  Grad: 3.9809  max=0.2991(module.dense_head.prediction_head.height.1.bias)  min: -0.5302(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7119, loss_cls=0.1380, loss_bbox=0.8208, matched_ious=0.5001, loss_iou=0.0942, loss_iou_reg=0.2342, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.21(1.26)  Time cost: 24:02/13:00 [7:50:24/14:24:52]  Acc_iter 22250       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.26)
2025-09-03 16:45:25,512   INFO  Train:   13/36 ( 36%) [1191/1759 ( 68%)]  Loss: 4.111 (3.74)  LR: 2.906e-03  Grad: 4.1430  max=0.5095(module.vfe.pfn_layers.0.linear.weight)  min: -0.3321(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7551, loss_cls=0.1389, loss_bbox=0.8827, matched_ious=0.4960, loss_iou=0.0955, loss_iou_reg=0.2346, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.26(1.26)  Time cost: 25:06/11:57 [7:51:28/14:24:08]  Acc_iter 22300       Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.26)
2025-09-03 16:46:28,324   INFO  Train:   13/36 ( 36%) [1241/1759 ( 71%)]  Loss: 4.144 (3.74)  LR: 2.909e-03  Grad: 4.6678  max=0.8948(module.vfe.pfn_layers.0.linear.weight)  min: -1.4361(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7247, loss_cls=0.1405, loss_bbox=0.8245, matched_ious=0.5010, loss_iou=0.0958, loss_iou_reg=0.2320, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.27(1.26)  Time cost: 26:09/10:54 [7:52:30/14:22:52]  Acc_iter 22350       Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.27(1.26)
2025-09-03 16:47:30,967   INFO  Train:   13/36 ( 36%) [1291/1759 ( 73%)]  Loss: 3.070 (3.74)  LR: 2.912e-03  Grad: 4.8068  max=0.8744(module.vfe.pfn_layers.0.linear.weight)  min: -1.4489(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7073, loss_cls=0.1337, loss_bbox=0.8306, matched_ious=0.5071, loss_iou=0.0943, loss_iou_reg=0.2306, d_time=0.00(0.01), f_time=1.16(1.26), b_time=1.16(1.26)  Time cost: 27:11/09:51 [7:53:33/14:21:32]  Acc_iter 22400       Data time: 0.00(0.01)  Forward time: 1.16(1.26)  Batch time: 1.16(1.26)
2025-09-03 16:48:33,603   INFO  Train:   13/36 ( 36%) [1341/1759 ( 76%)]  Loss: 3.755 (3.74)  LR: 2.915e-03  Grad: 4.8227  max=0.6339(module.vfe.pfn_layers.0.linear.weight)  min: -0.1490(module.backbone_3d.cls_conv.3.bias)  NaN: False  loss_hm=0.7481, loss_cls=0.1398, loss_bbox=0.8763, matched_ious=0.4872, loss_iou=0.0971, loss_iou_reg=0.2391, d_time=0.01(0.01), f_time=1.27(1.26), b_time=1.28(1.26)  Time cost: 28:14/08:47 [7:54:36/14:20:13]  Acc_iter 22450       Data time: 0.01(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.26)
2025-09-03 16:49:36,171   INFO  Train:   13/36 ( 36%) [1391/1759 ( 79%)]  Loss: 3.639 (3.74)  LR: 2.918e-03  Grad: 5.3244  max=0.3543(module.vfe.pfn_layers.0.linear.weight)  min: -1.2742(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6986, loss_cls=0.1337, loss_bbox=0.7897, matched_ious=0.5036, loss_iou=0.0926, loss_iou_reg=0.2331, d_time=0.00(0.01), f_time=1.24(1.25), b_time=1.24(1.26)  Time cost: 29:17/07:44 [7:55:38/14:18:53]  Acc_iter 22500       Data time: 0.00(0.01)  Forward time: 1.24(1.25)  Batch time: 1.24(1.26)
2025-09-03 16:50:39,561   INFO  Train:   13/36 ( 36%) [1441/1759 ( 82%)]  Loss: 3.638 (3.74)  LR: 2.921e-03  Grad: 4.2476  max=1.5001(module.vfe.pfn_layers.0.linear.weight)  min: -0.5209(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7154, loss_cls=0.1373, loss_bbox=0.8159, matched_ious=0.5002, loss_iou=0.0965, loss_iou_reg=0.2322, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.19(1.26)  Time cost: 30:20/06:41 [7:56:42/14:17:58]  Acc_iter 22550       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.26)
2025-09-03 16:51:42,135   INFO  Train:   13/36 ( 36%) [1491/1759 ( 85%)]  Loss: 3.991 (3.74)  LR: 2.923e-03  Grad: 4.2574  max=0.3302(module.vfe.pfn_layers.0.linear.weight)  min: -0.3898(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7386, loss_cls=0.1395, loss_bbox=0.7967, matched_ious=0.4983, loss_iou=0.0952, loss_iou_reg=0.2350, d_time=0.00(0.01), f_time=1.34(1.25), b_time=1.34(1.26)  Time cost: 31:23/05:38 [7:57:44/14:16:40]  Acc_iter 22600       Data time: 0.00(0.01)  Forward time: 1.34(1.25)  Batch time: 1.34(1.26)
2025-09-03 16:52:44,566   INFO  Train:   13/36 ( 36%) [1541/1759 ( 88%)]  Loss: 3.959 (3.74)  LR: 2.926e-03  Grad: 3.9986  max=0.4176(module.vfe.pfn_layers.0.linear.weight)  min: -0.6042(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7220, loss_cls=0.1335, loss_bbox=0.8395, matched_ious=0.5002, loss_iou=0.0963, loss_iou_reg=0.2314, d_time=0.00(0.01), f_time=1.29(1.25), b_time=1.29(1.26)  Time cost: 32:25/04:35 [7:58:47/14:15:19]  Acc_iter 22650       Data time: 0.00(0.01)  Forward time: 1.29(1.25)  Batch time: 1.29(1.26)
2025-09-03 16:53:49,255   INFO  Train:   13/36 ( 36%) [1591/1759 ( 90%)]  Loss: 3.461 (3.74)  LR: 2.929e-03  Grad: 2.5632  max=0.2906(module.vfe.pfn_layers.0.linear.weight)  min: -1.2354(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7423, loss_cls=0.1376, loss_bbox=0.8677, matched_ious=0.4892, loss_iou=0.0965, loss_iou_reg=0.2394, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.20(1.26)  Time cost: 33:30/03:32 [7:59:51/14:14:57]  Acc_iter 22700       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.26)
2025-09-03 16:54:52,600   INFO  Train:   13/36 ( 36%) [1641/1759 ( 93%)]  Loss: 3.427 (3.74)  LR: 2.931e-03  Grad: 2.3678  max=0.5422(module.vfe.pfn_layers.0.linear.weight)  min: -0.4829(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6967, loss_cls=0.1307, loss_bbox=0.8149, matched_ious=0.5058, loss_iou=0.0926, loss_iou_reg=0.2298, d_time=0.01(0.01), f_time=1.35(1.26), b_time=1.36(1.26)  Time cost: 34:33/02:29 [8:00:55/14:13:58]  Acc_iter 22750       Data time: 0.01(0.01)  Forward time: 1.35(1.26)  Batch time: 1.36(1.26)
2025-09-03 16:55:56,034   INFO  Train:   13/36 ( 36%) [1691/1759 ( 96%)]  Loss: 4.301 (3.73)  LR: 2.934e-03  Grad: 3.0630  max=1.9729(module.vfe.pfn_layers.0.linear.weight)  min: -0.1742(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7228, loss_cls=0.1357, loss_bbox=0.7953, matched_ious=0.4994, loss_iou=0.0972, loss_iou_reg=0.2360, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.21(1.26)  Time cost: 35:36/01:25 [8:01:58/14:13:02]  Acc_iter 22800       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.26)
2025-09-03 16:56:58,429   INFO  Train:   13/36 ( 36%) [1741/1759 ( 99%)]  Loss: 2.979 (3.73)  LR: 2.937e-03  Grad: 6.7747  max=6.4437(module.vfe.pfn_layers.0.linear.weight)  min: -0.3566(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7198, loss_cls=0.1343, loss_bbox=0.8386, matched_ious=0.5041, loss_iou=0.0945, loss_iou_reg=0.2311, d_time=0.00(0.01), f_time=1.15(1.26), b_time=1.15(1.26)  Time cost: 36:39/00:22 [8:03:00/14:11:42]  Acc_iter 22850       Data time: 0.00(0.01)  Forward time: 1.15(1.26)  Batch time: 1.15(1.26)
2025-09-03 16:57:18,893   INFO  Train:   13/36 ( 36%) [1758/1759 (100%)]  Loss: 3.912 (3.73)  LR: 2.937e-03  Grad: 2.1890  max=0.2938(module.vfe.pfn_layers.0.linear.weight)  min: -0.4259(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7489, loss_cls=0.1392, loss_bbox=0.8548, matched_ious=0.4983, loss_iou=0.0940, loss_iou_reg=0.2346, d_time=0.00(0.01), f_time=0.70(1.25), b_time=0.71(1.26)  Time cost: 36:59/00:01 [8:03:21/14:10:57]  Acc_iter 22867       Data time: 0.00(0.01)  Forward time: 0.70(1.25)  Batch time: 0.71(1.26)

                                               [Aepochs:  36%|███▌      | 13/36 [8:03:21<14:12:18, 2223.42s/it]epochs:  36%|███▌      | 13/36 [8:03:21<14:12:18, 2223.42s/it]epochs:  36%|███▌      | 13/36 [8:03:21<14:12:18, 2223.42s/it]epochs:  36%|███▌      | 13/36 [8:03:21<14:12:18, 2223.43s/it]epochs:  36%|███▌      | 13/36 [8:03:21<14:12:18, 2223.42s/it]epochs:  36%|███▌      | 13/36 [8:03:21<14:12:18, 2223.42s/it]epochs:  36%|███▌      | 13/36 [8:03:21<14:12:18, 2223.42s/it]epochs:  36%|███▌      | 13/36 [8:03:22<14:12:18, 2223.43s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 16:57:24,488   INFO  Train:   14/36 ( 39%) [   0/1759 (  0%)]  Loss: 3.332 (3.33)  LR: 2.938e-03  Grad: 2.0176  max=0.2976(module.vfe.pfn_layers.0.linear.weight)  min: -0.7321(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6640, loss_cls=0.1294, loss_bbox=0.6527, matched_ious=0.5275, loss_iou=0.0878, loss_iou_reg=0.2238, d_time=1.59(1.59), f_time=2.68(2.68), b_time=4.27(4.27)  Time cost: 00:03/1:53:38 [8:03:27/43:33:37]  Acc_iter 22868       Data time: 1.59(1.59)  Forward time: 2.68(2.68)  Batch time: 4.27(4.27)
2025-09-03 16:58:05,449   INFO  Train:   14/36 ( 39%) [  32/1759 (  2%)]  Loss: 3.664 (3.65)  LR: 2.939e-03  Grad: 3.2234  max=1.6122(module.vfe.pfn_layers.0.linear.weight)  min: -1.7287(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7111, loss_cls=0.1343, loss_bbox=0.8017, matched_ious=0.5000, loss_iou=0.0970, loss_iou_reg=0.2351, d_time=0.00(0.05), f_time=1.31(1.32), b_time=1.31(1.37)  Time cost: 00:44/39:06 [8:04:07/15:15:26]  Acc_iter 22900       Data time: 0.00(0.05)  Forward time: 1.31(1.32)  Batch time: 1.31(1.37)
2025-09-03 16:59:07,710   INFO  Train:   14/36 ( 39%) [  82/1759 (  5%)]  Loss: 4.506 (3.68)  LR: 2.942e-03  Grad: 2.0155  max=0.5715(module.vfe.pfn_layers.0.linear.weight)  min: -0.1750(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7302, loss_cls=0.1388, loss_bbox=0.8287, matched_ious=0.4985, loss_iou=0.0946, loss_iou_reg=0.2329, d_time=0.00(0.02), f_time=1.13(1.27), b_time=1.14(1.30)  Time cost: 01:47/36:03 [8:05:10/14:28:18]  Acc_iter 22950       Data time: 0.00(0.02)  Forward time: 1.13(1.27)  Batch time: 1.14(1.30)
2025-09-03 17:00:10,551   INFO  Train:   14/36 ( 39%) [ 132/1759 (  8%)]  Loss: 3.660 (3.69)  LR: 2.944e-03  Grad: 2.5454  max=0.2805(module.vfe.pfn_layers.0.linear.weight)  min: -1.0839(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7219, loss_cls=0.1362, loss_bbox=0.8222, matched_ious=0.4940, loss_iou=0.0949, loss_iou_reg=0.2349, d_time=0.00(0.02), f_time=1.27(1.26), b_time=1.28(1.28)  Time cost: 02:49/34:38 [8:06:13/14:18:44]  Acc_iter 23000       Data time: 0.00(0.02)  Forward time: 1.27(1.26)  Batch time: 1.28(1.28)
2025-09-03 17:01:13,415   INFO  Train:   14/36 ( 39%) [ 182/1759 ( 10%)]  Loss: 3.699 (3.69)  LR: 2.946e-03  Grad: 2.6253  max=1.3545(module.vfe.pfn_layers.0.linear.weight)  min: -0.9895(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7087, loss_cls=0.1366, loss_bbox=0.7956, matched_ious=0.5012, loss_iou=0.0955, loss_iou_reg=0.2339, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.28(1.27)  Time cost: 03:52/33:26 [8:07:15/14:13:55]  Acc_iter 23050       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.27)
2025-09-03 17:02:16,745   INFO  Train:   14/36 ( 39%) [ 232/1759 ( 13%)]  Loss: 4.021 (3.67)  LR: 2.949e-03  Grad: 2.1597  max=0.2539(module.vfe.pfn_layers.0.linear.weight)  min: -0.0717(module.backbone_3d.cls_conv.1.bias)  NaN: False  loss_hm=0.6829, loss_cls=0.1320, loss_bbox=0.8148, matched_ious=0.5057, loss_iou=0.0950, loss_iou_reg=0.2317, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 04:56/32:20 [8:08:19/14:12:04]  Acc_iter 23100       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-03 17:03:19,732   INFO  Train:   14/36 ( 39%) [ 282/1759 ( 16%)]  Loss: 4.078 (3.66)  LR: 2.951e-03  Grad: 2.5263  max=0.5356(module.vfe.pfn_layers.0.linear.weight)  min: -0.2023(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6997, loss_cls=0.1306, loss_bbox=0.7998, matched_ious=0.5065, loss_iou=0.0961, loss_iou_reg=0.2284, d_time=0.00(0.01), f_time=1.32(1.26), b_time=1.32(1.27)  Time cost: 05:59/31:14 [8:09:22/14:09:41]  Acc_iter 23150       Data time: 0.00(0.01)  Forward time: 1.32(1.26)  Batch time: 1.32(1.27)
2025-09-03 17:04:23,614   INFO  Train:   14/36 ( 39%) [ 332/1759 ( 19%)]  Loss: 3.018 (3.67)  LR: 2.953e-03  Grad: 2.8620  max=0.6649(module.vfe.pfn_layers.0.linear.weight)  min: -0.4519(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7083, loss_cls=0.1331, loss_bbox=0.8428, matched_ious=0.5051, loss_iou=0.0936, loss_iou_reg=0.2279, d_time=0.01(0.01), f_time=1.28(1.26), b_time=1.29(1.27)  Time cost: 07:03/30:12 [8:10:26/14:09:29]  Acc_iter 23200       Data time: 0.01(0.01)  Forward time: 1.28(1.26)  Batch time: 1.29(1.27)
2025-09-03 17:05:27,214   INFO  Train:   14/36 ( 39%) [ 382/1759 ( 22%)]  Loss: 4.652 (3.66)  LR: 2.955e-03  Grad: 3.0492  max=0.7952(module.vfe.pfn_layers.0.linear.weight)  min: -0.2390(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6972, loss_cls=0.1313, loss_bbox=0.7642, matched_ious=0.5084, loss_iou=0.0942, loss_iou_reg=0.2291, d_time=0.00(0.01), f_time=1.17(1.26), b_time=1.18(1.27)  Time cost: 08:06/29:09 [8:11:29/14:08:35]  Acc_iter 23250       Data time: 0.00(0.01)  Forward time: 1.17(1.26)  Batch time: 1.18(1.27)
2025-09-03 17:06:29,930   INFO  Train:   14/36 ( 39%) [ 432/1759 ( 25%)]  Loss: 3.867 (3.66)  LR: 2.957e-03  Grad: 3.6484  max=1.4839(module.vfe.pfn_layers.0.linear.weight)  min: -1.1068(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7166, loss_cls=0.1369, loss_bbox=0.8476, matched_ious=0.4966, loss_iou=0.0944, loss_iou_reg=0.2345, d_time=0.00(0.01), f_time=1.16(1.26), b_time=1.17(1.27)  Time cost: 09:09/28:03 [8:12:32/14:06:17]  Acc_iter 23300       Data time: 0.00(0.01)  Forward time: 1.16(1.26)  Batch time: 1.17(1.27)
2025-09-03 17:07:32,769   INFO  Train:   14/36 ( 39%) [ 482/1759 ( 27%)]  Loss: 3.410 (3.67)  LR: 2.959e-03  Grad: 3.4786  max=0.8563(module.vfe.pfn_layers.0.linear.weight)  min: -0.3081(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6928, loss_cls=0.1308, loss_bbox=0.8362, matched_ious=0.5037, loss_iou=0.0948, loss_iou_reg=0.2319, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.28(1.27)  Time cost: 10:12/26:58 [8:13:35/14:04:24]  Acc_iter 23350       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.27)
2025-09-03 17:08:36,406   INFO  Train:   14/36 ( 39%) [ 532/1759 ( 30%)]  Loss: 3.472 (3.67)  LR: 2.962e-03  Grad: 3.6845  max=0.5594(module.vfe.pfn_layers.0.linear.weight)  min: -0.3637(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7081, loss_cls=0.1314, loss_bbox=0.8444, matched_ious=0.4958, loss_iou=0.0975, loss_iou_reg=0.2370, d_time=0.00(0.01), f_time=1.35(1.26), b_time=1.35(1.27)  Time cost: 11:15/25:55 [8:14:38/14:03:41]  Acc_iter 23400       Data time: 0.00(0.01)  Forward time: 1.35(1.26)  Batch time: 1.35(1.27)
2025-09-03 17:09:39,580   INFO  Train:   14/36 ( 39%) [ 582/1759 ( 33%)]  Loss: 3.561 (3.68)  LR: 2.963e-03  Grad: 2.6909  max=0.6982(module.vfe.pfn_layers.0.linear.weight)  min: -0.6478(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7448, loss_cls=0.1360, loss_bbox=0.8549, matched_ious=0.4995, loss_iou=0.0960, loss_iou_reg=0.2336, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.26(1.27)  Time cost: 12:18/24:51 [8:15:42/14:02:22]  Acc_iter 23450       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.27)
2025-09-03 17:10:42,415   INFO  Train:   14/36 ( 39%) [ 632/1759 ( 36%)]  Loss: 3.412 (3.68)  LR: 2.965e-03  Grad: 2.4136  max=1.3201(module.vfe.pfn_layers.0.linear.weight)  min: -0.3366(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7042, loss_cls=0.1362, loss_bbox=0.7898, matched_ious=0.5002, loss_iou=0.0951, loss_iou_reg=0.2333, d_time=0.00(0.01), f_time=1.31(1.26), b_time=1.32(1.27)  Time cost: 13:21/23:47 [8:16:44/14:00:45]  Acc_iter 23500       Data time: 0.00(0.01)  Forward time: 1.31(1.26)  Batch time: 1.32(1.27)
2025-09-03 17:11:45,638   INFO  Train:   14/36 ( 39%) [ 682/1759 ( 39%)]  Loss: 3.342 (3.68)  LR: 2.967e-03  Grad: 2.4012  max=0.7758(module.vfe.pfn_layers.0.linear.weight)  min: -0.6349(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7015, loss_cls=0.1325, loss_bbox=0.7506, matched_ious=0.5139, loss_iou=0.0947, loss_iou_reg=0.2280, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.29(1.27)  Time cost: 14:25/22:44 [8:17:48/13:59:35]  Acc_iter 23550       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.29(1.27)
2025-09-03 17:12:48,844   INFO  Train:   14/36 ( 39%) [ 732/1759 ( 42%)]  Loss: 3.633 (3.68)  LR: 2.969e-03  Grad: 2.5273  max=0.3391(module.vfe.pfn_layers.0.linear.weight)  min: -0.4916(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7005, loss_cls=0.1310, loss_bbox=0.8189, matched_ious=0.4940, loss_iou=0.0984, loss_iou_reg=0.2351, d_time=0.01(0.01), f_time=1.25(1.26), b_time=1.26(1.27)  Time cost: 15:28/21:40 [8:18:51/13:58:25]  Acc_iter 23600       Data time: 0.01(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.27)
2025-09-03 17:13:51,620   INFO  Train:   14/36 ( 39%) [ 782/1759 ( 44%)]  Loss: 3.347 (3.68)  LR: 2.971e-03  Grad: 3.6050  max=0.1554(module.vfe.pfn_layers.0.linear.weight)  min: -2.3999(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7283, loss_cls=0.1402, loss_bbox=0.7729, matched_ious=0.4998, loss_iou=0.0976, loss_iou_reg=0.2345, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 16:31/20:36 [8:19:54/13:56:54]  Acc_iter 23650       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-03 17:14:56,041   INFO  Train:   14/36 ( 39%) [ 832/1759 ( 47%)]  Loss: 3.346 (3.68)  LR: 2.973e-03  Grad: 3.5121  max=1.1987(module.vfe.pfn_layers.0.linear.weight)  min: -1.3000(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6966, loss_cls=0.1315, loss_bbox=0.8171, matched_ious=0.5073, loss_iou=0.0949, loss_iou_reg=0.2298, d_time=0.58(0.01), f_time=1.39(1.26), b_time=1.97(1.27)  Time cost: 17:35/19:34 [8:20:58/13:56:45]  Acc_iter 23700       Data time: 0.58(0.01)  Forward time: 1.39(1.26)  Batch time: 1.97(1.27)
2025-09-03 17:15:59,124   INFO  Train:   14/36 ( 39%) [ 882/1759 ( 50%)]  Loss: 2.858 (3.67)  LR: 2.974e-03  Grad: 3.2145  max=0.2164(module.vfe.pfn_layers.0.linear.weight)  min: -0.6261(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7066, loss_cls=0.1342, loss_bbox=0.7700, matched_ious=0.5151, loss_iou=0.0955, loss_iou_reg=0.2253, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.22(1.27)  Time cost: 18:38/18:30 [8:22:01/13:55:30]  Acc_iter 23750       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.27)
2025-09-03 17:17:02,111   INFO  Train:   14/36 ( 39%) [ 932/1759 ( 53%)]  Loss: 3.164 (3.67)  LR: 2.976e-03  Grad: 2.8103  max=0.4797(module.vfe.pfn_layers.0.linear.weight)  min: -0.3216(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7070, loss_cls=0.1348, loss_bbox=0.7417, matched_ious=0.5051, loss_iou=0.0952, loss_iou_reg=0.2344, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 19:41/17:27 [8:23:04/13:54:12]  Acc_iter 23800       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 17:18:05,151   INFO  Train:   14/36 ( 39%) [ 982/1759 ( 56%)]  Loss: 3.709 (3.67)  LR: 2.977e-03  Grad: 3.1120  max=0.4108(module.vfe.pfn_layers.0.linear.weight)  min: -0.5580(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7100, loss_cls=0.1348, loss_bbox=0.8158, matched_ious=0.5033, loss_iou=0.0951, loss_iou_reg=0.2296, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.19(1.27)  Time cost: 20:44/16:23 [8:24:07/13:52:57]  Acc_iter 23850       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.27)
2025-09-03 17:19:07,683   INFO  Train:   14/36 ( 39%) [1032/1759 ( 59%)]  Loss: 3.975 (3.67)  LR: 2.979e-03  Grad: 3.5661  max=1.1606(module.vfe.pfn_layers.0.linear.weight)  min: -0.7634(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7318, loss_cls=0.1356, loss_bbox=0.8515, matched_ious=0.4913, loss_iou=0.0971, loss_iou_reg=0.2351, d_time=0.01(0.01), f_time=1.28(1.26), b_time=1.28(1.27)  Time cost: 21:47/15:19 [8:25:10/13:51:25]  Acc_iter 23900       Data time: 0.01(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.27)
2025-09-03 17:20:10,787   INFO  Train:   14/36 ( 39%) [1082/1759 ( 62%)]  Loss: 3.014 (3.68)  LR: 2.980e-03  Grad: 3.5302  max=0.3803(module.vfe.pfn_layers.0.linear.weight)  min: -0.2738(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.7457, loss_cls=0.1386, loss_bbox=0.8919, matched_ious=0.5016, loss_iou=0.0948, loss_iou_reg=0.2312, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 22:50/14:16 [8:26:13/13:50:15]  Acc_iter 23950       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-03 17:21:14,175   INFO  Train:   14/36 ( 39%) [1132/1759 ( 64%)]  Loss: 3.778 (3.68)  LR: 2.982e-03  Grad: 2.7673  max=0.8633(module.vfe.pfn_layers.0.linear.weight)  min: -0.2000(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7176, loss_cls=0.1355, loss_bbox=0.8101, matched_ious=0.4999, loss_iou=0.0964, loss_iou_reg=0.2348, d_time=0.00(0.01), f_time=1.33(1.26), b_time=1.33(1.27)  Time cost: 23:53/13:13 [8:27:16/13:49:17]  Acc_iter 24000       Data time: 0.00(0.01)  Forward time: 1.33(1.26)  Batch time: 1.33(1.27)
2025-09-03 17:22:17,062   INFO  Train:   14/36 ( 39%) [1182/1759 ( 67%)]  Loss: 3.724 (3.68)  LR: 2.983e-03  Grad: 5.9011  max=0.9830(module.vfe.pfn_layers.0.linear.weight)  min: -4.5956(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7162, loss_cls=0.1365, loss_bbox=0.8403, matched_ious=0.4944, loss_iou=0.0980, loss_iou_reg=0.2355, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.19(1.27)  Time cost: 24:56/12:09 [8:28:19/13:48:01]  Acc_iter 24050       Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.19(1.27)
2025-09-03 17:23:19,067   INFO  Train:   14/36 ( 39%) [1232/1759 ( 70%)]  Loss: 3.184 (3.68)  LR: 2.984e-03  Grad: 3.4534  max=1.3918(module.vfe.pfn_layers.0.linear.weight)  min: -0.3394(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7125, loss_cls=0.1315, loss_bbox=0.8184, matched_ious=0.5041, loss_iou=0.0970, loss_iou_reg=0.2310, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.28(1.26)  Time cost: 25:58/11:06 [8:29:21/13:46:18]  Acc_iter 24100       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.26)
2025-09-03 17:24:21,970   INFO  Train:   14/36 ( 39%) [1282/1759 ( 73%)]  Loss: 3.062 (3.68)  LR: 2.986e-03  Grad: 3.7067  max=0.8791(module.vfe.pfn_layers.0.linear.weight)  min: -1.2138(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7006, loss_cls=0.1298, loss_bbox=0.8013, matched_ious=0.4975, loss_iou=0.0957, loss_iou_reg=0.2347, d_time=0.00(0.01), f_time=1.33(1.26), b_time=1.33(1.26)  Time cost: 27:01/10:02 [8:30:24/13:45:06]  Acc_iter 24150       Data time: 0.00(0.01)  Forward time: 1.33(1.26)  Batch time: 1.33(1.26)
2025-09-03 17:25:25,928   INFO  Train:   14/36 ( 39%) [1332/1759 ( 76%)]  Loss: 3.904 (3.68)  LR: 2.987e-03  Grad: 4.1220  max=1.2218(module.vfe.pfn_layers.0.linear.weight)  min: -0.2794(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.7283, loss_cls=0.1395, loss_bbox=0.8239, matched_ious=0.4937, loss_iou=0.0973, loss_iou_reg=0.2372, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.20(1.26)  Time cost: 28:05/08:59 [8:31:28/13:44:25]  Acc_iter 24200       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.26)
2025-09-03 17:26:28,263   INFO  Train:   14/36 ( 39%) [1382/1759 ( 79%)]  Loss: 4.052 (3.69)  LR: 2.988e-03  Grad: 3.7860  max=0.8751(module.vfe.pfn_layers.0.linear.weight)  min: -0.7864(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7204, loss_cls=0.1355, loss_bbox=0.8055, matched_ious=0.4974, loss_iou=0.0945, loss_iou_reg=0.2332, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.21(1.26)  Time cost: 29:07/07:56 [8:32:30/13:42:57]  Acc_iter 24250       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.26)
2025-09-03 17:27:31,394   INFO  Train:   14/36 ( 39%) [1432/1759 ( 81%)]  Loss: 3.983 (3.69)  LR: 2.989e-03  Grad: 3.5779  max=0.4319(module.vfe.pfn_layers.0.linear.weight)  min: -0.4224(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7368, loss_cls=0.1401, loss_bbox=0.9005, matched_ious=0.5038, loss_iou=0.0952, loss_iou_reg=0.2305, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.27(1.26)  Time cost: 30:10/06:53 [8:33:33/13:41:53]  Acc_iter 24300       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.26)
2025-09-03 17:28:34,314   INFO  Train:   14/36 ( 39%) [1482/1759 ( 84%)]  Loss: 3.801 (3.69)  LR: 2.990e-03  Grad: 3.7679  max=0.3966(module.vfe.pfn_layers.0.linear.weight)  min: -0.4309(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6932, loss_cls=0.1286, loss_bbox=0.8115, matched_ious=0.4997, loss_iou=0.0940, loss_iou_reg=0.2359, d_time=0.01(0.01), f_time=1.32(1.26), b_time=1.33(1.26)  Time cost: 31:13/05:49 [8:34:36/13:40:43]  Acc_iter 24350       Data time: 0.01(0.01)  Forward time: 1.32(1.26)  Batch time: 1.33(1.26)
2025-09-03 17:29:36,814   INFO  Train:   14/36 ( 39%) [1532/1759 ( 87%)]  Loss: 4.615 (3.69)  LR: 2.991e-03  Grad: 4.0273  max=0.4687(module.vfe.pfn_layers.0.linear.weight)  min: -0.3730(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6952, loss_cls=0.1305, loss_bbox=0.7924, matched_ious=0.5044, loss_iou=0.0957, loss_iou_reg=0.2308, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.28(1.26)  Time cost: 32:16/04:46 [8:35:39/13:39:22]  Acc_iter 24400       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.26)
2025-09-03 17:30:39,788   INFO  Train:   14/36 ( 39%) [1582/1759 ( 90%)]  Loss: 3.582 (3.69)  LR: 2.992e-03  Grad: 4.3505  max=0.1908(module.vfe.pfn_layers.0.linear.weight)  min: -1.0537(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7169, loss_cls=0.1310, loss_bbox=0.8086, matched_ious=0.5054, loss_iou=0.0943, loss_iou_reg=0.2308, d_time=0.01(0.01), f_time=1.28(1.26), b_time=1.29(1.26)  Time cost: 33:19/03:43 [8:36:42/13:38:15]  Acc_iter 24450       Data time: 0.01(0.01)  Forward time: 1.28(1.26)  Batch time: 1.29(1.26)
2025-09-03 17:31:42,585   INFO  Train:   14/36 ( 39%) [1632/1759 ( 93%)]  Loss: 3.503 (3.69)  LR: 2.993e-03  Grad: 4.5786  max=0.9057(module.vfe.pfn_layers.0.linear.weight)  min: -0.7937(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7083, loss_cls=0.1317, loss_bbox=0.8197, matched_ious=0.5043, loss_iou=0.0954, loss_iou_reg=0.2311, d_time=0.02(0.01), f_time=1.34(1.26), b_time=1.36(1.26)  Time cost: 34:21/02:40 [8:37:45/13:37:03]  Acc_iter 24500       Data time: 0.02(0.01)  Forward time: 1.34(1.26)  Batch time: 1.36(1.26)
2025-09-03 17:32:45,279   INFO  Train:   14/36 ( 39%) [1682/1759 ( 96%)]  Loss: 3.756 (3.69)  LR: 2.994e-03  Grad: 4.7358  max=0.6911(module.vfe.pfn_layers.0.linear.weight)  min: -0.2147(module.dense_head.decoder.multihead_attn.in_proj_weight)  NaN: False  loss_hm=0.7047, loss_cls=0.1336, loss_bbox=0.7871, matched_ious=0.5051, loss_iou=0.0960, loss_iou_reg=0.2313, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.23(1.26)  Time cost: 35:24/01:37 [8:38:47/13:35:50]  Acc_iter 24550       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.26)
2025-09-03 17:33:48,270   INFO  Train:   14/36 ( 39%) [1732/1759 ( 98%)]  Loss: 3.583 (3.69)  LR: 2.994e-03  Grad: 3.9722  max=0.5598(module.vfe.pfn_layers.0.linear.weight)  min: -0.6910(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6967, loss_cls=0.1299, loss_bbox=0.7803, matched_ious=0.5075, loss_iou=0.0949, loss_iou_reg=0.2285, d_time=0.00(0.01), f_time=1.31(1.26), b_time=1.31(1.26)  Time cost: 36:27/00:34 [8:39:50/13:34:44]  Acc_iter 24600       Data time: 0.00(0.01)  Forward time: 1.31(1.26)  Batch time: 1.31(1.26)
2025-09-03 17:34:21,009   INFO  Train:   14/36 ( 39%) [1758/1759 (100%)]  Loss: 3.586 (3.68)  LR: 2.995e-03  Grad: 4.1505  max=0.3225(module.vfe.pfn_layers.0.linear.weight)  min: -0.4878(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6816, loss_cls=0.1244, loss_bbox=0.7765, matched_ious=0.5094, loss_iou=0.0977, loss_iou_reg=0.2331, d_time=0.00(0.01), f_time=0.70(1.26), b_time=0.70(1.26)  Time cost: 37:00/00:01 [8:40:23/13:34:09]  Acc_iter 24626       Data time: 0.00(0.01)  Forward time: 0.70(1.26)  Batch time: 0.70(1.26)

                                               [Aepochs:  39%|███▉      | 14/36 [8:40:23<13:35:06, 2223.02s/it]epochs:  39%|███▉      | 14/36 [8:40:23<13:35:06, 2223.03s/it]epochs:  39%|███▉      | 14/36 [8:40:24<13:35:06, 2223.03s/it]epochs:  39%|███▉      | 14/36 [8:40:24<13:35:06, 2223.04s/it]epochs:  39%|███▉      | 14/36 [8:40:24<13:35:06, 2223.03s/it]epochs:  39%|███▉      | 14/36 [8:40:24<13:35:06, 2223.03s/it]epochs:  39%|███▉      | 14/36 [8:40:24<13:35:06, 2223.03s/it]epochs:  39%|███▉      | 14/36 [8:40:24<13:35:06, 2223.02s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 17:34:26,770   INFO  Train:   15/36 ( 42%) [   0/1759 (  0%)]  Loss: 3.625 (3.62)  LR: 2.995e-03  Grad: 4.0945  max=0.3892(module.vfe.pfn_layers.0.linear.weight)  min: -0.4538(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6710, loss_cls=0.1334, loss_bbox=0.8203, matched_ious=0.4411, loss_iou=0.1071, loss_iou_reg=0.2593, d_time=1.54(1.54), f_time=2.87(2.87), b_time=4.42(4.42)  Time cost: 00:03/1:54:08 [8:40:29/41:51:02]  Acc_iter 24627       Data time: 1.54(1.54)  Forward time: 2.87(2.87)  Batch time: 4.42(4.42)
2025-09-03 17:34:55,729   INFO  Train:   15/36 ( 42%) [  23/1759 (  1%)]  Loss: 4.168 (3.79)  LR: 2.995e-03  Grad: 4.2989  max=0.9149(module.vfe.pfn_layers.0.linear.weight)  min: -0.4992(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7232, loss_cls=0.1324, loss_bbox=0.8521, matched_ious=0.5102, loss_iou=0.0954, loss_iou_reg=0.2238, d_time=0.01(0.07), f_time=1.40(1.32), b_time=1.41(1.39)  Time cost: 00:32/39:36 [8:40:58/14:42:21]  Acc_iter 24650       Data time: 0.01(0.07)  Forward time: 1.40(1.32)  Batch time: 1.41(1.39)
2025-09-03 17:35:59,961   INFO  Train:   15/36 ( 42%) [  73/1759 (  4%)]  Loss: 3.578 (3.73)  LR: 2.996e-03  Grad: 1.7451  max=0.2276(module.vfe.pfn_layers.0.linear.weight)  min: -0.5267(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7061, loss_cls=0.1304, loss_bbox=0.8221, matched_ious=0.5026, loss_iou=0.0953, loss_iou_reg=0.2305, d_time=0.01(0.03), f_time=1.30(1.29), b_time=1.31(1.32)  Time cost: 01:37/36:51 [8:42:02/14:04:34]  Acc_iter 24700       Data time: 0.01(0.03)  Forward time: 1.30(1.29)  Batch time: 1.31(1.32)
2025-09-03 17:37:02,604   INFO  Train:   15/36 ( 42%) [ 123/1759 (  7%)]  Loss: 3.847 (3.70)  LR: 2.997e-03  Grad: 2.4859  max=0.9459(module.vfe.pfn_layers.0.linear.weight)  min: -0.9919(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7062, loss_cls=0.1338, loss_bbox=0.8134, matched_ious=0.5123, loss_iou=0.0919, loss_iou_reg=0.2283, d_time=0.00(0.02), f_time=1.28(1.27), b_time=1.28(1.29)  Time cost: 02:39/35:07 [8:43:05/13:48:08]  Acc_iter 24750       Data time: 0.00(0.02)  Forward time: 1.28(1.27)  Batch time: 1.28(1.29)
2025-09-03 17:38:05,299   INFO  Train:   15/36 ( 42%) [ 173/1759 ( 10%)]  Loss: 3.390 (3.71)  LR: 2.997e-03  Grad: 2.2392  max=0.8154(module.vfe.pfn_layers.0.linear.weight)  min: -0.3226(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7164, loss_cls=0.1318, loss_bbox=0.8313, matched_ious=0.4956, loss_iou=0.0946, loss_iou_reg=0.2360, d_time=0.01(0.02), f_time=1.27(1.27), b_time=1.28(1.28)  Time cost: 03:42/33:47 [8:44:07/13:40:46]  Acc_iter 24800       Data time: 0.01(0.02)  Forward time: 1.27(1.27)  Batch time: 1.28(1.28)
2025-09-03 17:39:08,069   INFO  Train:   15/36 ( 42%) [ 223/1759 ( 13%)]  Loss: 3.462 (3.71)  LR: 2.998e-03  Grad: 2.5875  max=0.5135(module.vfe.pfn_layers.0.linear.weight)  min: -0.4612(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7074, loss_cls=0.1311, loss_bbox=0.8202, matched_ious=0.5061, loss_iou=0.0918, loss_iou_reg=0.2292, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.28(1.28)  Time cost: 04:45/32:35 [8:45:10/13:36:25]  Acc_iter 24850       Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.28(1.28)
2025-09-03 17:40:10,767   INFO  Train:   15/36 ( 42%) [ 273/1759 ( 16%)]  Loss: 3.097 (3.70)  LR: 2.998e-03  Grad: 2.7578  max=0.8904(module.vfe.pfn_layers.0.linear.weight)  min: -0.3064(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6899, loss_cls=0.1319, loss_bbox=0.8068, matched_ious=0.5020, loss_iou=0.0974, loss_iou_reg=0.2344, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 05:47/31:26 [8:46:13/13:33:06]  Acc_iter 24900       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 17:41:13,412   INFO  Train:   15/36 ( 42%) [ 323/1759 ( 18%)]  Loss: 4.261 (3.69)  LR: 2.999e-03  Grad: 3.1616  max=0.7371(module.vfe.pfn_layers.0.linear.weight)  min: -1.1570(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6906, loss_cls=0.1295, loss_bbox=0.7940, matched_ious=0.5093, loss_iou=0.0959, loss_iou_reg=0.2310, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.23(1.27)  Time cost: 06:50/30:19 [8:47:15/13:30:24]  Acc_iter 24950       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.27)
2025-09-03 17:42:15,757   INFO  Train:   15/36 ( 42%) [ 373/1759 ( 21%)]  Loss: 3.493 (3.70)  LR: 2.999e-03  Grad: 3.1435  max=0.2229(module.vfe.pfn_layers.0.linear.weight)  min: -0.5351(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7042, loss_cls=0.1316, loss_bbox=0.8604, matched_ious=0.5009, loss_iou=0.0951, loss_iou_reg=0.2315, d_time=0.00(0.01), f_time=1.32(1.26), b_time=1.32(1.27)  Time cost: 07:52/29:12 [8:48:18/13:27:37]  Acc_iter 25000       Data time: 0.00(0.01)  Forward time: 1.32(1.26)  Batch time: 1.32(1.27)
2025-09-03 17:43:18,813   INFO  Train:   15/36 ( 42%) [ 423/1759 ( 24%)]  Loss: 2.979 (3.69)  LR: 2.999e-03  Grad: 2.4184  max=0.5160(module.vfe.pfn_layers.0.linear.weight)  min: -0.2755(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.7191, loss_cls=0.1372, loss_bbox=0.7791, matched_ious=0.5050, loss_iou=0.0951, loss_iou_reg=0.2308, d_time=0.00(0.01), f_time=1.29(1.26), b_time=1.30(1.27)  Time cost: 08:55/28:08 [8:49:21/13:26:19]  Acc_iter 25050       Data time: 0.00(0.01)  Forward time: 1.29(1.26)  Batch time: 1.30(1.27)
2025-09-03 17:44:23,251   INFO  Train:   15/36 ( 42%) [ 473/1759 ( 27%)]  Loss: 3.387 (3.70)  LR: 2.999e-03  Grad: 2.5353  max=0.2174(module.vfe.pfn_layers.0.linear.weight)  min: -0.4836(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7334, loss_cls=0.1365, loss_bbox=0.8425, matched_ious=0.5075, loss_iou=0.0939, loss_iou_reg=0.2267, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.19(1.27)  Time cost: 10:00/27:08 [8:50:25/13:26:56]  Acc_iter 25100       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.27)
2025-09-03 17:45:26,679   INFO  Train:   15/36 ( 42%) [ 523/1759 ( 30%)]  Loss: 4.175 (3.70)  LR: 3.000e-03  Grad: 3.1214  max=0.2472(module.vfe.pfn_layers.0.linear.weight)  min: -1.6989(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7065, loss_cls=0.1342, loss_bbox=0.8056, matched_ious=0.5093, loss_iou=0.0942, loss_iou_reg=0.2282, d_time=0.00(0.01), f_time=1.30(1.26), b_time=1.30(1.27)  Time cost: 11:03/26:05 [8:51:29/13:25:58]  Acc_iter 25150       Data time: 0.00(0.01)  Forward time: 1.30(1.26)  Batch time: 1.30(1.27)
2025-09-03 17:46:29,749   INFO  Train:   15/36 ( 42%) [ 573/1759 ( 33%)]  Loss: 3.491 (3.69)  LR: 3.000e-03  Grad: 2.7197  max=0.2404(module.vfe.pfn_layers.0.linear.weight)  min: -0.9219(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6827, loss_cls=0.1268, loss_bbox=0.7681, matched_ious=0.5109, loss_iou=0.0957, loss_iou_reg=0.2282, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 12:06/25:01 [8:52:32/13:24:38]  Acc_iter 25200       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 17:47:32,664   INFO  Train:   15/36 ( 42%) [ 623/1759 ( 35%)]  Loss: 3.834 (3.67)  LR: 3.000e-03  Grad: 2.9065  max=0.5934(module.vfe.pfn_layers.0.linear.weight)  min: -0.3363(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6754, loss_cls=0.1302, loss_bbox=0.7473, matched_ious=0.5144, loss_iou=0.0966, loss_iou_reg=0.2283, d_time=0.02(0.01), f_time=1.26(1.26), b_time=1.28(1.27)  Time cost: 13:09/23:57 [8:53:35/13:23:09]  Acc_iter 25250       Data time: 0.02(0.01)  Forward time: 1.26(1.26)  Batch time: 1.28(1.27)
2025-09-03 17:48:36,051   INFO  Train:   15/36 ( 42%) [ 673/1759 ( 38%)]  Loss: 4.143 (3.67)  LR: 3.000e-03  Grad: 3.3897  max=1.3495(module.vfe.pfn_layers.0.linear.weight)  min: -0.5154(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7000, loss_cls=0.1298, loss_bbox=0.8346, matched_ious=0.5060, loss_iou=0.0947, loss_iou_reg=0.2309, d_time=0.00(0.01), f_time=1.32(1.26), b_time=1.32(1.27)  Time cost: 14:13/22:54 [8:54:38/13:22:13]  Acc_iter 25300       Data time: 0.00(0.01)  Forward time: 1.32(1.26)  Batch time: 1.32(1.27)
2025-09-03 17:49:39,073   INFO  Train:   15/36 ( 42%) [ 723/1759 ( 41%)]  Loss: 3.536 (3.67)  LR: 3.000e-03  Grad: 3.4919  max=0.1535(module.vfe.pfn_layers.0.linear.weight)  min: -1.9115(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6983, loss_cls=0.1316, loss_bbox=0.7698, matched_ious=0.5102, loss_iou=0.0942, loss_iou_reg=0.2283, d_time=0.00(0.01), f_time=1.32(1.26), b_time=1.33(1.27)  Time cost: 15:16/21:51 [8:55:41/13:20:56]  Acc_iter 25350       Data time: 0.00(0.01)  Forward time: 1.32(1.26)  Batch time: 1.33(1.27)
2025-09-03 17:50:42,014   INFO  Train:   15/36 ( 42%) [ 773/1759 ( 44%)]  Loss: 3.967 (3.67)  LR: 3.000e-03  Grad: 3.3950  max=0.7358(module.vfe.pfn_layers.0.linear.weight)  min: -0.5624(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7024, loss_cls=0.1328, loss_bbox=0.7966, matched_ious=0.5053, loss_iou=0.0947, loss_iou_reg=0.2311, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.19(1.27)  Time cost: 16:19/20:47 [8:56:44/13:19:36]  Acc_iter 25400       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.27)
2025-09-03 17:51:44,777   INFO  Train:   15/36 ( 42%) [ 823/1759 ( 47%)]  Loss: 3.247 (3.67)  LR: 3.000e-03  Grad: 3.8052  max=1.5687(module.vfe.pfn_layers.0.linear.weight)  min: -0.2531(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6924, loss_cls=0.1283, loss_bbox=0.7932, matched_ious=0.5084, loss_iou=0.0934, loss_iou_reg=0.2284, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.27(1.27)  Time cost: 17:21/19:43 [8:57:47/13:18:10]  Acc_iter 25450       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.27)
2025-09-03 17:52:47,354   INFO  Train:   15/36 ( 42%) [ 873/1759 ( 50%)]  Loss: 3.119 (3.67)  LR: 3.000e-03  Grad: 4.3647  max=0.2450(module.vfe.pfn_layers.0.linear.weight)  min: -2.1395(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6984, loss_cls=0.1305, loss_bbox=0.8228, matched_ious=0.5072, loss_iou=0.0950, loss_iou_reg=0.2296, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.26(1.26)  Time cost: 18:24/18:39 [8:58:49/13:16:39]  Acc_iter 25500       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.26)
2025-09-03 17:53:51,064   INFO  Train:   15/36 ( 42%) [ 923/1759 ( 52%)]  Loss: 3.019 (3.67)  LR: 3.000e-03  Grad: 2.8643  max=0.4811(module.vfe.pfn_layers.0.linear.weight)  min: -0.7400(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6992, loss_cls=0.1299, loss_bbox=0.8009, matched_ious=0.5104, loss_iou=0.0922, loss_iou_reg=0.2269, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.25(1.26)  Time cost: 19:28/17:36 [8:59:53/13:15:57]  Acc_iter 25550       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.26)
2025-09-03 17:54:54,098   INFO  Train:   15/36 ( 42%) [ 973/1759 ( 55%)]  Loss: 2.935 (3.67)  LR: 3.000e-03  Grad: 3.1004  max=0.3621(module.vfe.pfn_layers.0.linear.weight)  min: -0.5085(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7060, loss_cls=0.1297, loss_bbox=0.8114, matched_ious=0.5050, loss_iou=0.0952, loss_iou_reg=0.2343, d_time=0.00(0.01), f_time=1.36(1.26), b_time=1.36(1.26)  Time cost: 20:31/16:33 [9:00:56/13:14:47]  Acc_iter 25600       Data time: 0.00(0.01)  Forward time: 1.36(1.26)  Batch time: 1.36(1.26)
2025-09-03 17:55:56,940   INFO  Train:   15/36 ( 42%) [1023/1759 ( 58%)]  Loss: 3.471 (3.67)  LR: 2.999e-03  Grad: 3.3650  max=0.6990(module.vfe.pfn_layers.0.linear.weight)  min: -0.5032(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7030, loss_cls=0.1297, loss_bbox=0.8105, matched_ious=0.5096, loss_iou=0.0945, loss_iou_reg=0.2298, d_time=0.01(0.01), f_time=1.16(1.26), b_time=1.17(1.26)  Time cost: 21:34/15:30 [9:01:59/13:13:31]  Acc_iter 25650       Data time: 0.01(0.01)  Forward time: 1.16(1.26)  Batch time: 1.17(1.26)
2025-09-03 17:57:01,301   INFO  Train:   15/36 ( 42%) [1073/1759 ( 61%)]  Loss: 3.746 (3.67)  LR: 2.999e-03  Grad: 1.5977  max=0.5541(module.vfe.pfn_layers.0.linear.weight)  min: -0.1283(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7125, loss_cls=0.1335, loss_bbox=0.7964, matched_ious=0.5128, loss_iou=0.0948, loss_iou_reg=0.2271, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.26(1.27)  Time cost: 22:38/14:27 [9:03:03/13:13:09]  Acc_iter 25700       Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.27)
2025-09-03 17:58:03,730   INFO  Train:   15/36 ( 42%) [1123/1759 ( 64%)]  Loss: 3.482 (3.67)  LR: 2.999e-03  Grad: 2.1569  max=1.0244(module.vfe.pfn_layers.0.linear.weight)  min: -0.3133(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7025, loss_cls=0.1311, loss_bbox=0.8204, matched_ious=0.5027, loss_iou=0.0944, loss_iou_reg=0.2312, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.23(1.26)  Time cost: 23:40/13:23 [9:04:06/13:11:38]  Acc_iter 25750       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.26)
2025-09-03 17:59:06,772   INFO  Train:   15/36 ( 42%) [1173/1759 ( 67%)]  Loss: 4.551 (3.67)  LR: 2.999e-03  Grad: 2.0758  max=0.3689(module.vfe.pfn_layers.0.linear.weight)  min: -0.1929(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6919, loss_cls=0.1321, loss_bbox=0.8045, matched_ious=0.5085, loss_iou=0.0940, loss_iou_reg=0.2291, d_time=0.00(0.01), f_time=1.31(1.26), b_time=1.31(1.26)  Time cost: 24:43/12:20 [9:05:09/13:10:30]  Acc_iter 25800       Data time: 0.00(0.01)  Forward time: 1.31(1.26)  Batch time: 1.31(1.26)
2025-09-03 18:00:09,251   INFO  Train:   15/36 ( 42%) [1223/1759 ( 70%)]  Loss: 3.954 (3.66)  LR: 2.999e-03  Grad: 2.3823  max=0.8547(module.vfe.pfn_layers.0.linear.weight)  min: -0.1734(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.6886, loss_cls=0.1301, loss_bbox=0.7615, matched_ious=0.5027, loss_iou=0.0926, loss_iou_reg=0.2367, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.23(1.26)  Time cost: 25:46/11:17 [9:06:11/13:09:05]  Acc_iter 25850       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.26)
2025-09-03 18:01:12,105   INFO  Train:   15/36 ( 42%) [1273/1759 ( 72%)]  Loss: 3.454 (3.66)  LR: 2.998e-03  Grad: 2.0882  max=0.4518(module.vfe.pfn_layers.0.linear.weight)  min: -0.6357(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7206, loss_cls=0.1334, loss_bbox=0.8616, matched_ious=0.5030, loss_iou=0.0943, loss_iou_reg=0.2316, d_time=0.01(0.01), f_time=1.36(1.26), b_time=1.37(1.26)  Time cost: 26:49/10:13 [9:07:14/13:07:52]  Acc_iter 25900       Data time: 0.01(0.01)  Forward time: 1.36(1.26)  Batch time: 1.37(1.26)
2025-09-03 18:02:14,641   INFO  Train:   15/36 ( 42%) [1323/1759 ( 75%)]  Loss: 3.297 (3.66)  LR: 2.998e-03  Grad: 3.2352  max=2.3112(module.vfe.pfn_layers.0.linear.weight)  min: -0.2945(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.7007, loss_cls=0.1288, loss_bbox=0.8134, matched_ious=0.5125, loss_iou=0.0948, loss_iou_reg=0.2277, d_time=0.00(0.01), f_time=1.06(1.26), b_time=1.06(1.26)  Time cost: 27:51/09:10 [9:08:17/13:06:32]  Acc_iter 25950       Data time: 0.00(0.01)  Forward time: 1.06(1.26)  Batch time: 1.06(1.26)
2025-09-03 18:03:17,856   INFO  Train:   15/36 ( 42%) [1373/1759 ( 78%)]  Loss: 3.601 (3.66)  LR: 2.998e-03  Grad: 2.8275  max=0.2136(module.vfe.pfn_layers.0.linear.weight)  min: -1.3400(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6719, loss_cls=0.1254, loss_bbox=0.7553, matched_ious=0.5066, loss_iou=0.0950, loss_iou_reg=0.2305, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.19(1.26)  Time cost: 28:54/08:07 [9:09:20/13:05:31]  Acc_iter 26000       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.26)
2025-09-03 18:04:20,913   INFO  Train:   15/36 ( 42%) [1423/1759 ( 81%)]  Loss: 3.503 (3.65)  LR: 2.997e-03  Grad: 2.6852  max=0.6242(module.vfe.pfn_layers.0.linear.weight)  min: -0.3047(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6762, loss_cls=0.1290, loss_bbox=0.7476, matched_ious=0.5129, loss_iou=0.0931, loss_iou_reg=0.2280, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.26(1.26)  Time cost: 29:58/07:04 [9:10:23/13:04:25]  Acc_iter 26050       Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.26)
2025-09-03 18:05:24,697   INFO  Train:   15/36 ( 42%) [1473/1759 ( 84%)]  Loss: 3.178 (3.65)  LR: 2.997e-03  Grad: 3.0092  max=0.4988(module.vfe.pfn_layers.0.linear.weight)  min: -0.2625(module.dense_head.prediction_head.height.1.bias)  NaN: False  loss_hm=0.7126, loss_cls=0.1353, loss_bbox=0.7706, matched_ious=0.5024, loss_iou=0.0967, loss_iou_reg=0.2322, d_time=0.00(0.01), f_time=1.37(1.26), b_time=1.38(1.26)  Time cost: 31:01/06:01 [9:11:27/13:03:39]  Acc_iter 26100       Data time: 0.00(0.01)  Forward time: 1.37(1.26)  Batch time: 1.38(1.26)
2025-09-03 18:06:27,514   INFO  Train:   15/36 ( 42%) [1523/1759 ( 87%)]  Loss: 3.409 (3.65)  LR: 2.997e-03  Grad: 3.3283  max=0.1808(module.vfe.pfn_layers.0.linear.weight)  min: -1.1012(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6899, loss_cls=0.1286, loss_bbox=0.7223, matched_ious=0.5176, loss_iou=0.0925, loss_iou_reg=0.2261, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.20(1.26)  Time cost: 32:04/04:58 [9:12:30/13:02:27]  Acc_iter 26150       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.20(1.26)
2025-09-03 18:07:31,913   INFO  Train:   15/36 ( 42%) [1573/1759 ( 89%)]  Loss: 4.187 (3.65)  LR: 2.996e-03  Grad: 2.6286  max=0.6117(module.vfe.pfn_layers.0.linear.weight)  min: -0.5693(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6957, loss_cls=0.1260, loss_bbox=0.8008, matched_ious=0.5039, loss_iou=0.0958, loss_iou_reg=0.2319, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.25(1.26)  Time cost: 33:09/03:55 [9:13:34/13:01:54]  Acc_iter 26200       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.26)
2025-09-03 18:08:34,787   INFO  Train:   15/36 ( 42%) [1623/1759 ( 92%)]  Loss: 3.665 (3.65)  LR: 2.996e-03  Grad: 2.6362  max=0.4165(module.vfe.pfn_layers.0.linear.weight)  min: -0.4692(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6813, loss_cls=0.1269, loss_bbox=0.7199, matched_ious=0.5204, loss_iou=0.0918, loss_iou_reg=0.2258, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.28(1.26)  Time cost: 34:11/02:51 [9:14:37/13:00:43]  Acc_iter 26250       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.26)
2025-09-03 18:09:38,222   INFO  Train:   15/36 ( 42%) [1673/1759 ( 95%)]  Loss: 3.852 (3.65)  LR: 2.995e-03  Grad: 3.0856  max=0.5035(module.vfe.pfn_layers.0.linear.weight)  min: -0.2320(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6980, loss_cls=0.1298, loss_bbox=0.7594, matched_ious=0.5134, loss_iou=0.0937, loss_iou_reg=0.2271, d_time=0.01(0.01), f_time=1.36(1.26), b_time=1.36(1.26)  Time cost: 35:15/01:48 [9:15:40/12:59:46]  Acc_iter 26300       Data time: 0.01(0.01)  Forward time: 1.36(1.26)  Batch time: 1.36(1.26)
2025-09-03 18:10:40,691   INFO  Train:   15/36 ( 42%) [1723/1759 ( 98%)]  Loss: 2.764 (3.64)  LR: 2.995e-03  Grad: 1.8998  max=0.5597(module.vfe.pfn_layers.0.linear.weight)  min: -0.2971(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6669, loss_cls=0.1282, loss_bbox=0.7583, matched_ious=0.5043, loss_iou=0.0951, loss_iou_reg=0.2343, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.26)  Time cost: 36:17/00:45 [9:16:43/12:58:28]  Acc_iter 26350       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.26)
2025-09-03 18:11:23,824   INFO  Train:   15/36 ( 42%) [1758/1759 (100%)]  Loss: 3.488 (3.64)  LR: 2.994e-03  Grad: 2.2950  max=0.4237(module.vfe.pfn_layers.0.linear.weight)  min: -0.6891(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6647, loss_cls=0.1259, loss_bbox=0.7363, matched_ious=0.5054, loss_iou=0.0944, loss_iou_reg=0.2335, d_time=0.00(0.01), f_time=0.74(1.26), b_time=0.74(1.26)  Time cost: 37:00/00:01 [9:17:26/12:57:20]  Acc_iter 26385       Data time: 0.00(0.01)  Forward time: 0.74(1.26)  Batch time: 0.74(1.26)

                                               [Aepochs:  42%|████▏     | 15/36 [9:17:26<12:58:01, 2222.94s/it]epochs:  42%|████▏     | 15/36 [9:17:26<12:58:02, 2222.97s/it]epochs:  42%|████▏     | 15/36 [9:17:26<12:58:02, 2222.97s/it]epochs:  42%|████▏     | 15/36 [9:17:26<12:58:02, 2222.98s/it]epochs:  42%|████▏     | 15/36 [9:17:26<12:58:02, 2222.97s/it]epochs:  42%|████▏     | 15/36 [9:17:26<12:58:02, 2222.97s/it]epochs:  42%|████▏     | 15/36 [9:17:26<12:58:02, 2222.97s/it]epochs:  42%|████▏     | 15/36 [9:17:27<12:58:02, 2222.99s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 18:11:29,413   INFO  Train:   16/36 ( 44%) [   0/1759 (  0%)]  Loss: 3.303 (3.30)  LR: 2.994e-03  Grad: 3.8524  max=2.1895(module.vfe.pfn_layers.0.linear.weight)  min: -2.3601(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6863, loss_cls=0.1275, loss_bbox=0.5590, matched_ious=0.5196, loss_iou=0.1088, loss_iou_reg=0.2281, d_time=1.49(1.49), f_time=2.72(2.72), b_time=4.21(4.21)  Time cost: 00:03/1:45:52 [9:17:31/37:03:30]  Acc_iter 26386       Data time: 1.49(1.49)  Forward time: 2.72(2.72)  Batch time: 4.21(4.21)
2025-09-03 18:11:47,264   INFO  Train:   16/36 ( 44%) [  14/1759 (  1%)]  Loss: 3.368 (3.32)  LR: 2.994e-03  Grad: 2.1059  max=0.3462(module.vfe.pfn_layers.0.linear.weight)  min: -0.5679(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6518, loss_cls=0.1175, loss_bbox=0.7433, matched_ious=0.5221, loss_iou=0.0908, loss_iou_reg=0.2243, d_time=0.01(0.10), f_time=1.31(1.37), b_time=1.31(1.47)  Time cost: 00:21/41:36 [9:17:49/14:40:29]  Acc_iter 26400       Data time: 0.01(0.10)  Forward time: 1.31(1.37)  Batch time: 1.31(1.47)
2025-09-03 18:12:50,937   INFO  Train:   16/36 ( 44%) [  64/1759 (  4%)]  Loss: 3.713 (3.53)  LR: 2.994e-03  Grad: 2.4709  max=0.5553(module.vfe.pfn_layers.0.linear.weight)  min: -0.6211(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6947, loss_cls=0.1307, loss_bbox=0.7796, matched_ious=0.5073, loss_iou=0.0929, loss_iou_reg=0.2313, d_time=0.00(0.04), f_time=1.27(1.28), b_time=1.27(1.32)  Time cost: 01:25/37:00 [9:18:53/13:24:58]  Acc_iter 26450       Data time: 0.00(0.04)  Forward time: 1.27(1.28)  Batch time: 1.27(1.32)
2025-09-03 18:13:52,846   INFO  Train:   16/36 ( 44%) [ 114/1759 (  6%)]  Loss: 3.356 (3.57)  LR: 2.993e-03  Grad: 2.5524  max=0.4377(module.vfe.pfn_layers.0.linear.weight)  min: -0.6499(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6886, loss_cls=0.1281, loss_bbox=0.7996, matched_ious=0.5026, loss_iou=0.0958, loss_iou_reg=0.2330, d_time=0.00(0.02), f_time=1.29(1.26), b_time=1.29(1.28)  Time cost: 02:27/35:03 [9:19:55/13:04:46]  Acc_iter 26500       Data time: 0.00(0.02)  Forward time: 1.29(1.26)  Batch time: 1.29(1.28)
2025-09-03 18:14:55,329   INFO  Train:   16/36 ( 44%) [ 164/1759 (  9%)]  Loss: 3.443 (3.59)  LR: 2.992e-03  Grad: 2.6441  max=0.3173(module.vfe.pfn_layers.0.linear.weight)  min: -0.3592(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7040, loss_cls=0.1299, loss_bbox=0.7740, matched_ious=0.5119, loss_iou=0.0933, loss_iou_reg=0.2294, d_time=0.00(0.02), f_time=1.28(1.25), b_time=1.28(1.27)  Time cost: 03:29/33:45 [9:20:57/12:58:19]  Acc_iter 26550       Data time: 0.00(0.02)  Forward time: 1.28(1.25)  Batch time: 1.28(1.27)
2025-09-03 18:15:58,315   INFO  Train:   16/36 ( 44%) [ 214/1759 ( 12%)]  Loss: 3.275 (3.58)  LR: 2.992e-03  Grad: 3.1865  max=1.1865(module.vfe.pfn_layers.0.linear.weight)  min: -0.2327(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6687, loss_cls=0.1277, loss_bbox=0.7680, matched_ious=0.5106, loss_iou=0.0941, loss_iou_reg=0.2291, d_time=0.00(0.02), f_time=1.14(1.25), b_time=1.14(1.27)  Time cost: 04:32/32:38 [9:22:00/12:55:49]  Acc_iter 26600       Data time: 0.00(0.02)  Forward time: 1.14(1.25)  Batch time: 1.14(1.27)
2025-09-03 18:17:01,539   INFO  Train:   16/36 ( 44%) [ 264/1759 ( 15%)]  Loss: 3.343 (3.58)  LR: 2.991e-03  Grad: 2.3312  max=1.0456(module.vfe.pfn_layers.0.linear.weight)  min: -0.5076(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6857, loss_cls=0.1328, loss_bbox=0.7410, matched_ious=0.5121, loss_iou=0.0925, loss_iou_reg=0.2288, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.21(1.27)  Time cost: 05:35/31:34 [9:23:04/12:54:24]  Acc_iter 26650       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.21(1.27)
2025-09-03 18:18:04,721   INFO  Train:   16/36 ( 44%) [ 314/1759 ( 18%)]  Loss: 3.402 (3.58)  LR: 2.990e-03  Grad: 1.8241  max=0.6136(module.vfe.pfn_layers.0.linear.weight)  min: -0.6487(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6688, loss_cls=0.1246, loss_bbox=0.7748, matched_ious=0.5137, loss_iou=0.0936, loss_iou_reg=0.2263, d_time=0.00(0.01), f_time=1.44(1.26), b_time=1.45(1.27)  Time cost: 06:38/30:29 [9:24:07/12:53:02]  Acc_iter 26700       Data time: 0.00(0.01)  Forward time: 1.44(1.26)  Batch time: 1.45(1.27)
2025-09-03 18:19:07,592   INFO  Train:   16/36 ( 44%) [ 364/1759 ( 21%)]  Loss: 3.370 (3.58)  LR: 2.990e-03  Grad: 2.1026  max=0.6875(module.vfe.pfn_layers.0.linear.weight)  min: -0.4375(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6782, loss_cls=0.1281, loss_bbox=0.7920, matched_ious=0.5087, loss_iou=0.0941, loss_iou_reg=0.2301, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 07:41/29:24 [9:25:10/12:51:13]  Acc_iter 26750       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-03 18:20:10,665   INFO  Train:   16/36 ( 44%) [ 414/1759 ( 24%)]  Loss: 3.833 (3.58)  LR: 2.989e-03  Grad: 6.6173  max=3.6487(module.vfe.pfn_layers.0.linear.weight)  min: -4.4504(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7074, loss_cls=0.1258, loss_bbox=0.7817, matched_ious=0.5111, loss_iou=0.0951, loss_iou_reg=0.2275, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 08:44/28:21 [9:26:13/12:49:54]  Acc_iter 26800       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-03 18:21:13,216   INFO  Train:   16/36 ( 44%) [ 464/1759 ( 26%)]  Loss: 3.431 (3.59)  LR: 2.988e-03  Grad: 2.5169  max=1.0337(module.vfe.pfn_layers.0.linear.weight)  min: -0.1739(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6920, loss_cls=0.1296, loss_bbox=0.7627, matched_ious=0.5027, loss_iou=0.0952, loss_iou_reg=0.2330, d_time=0.01(0.01), f_time=1.22(1.25), b_time=1.23(1.26)  Time cost: 09:47/27:15 [9:27:15/12:47:57]  Acc_iter 26850       Data time: 0.01(0.01)  Forward time: 1.22(1.25)  Batch time: 1.23(1.26)
2025-09-03 18:22:16,826   INFO  Train:   16/36 ( 44%) [ 514/1759 ( 29%)]  Loss: 3.100 (3.58)  LR: 2.987e-03  Grad: 2.6146  max=0.6088(module.vfe.pfn_layers.0.linear.weight)  min: -0.4838(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6902, loss_cls=0.1309, loss_bbox=0.7546, matched_ious=0.5097, loss_iou=0.0941, loss_iou_reg=0.2290, d_time=0.00(0.01), f_time=2.14(1.26), b_time=2.14(1.27)  Time cost: 10:51/26:13 [9:28:19/12:47:25]  Acc_iter 26900       Data time: 0.00(0.01)  Forward time: 2.14(1.26)  Batch time: 2.14(1.27)
2025-09-03 18:23:19,435   INFO  Train:   16/36 ( 44%) [ 564/1759 ( 32%)]  Loss: 3.575 (3.58)  LR: 2.987e-03  Grad: 4.0244  max=0.5336(module.vfe.pfn_layers.0.linear.weight)  min: -1.9432(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7025, loss_cls=0.1311, loss_bbox=0.7658, matched_ious=0.5241, loss_iou=0.0941, loss_iou_reg=0.2227, d_time=0.00(0.01), f_time=1.24(1.25), b_time=1.25(1.26)  Time cost: 11:53/25:09 [9:29:21/12:45:44]  Acc_iter 26950       Data time: 0.00(0.01)  Forward time: 1.24(1.25)  Batch time: 1.25(1.26)
2025-09-03 18:24:22,580   INFO  Train:   16/36 ( 44%) [ 614/1759 ( 35%)]  Loss: 4.189 (3.58)  LR: 2.986e-03  Grad: 2.9946  max=0.4303(module.vfe.pfn_layers.0.linear.weight)  min: -0.5240(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6650, loss_cls=0.1212, loss_bbox=0.7266, matched_ious=0.5167, loss_iou=0.0944, loss_iou_reg=0.2280, d_time=0.00(0.01), f_time=1.38(1.26), b_time=1.39(1.26)  Time cost: 12:56/24:06 [9:30:25/12:44:40]  Acc_iter 27000       Data time: 0.00(0.01)  Forward time: 1.38(1.26)  Batch time: 1.39(1.26)
2025-09-03 18:25:25,602   INFO  Train:   16/36 ( 44%) [ 664/1759 ( 38%)]  Loss: 3.484 (3.58)  LR: 2.985e-03  Grad: 4.2690  max=0.7944(module.vfe.pfn_layers.0.linear.weight)  min: -2.8081(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6859, loss_cls=0.1232, loss_bbox=0.7692, matched_ious=0.5168, loss_iou=0.0951, loss_iou_reg=0.2259, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.27(1.26)  Time cost: 13:59/23:02 [9:31:28/12:43:30]  Acc_iter 27050       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.26)
2025-09-03 18:26:29,155   INFO  Train:   16/36 ( 44%) [ 714/1759 ( 41%)]  Loss: 3.771 (3.58)  LR: 2.984e-03  Grad: 3.3771  max=0.5270(module.vfe.pfn_layers.0.linear.weight)  min: -0.2772(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6686, loss_cls=0.1237, loss_bbox=0.7671, matched_ious=0.5159, loss_iou=0.0947, loss_iou_reg=0.2278, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.28(1.26)  Time cost: 15:03/22:00 [9:32:31/12:42:47]  Acc_iter 27100       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.26)
2025-09-03 18:27:30,894   INFO  Train:   16/36 ( 44%) [ 764/1759 ( 43%)]  Loss: 4.281 (3.58)  LR: 2.983e-03  Grad: 4.8715  max=1.1181(module.vfe.pfn_layers.0.linear.weight)  min: -2.9800(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6868, loss_cls=0.1275, loss_bbox=0.8084, matched_ious=0.5101, loss_iou=0.0949, loss_iou_reg=0.2282, d_time=0.00(0.01), f_time=1.19(1.25), b_time=1.19(1.26)  Time cost: 16:05/20:55 [9:33:33/12:40:36]  Acc_iter 27150       Data time: 0.00(0.01)  Forward time: 1.19(1.25)  Batch time: 1.19(1.26)
2025-09-03 18:28:33,082   INFO  Train:   16/36 ( 44%) [ 814/1759 ( 46%)]  Loss: 3.452 (3.58)  LR: 2.982e-03  Grad: 3.4833  max=0.2835(module.vfe.pfn_layers.0.linear.weight)  min: -0.6248(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6656, loss_cls=0.1212, loss_bbox=0.7708, matched_ious=0.5172, loss_iou=0.0915, loss_iou_reg=0.2280, d_time=0.00(0.01), f_time=1.23(1.25), b_time=1.24(1.26)  Time cost: 17:07/19:51 [9:34:35/12:38:54]  Acc_iter 27200       Data time: 0.00(0.01)  Forward time: 1.23(1.25)  Batch time: 1.24(1.26)
2025-09-03 18:29:36,209   INFO  Train:   16/36 ( 44%) [ 864/1759 ( 49%)]  Loss: 3.175 (3.57)  LR: 2.981e-03  Grad: 3.9161  max=1.3179(module.vfe.pfn_layers.0.linear.weight)  min: -0.4660(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6792, loss_cls=0.1259, loss_bbox=0.7545, matched_ious=0.5145, loss_iou=0.0933, loss_iou_reg=0.2261, d_time=0.00(0.01), f_time=1.28(1.25), b_time=1.28(1.26)  Time cost: 18:10/18:48 [9:35:38/12:37:55]  Acc_iter 27250       Data time: 0.00(0.01)  Forward time: 1.28(1.25)  Batch time: 1.28(1.26)
2025-09-03 18:30:39,012   INFO  Train:   16/36 ( 44%) [ 914/1759 ( 52%)]  Loss: 3.448 (3.57)  LR: 2.980e-03  Grad: 3.3154  max=0.4898(module.vfe.pfn_layers.0.linear.weight)  min: -0.2484(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6615, loss_cls=0.1231, loss_bbox=0.7457, matched_ious=0.5127, loss_iou=0.0940, loss_iou_reg=0.2281, d_time=0.00(0.01), f_time=1.20(1.25), b_time=1.20(1.26)  Time cost: 19:13/17:44 [9:36:41/12:36:43]  Acc_iter 27300       Data time: 0.00(0.01)  Forward time: 1.20(1.25)  Batch time: 1.20(1.26)
2025-09-03 18:31:42,408   INFO  Train:   16/36 ( 44%) [ 964/1759 ( 55%)]  Loss: 3.844 (3.58)  LR: 2.979e-03  Grad: 4.1369  max=1.3986(module.vfe.pfn_layers.0.linear.weight)  min: -1.0438(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7034, loss_cls=0.1267, loss_bbox=0.8221, matched_ious=0.5097, loss_iou=0.0951, loss_iou_reg=0.2279, d_time=0.00(0.01), f_time=1.32(1.25), b_time=1.32(1.26)  Time cost: 20:16/16:42 [9:37:44/12:35:54]  Acc_iter 27350       Data time: 0.00(0.01)  Forward time: 1.32(1.25)  Batch time: 1.32(1.26)
2025-09-03 18:32:47,386   INFO  Train:   16/36 ( 44%) [1014/1759 ( 58%)]  Loss: 3.215 (3.58)  LR: 2.978e-03  Grad: 3.5203  max=0.4873(module.vfe.pfn_layers.0.linear.weight)  min: -0.2977(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6753, loss_cls=0.1288, loss_bbox=0.7373, matched_ious=0.5125, loss_iou=0.0942, loss_iou_reg=0.2293, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.27(1.26)  Time cost: 21:21/15:40 [9:38:49/12:36:00]  Acc_iter 27400       Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.27(1.26)
2025-09-03 18:33:50,627   INFO  Train:   16/36 ( 44%) [1064/1759 ( 60%)]  Loss: 3.531 (3.58)  LR: 2.977e-03  Grad: 3.6757  max=0.4870(module.vfe.pfn_layers.0.linear.weight)  min: -0.1412(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6900, loss_cls=0.1287, loss_bbox=0.7738, matched_ious=0.5204, loss_iou=0.0934, loss_iou_reg=0.2234, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.23(1.26)  Time cost: 22:24/14:37 [9:39:53/12:35:01]  Acc_iter 27450       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.26)
2025-09-03 18:34:54,501   INFO  Train:   16/36 ( 44%) [1114/1759 ( 63%)]  Loss: 3.411 (3.57)  LR: 2.976e-03  Grad: 3.9101  max=0.2304(module.vfe.pfn_layers.0.linear.weight)  min: -0.3818(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6778, loss_cls=0.1246, loss_bbox=0.7529, matched_ious=0.5207, loss_iou=0.0941, loss_iou_reg=0.2250, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.29(1.26)  Time cost: 23:28/13:34 [9:40:57/12:34:21]  Acc_iter 27500       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.29(1.26)
2025-09-03 18:35:58,345   INFO  Train:   16/36 ( 44%) [1164/1759 ( 66%)]  Loss: 4.430 (3.57)  LR: 2.975e-03  Grad: 4.2606  max=0.7506(module.vfe.pfn_layers.0.linear.weight)  min: -0.8239(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6664, loss_cls=0.1282, loss_bbox=0.7536, matched_ious=0.5110, loss_iou=0.0969, loss_iou_reg=0.2301, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.23(1.26)  Time cost: 24:32/12:32 [9:42:00/12:33:39]  Acc_iter 27550       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.26)
2025-09-03 18:37:01,843   INFO  Train:   16/36 ( 44%) [1214/1759 ( 69%)]  Loss: 3.671 (3.57)  LR: 2.974e-03  Grad: 4.5066  max=0.8738(module.vfe.pfn_layers.0.linear.weight)  min: -0.6525(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6654, loss_cls=0.1249, loss_bbox=0.7503, matched_ious=0.5190, loss_iou=0.0944, loss_iou_reg=0.2264, d_time=0.00(0.01), f_time=1.30(1.26), b_time=1.31(1.26)  Time cost: 25:36/11:29 [9:43:04/12:32:44]  Acc_iter 27600       Data time: 0.00(0.01)  Forward time: 1.30(1.26)  Batch time: 1.31(1.26)
2025-09-03 18:38:05,109   INFO  Train:   16/36 ( 44%) [1264/1759 ( 72%)]  Loss: 4.332 (3.57)  LR: 2.972e-03  Grad: 4.5900  max=0.3110(module.vfe.pfn_layers.0.linear.weight)  min: -0.3311(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6767, loss_cls=0.1242, loss_bbox=0.7520, matched_ious=0.5176, loss_iou=0.0934, loss_iou_reg=0.2267, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.25(1.26)  Time cost: 26:39/10:25 [9:44:07/12:31:42]  Acc_iter 27650       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.26)
2025-09-03 18:39:07,821   INFO  Train:   16/36 ( 44%) [1314/1759 ( 75%)]  Loss: 3.794 (3.57)  LR: 2.971e-03  Grad: 2.8994  max=0.5898(module.vfe.pfn_layers.0.linear.weight)  min: -0.2003(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6849, loss_cls=0.1271, loss_bbox=0.7568, matched_ious=0.5116, loss_iou=0.0947, loss_iou_reg=0.2283, d_time=0.00(0.01), f_time=1.16(1.26), b_time=1.17(1.26)  Time cost: 27:42/09:22 [9:45:10/12:30:26]  Acc_iter 27700       Data time: 0.00(0.01)  Forward time: 1.16(1.26)  Batch time: 1.17(1.26)
2025-09-03 18:40:10,220   INFO  Train:   16/36 ( 44%) [1364/1759 ( 78%)]  Loss: 3.318 (3.57)  LR: 2.970e-03  Grad: 3.0809  max=0.3563(module.vfe.pfn_layers.0.linear.weight)  min: -0.2487(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6844, loss_cls=0.1275, loss_bbox=0.7693, matched_ious=0.5093, loss_iou=0.0944, loss_iou_reg=0.2301, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.22(1.26)  Time cost: 28:44/08:19 [9:46:12/12:29:02]  Acc_iter 27750       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.26)
2025-09-03 18:41:12,884   INFO  Train:   16/36 ( 44%) [1414/1759 ( 80%)]  Loss: 3.244 (3.56)  LR: 2.969e-03  Grad: 1.5884  max=0.3797(module.vfe.pfn_layers.0.linear.weight)  min: -0.3244(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6687, loss_cls=0.1269, loss_bbox=0.7537, matched_ious=0.5195, loss_iou=0.0925, loss_iou_reg=0.2255, d_time=0.00(0.01), f_time=1.15(1.26), b_time=1.16(1.26)  Time cost: 29:47/07:15 [9:47:15/12:27:46]  Acc_iter 27800       Data time: 0.00(0.01)  Forward time: 1.15(1.26)  Batch time: 1.16(1.26)
2025-09-03 18:42:16,790   INFO  Train:   16/36 ( 44%) [1464/1759 ( 83%)]  Loss: 4.365 (3.57)  LR: 2.968e-03  Grad: 2.2801  max=1.1443(module.vfe.pfn_layers.0.linear.weight)  min: -0.1561(module.backbone_3d.stage.blocks.2.output_norms.1.weight)  NaN: False  loss_hm=0.7196, loss_cls=0.1343, loss_bbox=0.8163, matched_ious=0.5122, loss_iou=0.0947, loss_iou_reg=0.2269, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.26(1.26)  Time cost: 30:50/06:12 [9:48:19/12:27:01]  Acc_iter 27850       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.26)
2025-09-03 18:43:18,986   INFO  Train:   16/36 ( 44%) [1514/1759 ( 86%)]  Loss: 3.932 (3.57)  LR: 2.966e-03  Grad: 2.2953  max=0.4868(module.vfe.pfn_layers.0.linear.weight)  min: -0.7062(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6957, loss_cls=0.1249, loss_bbox=0.7931, matched_ious=0.5127, loss_iou=0.0907, loss_iou_reg=0.2268, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.23(1.26)  Time cost: 31:53/05:09 [9:49:21/12:25:35]  Acc_iter 27900       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.26)
2025-09-03 18:44:21,170   INFO  Train:   16/36 ( 44%) [1564/1759 ( 89%)]  Loss: 3.823 (3.57)  LR: 2.965e-03  Grad: 2.2463  max=0.1449(module.vfe.pfn_layers.0.linear.weight)  min: -0.5018(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6601, loss_cls=0.1219, loss_bbox=0.7307, matched_ious=0.5129, loss_iou=0.0938, loss_iou_reg=0.2301, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.21(1.26)  Time cost: 32:55/04:06 [9:50:23/12:24:10]  Acc_iter 27950       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.26)
2025-09-03 18:45:23,378   INFO  Train:   16/36 ( 44%) [1614/1759 ( 92%)]  Loss: 3.784 (3.57)  LR: 2.964e-03  Grad: 2.4633  max=0.5050(module.vfe.pfn_layers.0.linear.weight)  min: -0.3416(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6764, loss_cls=0.1228, loss_bbox=0.7288, matched_ious=0.5103, loss_iou=0.0941, loss_iou_reg=0.2313, d_time=0.01(0.01), f_time=1.40(1.26), b_time=1.40(1.26)  Time cost: 33:57/03:02 [9:51:25/12:22:48]  Acc_iter 28000       Data time: 0.01(0.01)  Forward time: 1.40(1.26)  Batch time: 1.40(1.26)
2025-09-03 18:46:25,966   INFO  Train:   16/36 ( 44%) [1664/1759 ( 95%)]  Loss: 3.581 (3.57)  LR: 2.962e-03  Grad: 2.6640  max=0.3729(module.vfe.pfn_layers.0.linear.weight)  min: -0.4125(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6867, loss_cls=0.1208, loss_bbox=0.7966, matched_ious=0.5148, loss_iou=0.0958, loss_iou_reg=0.2260, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.25(1.26)  Time cost: 35:00/01:59 [9:52:28/12:21:34]  Acc_iter 28050       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.26)
2025-09-03 18:47:29,041   INFO  Train:   16/36 ( 44%) [1714/1759 ( 97%)]  Loss: 3.914 (3.57)  LR: 2.961e-03  Grad: 2.5153  max=0.1392(module.dense_head.prediction_head.iou.1.bias)  min: -0.4444(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6748, loss_cls=0.1284, loss_bbox=0.7390, matched_ious=0.5152, loss_iou=0.0932, loss_iou_reg=0.2280, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.22(1.26)  Time cost: 36:03/00:56 [9:53:31/12:20:31]  Acc_iter 28100       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.26)
2025-09-03 18:48:24,563   INFO  Train:   16/36 ( 44%) [1758/1759 (100%)]  Loss: 3.785 (3.57)  LR: 2.960e-03  Grad: 2.9886  max=0.6402(module.backbone_3d.cls_conv.3.bias)  min: -0.2244(module.backbone_3d.cls_conv.1.bias)  NaN: False  loss_hm=0.6778, loss_cls=0.1261, loss_bbox=0.7620, matched_ious=0.5142, loss_iou=0.0921, loss_iou_reg=0.2274, d_time=0.00(0.01), f_time=0.86(1.26), b_time=0.86(1.26)  Time cost: 36:58/00:01 [9:54:27/12:19:36]  Acc_iter 28144       Data time: 0.00(0.01)  Forward time: 0.86(1.26)  Batch time: 0.86(1.26)

                                               [Aepochs:  44%|████▍     | 16/36 [9:54:27<12:20:45, 2222.28s/it]epochs:  44%|████▍     | 16/36 [9:54:27<12:20:45, 2222.29s/it]epochs:  44%|████▍     | 16/36 [9:54:27<12:20:45, 2222.30s/it]epochs:  44%|████▍     | 16/36 [9:54:27<12:20:45, 2222.30s/it]epochs:  44%|████▍     | 16/36 [9:54:27<12:20:45, 2222.30s/it]epochs:  44%|████▍     | 16/36 [9:54:27<12:20:45, 2222.30s/it]epochs:  44%|████▍     | 16/36 [9:54:27<12:20:45, 2222.30s/it]epochs:  44%|████▍     | 16/36 [9:54:27<12:20:45, 2222.30s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 18:48:30,011   INFO  Train:   17/36 ( 47%) [   0/1759 (  0%)]  Loss: 3.118 (3.12)  LR: 2.960e-03  Grad: 2.7772  max=0.7165(module.vfe.pfn_layers.0.linear.weight)  min: -0.4777(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7397, loss_cls=0.1377, loss_bbox=0.5953, matched_ious=0.5503, loss_iou=0.0929, loss_iou_reg=0.2117, d_time=1.50(1.50), f_time=2.59(2.59), b_time=4.09(4.09)  Time cost: 00:03/1:40:57 [9:54:32/33:39:00]  Acc_iter 28145       Data time: 1.50(1.50)  Forward time: 2.59(2.59)  Batch time: 4.09(4.09)
2025-09-03 18:48:37,639   INFO  Train:   17/36 ( 47%) [   5/1759 (  0%)]  Loss: 4.551 (3.55)  LR: 2.959e-03  Grad: 2.6937  max=0.3520(module.vfe.pfn_layers.0.linear.weight)  min: -0.2465(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7035, loss_cls=0.1227, loss_bbox=0.9188, matched_ious=0.5074, loss_iou=0.1014, loss_iou_reg=0.2283, d_time=0.00(0.26), f_time=1.44(1.70), b_time=1.45(1.95)  Time cost: 00:11/53:56 [9:54:40/18:01:43]  Acc_iter 28150       Data time: 0.00(0.26)  Forward time: 1.44(1.70)  Batch time: 1.45(1.95)
2025-09-03 18:49:40,466   INFO  Train:   17/36 ( 47%) [  55/1759 (  3%)]  Loss: 3.168 (3.46)  LR: 2.958e-03  Grad: 2.5945  max=0.4442(module.vfe.pfn_layers.0.linear.weight)  min: -0.4090(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6444, loss_cls=0.1178, loss_bbox=0.7506, matched_ious=0.5214, loss_iou=0.0937, loss_iou_reg=0.2239, d_time=0.00(0.03), f_time=1.19(1.30), b_time=1.20(1.33)  Time cost: 01:13/37:28 [9:55:42/12:52:31]  Acc_iter 28200       Data time: 0.00(0.03)  Forward time: 1.19(1.30)  Batch time: 1.20(1.33)
2025-09-03 18:50:43,383   INFO  Train:   17/36 ( 47%) [ 105/1759 (  6%)]  Loss: 4.165 (3.52)  LR: 2.956e-03  Grad: 2.8079  max=0.4636(module.vfe.pfn_layers.0.linear.weight)  min: -0.3158(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6897, loss_cls=0.1272, loss_bbox=0.7713, matched_ious=0.5173, loss_iou=0.0935, loss_iou_reg=0.2247, d_time=0.01(0.02), f_time=1.35(1.28), b_time=1.35(1.30)  Time cost: 02:16/35:34 [9:56:45/12:34:31]  Acc_iter 28250       Data time: 0.01(0.02)  Forward time: 1.35(1.28)  Batch time: 1.35(1.30)
2025-09-03 18:51:46,882   INFO  Train:   17/36 ( 47%) [ 155/1759 (  9%)]  Loss: 3.731 (3.53)  LR: 2.955e-03  Grad: 3.1063  max=0.5602(module.vfe.pfn_layers.0.linear.weight)  min: -0.4332(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6641, loss_cls=0.1240, loss_bbox=0.7541, matched_ious=0.5211, loss_iou=0.0950, loss_iou_reg=0.2239, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.29(1.29)  Time cost: 03:20/34:19 [9:57:49/12:29:34]  Acc_iter 28300       Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.29(1.29)
2025-09-03 18:52:50,686   INFO  Train:   17/36 ( 47%) [ 205/1759 ( 12%)]  Loss: 3.416 (3.51)  LR: 2.953e-03  Grad: 3.3072  max=0.3897(module.vfe.pfn_layers.0.linear.weight)  min: -0.3172(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6378, loss_cls=0.1162, loss_bbox=0.7612, matched_ious=0.5208, loss_iou=0.0905, loss_iou_reg=0.2238, d_time=0.00(0.01), f_time=1.28(1.27), b_time=1.29(1.29)  Time cost: 04:24/33:12 [9:58:53/12:27:22]  Acc_iter 28350       Data time: 0.00(0.01)  Forward time: 1.28(1.27)  Batch time: 1.29(1.29)
2025-09-03 18:53:53,890   INFO  Train:   17/36 ( 47%) [ 255/1759 ( 14%)]  Loss: 3.513 (3.51)  LR: 2.952e-03  Grad: 4.4418  max=2.8277(module.vfe.pfn_layers.0.linear.weight)  min: -1.6830(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6415, loss_cls=0.1216, loss_bbox=0.7181, matched_ious=0.5201, loss_iou=0.0939, loss_iou_reg=0.2237, d_time=0.02(0.01), f_time=1.16(1.27), b_time=1.18(1.28)  Time cost: 05:27/32:03 [9:59:56/12:24:15]  Acc_iter 28400       Data time: 0.02(0.01)  Forward time: 1.16(1.27)  Batch time: 1.18(1.28)
2025-09-03 18:54:56,907   INFO  Train:   17/36 ( 47%) [ 305/1759 ( 17%)]  Loss: 3.747 (3.53)  LR: 2.950e-03  Grad: 1.9583  max=0.2101(module.vfe.pfn_layers.0.linear.weight)  min: -0.2900(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6814, loss_cls=0.1268, loss_bbox=0.7540, matched_ious=0.5132, loss_iou=0.0931, loss_iou_reg=0.2274, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.21(1.28)  Time cost: 06:30/30:54 [10:00:59/12:21:27]  Acc_iter 28450       Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.21(1.28)
2025-09-03 18:55:59,060   INFO  Train:   17/36 ( 47%) [ 355/1759 ( 20%)]  Loss: 4.134 (3.52)  LR: 2.949e-03  Grad: 2.9250  max=1.4870(module.vfe.pfn_layers.0.linear.weight)  min: -0.9335(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6565, loss_cls=0.1220, loss_bbox=0.7781, matched_ious=0.5181, loss_iou=0.0941, loss_iou_reg=0.2276, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 07:32/29:44 [10:02:01/12:17:44]  Acc_iter 28500       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-03 18:57:00,853   INFO  Train:   17/36 ( 47%) [ 405/1759 ( 23%)]  Loss: 4.048 (3.53)  LR: 2.947e-03  Grad: 2.4431  max=0.3321(module.vfe.pfn_layers.0.linear.weight)  min: -0.3534(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6797, loss_cls=0.1246, loss_bbox=0.7687, matched_ious=0.5130, loss_iou=0.0932, loss_iou_reg=0.2287, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.21(1.27)  Time cost: 08:34/28:35 [10:03:03/12:14:09]  Acc_iter 28550       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.27)
2025-09-03 18:58:03,379   INFO  Train:   17/36 ( 47%) [ 455/1759 ( 26%)]  Loss: 4.074 (3.53)  LR: 2.946e-03  Grad: 2.6994  max=0.4599(module.vfe.pfn_layers.0.linear.weight)  min: -0.5326(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6834, loss_cls=0.1272, loss_bbox=0.7183, matched_ious=0.5225, loss_iou=0.0909, loss_iou_reg=0.2247, d_time=0.00(0.01), f_time=1.30(1.26), b_time=1.31(1.27)  Time cost: 09:36/27:29 [10:04:05/12:12:04]  Acc_iter 28600       Data time: 0.00(0.01)  Forward time: 1.30(1.26)  Batch time: 1.31(1.27)
2025-09-03 18:59:06,860   INFO  Train:   17/36 ( 47%) [ 505/1759 ( 29%)]  Loss: 3.598 (3.53)  LR: 2.944e-03  Grad: 2.9677  max=0.4655(module.vfe.pfn_layers.0.linear.weight)  min: -0.7506(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7037, loss_cls=0.1303, loss_bbox=0.7510, matched_ious=0.5166, loss_iou=0.0972, loss_iou_reg=0.2262, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 10:40/26:26 [10:05:09/12:11:17]  Acc_iter 28650       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 19:00:09,892   INFO  Train:   17/36 ( 47%) [ 555/1759 ( 32%)]  Loss: 4.456 (3.54)  LR: 2.942e-03  Grad: 2.2100  max=0.3574(module.vfe.pfn_layers.0.linear.weight)  min: -0.6147(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6886, loss_cls=0.1289, loss_bbox=0.7486, matched_ious=0.5130, loss_iou=0.0925, loss_iou_reg=0.2304, d_time=0.01(0.01), f_time=1.20(1.26), b_time=1.21(1.27)  Time cost: 11:43/25:23 [10:06:12/12:09:59]  Acc_iter 28700       Data time: 0.01(0.01)  Forward time: 1.20(1.26)  Batch time: 1.21(1.27)
2025-09-03 19:01:13,252   INFO  Train:   17/36 ( 47%) [ 605/1759 ( 34%)]  Loss: 3.327 (3.54)  LR: 2.940e-03  Grad: 2.4764  max=0.8750(module.vfe.pfn_layers.0.linear.weight)  min: -0.2161(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6728, loss_cls=0.1241, loss_bbox=0.7633, matched_ious=0.5121, loss_iou=0.0934, loss_iou_reg=0.2300, d_time=0.01(0.01), f_time=1.17(1.26), b_time=1.18(1.27)  Time cost: 12:46/24:19 [10:07:15/12:09:02]  Acc_iter 28750       Data time: 0.01(0.01)  Forward time: 1.17(1.26)  Batch time: 1.18(1.27)
2025-09-03 19:02:15,835   INFO  Train:   17/36 ( 47%) [ 655/1759 ( 37%)]  Loss: 3.158 (3.53)  LR: 2.939e-03  Grad: 2.5762  max=0.2492(module.vfe.pfn_layers.0.linear.weight)  min: -0.3838(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6548, loss_cls=0.1223, loss_bbox=0.7183, matched_ious=0.5182, loss_iou=0.0953, loss_iou_reg=0.2273, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 13:49/23:15 [10:08:18/12:07:23]  Acc_iter 28800       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-03 19:03:18,822   INFO  Train:   17/36 ( 47%) [ 705/1759 ( 40%)]  Loss: 2.876 (3.53)  LR: 2.937e-03  Grad: 2.9364  max=0.4328(module.vfe.pfn_layers.0.linear.weight)  min: -0.7159(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6675, loss_cls=0.1255, loss_bbox=0.7950, matched_ious=0.5025, loss_iou=0.0945, loss_iou_reg=0.2312, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.23(1.26)  Time cost: 14:52/22:12 [10:09:21/12:06:10]  Acc_iter 28850       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.26)
2025-09-03 19:04:21,985   INFO  Train:   17/36 ( 47%) [ 755/1759 ( 43%)]  Loss: 3.129 (3.53)  LR: 2.935e-03  Grad: 2.9946  max=0.2250(module.vfe.pfn_layers.0.linear.weight)  min: -0.5528(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6633, loss_cls=0.1229, loss_bbox=0.7506, matched_ious=0.5158, loss_iou=0.0946, loss_iou_reg=0.2281, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.27(1.26)  Time cost: 15:55/21:08 [10:10:24/12:05:05]  Acc_iter 28900       Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.27(1.26)
2025-09-03 19:05:25,027   INFO  Train:   17/36 ( 47%) [ 805/1759 ( 46%)]  Loss: 3.557 (3.53)  LR: 2.933e-03  Grad: 3.2110  max=0.5938(module.vfe.pfn_layers.0.linear.weight)  min: -0.5060(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6695, loss_cls=0.1198, loss_bbox=0.7446, matched_ious=0.5147, loss_iou=0.0934, loss_iou_reg=0.2276, d_time=0.01(0.01), f_time=1.31(1.26), b_time=1.31(1.26)  Time cost: 16:58/20:05 [10:11:27/12:03:56]  Acc_iter 28950       Data time: 0.01(0.01)  Forward time: 1.31(1.26)  Batch time: 1.31(1.26)
2025-09-03 19:06:28,305   INFO  Train:   17/36 ( 47%) [ 855/1759 ( 49%)]  Loss: 3.758 (3.53)  LR: 2.931e-03  Grad: 3.3832  max=0.2883(module.vfe.pfn_layers.0.linear.weight)  min: -0.3882(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6904, loss_cls=0.1293, loss_bbox=0.7945, matched_ious=0.5121, loss_iou=0.0948, loss_iou_reg=0.2280, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.25(1.26)  Time cost: 18:01/19:02 [10:12:30/12:02:56]  Acc_iter 29000       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.26)
2025-09-03 19:07:31,002   INFO  Train:   17/36 ( 47%) [ 905/1759 ( 51%)]  Loss: 3.274 (3.54)  LR: 2.930e-03  Grad: 3.5841  max=0.3117(module.vfe.pfn_layers.0.linear.weight)  min: -0.2156(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6889, loss_cls=0.1227, loss_bbox=0.7994, matched_ious=0.5133, loss_iou=0.0930, loss_iou_reg=0.2264, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.26(1.26)  Time cost: 19:04/17:58 [10:13:33/12:01:35]  Acc_iter 29050       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.26)
2025-09-03 19:08:34,060   INFO  Train:   17/36 ( 47%) [ 955/1759 ( 54%)]  Loss: 3.590 (3.54)  LR: 2.928e-03  Grad: 4.1003  max=0.9541(module.vfe.pfn_layers.0.linear.weight)  min: -0.5608(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6799, loss_cls=0.1239, loss_bbox=0.7816, matched_ious=0.5203, loss_iou=0.0915, loss_iou_reg=0.2233, d_time=0.01(0.01), f_time=1.28(1.26), b_time=1.29(1.26)  Time cost: 20:07/16:55 [10:14:36/12:00:28]  Acc_iter 29100       Data time: 0.01(0.01)  Forward time: 1.28(1.26)  Batch time: 1.29(1.26)
2025-09-03 19:09:37,135   INFO  Train:   17/36 ( 47%) [1005/1759 ( 57%)]  Loss: 3.893 (3.54)  LR: 2.926e-03  Grad: 4.0598  max=0.5661(module.vfe.pfn_layers.0.linear.weight)  min: -0.3011(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6543, loss_cls=0.1256, loss_bbox=0.7431, matched_ious=0.5210, loss_iou=0.0922, loss_iou_reg=0.2240, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.28(1.26)  Time cost: 21:10/15:52 [10:15:39/11:59:22]  Acc_iter 29150       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.26)
2025-09-03 19:10:40,066   INFO  Train:   17/36 ( 47%) [1055/1759 ( 60%)]  Loss: 3.179 (3.54)  LR: 2.924e-03  Grad: 10.0000  max=4.6304(module.vfe.pfn_layers.0.linear.weight)  min: -7.9628(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6656, loss_cls=0.1248, loss_bbox=0.7721, matched_ious=0.5101, loss_iou=0.0941, loss_iou_reg=0.2292, d_time=0.00(0.01), f_time=1.14(1.26), b_time=1.14(1.26)  Time cost: 22:13/14:48 [10:16:42/11:58:12]  Acc_iter 29200       Data time: 0.00(0.01)  Forward time: 1.14(1.26)  Batch time: 1.14(1.26)
2025-09-03 19:11:42,726   INFO  Train:   17/36 ( 47%) [1105/1759 ( 63%)]  Loss: 3.675 (3.54)  LR: 2.922e-03  Grad: 4.1534  max=0.5376(module.vfe.pfn_layers.0.linear.weight)  min: -0.3679(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6794, loss_cls=0.1245, loss_bbox=0.7418, matched_ious=0.5204, loss_iou=0.0934, loss_iou_reg=0.2250, d_time=0.00(0.01), f_time=1.31(1.26), b_time=1.32(1.26)  Time cost: 23:16/13:45 [10:17:45/11:56:54]  Acc_iter 29250       Data time: 0.00(0.01)  Forward time: 1.31(1.26)  Batch time: 1.32(1.26)
2025-09-03 19:12:45,802   INFO  Train:   17/36 ( 47%) [1155/1759 ( 66%)]  Loss: 2.985 (3.54)  LR: 2.920e-03  Grad: 4.4413  max=0.8770(module.vfe.pfn_layers.0.linear.weight)  min: -0.2966(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6838, loss_cls=0.1241, loss_bbox=0.8123, matched_ious=0.5078, loss_iou=0.0948, loss_iou_reg=0.2281, d_time=0.00(0.01), f_time=1.29(1.26), b_time=1.30(1.26)  Time cost: 24:19/12:42 [10:18:48/11:55:50]  Acc_iter 29300       Data time: 0.00(0.01)  Forward time: 1.29(1.26)  Batch time: 1.30(1.26)
2025-09-03 19:13:48,827   INFO  Train:   17/36 ( 47%) [1205/1759 ( 69%)]  Loss: 3.950 (3.54)  LR: 2.918e-03  Grad: 4.6046  max=0.4893(module.vfe.pfn_layers.0.linear.weight)  min: -0.6126(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6617, loss_cls=0.1236, loss_bbox=0.7407, matched_ious=0.5145, loss_iou=0.0934, loss_iou_reg=0.2260, d_time=0.00(0.01), f_time=1.16(1.26), b_time=1.17(1.26)  Time cost: 25:22/11:39 [10:19:51/11:54:44]  Acc_iter 29350       Data time: 0.00(0.01)  Forward time: 1.16(1.26)  Batch time: 1.17(1.26)
2025-09-03 19:14:51,425   INFO  Train:   17/36 ( 47%) [1255/1759 ( 71%)]  Loss: 3.607 (3.54)  LR: 2.916e-03  Grad: 4.9737  max=1.4883(module.vfe.pfn_layers.0.linear.weight)  min: -0.1532(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6910, loss_cls=0.1292, loss_bbox=0.7774, matched_ious=0.5103, loss_iou=0.0910, loss_iou_reg=0.2287, d_time=0.00(0.01), f_time=1.29(1.26), b_time=1.29(1.26)  Time cost: 26:24/10:35 [10:20:53/11:53:27]  Acc_iter 29400       Data time: 0.00(0.01)  Forward time: 1.29(1.26)  Batch time: 1.29(1.26)
2025-09-03 19:15:54,034   INFO  Train:   17/36 ( 47%) [1305/1759 ( 74%)]  Loss: 3.723 (3.54)  LR: 2.914e-03  Grad: 4.8268  max=0.1960(module.vfe.pfn_layers.0.linear.weight)  min: -0.3422(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6654, loss_cls=0.1250, loss_bbox=0.7200, matched_ious=0.5167, loss_iou=0.0939, loss_iou_reg=0.2249, d_time=0.02(0.01), f_time=1.34(1.25), b_time=1.36(1.26)  Time cost: 27:27/09:32 [10:21:56/11:52:11]  Acc_iter 29450       Data time: 0.02(0.01)  Forward time: 1.34(1.25)  Batch time: 1.36(1.26)
2025-09-03 19:16:57,425   INFO  Train:   17/36 ( 47%) [1355/1759 ( 77%)]  Loss: 3.285 (3.54)  LR: 2.912e-03  Grad: 5.2074  max=1.0496(module.vfe.pfn_layers.0.linear.weight)  min: -0.2155(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6761, loss_cls=0.1239, loss_bbox=0.7990, matched_ious=0.5075, loss_iou=0.0926, loss_iou_reg=0.2305, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.27(1.26)  Time cost: 28:30/08:29 [10:22:59/11:51:16]  Acc_iter 29500       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.26)
2025-09-03 19:18:00,273   INFO  Train:   17/36 ( 47%) [1405/1759 ( 80%)]  Loss: 3.894 (3.54)  LR: 2.910e-03  Grad: 5.3532  max=0.4859(module.vfe.pfn_layers.0.linear.weight)  min: -0.7251(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6953, loss_cls=0.1257, loss_bbox=0.7933, matched_ious=0.5181, loss_iou=0.0936, loss_iou_reg=0.2232, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.20(1.26)  Time cost: 29:33/07:26 [10:24:02/11:50:08]  Acc_iter 29550       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.26)
2025-09-03 19:19:03,159   INFO  Train:   17/36 ( 47%) [1455/1759 ( 83%)]  Loss: 3.814 (3.54)  LR: 2.907e-03  Grad: 5.8975  max=1.2574(module.vfe.pfn_layers.0.linear.weight)  min: -1.5475(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6757, loss_cls=0.1214, loss_bbox=0.7862, matched_ious=0.5054, loss_iou=0.0959, loss_iou_reg=0.2317, d_time=0.01(0.01), f_time=1.42(1.26), b_time=1.42(1.26)  Time cost: 30:36/06:23 [10:25:05/11:49:00]  Acc_iter 29600       Data time: 0.01(0.01)  Forward time: 1.42(1.26)  Batch time: 1.42(1.26)
2025-09-03 19:20:06,800   INFO  Train:   17/36 ( 47%) [1505/1759 ( 86%)]  Loss: 3.944 (3.54)  LR: 2.905e-03  Grad: 6.4052  max=2.2560(module.vfe.pfn_layers.0.linear.weight)  min: -0.7454(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6674, loss_cls=0.1226, loss_bbox=0.8251, matched_ious=0.5130, loss_iou=0.0928, loss_iou_reg=0.2271, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.23(1.26)  Time cost: 31:40/05:20 [10:26:09/11:48:10]  Acc_iter 29650       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.26)
2025-09-03 19:21:10,623   INFO  Train:   17/36 ( 47%) [1555/1759 ( 88%)]  Loss: 3.618 (3.54)  LR: 2.903e-03  Grad: 3.5863  max=0.4367(module.vfe.pfn_layers.0.linear.weight)  min: -0.1587(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6751, loss_cls=0.1229, loss_bbox=0.7675, matched_ious=0.5144, loss_iou=0.0946, loss_iou_reg=0.2261, d_time=0.00(0.01), f_time=1.30(1.26), b_time=1.30(1.26)  Time cost: 32:44/04:17 [10:27:13/11:47:23]  Acc_iter 29700       Data time: 0.00(0.01)  Forward time: 1.30(1.26)  Batch time: 1.30(1.26)
2025-09-03 19:22:13,274   INFO  Train:   17/36 ( 47%) [1605/1759 ( 91%)]  Loss: 3.081 (3.54)  LR: 2.901e-03  Grad: 4.6024  max=1.7515(module.vfe.pfn_layers.0.linear.weight)  min: -1.6930(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6623, loss_cls=0.1219, loss_bbox=0.7683, matched_ious=0.5198, loss_iou=0.0919, loss_iou_reg=0.2245, d_time=0.00(0.01), f_time=1.34(1.26), b_time=1.35(1.26)  Time cost: 33:46/03:14 [10:28:15/11:46:10]  Acc_iter 29750       Data time: 0.00(0.01)  Forward time: 1.34(1.26)  Batch time: 1.35(1.26)
2025-09-03 19:23:15,898   INFO  Train:   17/36 ( 47%) [1655/1759 ( 94%)]  Loss: 3.311 (3.54)  LR: 2.899e-03  Grad: 4.1262  max=0.6289(module.vfe.pfn_layers.0.linear.weight)  min: -0.4658(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6536, loss_cls=0.1204, loss_bbox=0.7252, matched_ious=0.5143, loss_iou=0.0949, loss_iou_reg=0.2270, d_time=0.01(0.01), f_time=1.23(1.26), b_time=1.24(1.26)  Time cost: 34:49/02:11 [10:29:18/11:44:57]  Acc_iter 29800       Data time: 0.01(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.26)
2025-09-03 19:24:18,863   INFO  Train:   17/36 ( 47%) [1705/1759 ( 97%)]  Loss: 3.402 (3.54)  LR: 2.896e-03  Grad: 4.3501  max=0.6804(module.vfe.pfn_layers.0.linear.weight)  min: -0.4446(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6733, loss_cls=0.1265, loss_bbox=0.7558, matched_ious=0.5147, loss_iou=0.0948, loss_iou_reg=0.2273, d_time=0.00(0.01), f_time=1.16(1.26), b_time=1.16(1.26)  Time cost: 35:52/01:08 [10:30:21/11:43:52]  Acc_iter 29850       Data time: 0.00(0.01)  Forward time: 1.16(1.26)  Batch time: 1.16(1.26)
2025-09-03 19:25:21,694   INFO  Train:   17/36 ( 47%) [1755/1759 (100%)]  Loss: 3.170 (3.54)  LR: 2.894e-03  Grad: 4.5605  max=0.4929(module.vfe.pfn_layers.0.linear.weight)  min: -0.6361(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6603, loss_cls=0.1250, loss_bbox=0.7035, matched_ious=0.5205, loss_iou=0.0919, loss_iou_reg=0.2228, d_time=0.01(0.01), f_time=1.18(1.26), b_time=1.19(1.26)  Time cost: 36:55/00:05 [10:31:24/11:42:44]  Acc_iter 29900       Data time: 0.01(0.01)  Forward time: 1.18(1.26)  Batch time: 1.19(1.26)
2025-09-03 19:25:25,007   INFO  Train:   17/36 ( 47%) [1758/1759 (100%)]  Loss: 3.228 (3.54)  LR: 2.894e-03  Grad: 4.5717  max=0.5161(module.vfe.pfn_layers.0.linear.weight)  min: -0.4478(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7064, loss_cls=0.1369, loss_bbox=0.6813, matched_ious=0.5482, loss_iou=0.0891, loss_iou_reg=0.2122, d_time=0.03(0.01), f_time=0.72(1.25), b_time=0.75(1.26)  Time cost: 36:58/00:01 [10:31:27/11:42:31]  Acc_iter 29903       Data time: 0.03(0.01)  Forward time: 0.72(1.25)  Batch time: 0.75(1.26)

                                               [Aepochs:  47%|████▋     | 17/36 [10:31:27<11:43:32, 2221.72s/it]epochs:  47%|████▋     | 17/36 [10:31:27<11:43:32, 2221.74s/it]epochs:  47%|████▋     | 17/36 [10:31:28<11:43:33, 2221.75s/it]epochs:  47%|████▋     | 17/36 [10:31:28<11:43:33, 2221.76s/it]epochs:  47%|████▋     | 17/36 [10:31:28<11:43:33, 2221.75s/it]epochs:  47%|████▋     | 17/36 [10:31:28<11:43:33, 2221.75s/it]epochs:  47%|████▋     | 17/36 [10:31:28<11:43:33, 2221.75s/it]epochs:  47%|████▋     | 17/36 [10:31:28<11:43:33, 2221.77s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 19:25:30,473   INFO  Train:   18/36 ( 50%) [   0/1759 (  0%)]  Loss: 3.654 (3.65)  LR: 2.894e-03  Grad: 4.5481  max=0.3955(module.vfe.pfn_layers.0.linear.weight)  min: -0.4933(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7169, loss_cls=0.1318, loss_bbox=0.9110, matched_ious=0.5157, loss_iou=0.0852, loss_iou_reg=0.2159, d_time=1.31(1.31), f_time=2.74(2.74), b_time=4.05(4.05)  Time cost: 00:03/1:34:28 [10:31:32/29:55:09]  Acc_iter 29904       Data time: 1.31(1.31)  Forward time: 2.74(2.74)  Batch time: 4.05(4.05)
2025-09-03 19:26:28,232   INFO  Train:   18/36 ( 50%) [  46/1759 (  3%)]  Loss: 3.705 (3.49)  LR: 2.892e-03  Grad: 4.7080  max=0.3977(module.vfe.pfn_layers.0.linear.weight)  min: -0.3366(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6529, loss_cls=0.1204, loss_bbox=0.7505, matched_ious=0.5206, loss_iou=0.0915, loss_iou_reg=0.2234, d_time=0.00(0.03), f_time=1.14(1.28), b_time=1.15(1.32)  Time cost: 01:00/37:02 [10:32:30/12:01:43]  Acc_iter 29950       Data time: 0.00(0.03)  Forward time: 1.14(1.28)  Batch time: 1.15(1.32)
2025-09-03 19:27:31,174   INFO  Train:   18/36 ( 50%) [  96/1759 (  5%)]  Loss: 3.758 (3.47)  LR: 2.890e-03  Grad: 5.1219  max=1.2324(module.vfe.pfn_layers.0.linear.weight)  min: -0.5132(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6517, loss_cls=0.1244, loss_bbox=0.7469, matched_ious=0.5116, loss_iou=0.0952, loss_iou_reg=0.2280, d_time=0.00(0.02), f_time=1.27(1.27), b_time=1.28(1.29)  Time cost: 02:03/35:24 [10:33:33/11:49:34]  Acc_iter 30000       Data time: 0.00(0.02)  Forward time: 1.27(1.27)  Batch time: 1.28(1.29)
2025-09-03 19:28:34,119   INFO  Train:   18/36 ( 50%) [ 146/1759 (  8%)]  Loss: 3.913 (3.47)  LR: 2.887e-03  Grad: 3.8994  max=1.0361(module.vfe.pfn_layers.0.linear.weight)  min: -0.3445(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6568, loss_cls=0.1202, loss_bbox=0.7244, matched_ious=0.5237, loss_iou=0.0923, loss_iou_reg=0.2238, d_time=0.00(0.01), f_time=1.33(1.26), b_time=1.34(1.28)  Time cost: 03:06/34:10 [10:34:36/11:44:59]  Acc_iter 30050       Data time: 0.00(0.01)  Forward time: 1.33(1.26)  Batch time: 1.34(1.28)
2025-09-03 19:29:37,517   INFO  Train:   18/36 ( 50%) [ 196/1759 ( 11%)]  Loss: 3.321 (3.48)  LR: 2.885e-03  Grad: 4.2817  max=1.0727(module.vfe.pfn_layers.0.linear.weight)  min: -0.6107(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6720, loss_cls=0.1256, loss_bbox=0.7666, matched_ious=0.5098, loss_iou=0.0944, loss_iou_reg=0.2294, d_time=0.00(0.01), f_time=1.36(1.26), b_time=1.37(1.27)  Time cost: 04:10/33:05 [10:35:40/11:43:28]  Acc_iter 30100       Data time: 0.00(0.01)  Forward time: 1.36(1.26)  Batch time: 1.37(1.27)
2025-09-03 19:30:43,104   INFO  Train:   18/36 ( 50%) [ 246/1759 ( 14%)]  Loss: 3.044 (3.48)  LR: 2.882e-03  Grad: 1.7432  max=0.3156(module.vfe.pfn_layers.0.linear.weight)  min: -0.1905(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6507, loss_cls=0.1207, loss_bbox=0.7381, matched_ious=0.5217, loss_iou=0.0920, loss_iou_reg=0.2254, d_time=0.01(0.01), f_time=1.28(1.27), b_time=1.29(1.28)  Time cost: 05:15/32:14 [10:36:45/11:47:02]  Acc_iter 30150       Data time: 0.01(0.01)  Forward time: 1.28(1.27)  Batch time: 1.29(1.28)
2025-09-03 19:31:45,867   INFO  Train:   18/36 ( 50%) [ 296/1759 ( 17%)]  Loss: 3.852 (3.47)  LR: 2.880e-03  Grad: 2.2955  max=0.5090(module.vfe.pfn_layers.0.linear.weight)  min: -0.4401(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6416, loss_cls=0.1195, loss_bbox=0.7042, matched_ious=0.5210, loss_iou=0.0926, loss_iou_reg=0.2252, d_time=0.00(0.01), f_time=1.21(1.27), b_time=1.22(1.28)  Time cost: 06:18/31:05 [10:37:48/11:43:47]  Acc_iter 30200       Data time: 0.00(0.01)  Forward time: 1.21(1.27)  Batch time: 1.22(1.28)
2025-09-03 19:32:48,584   INFO  Train:   18/36 ( 50%) [ 346/1759 ( 20%)]  Loss: 3.545 (3.47)  LR: 2.878e-03  Grad: 2.6076  max=0.2896(module.vfe.pfn_layers.0.linear.weight)  min: -1.1379(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6539, loss_cls=0.1212, loss_bbox=0.7125, matched_ious=0.5151, loss_iou=0.0948, loss_iou_reg=0.2301, d_time=0.00(0.01), f_time=1.24(1.27), b_time=1.25(1.27)  Time cost: 07:21/29:57 [10:38:51/11:41:06]  Acc_iter 30250       Data time: 0.00(0.01)  Forward time: 1.24(1.27)  Batch time: 1.25(1.27)
2025-09-03 19:33:51,150   INFO  Train:   18/36 ( 50%) [ 396/1759 ( 23%)]  Loss: 3.584 (3.47)  LR: 2.875e-03  Grad: 2.5843  max=0.4053(module.vfe.pfn_layers.0.linear.weight)  min: -0.2326(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6574, loss_cls=0.1223, loss_bbox=0.7243, matched_ious=0.5176, loss_iou=0.0932, loss_iou_reg=0.2258, d_time=0.01(0.01), f_time=1.29(1.26), b_time=1.30(1.27)  Time cost: 08:23/28:50 [10:39:53/11:38:37]  Acc_iter 30300       Data time: 0.01(0.01)  Forward time: 1.29(1.26)  Batch time: 1.30(1.27)
2025-09-03 19:34:54,360   INFO  Train:   18/36 ( 50%) [ 446/1759 ( 25%)]  Loss: 3.757 (3.47)  LR: 2.873e-03  Grad: 2.7791  max=0.3609(module.vfe.pfn_layers.0.linear.weight)  min: -0.5843(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6668, loss_cls=0.1234, loss_bbox=0.7515, matched_ious=0.5170, loss_iou=0.0911, loss_iou_reg=0.2252, d_time=0.00(0.01), f_time=1.37(1.26), b_time=1.37(1.27)  Time cost: 09:27/27:45 [10:40:56/11:37:15]  Acc_iter 30350       Data time: 0.00(0.01)  Forward time: 1.37(1.26)  Batch time: 1.37(1.27)
2025-09-03 19:35:57,128   INFO  Train:   18/36 ( 50%) [ 496/1759 ( 28%)]  Loss: 3.884 (3.47)  LR: 2.870e-03  Grad: 2.9167  max=0.1979(module.vfe.pfn_layers.0.linear.weight)  min: -0.2221(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6542, loss_cls=0.1218, loss_bbox=0.7294, matched_ious=0.5181, loss_iou=0.0912, loss_iou_reg=0.2270, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.28(1.27)  Time cost: 10:29/26:40 [10:41:59/11:35:27]  Acc_iter 30400       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.27)
2025-09-03 19:37:00,969   INFO  Train:   18/36 ( 50%) [ 546/1759 ( 31%)]  Loss: 3.373 (3.47)  LR: 2.868e-03  Grad: 3.2172  max=0.4174(module.vfe.pfn_layers.0.linear.weight)  min: -0.4341(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6376, loss_cls=0.1207, loss_bbox=0.7167, matched_ious=0.5129, loss_iou=0.0945, loss_iou_reg=0.2303, d_time=0.00(0.01), f_time=1.32(1.26), b_time=1.32(1.27)  Time cost: 11:33/25:38 [10:43:03/11:34:52]  Acc_iter 30450       Data time: 0.00(0.01)  Forward time: 1.32(1.26)  Batch time: 1.32(1.27)
2025-09-03 19:38:03,342   INFO  Train:   18/36 ( 50%) [ 596/1759 ( 34%)]  Loss: 3.396 (3.48)  LR: 2.865e-03  Grad: 3.5249  max=0.7490(module.vfe.pfn_layers.0.linear.weight)  min: -0.5607(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6755, loss_cls=0.1229, loss_bbox=0.7361, matched_ious=0.5207, loss_iou=0.0923, loss_iou_reg=0.2254, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.23(1.27)  Time cost: 12:36/24:32 [10:44:05/11:32:52]  Acc_iter 30500       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.23(1.27)
2025-09-03 19:39:05,917   INFO  Train:   18/36 ( 50%) [ 646/1759 ( 37%)]  Loss: 3.983 (3.47)  LR: 2.862e-03  Grad: 3.0554  max=0.1557(module.vfe.pfn_layers.0.norm.weight)  min: -0.2702(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6464, loss_cls=0.1215, loss_bbox=0.7270, matched_ious=0.5108, loss_iou=0.0933, loss_iou_reg=0.2280, d_time=0.00(0.01), f_time=1.31(1.26), b_time=1.31(1.27)  Time cost: 13:38/23:28 [10:45:08/11:31:11]  Acc_iter 30550       Data time: 0.00(0.01)  Forward time: 1.31(1.26)  Batch time: 1.31(1.27)
2025-09-03 19:40:09,555   INFO  Train:   18/36 ( 50%) [ 696/1759 ( 40%)]  Loss: 3.012 (3.48)  LR: 2.860e-03  Grad: 3.4889  max=0.5621(module.vfe.pfn_layers.0.linear.weight)  min: -0.7613(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6620, loss_cls=0.1231, loss_bbox=0.7869, matched_ious=0.5183, loss_iou=0.0932, loss_iou_reg=0.2238, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.26(1.27)  Time cost: 14:42/22:25 [10:46:12/11:30:25]  Acc_iter 30600       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.27)
2025-09-03 19:41:14,246   INFO  Train:   18/36 ( 50%) [ 746/1759 ( 42%)]  Loss: 3.963 (3.49)  LR: 2.857e-03  Grad: 3.4837  max=0.3395(module.vfe.pfn_layers.0.linear.weight)  min: -0.2888(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6919, loss_cls=0.1273, loss_bbox=0.7938, matched_ious=0.5062, loss_iou=0.0950, loss_iou_reg=0.2325, d_time=0.00(0.01), f_time=1.31(1.26), b_time=1.32(1.27)  Time cost: 15:46/21:24 [10:47:16/11:30:23]  Acc_iter 30650       Data time: 0.00(0.01)  Forward time: 1.31(1.26)  Batch time: 1.32(1.27)
2025-09-03 19:42:16,841   INFO  Train:   18/36 ( 50%) [ 796/1759 ( 45%)]  Loss: 4.000 (3.48)  LR: 2.855e-03  Grad: 3.8799  max=0.2620(module.vfe.pfn_layers.0.linear.weight)  min: -1.1181(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6504, loss_cls=0.1192, loss_bbox=0.6883, matched_ious=0.5289, loss_iou=0.0892, loss_iou_reg=0.2216, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.19(1.27)  Time cost: 16:49/20:19 [10:48:19/11:28:47]  Acc_iter 30700       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.27)
2025-09-03 19:43:20,383   INFO  Train:   18/36 ( 50%) [ 846/1759 ( 48%)]  Loss: 3.395 (3.49)  LR: 2.852e-03  Grad: 3.9651  max=0.3274(module.vfe.pfn_layers.0.linear.weight)  min: -0.3231(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6795, loss_cls=0.1259, loss_bbox=0.7935, matched_ious=0.5158, loss_iou=0.0943, loss_iou_reg=0.2280, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.23(1.27)  Time cost: 17:53/19:16 [10:49:22/11:27:51]  Acc_iter 30750       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.27)
2025-09-03 19:44:23,016   INFO  Train:   18/36 ( 50%) [ 896/1759 ( 51%)]  Loss: 3.962 (3.49)  LR: 2.849e-03  Grad: 6.3728  max=3.3976(module.vfe.pfn_layers.0.linear.weight)  min: -3.2772(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6616, loss_cls=0.1202, loss_bbox=0.7511, matched_ious=0.5212, loss_iou=0.0936, loss_iou_reg=0.2266, d_time=0.00(0.01), f_time=1.34(1.26), b_time=1.34(1.27)  Time cost: 18:55/18:12 [10:50:25/11:26:22]  Acc_iter 30800       Data time: 0.00(0.01)  Forward time: 1.34(1.26)  Batch time: 1.34(1.27)
2025-09-03 19:45:24,727   INFO  Train:   18/36 ( 50%) [ 946/1759 ( 54%)]  Loss: 3.613 (3.49)  LR: 2.846e-03  Grad: 4.1790  max=0.2640(module.vfe.pfn_layers.0.linear.weight)  min: -0.5496(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6763, loss_cls=0.1237, loss_bbox=0.7627, matched_ious=0.5164, loss_iou=0.0942, loss_iou_reg=0.2269, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.23(1.27)  Time cost: 19:57/17:08 [10:51:27/11:24:24]  Acc_iter 30850       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.27)
2025-09-03 19:46:27,380   INFO  Train:   18/36 ( 50%) [ 996/1759 ( 57%)]  Loss: 3.939 (3.49)  LR: 2.844e-03  Grad: 4.4161  max=0.5620(module.vfe.pfn_layers.0.linear.weight)  min: -0.4100(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6438, loss_cls=0.1215, loss_bbox=0.7308, matched_ious=0.5134, loss_iou=0.0943, loss_iou_reg=0.2272, d_time=0.00(0.01), f_time=1.13(1.26), b_time=1.14(1.26)  Time cost: 21:00/16:04 [10:52:29/11:23:02]  Acc_iter 30900       Data time: 0.00(0.01)  Forward time: 1.13(1.26)  Batch time: 1.14(1.26)
2025-09-03 19:47:30,686   INFO  Train:   18/36 ( 50%) [1046/1759 ( 59%)]  Loss: 3.182 (3.49)  LR: 2.841e-03  Grad: 3.5399  max=0.4248(module.vfe.pfn_layers.0.linear.weight)  min: -0.5460(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6625, loss_cls=0.1181, loss_bbox=0.7951, matched_ious=0.5076, loss_iou=0.0944, loss_iou_reg=0.2293, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.27(1.26)  Time cost: 22:03/15:01 [10:53:33/11:22:02]  Acc_iter 30950       Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.27(1.26)
2025-09-03 19:48:32,811   INFO  Train:   18/36 ( 50%) [1096/1759 ( 62%)]  Loss: 2.878 (3.48)  LR: 2.838e-03  Grad: 3.7154  max=0.4812(module.vfe.pfn_layers.0.linear.weight)  min: -0.2196(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6307, loss_cls=0.1167, loss_bbox=0.7556, matched_ious=0.5155, loss_iou=0.0929, loss_iou_reg=0.2261, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.22(1.26)  Time cost: 23:05/13:57 [10:54:35/11:20:27]  Acc_iter 31000       Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.26)
2025-09-03 19:49:36,701   INFO  Train:   18/36 ( 50%) [1146/1759 ( 65%)]  Loss: 3.880 (3.48)  LR: 2.835e-03  Grad: 3.9230  max=0.2647(module.vfe.pfn_layers.0.linear.weight)  min: -0.3677(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6286, loss_cls=0.1188, loss_bbox=0.7018, matched_ious=0.5241, loss_iou=0.0925, loss_iou_reg=0.2260, d_time=0.00(0.01), f_time=1.34(1.26), b_time=1.34(1.26)  Time cost: 24:09/12:54 [10:55:39/11:19:45]  Acc_iter 31050       Data time: 0.00(0.01)  Forward time: 1.34(1.26)  Batch time: 1.34(1.26)
2025-09-03 19:50:39,967   INFO  Train:   18/36 ( 50%) [1196/1759 ( 68%)]  Loss: 3.374 (3.48)  LR: 2.833e-03  Grad: 4.2231  max=0.3433(module.vfe.pfn_layers.0.linear.weight)  min: -0.5194(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6652, loss_cls=0.1248, loss_bbox=0.7269, matched_ious=0.5182, loss_iou=0.0953, loss_iou_reg=0.2264, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.22(1.26)  Time cost: 25:12/11:51 [10:56:42/11:18:44]  Acc_iter 31100       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.26)
2025-09-03 19:51:44,427   INFO  Train:   18/36 ( 50%) [1246/1759 ( 71%)]  Loss: 3.672 (3.48)  LR: 2.830e-03  Grad: 4.3638  max=0.3415(module.vfe.pfn_layers.0.linear.weight)  min: -0.2814(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6658, loss_cls=0.1211, loss_bbox=0.7727, matched_ious=0.5085, loss_iou=0.0948, loss_iou_reg=0.2295, d_time=0.01(0.01), f_time=1.17(1.26), b_time=1.18(1.27)  Time cost: 26:17/10:48 [10:57:46/11:18:14]  Acc_iter 31150       Data time: 0.01(0.01)  Forward time: 1.17(1.26)  Batch time: 1.18(1.27)
2025-09-03 19:52:46,834   INFO  Train:   18/36 ( 50%) [1296/1759 ( 74%)]  Loss: 3.913 (3.48)  LR: 2.827e-03  Grad: 4.5819  max=0.1724(module.dense_head.decoder.self_attn.in_proj_weight)  min: -0.5228(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6522, loss_cls=0.1182, loss_bbox=0.7274, matched_ious=0.5223, loss_iou=0.0914, loss_iou_reg=0.2241, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.23(1.26)  Time cost: 27:19/09:45 [10:58:49/11:16:50]  Acc_iter 31200       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.26)
2025-09-03 19:53:49,270   INFO  Train:   18/36 ( 50%) [1346/1759 ( 77%)]  Loss: 3.804 (3.49)  LR: 2.824e-03  Grad: 7.0213  max=5.1463(module.vfe.pfn_layers.0.linear.weight)  min: -1.0490(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6558, loss_cls=0.1173, loss_bbox=0.7658, matched_ious=0.5215, loss_iou=0.0947, loss_iou_reg=0.2235, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.25(1.26)  Time cost: 28:22/08:41 [10:59:51/11:15:28]  Acc_iter 31250       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.26)
2025-09-03 19:54:51,606   INFO  Train:   18/36 ( 50%) [1396/1759 ( 79%)]  Loss: 3.840 (3.49)  LR: 2.821e-03  Grad: 4.5588  max=0.4510(module.vfe.pfn_layers.0.linear.weight)  min: -0.4196(module.dense_head.heatmap_head.0.bn.bias)  NaN: False  loss_hm=0.6686, loss_cls=0.1245, loss_bbox=0.7191, matched_ious=0.5193, loss_iou=0.0926, loss_iou_reg=0.2249, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.28(1.26)  Time cost: 29:24/07:38 [11:00:54/11:14:06]  Acc_iter 31300       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.26)
2025-09-03 19:55:55,547   INFO  Train:   18/36 ( 50%) [1446/1759 ( 82%)]  Loss: 3.455 (3.48)  LR: 2.818e-03  Grad: 4.6753  max=0.7581(module.vfe.pfn_layers.0.linear.weight)  min: -0.1972(module.dense_head.prediction_head.dim.1.bias)  NaN: False  loss_hm=0.6319, loss_cls=0.1233, loss_bbox=0.7029, matched_ious=0.5167, loss_iou=0.0931, loss_iou_reg=0.2292, d_time=0.01(0.01), f_time=1.30(1.26), b_time=1.30(1.26)  Time cost: 30:28/06:35 [11:01:58/11:13:20]  Acc_iter 31350       Data time: 0.01(0.01)  Forward time: 1.30(1.26)  Batch time: 1.30(1.26)
2025-09-03 19:56:58,138   INFO  Train:   18/36 ( 50%) [1496/1759 ( 85%)]  Loss: 3.389 (3.48)  LR: 2.815e-03  Grad: 5.0073  max=0.6752(module.vfe.pfn_layers.0.linear.weight)  min: -1.2846(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6591, loss_cls=0.1230, loss_bbox=0.7149, matched_ious=0.5231, loss_iou=0.0931, loss_iou_reg=0.2240, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.25(1.26)  Time cost: 31:30/05:32 [11:03:00/11:12:05]  Acc_iter 31400       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.26)
2025-09-03 19:58:00,146   INFO  Train:   18/36 ( 50%) [1546/1759 ( 88%)]  Loss: 4.045 (3.48)  LR: 2.812e-03  Grad: 3.4150  max=0.5623(module.vfe.pfn_layers.0.linear.weight)  min: -0.2584(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6501, loss_cls=0.1189, loss_bbox=0.7274, matched_ious=0.5281, loss_iou=0.0903, loss_iou_reg=0.2213, d_time=0.01(0.01), f_time=1.24(1.26), b_time=1.26(1.26)  Time cost: 32:32/04:28 [11:04:02/11:10:38]  Acc_iter 31450       Data time: 0.01(0.01)  Forward time: 1.24(1.26)  Batch time: 1.26(1.26)
2025-09-03 19:59:03,480   INFO  Train:   18/36 ( 50%) [1596/1759 ( 91%)]  Loss: 3.795 (3.48)  LR: 2.809e-03  Grad: 3.6469  max=0.2692(module.vfe.pfn_layers.0.linear.weight)  min: -0.4895(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6557, loss_cls=0.1206, loss_bbox=0.7579, matched_ious=0.5235, loss_iou=0.0928, loss_iou_reg=0.2223, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.22(1.26)  Time cost: 33:36/03:25 [11:05:05/11:09:39]  Acc_iter 31500       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.26)
2025-09-03 20:00:06,612   INFO  Train:   18/36 ( 50%) [1646/1759 ( 94%)]  Loss: 2.929 (3.48)  LR: 2.806e-03  Grad: 3.8441  max=0.5950(module.vfe.pfn_layers.0.linear.weight)  min: -0.1352(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6448, loss_cls=0.1194, loss_bbox=0.7478, matched_ious=0.5233, loss_iou=0.0926, loss_iou_reg=0.2235, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.27(1.26)  Time cost: 34:39/02:22 [11:06:09/11:08:36]  Acc_iter 31550       Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.27(1.26)
2025-09-03 20:01:09,436   INFO  Train:   18/36 ( 50%) [1696/1759 ( 96%)]  Loss: 3.435 (3.48)  LR: 2.803e-03  Grad: 5.0713  max=0.5074(module.vfe.pfn_layers.0.linear.weight)  min: -3.0705(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6361, loss_cls=0.1156, loss_bbox=0.7067, matched_ious=0.5231, loss_iou=0.0918, loss_iou_reg=0.2236, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.22(1.26)  Time cost: 35:42/01:19 [11:07:11/11:07:27]  Acc_iter 31600       Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.26)
2025-09-03 20:02:13,264   INFO  Train:   18/36 ( 50%) [1746/1759 ( 99%)]  Loss: 2.988 (3.48)  LR: 2.800e-03  Grad: 4.2289  max=0.2957(module.vfe.pfn_layers.0.linear.weight)  min: -0.2555(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6803, loss_cls=0.1239, loss_bbox=0.7103, matched_ious=0.5198, loss_iou=0.0917, loss_iou_reg=0.2256, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.19(1.26)  Time cost: 36:46/00:16 [11:08:15/11:06:37]  Acc_iter 31650       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.26)
2025-09-03 20:02:27,756   INFO  Train:   18/36 ( 50%) [1758/1759 (100%)]  Loss: 3.187 (3.48)  LR: 2.799e-03  Grad: 4.3485  max=0.3187(module.vfe.pfn_layers.0.linear.weight)  min: -0.3681(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6372, loss_cls=0.1237, loss_bbox=0.6437, matched_ious=0.5251, loss_iou=0.0967, loss_iou_reg=0.2240, d_time=0.00(0.01), f_time=0.70(1.26), b_time=0.70(1.26)  Time cost: 37:00/00:01 [11:08:30/11:06:10]  Acc_iter 31662       Data time: 0.00(0.01)  Forward time: 0.70(1.26)  Batch time: 0.70(1.26)

                                               [Aepochs:  50%|█████     | 18/36 [11:08:30<11:06:36, 2222.03s/it]epochs:  50%|█████     | 18/36 [11:08:30<11:06:36, 2222.04s/it]epochs:  50%|█████     | 18/36 [11:08:30<11:06:37, 2222.06s/it]epochs:  50%|█████     | 18/36 [11:08:30<11:06:36, 2222.05s/it]epochs:  50%|█████     | 18/36 [11:08:30<11:06:36, 2222.04s/it]epochs:  50%|█████     | 18/36 [11:08:30<11:06:36, 2222.04s/it]epochs:  50%|█████     | 18/36 [11:08:30<11:06:36, 2222.04s/it]epochs:  50%|█████     | 18/36 [11:08:31<11:06:36, 2222.03s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 20:02:33,518   INFO  Train:   19/36 ( 53%) [   0/1759 (  0%)]  Loss: 3.687 (3.69)  LR: 2.799e-03  Grad: 4.3727  max=0.3587(module.vfe.pfn_layers.0.linear.weight)  min: -0.7378(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6305, loss_cls=0.1417, loss_bbox=0.6996, matched_ious=0.4848, loss_iou=0.0858, loss_iou_reg=0.2364, d_time=1.54(1.54), f_time=2.87(2.87), b_time=4.42(4.42)  Time cost: 00:04/1:58:04 [11:08:36/35:25:17]  Acc_iter 31663       Data time: 1.54(1.54)  Forward time: 2.87(2.87)  Batch time: 4.42(4.42)
2025-09-03 20:03:19,979   INFO  Train:   19/36 ( 53%) [  37/1759 (  2%)]  Loss: 2.939 (3.46)  LR: 2.797e-03  Grad: 4.4292  max=0.4182(module.vfe.pfn_layers.0.linear.weight)  min: -0.3828(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6609, loss_cls=0.1204, loss_bbox=0.7318, matched_ious=0.5280, loss_iou=0.0923, loss_iou_reg=0.2198, d_time=0.00(0.05), f_time=1.19(1.29), b_time=1.19(1.34)  Time cost: 00:50/38:07 [11:09:22/11:40:17]  Acc_iter 31700       Data time: 0.00(0.05)  Forward time: 1.19(1.29)  Batch time: 1.19(1.34)
2025-09-03 20:04:22,963   INFO  Train:   19/36 ( 53%) [  87/1759 (  5%)]  Loss: 3.373 (3.50)  LR: 2.794e-03  Grad: 4.7395  max=0.3149(module.vfe.pfn_layers.0.linear.weight)  min: -0.4230(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6680, loss_cls=0.1234, loss_bbox=0.7672, matched_ious=0.5246, loss_iou=0.0915, loss_iou_reg=0.2235, d_time=0.00(0.02), f_time=1.29(1.27), b_time=1.29(1.29)  Time cost: 01:53/35:55 [11:10:25/11:18:34]  Acc_iter 31750       Data time: 0.00(0.02)  Forward time: 1.29(1.27)  Batch time: 1.29(1.29)
2025-09-03 20:05:25,573   INFO  Train:   19/36 ( 53%) [ 137/1759 (  8%)]  Loss: 3.849 (3.48)  LR: 2.790e-03  Grad: 4.8777  max=0.2382(module.dense_head.prediction_head.height.1.bias)  min: -0.3331(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6446, loss_cls=0.1197, loss_bbox=0.7429, matched_ious=0.5166, loss_iou=0.0914, loss_iou_reg=0.2279, d_time=0.00(0.02), f_time=1.22(1.26), b_time=1.22(1.28)  Time cost: 02:56/34:29 [11:11:28/11:10:24]  Acc_iter 31800       Data time: 0.00(0.02)  Forward time: 1.22(1.26)  Batch time: 1.22(1.28)
2025-09-03 20:06:28,196   INFO  Train:   19/36 ( 53%) [ 187/1759 ( 11%)]  Loss: 3.432 (3.49)  LR: 2.787e-03  Grad: 4.1824  max=1.7990(module.vfe.pfn_layers.0.linear.weight)  min: -0.4539(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6499, loss_cls=0.1210, loss_bbox=0.7625, matched_ious=0.5206, loss_iou=0.0915, loss_iou_reg=0.2237, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.18(1.27)  Time cost: 03:58/33:15 [11:12:30/11:06:03]  Acc_iter 31850       Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.18(1.27)
2025-09-03 20:07:31,732   INFO  Train:   19/36 ( 53%) [ 237/1759 ( 13%)]  Loss: 3.176 (3.48)  LR: 2.784e-03  Grad: 4.0273  max=0.2779(module.vfe.pfn_layers.0.linear.weight)  min: -0.4150(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6518, loss_cls=0.1185, loss_bbox=0.7283, matched_ious=0.5304, loss_iou=0.0941, loss_iou_reg=0.2202, d_time=0.04(0.01), f_time=1.55(1.26), b_time=1.59(1.27)  Time cost: 05:02/32:12 [11:13:34/11:05:06]  Acc_iter 31900       Data time: 0.04(0.01)  Forward time: 1.55(1.26)  Batch time: 1.59(1.27)
2025-09-03 20:08:35,439   INFO  Train:   19/36 ( 53%) [ 287/1759 ( 16%)]  Loss: 2.925 (3.47)  LR: 2.781e-03  Grad: 4.1948  max=0.3095(module.vfe.pfn_layers.0.linear.weight)  min: -0.5044(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6505, loss_cls=0.1187, loss_bbox=0.7350, matched_ious=0.5187, loss_iou=0.0915, loss_iou_reg=0.2245, d_time=0.01(0.01), f_time=1.21(1.26), b_time=1.22(1.27)  Time cost: 06:05/31:10 [11:14:37/11:04:26]  Acc_iter 31950       Data time: 0.01(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.27)
2025-09-03 20:09:38,549   INFO  Train:   19/36 ( 53%) [ 337/1759 ( 19%)]  Loss: 3.329 (3.48)  LR: 2.778e-03  Grad: 4.7110  max=0.7751(module.vfe.pfn_layers.0.linear.weight)  min: -1.5553(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6591, loss_cls=0.1220, loss_bbox=0.7575, matched_ious=0.5158, loss_iou=0.0935, loss_iou_reg=0.2281, d_time=0.00(0.01), f_time=1.16(1.26), b_time=1.17(1.27)  Time cost: 07:09/30:05 [11:15:41/11:02:44]  Acc_iter 32000       Data time: 0.00(0.01)  Forward time: 1.16(1.26)  Batch time: 1.17(1.27)
2025-09-03 20:10:41,620   INFO  Train:   19/36 ( 53%) [ 387/1759 ( 22%)]  Loss: 3.533 (3.48)  LR: 2.774e-03  Grad: 4.7007  max=0.7172(module.vfe.pfn_layers.0.linear.weight)  min: -0.6066(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6298, loss_cls=0.1204, loss_bbox=0.7352, matched_ious=0.5218, loss_iou=0.0932, loss_iou_reg=0.2265, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.20(1.27)  Time cost: 08:12/29:00 [11:16:44/11:01:08]  Acc_iter 32050       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.20(1.27)
2025-09-03 20:11:44,628   INFO  Train:   19/36 ( 53%) [ 437/1759 ( 25%)]  Loss: 3.905 (3.47)  LR: 2.771e-03  Grad: 4.9245  max=0.3086(module.vfe.pfn_layers.0.linear.weight)  min: -0.8019(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6320, loss_cls=0.1189, loss_bbox=0.7099, matched_ious=0.5245, loss_iou=0.0927, loss_iou_reg=0.2230, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.22(1.27)  Time cost: 09:15/27:55 [11:17:47/10:59:35]  Acc_iter 32100       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.22(1.27)
2025-09-03 20:12:48,873   INFO  Train:   19/36 ( 53%) [ 487/1759 ( 28%)]  Loss: 3.677 (3.47)  LR: 2.768e-03  Grad: 7.1417  max=4.2391(module.vfe.pfn_layers.0.linear.weight)  min: -2.4565(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6569, loss_cls=0.1176, loss_bbox=0.7783, matched_ious=0.5140, loss_iou=0.0959, loss_iou_reg=0.2253, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 10:19/26:54 [11:18:51/10:59:28]  Acc_iter 32150       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 20:13:51,987   INFO  Train:   19/36 ( 53%) [ 537/1759 ( 31%)]  Loss: 3.695 (3.47)  LR: 2.764e-03  Grad: 4.6155  max=0.6739(module.vfe.pfn_layers.0.linear.weight)  min: -0.6991(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6661, loss_cls=0.1183, loss_bbox=0.7658, matched_ious=0.5265, loss_iou=0.0917, loss_iou_reg=0.2200, d_time=0.00(0.01), f_time=1.33(1.26), b_time=1.33(1.27)  Time cost: 11:22/25:50 [11:19:54/10:58:04]  Acc_iter 32200       Data time: 0.00(0.01)  Forward time: 1.33(1.26)  Batch time: 1.33(1.27)
2025-09-03 20:14:54,256   INFO  Train:   19/36 ( 53%) [ 587/1759 ( 33%)]  Loss: 3.200 (3.47)  LR: 2.761e-03  Grad: 4.7715  max=0.4276(module.vfe.pfn_layers.0.linear.weight)  min: -0.4803(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6399, loss_cls=0.1187, loss_bbox=0.7257, matched_ious=0.5219, loss_iou=0.0921, loss_iou_reg=0.2235, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.25(1.27)  Time cost: 12:24/24:44 [11:20:56/10:55:59]  Acc_iter 32250       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.27)
2025-09-03 20:15:57,397   INFO  Train:   19/36 ( 53%) [ 637/1759 ( 36%)]  Loss: 4.361 (3.47)  LR: 2.758e-03  Grad: 2.0010  max=0.4992(module.vfe.pfn_layers.0.linear.weight)  min: -0.8251(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6591, loss_cls=0.1214, loss_bbox=0.7009, matched_ious=0.5192, loss_iou=0.0946, loss_iou_reg=0.2271, d_time=0.01(0.01), f_time=1.18(1.26), b_time=1.19(1.27)  Time cost: 13:27/23:40 [11:21:59/10:54:47]  Acc_iter 32300       Data time: 0.01(0.01)  Forward time: 1.18(1.26)  Batch time: 1.19(1.27)
2025-09-03 20:17:00,132   INFO  Train:   19/36 ( 53%) [ 687/1759 ( 39%)]  Loss: 3.698 (3.46)  LR: 2.754e-03  Grad: 1.7109  max=0.9157(module.vfe.pfn_layers.0.linear.weight)  min: -0.3815(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6637, loss_cls=0.1235, loss_bbox=0.7302, matched_ious=0.5240, loss_iou=0.0940, loss_iou_reg=0.2254, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.27)  Time cost: 14:30/22:36 [11:23:02/10:53:17]  Acc_iter 32350       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.27)
2025-09-03 20:18:03,391   INFO  Train:   19/36 ( 53%) [ 737/1759 ( 42%)]  Loss: 3.486 (3.46)  LR: 2.751e-03  Grad: 1.6033  max=0.2833(module.vfe.pfn_layers.0.linear.weight)  min: -0.3857(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6521, loss_cls=0.1214, loss_bbox=0.7041, matched_ious=0.5230, loss_iou=0.0928, loss_iou_reg=0.2238, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 15:33/21:33 [11:24:05/10:52:13]  Acc_iter 32400       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-03 20:19:04,862   INFO  Train:   19/36 ( 53%) [ 787/1759 ( 45%)]  Loss: 3.181 (3.46)  LR: 2.747e-03  Grad: 2.1144  max=0.1976(module.vfe.pfn_layers.0.linear.weight)  min: -1.1134(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6080, loss_cls=0.1105, loss_bbox=0.7135, matched_ious=0.5258, loss_iou=0.0929, loss_iou_reg=0.2228, d_time=0.01(0.01), f_time=1.22(1.26), b_time=1.23(1.26)  Time cost: 16:35/20:27 [11:25:07/10:50:00]  Acc_iter 32450       Data time: 0.01(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.26)
2025-09-03 20:20:07,702   INFO  Train:   19/36 ( 53%) [ 837/1759 ( 48%)]  Loss: 3.136 (3.46)  LR: 2.744e-03  Grad: 2.1994  max=1.4012(module.vfe.pfn_layers.0.linear.weight)  min: -0.3433(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6419, loss_cls=0.1170, loss_bbox=0.7482, matched_ious=0.5261, loss_iou=0.0913, loss_iou_reg=0.2206, d_time=0.00(0.01), f_time=1.29(1.26), b_time=1.29(1.26)  Time cost: 17:38/19:24 [11:26:10/10:48:45]  Acc_iter 32500       Data time: 0.00(0.01)  Forward time: 1.29(1.26)  Batch time: 1.29(1.26)
2025-09-03 20:21:10,673   INFO  Train:   19/36 ( 53%) [ 887/1759 ( 50%)]  Loss: 3.575 (3.46)  LR: 2.741e-03  Grad: 2.2540  max=0.9656(module.vfe.pfn_layers.0.linear.weight)  min: -0.3539(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6468, loss_cls=0.1183, loss_bbox=0.6943, matched_ious=0.5245, loss_iou=0.0925, loss_iou_reg=0.2228, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.26(1.26)  Time cost: 18:41/18:20 [11:27:13/10:47:36]  Acc_iter 32550       Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.26)
2025-09-03 20:22:13,206   INFO  Train:   19/36 ( 53%) [ 937/1759 ( 53%)]  Loss: 2.861 (3.46)  LR: 2.737e-03  Grad: 2.5868  max=0.9273(module.vfe.pfn_layers.0.linear.weight)  min: -1.0586(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6417, loss_cls=0.1166, loss_bbox=0.7711, matched_ious=0.5204, loss_iou=0.0930, loss_iou_reg=0.2243, d_time=0.02(0.01), f_time=1.21(1.26), b_time=1.23(1.26)  Time cost: 19:43/17:17 [11:28:15/10:46:13]  Acc_iter 32600       Data time: 0.02(0.01)  Forward time: 1.21(1.26)  Batch time: 1.23(1.26)
2025-09-03 20:23:17,434   INFO  Train:   19/36 ( 53%) [ 987/1759 ( 56%)]  Loss: 2.898 (3.46)  LR: 2.734e-03  Grad: 2.6350  max=0.5672(module.vfe.pfn_layers.0.linear.weight)  min: -0.8879(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6572, loss_cls=0.1200, loss_bbox=0.7330, matched_ious=0.5249, loss_iou=0.0938, loss_iou_reg=0.2230, d_time=0.00(0.01), f_time=1.34(1.26), b_time=1.34(1.26)  Time cost: 20:47/16:15 [11:29:19/10:45:45]  Acc_iter 32650       Data time: 0.00(0.01)  Forward time: 1.34(1.26)  Batch time: 1.34(1.26)
2025-09-03 20:24:20,176   INFO  Train:   19/36 ( 53%) [1037/1759 ( 59%)]  Loss: 3.009 (3.46)  LR: 2.730e-03  Grad: 2.6369  max=0.2620(module.backbone_3d.cls_conv.3.bias)  min: -0.5034(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6377, loss_cls=0.1153, loss_bbox=0.7278, matched_ious=0.5279, loss_iou=0.0903, loss_iou_reg=0.2216, d_time=0.00(0.01), f_time=1.17(1.26), b_time=1.18(1.26)  Time cost: 21:50/15:11 [11:30:22/10:44:30]  Acc_iter 32700       Data time: 0.00(0.01)  Forward time: 1.17(1.26)  Batch time: 1.18(1.26)
2025-09-03 20:25:22,440   INFO  Train:   19/36 ( 53%) [1087/1759 ( 62%)]  Loss: 2.930 (3.45)  LR: 2.726e-03  Grad: 3.2710  max=0.6688(module.vfe.pfn_layers.0.linear.weight)  min: -1.3547(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6311, loss_cls=0.1143, loss_bbox=0.7186, matched_ious=0.5302, loss_iou=0.0914, loss_iou_reg=0.2223, d_time=0.00(0.01), f_time=1.30(1.26), b_time=1.30(1.26)  Time cost: 22:52/14:07 [11:31:24/10:43:02]  Acc_iter 32750       Data time: 0.00(0.01)  Forward time: 1.30(1.26)  Batch time: 1.30(1.26)
2025-09-03 20:26:25,011   INFO  Train:   19/36 ( 53%) [1137/1759 ( 65%)]  Loss: 3.411 (3.45)  LR: 2.723e-03  Grad: 3.1941  max=0.6298(module.vfe.pfn_layers.0.linear.weight)  min: -0.5535(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6340, loss_cls=0.1133, loss_bbox=0.7252, matched_ious=0.5245, loss_iou=0.0913, loss_iou_reg=0.2239, d_time=0.01(0.01), f_time=1.17(1.25), b_time=1.18(1.26)  Time cost: 23:55/13:04 [11:32:27/10:41:45]  Acc_iter 32800       Data time: 0.01(0.01)  Forward time: 1.17(1.25)  Batch time: 1.18(1.26)
2025-09-03 20:27:28,142   INFO  Train:   19/36 ( 53%) [1187/1759 ( 67%)]  Loss: 4.357 (3.45)  LR: 2.719e-03  Grad: 3.0353  max=1.7155(module.vfe.pfn_layers.0.linear.weight)  min: -0.2515(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6323, loss_cls=0.1164, loss_bbox=0.7187, matched_ious=0.5253, loss_iou=0.0902, loss_iou_reg=0.2216, d_time=0.00(0.01), f_time=1.34(1.25), b_time=1.34(1.26)  Time cost: 24:58/12:01 [11:33:30/10:40:43]  Acc_iter 32850       Data time: 0.00(0.01)  Forward time: 1.34(1.25)  Batch time: 1.34(1.26)
2025-09-03 20:28:31,549   INFO  Train:   19/36 ( 53%) [1237/1759 ( 70%)]  Loss: 3.130 (3.45)  LR: 2.716e-03  Grad: 2.4396  max=0.5284(module.vfe.pfn_layers.0.linear.weight)  min: -0.6875(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6523, loss_cls=0.1183, loss_bbox=0.7281, matched_ious=0.5189, loss_iou=0.0936, loss_iou_reg=0.2259, d_time=0.00(0.01), f_time=1.39(1.25), b_time=1.39(1.26)  Time cost: 26:02/10:58 [11:34:34/10:39:48]  Acc_iter 32900       Data time: 0.00(0.01)  Forward time: 1.39(1.25)  Batch time: 1.39(1.26)
2025-09-03 20:29:34,565   INFO  Train:   19/36 ( 53%) [1287/1759 ( 73%)]  Loss: 3.464 (3.45)  LR: 2.712e-03  Grad: 2.8254  max=1.1894(module.vfe.pfn_layers.0.linear.weight)  min: -0.7807(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6597, loss_cls=0.1208, loss_bbox=0.7225, matched_ious=0.5254, loss_iou=0.0917, loss_iou_reg=0.2229, d_time=0.00(0.01), f_time=1.18(1.25), b_time=1.18(1.26)  Time cost: 27:05/09:55 [11:35:37/10:38:44]  Acc_iter 32950       Data time: 0.00(0.01)  Forward time: 1.18(1.25)  Batch time: 1.18(1.26)
2025-09-03 20:30:37,589   INFO  Train:   19/36 ( 53%) [1337/1759 ( 76%)]  Loss: 3.497 (3.45)  LR: 2.708e-03  Grad: 2.9671  max=0.9981(module.vfe.pfn_layers.0.linear.weight)  min: -0.5566(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6644, loss_cls=0.1221, loss_bbox=0.7520, matched_ious=0.5235, loss_iou=0.0927, loss_iou_reg=0.2235, d_time=0.01(0.01), f_time=1.26(1.25), b_time=1.26(1.26)  Time cost: 28:08/08:52 [11:36:40/10:37:39]  Acc_iter 33000       Data time: 0.01(0.01)  Forward time: 1.26(1.25)  Batch time: 1.26(1.26)
2025-09-03 20:31:40,407   INFO  Train:   19/36 ( 53%) [1387/1759 ( 79%)]  Loss: 3.463 (3.45)  LR: 2.705e-03  Grad: 2.7969  max=0.3577(module.vfe.pfn_layers.0.linear.weight)  min: -0.2099(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6504, loss_cls=0.1170, loss_bbox=0.7161, matched_ious=0.5214, loss_iou=0.0918, loss_iou_reg=0.2241, d_time=0.00(0.01), f_time=1.15(1.25), b_time=1.15(1.26)  Time cost: 29:10/07:49 [11:37:42/10:36:30]  Acc_iter 33050       Data time: 0.00(0.01)  Forward time: 1.15(1.25)  Batch time: 1.15(1.26)
2025-09-03 20:32:43,948   INFO  Train:   19/36 ( 53%) [1437/1759 ( 82%)]  Loss: 3.503 (3.45)  LR: 2.701e-03  Grad: 3.1023  max=0.3856(module.vfe.pfn_layers.0.linear.weight)  min: -0.4993(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6425, loss_cls=0.1168, loss_bbox=0.7304, matched_ious=0.5260, loss_iou=0.0943, loss_iou_reg=0.2226, d_time=0.01(0.01), f_time=1.25(1.25), b_time=1.26(1.26)  Time cost: 30:14/06:46 [11:38:46/10:35:37]  Acc_iter 33100       Data time: 0.01(0.01)  Forward time: 1.25(1.25)  Batch time: 1.26(1.26)
2025-09-03 20:33:47,361   INFO  Train:   19/36 ( 53%) [1487/1759 ( 85%)]  Loss: 3.787 (3.45)  LR: 2.697e-03  Grad: 3.2517  max=0.6424(module.vfe.pfn_layers.0.linear.weight)  min: -0.2796(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6448, loss_cls=0.1155, loss_bbox=0.7214, matched_ious=0.5269, loss_iou=0.0932, loss_iou_reg=0.2212, d_time=0.00(0.01), f_time=1.23(1.26), b_time=1.24(1.26)  Time cost: 31:17/05:43 [11:39:49/10:34:41]  Acc_iter 33150       Data time: 0.00(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.26)
2025-09-03 20:34:50,386   INFO  Train:   19/36 ( 53%) [1537/1759 ( 87%)]  Loss: 3.860 (3.44)  LR: 2.693e-03  Grad: 4.5682  max=2.5683(module.vfe.pfn_layers.0.linear.weight)  min: -1.3660(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6160, loss_cls=0.1130, loss_bbox=0.6848, matched_ious=0.5271, loss_iou=0.0925, loss_iou_reg=0.2250, d_time=0.02(0.01), f_time=1.16(1.26), b_time=1.17(1.26)  Time cost: 32:20/04:40 [11:40:52/10:33:36]  Acc_iter 33200       Data time: 0.02(0.01)  Forward time: 1.16(1.26)  Batch time: 1.17(1.26)
2025-09-03 20:35:52,352   INFO  Train:   19/36 ( 53%) [1587/1759 ( 90%)]  Loss: 3.460 (3.44)  LR: 2.690e-03  Grad: 3.7530  max=0.1702(module.dense_head.prediction_head.height.1.bias)  min: -0.9653(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6198, loss_cls=0.1169, loss_bbox=0.7198, matched_ious=0.5254, loss_iou=0.0912, loss_iou_reg=0.2237, d_time=0.00(0.01), f_time=1.36(1.25), b_time=1.36(1.26)  Time cost: 33:22/03:36 [11:41:54/10:32:11]  Acc_iter 33250       Data time: 0.00(0.01)  Forward time: 1.36(1.25)  Batch time: 1.36(1.26)
2025-09-03 20:36:55,483   INFO  Train:   19/36 ( 53%) [1637/1759 ( 93%)]  Loss: 2.684 (3.44)  LR: 2.686e-03  Grad: 3.3778  max=0.3518(module.vfe.pfn_layers.0.linear.weight)  min: -1.9042(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6528, loss_cls=0.1206, loss_bbox=0.7173, matched_ious=0.5246, loss_iou=0.0933, loss_iou_reg=0.2237, d_time=0.00(0.01), f_time=1.29(1.25), b_time=1.29(1.26)  Time cost: 34:25/02:33 [11:42:58/10:31:10]  Acc_iter 33300       Data time: 0.00(0.01)  Forward time: 1.29(1.25)  Batch time: 1.29(1.26)
2025-09-03 20:37:59,598   INFO  Train:   19/36 ( 53%) [1687/1759 ( 96%)]  Loss: 3.297 (3.44)  LR: 2.682e-03  Grad: 2.6653  max=0.4171(module.vfe.pfn_layers.0.linear.weight)  min: -0.1930(module.backbone_3d.cls_conv.1.weight)  NaN: False  loss_hm=0.6576, loss_cls=0.1200, loss_bbox=0.7270, matched_ious=0.5231, loss_iou=0.0924, loss_iou_reg=0.2238, d_time=0.01(0.01), f_time=1.30(1.26), b_time=1.31(1.26)  Time cost: 35:30/01:30 [11:44:02/10:30:25]  Acc_iter 33350       Data time: 0.01(0.01)  Forward time: 1.30(1.26)  Batch time: 1.31(1.26)
2025-09-03 20:39:02,797   INFO  Train:   19/36 ( 53%) [1737/1759 ( 99%)]  Loss: 2.947 (3.44)  LR: 2.678e-03  Grad: 2.8927  max=0.2188(module.vfe.pfn_layers.0.linear.weight)  min: -0.2323(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6428, loss_cls=0.1165, loss_bbox=0.6871, matched_ious=0.5274, loss_iou=0.0905, loss_iou_reg=0.2227, d_time=0.00(0.01), f_time=1.18(1.26), b_time=1.18(1.26)  Time cost: 36:33/00:27 [11:45:05/10:29:24]  Acc_iter 33400       Data time: 0.00(0.01)  Forward time: 1.18(1.26)  Batch time: 1.18(1.26)
2025-09-03 20:39:28,025   INFO  Train:   19/36 ( 53%) [1758/1759 (100%)]  Loss: 3.529 (3.44)  LR: 2.677e-03  Grad: 3.1494  max=0.4294(module.vfe.pfn_layers.0.linear.weight)  min: -0.4010(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6228, loss_cls=0.1149, loss_bbox=0.6387, matched_ious=0.5303, loss_iou=0.0921, loss_iou_reg=0.2233, d_time=0.00(0.01), f_time=0.77(1.25), b_time=0.77(1.26)  Time cost: 36:58/00:01 [11:45:30/10:28:36]  Acc_iter 33421       Data time: 0.00(0.01)  Forward time: 0.77(1.25)  Batch time: 0.77(1.26)

                                               [Aepochs:  53%|█████▎    | 19/36 [11:45:30<10:29:25, 2221.48s/it]epochs:  53%|█████▎    | 19/36 [11:45:31<10:29:25, 2221.51s/it]epochs:  53%|█████▎    | 19/36 [11:45:31<10:29:26, 2221.54s/it]epochs:  53%|█████▎    | 19/36 [11:45:31<10:29:25, 2221.51s/it]epochs:  53%|█████▎    | 19/36 [11:45:31<10:29:25, 2221.51s/it]epochs:  53%|█████▎    | 19/36 [11:45:31<10:29:25, 2221.52s/it]epochs:  53%|█████▎    | 19/36 [11:45:31<10:29:25, 2221.53s/it]epochs:  53%|█████▎    | 19/36 [11:45:31<10:29:25, 2221.52s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 20:39:33,703   INFO  Train:   20/36 ( 56%) [   0/1759 (  0%)]  Loss: 2.570 (2.57)  LR: 2.677e-03  Grad: 3.0252  max=0.4483(module.vfe.pfn_layers.0.linear.weight)  min: -0.5895(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5211, loss_cls=0.1062, loss_bbox=0.4844, matched_ious=0.5712, loss_iou=0.0837, loss_iou_reg=0.2057, d_time=1.50(1.50), f_time=2.78(2.78), b_time=4.28(4.28)  Time cost: 00:03/1:48:14 [11:45:36/30:40:08]  Acc_iter 33422       Data time: 1.50(1.50)  Forward time: 2.78(2.78)  Batch time: 4.28(4.28)
2025-09-03 20:40:09,439   INFO  Train:   20/36 ( 56%) [  28/1759 (  2%)]  Loss: 3.553 (3.35)  LR: 2.674e-03  Grad: 3.2858  max=0.6421(module.vfe.pfn_layers.0.linear.weight)  min: -0.6463(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6411, loss_cls=0.1214, loss_bbox=0.7116, matched_ious=0.5311, loss_iou=0.0927, loss_iou_reg=0.2188, d_time=0.00(0.06), f_time=1.34(1.32), b_time=1.34(1.38)  Time cost: 00:39/39:13 [11:46:11/11:16:57]  Acc_iter 33450       Data time: 0.00(0.06)  Forward time: 1.34(1.32)  Batch time: 1.34(1.38)
2025-09-03 20:41:12,379   INFO  Train:   20/36 ( 56%) [  78/1759 (  4%)]  Loss: 3.312 (3.42)  LR: 2.671e-03  Grad: 3.3332  max=0.4852(module.vfe.pfn_layers.0.linear.weight)  min: -0.2351(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6495, loss_cls=0.1205, loss_bbox=0.7312, matched_ious=0.5154, loss_iou=0.0930, loss_iou_reg=0.2262, d_time=0.00(0.02), f_time=1.27(1.28), b_time=1.27(1.30)  Time cost: 01:42/36:18 [11:47:14/10:44:06]  Acc_iter 33500       Data time: 0.00(0.02)  Forward time: 1.27(1.28)  Batch time: 1.27(1.30)
2025-09-03 20:42:15,112   INFO  Train:   20/36 ( 56%) [ 128/1759 (  7%)]  Loss: 2.998 (3.42)  LR: 2.667e-03  Grad: 3.6113  max=0.6320(module.vfe.pfn_layers.0.linear.weight)  min: -0.7372(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6533, loss_cls=0.1178, loss_bbox=0.7032, matched_ious=0.5345, loss_iou=0.0906, loss_iou_reg=0.2195, d_time=0.00(0.02), f_time=1.30(1.27), b_time=1.30(1.28)  Time cost: 02:45/34:47 [11:48:17/10:35:07]  Acc_iter 33550       Data time: 0.00(0.02)  Forward time: 1.30(1.27)  Batch time: 1.30(1.28)
2025-09-03 20:43:18,161   INFO  Train:   20/36 ( 56%) [ 178/1759 ( 10%)]  Loss: 4.081 (3.42)  LR: 2.663e-03  Grad: 3.9939  max=0.9819(module.vfe.pfn_layers.0.linear.weight)  min: -1.2426(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6504, loss_cls=0.1166, loss_bbox=0.7372, matched_ious=0.5274, loss_iou=0.0936, loss_iou_reg=0.2205, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.28(1.28)  Time cost: 03:48/33:35 [11:49:20/10:31:26]  Acc_iter 33600       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.28(1.28)
2025-09-03 20:44:22,206   INFO  Train:   20/36 ( 56%) [ 228/1759 ( 13%)]  Loss: 3.268 (3.41)  LR: 2.659e-03  Grad: 3.8979  max=0.4684(module.vfe.pfn_layers.0.linear.weight)  min: -0.1642(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6471, loss_cls=0.1183, loss_bbox=0.7076, matched_ious=0.5289, loss_iou=0.0919, loss_iou_reg=0.2213, d_time=0.00(0.01), f_time=1.23(1.27), b_time=1.23(1.28)  Time cost: 04:52/32:33 [11:50:24/10:31:03]  Acc_iter 33650       Data time: 0.00(0.01)  Forward time: 1.23(1.27)  Batch time: 1.23(1.28)
2025-09-03 20:45:25,170   INFO  Train:   20/36 ( 56%) [ 278/1759 ( 16%)]  Loss: 3.744 (3.42)  LR: 2.655e-03  Grad: 4.3903  max=1.2117(module.vfe.pfn_layers.0.linear.weight)  min: -0.3209(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6376, loss_cls=0.1163, loss_bbox=0.7157, matched_ious=0.5221, loss_iou=0.0929, loss_iou_reg=0.2242, d_time=0.00(0.01), f_time=1.35(1.26), b_time=1.35(1.28)  Time cost: 05:55/31:25 [11:51:27/10:28:31]  Acc_iter 33700       Data time: 0.00(0.01)  Forward time: 1.35(1.26)  Batch time: 1.35(1.28)
2025-09-03 20:46:28,191   INFO  Train:   20/36 ( 56%) [ 328/1759 ( 19%)]  Loss: 4.569 (3.42)  LR: 2.651e-03  Grad: 4.4450  max=0.2434(module.vfe.pfn_layers.0.linear.weight)  min: -0.8364(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6437, loss_cls=0.1165, loss_bbox=0.7492, matched_ious=0.5287, loss_iou=0.0901, loss_iou_reg=0.2193, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.27(1.27)  Time cost: 06:58/30:18 [11:52:30/10:26:31]  Acc_iter 33750       Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.27(1.27)
2025-09-03 20:47:31,630   INFO  Train:   20/36 ( 56%) [ 378/1759 ( 21%)]  Loss: 4.516 (3.43)  LR: 2.647e-03  Grad: 4.5656  max=0.5954(module.vfe.pfn_layers.0.linear.weight)  min: -0.1460(module.dense_head.decoder.self_attn.in_proj_weight)  NaN: False  loss_hm=0.6426, loss_cls=0.1161, loss_bbox=0.7630, matched_ious=0.5233, loss_iou=0.0928, loss_iou_reg=0.2243, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.26(1.27)  Time cost: 08:01/29:14 [11:53:34/10:25:19]  Acc_iter 33800       Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.27)
2025-09-03 20:48:34,040   INFO  Train:   20/36 ( 56%) [ 428/1759 ( 24%)]  Loss: 3.251 (3.43)  LR: 2.643e-03  Grad: 4.8356  max=0.7900(module.vfe.pfn_layers.0.linear.weight)  min: -0.5565(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6408, loss_cls=0.1166, loss_bbox=0.7310, matched_ious=0.5227, loss_iou=0.0939, loss_iou_reg=0.2250, d_time=0.00(0.01), f_time=1.33(1.26), b_time=1.34(1.27)  Time cost: 09:04/28:07 [11:54:36/10:22:58]  Acc_iter 33850       Data time: 0.00(0.01)  Forward time: 1.33(1.26)  Batch time: 1.34(1.27)
2025-09-03 20:49:36,524   INFO  Train:   20/36 ( 56%) [ 478/1759 ( 27%)]  Loss: 3.167 (3.43)  LR: 2.639e-03  Grad: 4.9425  max=0.4186(module.vfe.pfn_layers.0.linear.weight)  min: -0.1933(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6224, loss_cls=0.1109, loss_bbox=0.7231, matched_ious=0.5300, loss_iou=0.0916, loss_iou_reg=0.2204, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.25(1.27)  Time cost: 10:06/27:02 [11:55:39/10:20:58]  Acc_iter 33900       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.27)
2025-09-03 20:50:39,170   INFO  Train:   20/36 ( 56%) [ 528/1759 ( 30%)]  Loss: 3.426 (3.43)  LR: 2.635e-03  Grad: 5.1701  max=0.4255(module.vfe.pfn_layers.0.linear.weight)  min: -0.2684(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6513, loss_cls=0.1218, loss_bbox=0.7121, matched_ious=0.5182, loss_iou=0.0918, loss_iou_reg=0.2284, d_time=0.01(0.01), f_time=1.18(1.26), b_time=1.19(1.27)  Time cost: 11:09/25:57 [11:56:41/10:19:17]  Acc_iter 33950       Data time: 0.01(0.01)  Forward time: 1.18(1.26)  Batch time: 1.19(1.27)
2025-09-03 20:51:41,621   INFO  Train:   20/36 ( 56%) [ 578/1759 ( 33%)]  Loss: 3.219 (3.42)  LR: 2.631e-03  Grad: 5.5079  max=0.5507(module.vfe.pfn_layers.0.linear.weight)  min: -0.9088(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6147, loss_cls=0.1132, loss_bbox=0.6845, matched_ious=0.5325, loss_iou=0.0900, loss_iou_reg=0.2204, d_time=0.00(0.01), f_time=1.16(1.26), b_time=1.16(1.26)  Time cost: 12:11/24:52 [11:57:44/10:17:34]  Acc_iter 34000       Data time: 0.00(0.01)  Forward time: 1.16(1.26)  Batch time: 1.16(1.26)
2025-09-03 20:52:44,419   INFO  Train:   20/36 ( 56%) [ 628/1759 ( 36%)]  Loss: 3.122 (3.41)  LR: 2.627e-03  Grad: 5.7437  max=0.6077(module.vfe.pfn_layers.0.linear.weight)  min: -0.8562(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6275, loss_cls=0.1169, loss_bbox=0.6855, matched_ious=0.5323, loss_iou=0.0911, loss_iou_reg=0.2209, d_time=0.01(0.01), f_time=1.34(1.26), b_time=1.34(1.26)  Time cost: 13:14/23:48 [11:58:46/10:16:13]  Acc_iter 34050       Data time: 0.01(0.01)  Forward time: 1.34(1.26)  Batch time: 1.34(1.26)
2025-09-03 20:53:46,223   INFO  Train:   20/36 ( 56%) [ 678/1759 ( 39%)]  Loss: 2.844 (3.40)  LR: 2.623e-03  Grad: 5.7701  max=0.3052(module.vfe.pfn_layers.0.linear.weight)  min: -0.5032(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6278, loss_cls=0.1181, loss_bbox=0.7076, matched_ious=0.5311, loss_iou=0.0931, loss_iou_reg=0.2203, d_time=0.00(0.01), f_time=1.26(1.25), b_time=1.27(1.26)  Time cost: 14:16/22:43 [11:59:48/10:14:12]  Acc_iter 34100       Data time: 0.00(0.01)  Forward time: 1.26(1.25)  Batch time: 1.27(1.26)
2025-09-03 20:54:50,312   INFO  Train:   20/36 ( 56%) [ 728/1759 ( 41%)]  Loss: 3.872 (3.41)  LR: 2.618e-03  Grad: 6.1605  max=1.1965(module.vfe.pfn_layers.0.linear.weight)  min: -0.6251(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6358, loss_cls=0.1170, loss_bbox=0.7153, matched_ious=0.5237, loss_iou=0.0933, loss_iou_reg=0.2244, d_time=0.00(0.01), f_time=1.12(1.26), b_time=1.13(1.26)  Time cost: 15:20/21:41 [12:00:52/10:13:50]  Acc_iter 34150       Data time: 0.00(0.01)  Forward time: 1.12(1.26)  Batch time: 1.13(1.26)
2025-09-03 20:55:52,753   INFO  Train:   20/36 ( 56%) [ 778/1759 ( 44%)]  Loss: 3.130 (3.41)  LR: 2.614e-03  Grad: 3.1347  max=0.5886(module.vfe.pfn_layers.0.linear.weight)  min: -0.5203(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6536, loss_cls=0.1190, loss_bbox=0.7233, matched_ious=0.5236, loss_iou=0.0921, loss_iou_reg=0.2250, d_time=0.00(0.01), f_time=1.13(1.26), b_time=1.13(1.26)  Time cost: 16:22/20:37 [12:01:55/10:12:22]  Acc_iter 34200       Data time: 0.00(0.01)  Forward time: 1.13(1.26)  Batch time: 1.13(1.26)
2025-09-03 20:56:55,796   INFO  Train:   20/36 ( 56%) [ 828/1759 ( 47%)]  Loss: 2.963 (3.41)  LR: 2.610e-03  Grad: 3.2667  max=0.3234(module.vfe.pfn_layers.0.linear.weight)  min: -0.1322(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6233, loss_cls=0.1151, loss_bbox=0.7026, matched_ious=0.5298, loss_iou=0.0912, loss_iou_reg=0.2217, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.25(1.26)  Time cost: 17:25/19:34 [12:02:58/10:11:18]  Acc_iter 34250       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.25(1.26)
2025-09-03 20:57:58,064   INFO  Train:   20/36 ( 56%) [ 878/1759 ( 50%)]  Loss: 3.496 (3.41)  LR: 2.606e-03  Grad: 3.5907  max=0.1805(module.vfe.pfn_layers.0.linear.weight)  min: -0.5892(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6373, loss_cls=0.1186, loss_bbox=0.7329, matched_ious=0.5246, loss_iou=0.0913, loss_iou_reg=0.2246, d_time=0.01(0.01), f_time=1.25(1.25), b_time=1.26(1.26)  Time cost: 18:28/18:30 [12:04:00/10:09:48]  Acc_iter 34300       Data time: 0.01(0.01)  Forward time: 1.25(1.25)  Batch time: 1.26(1.26)
2025-09-03 20:59:00,806   INFO  Train:   20/36 ( 56%) [ 928/1759 ( 53%)]  Loss: 2.691 (3.40)  LR: 2.602e-03  Grad: 3.8699  max=0.7456(module.vfe.pfn_layers.0.linear.weight)  min: -0.5417(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6197, loss_cls=0.1135, loss_bbox=0.7011, matched_ious=0.5247, loss_iou=0.0924, loss_iou_reg=0.2225, d_time=0.00(0.01), f_time=1.21(1.25), b_time=1.21(1.26)  Time cost: 19:30/17:27 [12:05:03/10:08:36]  Acc_iter 34350       Data time: 0.00(0.01)  Forward time: 1.21(1.25)  Batch time: 1.21(1.26)
2025-09-03 21:00:03,238   INFO  Train:   20/36 ( 56%) [ 978/1759 ( 56%)]  Loss: 3.271 (3.40)  LR: 2.598e-03  Grad: 4.2233  max=0.6827(module.vfe.pfn_layers.0.linear.weight)  min: -0.9517(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6283, loss_cls=0.1128, loss_bbox=0.6644, matched_ious=0.5331, loss_iou=0.0916, loss_iou_reg=0.2213, d_time=0.00(0.01), f_time=1.22(1.25), b_time=1.23(1.26)  Time cost: 20:33/16:23 [12:06:05/10:07:16]  Acc_iter 34400       Data time: 0.00(0.01)  Forward time: 1.22(1.25)  Batch time: 1.23(1.26)
2025-09-03 21:01:06,095   INFO  Train:   20/36 ( 56%) [1028/1759 ( 58%)]  Loss: 3.848 (3.39)  LR: 2.593e-03  Grad: 4.2133  max=0.8514(module.vfe.pfn_layers.0.linear.weight)  min: -0.4387(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6248, loss_cls=0.1123, loss_bbox=0.7085, matched_ious=0.5317, loss_iou=0.0921, loss_iou_reg=0.2203, d_time=0.00(0.01), f_time=1.30(1.25), b_time=1.30(1.26)  Time cost: 21:36/15:20 [12:07:08/10:06:09]  Acc_iter 34450       Data time: 0.00(0.01)  Forward time: 1.30(1.25)  Batch time: 1.30(1.26)
2025-09-03 21:02:08,623   INFO  Train:   20/36 ( 56%) [1078/1759 ( 61%)]  Loss: 3.010 (3.39)  LR: 2.589e-03  Grad: 4.4094  max=0.3526(module.vfe.pfn_layers.0.linear.weight)  min: -0.3406(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6305, loss_cls=0.1165, loss_bbox=0.6979, matched_ious=0.5332, loss_iou=0.0927, loss_iou_reg=0.2200, d_time=0.00(0.01), f_time=1.19(1.25), b_time=1.20(1.26)  Time cost: 22:38/14:17 [12:08:11/10:04:54]  Acc_iter 34500       Data time: 0.00(0.01)  Forward time: 1.19(1.25)  Batch time: 1.20(1.26)
2025-09-03 21:03:11,352   INFO  Train:   20/36 ( 56%) [1128/1759 ( 64%)]  Loss: 2.947 (3.39)  LR: 2.585e-03  Grad: 5.1901  max=0.2139(module.vfe.pfn_layers.0.linear.weight)  min: -1.7964(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6224, loss_cls=0.1193, loss_bbox=0.6871, matched_ious=0.5320, loss_iou=0.0902, loss_iou_reg=0.2209, d_time=0.00(0.01), f_time=1.35(1.25), b_time=1.35(1.26)  Time cost: 23:41/13:14 [12:09:13/10:03:45]  Acc_iter 34550       Data time: 0.00(0.01)  Forward time: 1.35(1.25)  Batch time: 1.35(1.26)
2025-09-03 21:04:13,875   INFO  Train:   20/36 ( 56%) [1178/1759 ( 67%)]  Loss: 3.348 (3.39)  LR: 2.581e-03  Grad: 4.7754  max=0.2063(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.4077(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6665, loss_cls=0.1222, loss_bbox=0.7691, matched_ious=0.5197, loss_iou=0.0940, loss_iou_reg=0.2246, d_time=0.00(0.01), f_time=1.29(1.25), b_time=1.30(1.26)  Time cost: 24:43/12:11 [12:10:16/10:02:32]  Acc_iter 34600       Data time: 0.00(0.01)  Forward time: 1.29(1.25)  Batch time: 1.30(1.26)
2025-09-03 21:05:17,203   INFO  Train:   20/36 ( 56%) [1228/1759 ( 70%)]  Loss: 3.792 (3.39)  LR: 2.576e-03  Grad: 4.9994  max=0.3852(module.vfe.pfn_layers.0.linear.weight)  min: -0.4973(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6193, loss_cls=0.1123, loss_bbox=0.7018, matched_ious=0.5278, loss_iou=0.0921, loss_iou_reg=0.2210, d_time=0.00(0.01), f_time=1.20(1.25), b_time=1.20(1.26)  Time cost: 25:47/11:08 [12:11:19/10:01:38]  Acc_iter 34650       Data time: 0.00(0.01)  Forward time: 1.20(1.25)  Batch time: 1.20(1.26)
2025-09-03 21:06:19,942   INFO  Train:   20/36 ( 56%) [1278/1759 ( 73%)]  Loss: 3.713 (3.39)  LR: 2.572e-03  Grad: 5.1854  max=0.2181(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.3587(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6307, loss_cls=0.1163, loss_bbox=0.7230, matched_ious=0.5277, loss_iou=0.0933, loss_iou_reg=0.2217, d_time=0.00(0.01), f_time=1.21(1.25), b_time=1.21(1.26)  Time cost: 26:49/10:05 [12:12:22/10:00:31]  Acc_iter 34700       Data time: 0.00(0.01)  Forward time: 1.21(1.25)  Batch time: 1.21(1.26)
2025-09-03 21:07:23,380   INFO  Train:   20/36 ( 56%) [1328/1759 ( 75%)]  Loss: 3.132 (3.39)  LR: 2.568e-03  Grad: 5.4327  max=0.3909(module.vfe.pfn_layers.0.linear.weight)  min: -0.2418(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6068, loss_cls=0.1123, loss_bbox=0.6594, matched_ious=0.5315, loss_iou=0.0910, loss_iou_reg=0.2236, d_time=0.00(0.01), f_time=1.16(1.25), b_time=1.16(1.26)  Time cost: 27:53/09:02 [12:13:25/9:59:39]  Acc_iter 34750       Data time: 0.00(0.01)  Forward time: 1.16(1.25)  Batch time: 1.16(1.26)
2025-09-03 21:08:26,829   INFO  Train:   20/36 ( 56%) [1378/1759 ( 78%)]  Loss: 3.484 (3.39)  LR: 2.563e-03  Grad: 5.6272  max=0.2224(module.dense_head.decoder.multihead_attn.in_proj_weight)  min: -0.2640(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6365, loss_cls=0.1124, loss_bbox=0.7564, matched_ious=0.5312, loss_iou=0.0913, loss_iou_reg=0.2205, d_time=0.01(0.01), f_time=1.19(1.25), b_time=1.20(1.26)  Time cost: 28:56/07:59 [12:14:29/9:58:46]  Acc_iter 34800       Data time: 0.01(0.01)  Forward time: 1.19(1.25)  Batch time: 1.20(1.26)
2025-09-03 21:09:29,863   INFO  Train:   20/36 ( 56%) [1428/1759 ( 81%)]  Loss: 2.802 (3.39)  LR: 2.559e-03  Grad: 4.5200  max=0.4772(module.vfe.pfn_layers.0.linear.weight)  min: -1.3942(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6172, loss_cls=0.1102, loss_bbox=0.6961, matched_ious=0.5340, loss_iou=0.0922, loss_iou_reg=0.2172, d_time=0.00(0.01), f_time=1.29(1.25), b_time=1.29(1.26)  Time cost: 29:59/06:56 [12:15:32/9:57:44]  Acc_iter 34850       Data time: 0.00(0.01)  Forward time: 1.29(1.25)  Batch time: 1.29(1.26)
2025-09-03 21:10:32,680   INFO  Train:   20/36 ( 56%) [1478/1759 ( 84%)]  Loss: 3.547 (3.39)  LR: 2.554e-03  Grad: 4.5668  max=0.5228(module.vfe.pfn_layers.0.linear.weight)  min: -0.7702(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6358, loss_cls=0.1174, loss_bbox=0.7545, matched_ious=0.5232, loss_iou=0.0930, loss_iou_reg=0.2237, d_time=0.00(0.01), f_time=1.20(1.25), b_time=1.20(1.26)  Time cost: 31:02/05:53 [12:16:35/9:56:38]  Acc_iter 34900       Data time: 0.00(0.01)  Forward time: 1.20(1.25)  Batch time: 1.20(1.26)
2025-09-03 21:11:35,319   INFO  Train:   20/36 ( 56%) [1528/1759 ( 87%)]  Loss: 2.982 (3.38)  LR: 2.550e-03  Grad: 4.7421  max=0.6645(module.vfe.pfn_layers.0.linear.weight)  min: -0.4293(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6187, loss_cls=0.1090, loss_bbox=0.7068, matched_ious=0.5311, loss_iou=0.0898, loss_iou_reg=0.2187, d_time=0.01(0.01), f_time=1.23(1.25), b_time=1.24(1.26)  Time cost: 32:05/04:50 [12:17:37/9:55:29]  Acc_iter 34950       Data time: 0.01(0.01)  Forward time: 1.23(1.25)  Batch time: 1.24(1.26)
2025-09-03 21:12:38,384   INFO  Train:   20/36 ( 56%) [1578/1759 ( 90%)]  Loss: 2.968 (3.38)  LR: 2.546e-03  Grad: 4.9799  max=0.7949(module.vfe.pfn_layers.0.linear.weight)  min: -0.3689(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6301, loss_cls=0.1148, loss_bbox=0.7085, matched_ious=0.5297, loss_iou=0.0933, loss_iou_reg=0.2210, d_time=0.00(0.01), f_time=1.24(1.25), b_time=1.24(1.26)  Time cost: 33:08/03:47 [12:18:40/9:54:28]  Acc_iter 35000       Data time: 0.00(0.01)  Forward time: 1.24(1.25)  Batch time: 1.24(1.26)
2025-09-03 21:13:41,049   INFO  Train:   20/36 ( 56%) [1628/1759 ( 93%)]  Loss: 3.884 (3.39)  LR: 2.541e-03  Grad: 5.1002  max=0.3252(module.vfe.pfn_layers.0.linear.weight)  min: -0.6245(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6360, loss_cls=0.1158, loss_bbox=0.7414, matched_ious=0.5211, loss_iou=0.0941, loss_iou_reg=0.2245, d_time=0.00(0.01), f_time=1.24(1.25), b_time=1.25(1.26)  Time cost: 34:11/02:44 [12:19:43/9:53:20]  Acc_iter 35050       Data time: 0.00(0.01)  Forward time: 1.24(1.25)  Batch time: 1.25(1.26)
2025-09-03 21:14:43,889   INFO  Train:   20/36 ( 56%) [1678/1759 ( 95%)]  Loss: 3.856 (3.39)  LR: 2.537e-03  Grad: 4.5620  max=0.2250(module.vfe.pfn_layers.0.linear.weight)  min: -0.4349(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6281, loss_cls=0.1165, loss_bbox=0.7124, matched_ious=0.5265, loss_iou=0.0916, loss_iou_reg=0.2222, d_time=0.01(0.01), f_time=1.21(1.25), b_time=1.22(1.26)  Time cost: 35:13/01:41 [12:20:46/9:52:15]  Acc_iter 35100       Data time: 0.01(0.01)  Forward time: 1.21(1.25)  Batch time: 1.22(1.26)
2025-09-03 21:15:48,596   INFO  Train:   20/36 ( 56%) [1728/1759 ( 98%)]  Loss: 2.947 (3.39)  LR: 2.532e-03  Grad: 4.9992  max=0.4610(module.vfe.pfn_layers.0.linear.weight)  min: -1.2112(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6285, loss_cls=0.1170, loss_bbox=0.6810, matched_ious=0.5280, loss_iou=0.0915, loss_iou_reg=0.2230, d_time=0.00(0.01), f_time=1.24(1.25), b_time=1.24(1.26)  Time cost: 36:18/00:39 [12:21:51/9:51:41]  Acc_iter 35150       Data time: 0.00(0.01)  Forward time: 1.24(1.25)  Batch time: 1.24(1.26)
2025-09-03 21:16:26,581   INFO  Train:   20/36 ( 56%) [1758/1759 (100%)]  Loss: 3.056 (3.38)  LR: 2.529e-03  Grad: 5.1367  max=0.8132(module.vfe.pfn_layers.0.linear.weight)  min: -0.3635(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5970, loss_cls=0.1093, loss_bbox=0.6960, matched_ious=0.5281, loss_iou=0.0921, loss_iou_reg=0.2249, d_time=0.01(0.01), f_time=0.84(1.25), b_time=0.85(1.26)  Time cost: 36:56/00:01 [12:22:29/9:51:06]  Acc_iter 35180       Data time: 0.01(0.01)  Forward time: 0.84(1.25)  Batch time: 0.85(1.26)

                                               [Aepochs:  56%|█████▌    | 20/36 [12:22:29<9:52:09, 2220.60s/it] epochs:  56%|█████▌    | 20/36 [12:22:29<9:52:10, 2220.63s/it] epochs:  56%|█████▌    | 20/36 [12:22:29<9:52:09, 2220.62s/it] epochs:  56%|█████▌    | 20/36 [12:22:29<9:52:10, 2220.65s/it] epochs:  56%|█████▌    | 20/36 [12:22:29<9:52:09, 2220.62s/it] epochs:  56%|█████▌    | 20/36 [12:22:29<9:52:09, 2220.62s/it] epochs:  56%|█████▌    | 20/36 [12:22:29<9:52:10, 2220.63s/it] epochs:  56%|█████▌    | 20/36 [12:22:29<9:52:09, 2220.61s/it] 
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 21:16:32,167   INFO  Train:   21/36 ( 58%) [   0/1759 (  0%)]  Loss: 3.613 (3.61)  LR: 2.529e-03  Grad: 4.9459  max=0.2883(module.vfe.pfn_layers.0.linear.weight)  min: -0.4739(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6342, loss_cls=0.1023, loss_bbox=0.7837, matched_ious=0.5195, loss_iou=0.0943, loss_iou_reg=0.2235, d_time=1.68(1.68), f_time=2.55(2.55), b_time=4.24(4.24)  Time cost: 00:03/1:51:53 [12:22:34/29:50:17]  Acc_iter 35181       Data time: 1.68(1.68)  Forward time: 2.55(2.55)  Batch time: 4.24(4.24)
2025-09-03 21:16:56,314   INFO  Train:   21/36 ( 58%) [  19/1759 (  1%)]  Loss: 3.418 (3.59)  LR: 2.528e-03  Grad: 5.3479  max=1.2664(module.vfe.pfn_layers.0.linear.weight)  min: -0.7922(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.7031, loss_cls=0.1266, loss_bbox=0.8050, matched_ious=0.5169, loss_iou=0.0925, loss_iou_reg=0.2254, d_time=0.00(0.09), f_time=1.27(1.33), b_time=1.28(1.42)  Time cost: 00:27/40:32 [12:22:58/10:55:26]  Acc_iter 35200       Data time: 0.00(0.09)  Forward time: 1.27(1.33)  Batch time: 1.28(1.42)
2025-09-03 21:17:58,782   INFO  Train:   21/36 ( 58%) [  69/1759 (  4%)]  Loss: 4.183 (3.42)  LR: 2.523e-03  Grad: 5.6915  max=2.0699(module.vfe.pfn_layers.0.linear.weight)  min: -0.3913(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6374, loss_cls=0.1157, loss_bbox=0.6988, matched_ious=0.5211, loss_iou=0.0927, loss_iou_reg=0.2260, d_time=0.00(0.03), f_time=1.25(1.27), b_time=1.25(1.30)  Time cost: 01:30/36:23 [12:24:01/10:04:30]  Acc_iter 35250       Data time: 0.00(0.03)  Forward time: 1.25(1.27)  Batch time: 1.25(1.30)
2025-09-03 21:19:01,892   INFO  Train:   21/36 ( 58%) [ 119/1759 (  7%)]  Loss: 3.086 (3.37)  LR: 2.519e-03  Grad: 6.5467  max=2.1042(module.vfe.pfn_layers.0.linear.weight)  min: -2.9083(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6189, loss_cls=0.1113, loss_bbox=0.6918, matched_ious=0.5286, loss_iou=0.0912, loss_iou_reg=0.2233, d_time=0.02(0.02), f_time=1.24(1.26), b_time=1.26(1.28)  Time cost: 02:33/34:58 [12:25:04/9:57:38]  Acc_iter 35300       Data time: 0.02(0.02)  Forward time: 1.24(1.26)  Batch time: 1.26(1.28)
2025-09-03 21:20:04,741   INFO  Train:   21/36 ( 58%) [ 169/1759 ( 10%)]  Loss: 3.584 (3.35)  LR: 2.514e-03  Grad: 3.5809  max=0.5003(module.vfe.pfn_layers.0.linear.weight)  min: -0.2483(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6076, loss_cls=0.1141, loss_bbox=0.6949, matched_ious=0.5244, loss_iou=0.0913, loss_iou_reg=0.2233, d_time=0.00(0.02), f_time=1.30(1.26), b_time=1.30(1.28)  Time cost: 03:36/33:43 [12:26:07/9:53:28]  Acc_iter 35350       Data time: 0.00(0.02)  Forward time: 1.30(1.26)  Batch time: 1.30(1.28)
2025-09-03 21:21:07,489   INFO  Train:   21/36 ( 58%) [ 219/1759 ( 12%)]  Loss: 2.876 (3.35)  LR: 2.509e-03  Grad: 3.8041  max=0.2994(module.vfe.pfn_layers.0.linear.weight)  min: -0.2364(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6189, loss_cls=0.1168, loss_bbox=0.7164, matched_ious=0.5251, loss_iou=0.0916, loss_iou_reg=0.2236, d_time=0.00(0.01), f_time=1.34(1.26), b_time=1.35(1.27)  Time cost: 04:39/32:33 [12:27:10/9:50:31]  Acc_iter 35400       Data time: 0.00(0.01)  Forward time: 1.34(1.26)  Batch time: 1.35(1.27)
2025-09-03 21:22:10,295   INFO  Train:   21/36 ( 58%) [ 269/1759 ( 15%)]  Loss: 3.718 (3.35)  LR: 2.505e-03  Grad: 3.9853  max=0.3361(module.vfe.pfn_layers.0.linear.weight)  min: -0.3190(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6185, loss_cls=0.1130, loss_bbox=0.6902, matched_ious=0.5357, loss_iou=0.0910, loss_iou_reg=0.2211, d_time=0.00(0.01), f_time=1.17(1.26), b_time=1.18(1.27)  Time cost: 05:41/31:27 [12:28:12/9:48:22]  Acc_iter 35450       Data time: 0.00(0.01)  Forward time: 1.17(1.26)  Batch time: 1.18(1.27)
2025-09-03 21:23:13,224   INFO  Train:   21/36 ( 58%) [ 319/1759 ( 18%)]  Loss: 3.324 (3.34)  LR: 2.500e-03  Grad: 4.1723  max=0.5116(module.vfe.pfn_layers.0.linear.weight)  min: -0.2208(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6297, loss_cls=0.1173, loss_bbox=0.7115, matched_ious=0.5313, loss_iou=0.0903, loss_iou_reg=0.2195, d_time=0.00(0.01), f_time=1.19(1.26), b_time=1.19(1.27)  Time cost: 06:44/30:21 [12:29:15/9:46:44]  Acc_iter 35500       Data time: 0.00(0.01)  Forward time: 1.19(1.26)  Batch time: 1.19(1.27)
2025-09-03 21:24:15,402   INFO  Train:   21/36 ( 58%) [ 369/1759 ( 21%)]  Loss: 2.906 (3.35)  LR: 2.496e-03  Grad: 3.0768  max=1.1343(module.vfe.pfn_layers.0.linear.weight)  min: -0.7984(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6219, loss_cls=0.1099, loss_bbox=0.7249, matched_ious=0.5283, loss_iou=0.0941, loss_iou_reg=0.2225, d_time=0.00(0.01), f_time=1.21(1.25), b_time=1.21(1.26)  Time cost: 07:47/29:14 [12:30:17/9:44:20]  Acc_iter 35550       Data time: 0.00(0.01)  Forward time: 1.21(1.25)  Batch time: 1.21(1.26)
2025-09-03 21:25:17,873   INFO  Train:   21/36 ( 58%) [ 419/1759 ( 24%)]  Loss: 4.022 (3.36)  LR: 2.491e-03  Grad: 3.0521  max=0.2041(module.backbone_3d.cls_conv.3.bias)  min: -0.3143(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6360, loss_cls=0.1154, loss_bbox=0.7417, matched_ious=0.5238, loss_iou=0.0921, loss_iou_reg=0.2235, d_time=0.02(0.01), f_time=1.22(1.25), b_time=1.25(1.26)  Time cost: 08:49/28:09 [12:31:20/9:42:34]  Acc_iter 35600       Data time: 0.02(0.01)  Forward time: 1.22(1.25)  Batch time: 1.25(1.26)
2025-09-03 21:26:22,715   INFO  Train:   21/36 ( 58%) [ 469/1759 ( 27%)]  Loss: 2.832 (3.35)  LR: 2.486e-03  Grad: 3.3642  max=0.3409(module.vfe.pfn_layers.0.linear.weight)  min: -0.8324(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6126, loss_cls=0.1121, loss_bbox=0.7154, matched_ious=0.5337, loss_iou=0.0913, loss_iou_reg=0.2195, d_time=0.00(0.01), f_time=1.25(1.26), b_time=1.26(1.27)  Time cost: 09:54/27:11 [12:32:25/9:43:17]  Acc_iter 35650       Data time: 0.00(0.01)  Forward time: 1.25(1.26)  Batch time: 1.26(1.27)
2025-09-03 21:27:25,556   INFO  Train:   21/36 ( 58%) [ 519/1759 ( 30%)]  Loss: 3.193 (3.36)  LR: 2.482e-03  Grad: 2.0788  max=0.1869(module.vfe.pfn_layers.0.linear.weight)  min: -0.8124(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6416, loss_cls=0.1172, loss_bbox=0.7256, matched_ious=0.5286, loss_iou=0.0932, loss_iou_reg=0.2240, d_time=0.00(0.01), f_time=1.27(1.26), b_time=1.27(1.26)  Time cost: 10:57/26:07 [12:33:28/9:41:54]  Acc_iter 35700       Data time: 0.00(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.26)
2025-09-03 21:28:28,298   INFO  Train:   21/36 ( 58%) [ 569/1759 ( 32%)]  Loss: 2.811 (3.35)  LR: 2.477e-03  Grad: 3.5244  max=1.9043(module.vfe.pfn_layers.0.linear.weight)  min: -1.7057(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6035, loss_cls=0.1095, loss_bbox=0.6969, matched_ious=0.5279, loss_iou=0.0906, loss_iou_reg=0.2222, d_time=0.01(0.01), f_time=1.23(1.26), b_time=1.24(1.26)  Time cost: 11:59/25:03 [12:34:30/9:40:29]  Acc_iter 35750       Data time: 0.01(0.01)  Forward time: 1.23(1.26)  Batch time: 1.24(1.26)
2025-09-03 21:29:31,633   INFO  Train:   21/36 ( 58%) [ 619/1759 ( 35%)]  Loss: 2.927 (3.35)  LR: 2.472e-03  Grad: 2.5617  max=0.3400(module.vfe.pfn_layers.0.linear.weight)  min: -0.7187(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6210, loss_cls=0.1133, loss_bbox=0.6832, matched_ious=0.5321, loss_iou=0.0943, loss_iou_reg=0.2216, d_time=0.01(0.01), f_time=1.26(1.26), b_time=1.27(1.26)  Time cost: 13:03/24:00 [12:35:34/9:39:34]  Acc_iter 35800       Data time: 0.01(0.01)  Forward time: 1.26(1.26)  Batch time: 1.27(1.26)
2025-09-03 21:30:33,820   INFO  Train:   21/36 ( 58%) [ 669/1759 ( 38%)]  Loss: 3.229 (3.35)  LR: 2.467e-03  Grad: 2.7461  max=0.6365(module.vfe.pfn_layers.0.linear.weight)  min: -0.2301(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6340, loss_cls=0.1160, loss_bbox=0.7056, matched_ious=0.5306, loss_iou=0.0921, loss_iou_reg=0.2207, d_time=0.02(0.01), f_time=1.29(1.25), b_time=1.30(1.26)  Time cost: 14:05/22:55 [12:36:36/9:37:50]  Acc_iter 35850       Data time: 0.02(0.01)  Forward time: 1.29(1.25)  Batch time: 1.30(1.26)
2025-09-03 21:31:36,644   INFO  Train:   21/36 ( 58%) [ 719/1759 ( 41%)]  Loss: 3.351 (3.35)  LR: 2.463e-03  Grad: 2.8775  max=0.3698(module.vfe.pfn_layers.0.linear.weight)  min: -0.6939(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6231, loss_cls=0.1134, loss_bbox=0.7350, matched_ious=0.5250, loss_iou=0.0918, loss_iou_reg=0.2244, d_time=0.00(0.01), f_time=1.30(1.25), b_time=1.30(1.26)  Time cost: 15:08/21:51 [12:37:39/9:36:37]  Acc_iter 35900       Data time: 0.00(0.01)  Forward time: 1.30(1.25)  Batch time: 1.30(1.26)
2025-09-03 21:32:39,235   INFO  Train:   21/36 ( 58%) [ 769/1759 ( 44%)]  Loss: 3.644 (3.35)  LR: 2.458e-03  Grad: 1.7881  max=0.3457(module.vfe.pfn_layers.0.linear.weight)  min: -0.5653(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5941, loss_cls=0.1118, loss_bbox=0.6668, matched_ious=0.5242, loss_iou=0.0933, loss_iou_reg=0.2258, d_time=0.00(0.01), f_time=1.17(1.25), b_time=1.17(1.26)  Time cost: 16:10/20:48 [12:38:41/9:35:16]  Acc_iter 35950       Data time: 0.00(0.01)  Forward time: 1.17(1.25)  Batch time: 1.17(1.26)
2025-09-03 21:33:41,706   INFO  Train:   21/36 ( 58%) [ 819/1759 ( 47%)]  Loss: 3.169 (3.35)  LR: 2.453e-03  Grad: 2.1158  max=0.3113(module.vfe.pfn_layers.0.linear.weight)  min: -0.4707(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6229, loss_cls=0.1136, loss_bbox=0.7056, matched_ious=0.5221, loss_iou=0.0939, loss_iou_reg=0.2236, d_time=0.01(0.01), f_time=1.28(1.25), b_time=1.28(1.26)  Time cost: 17:13/19:44 [12:39:44/9:33:54]  Acc_iter 36000       Data time: 0.01(0.01)  Forward time: 1.28(1.25)  Batch time: 1.28(1.26)
2025-09-03 21:34:44,828   INFO  Train:   21/36 ( 58%) [ 869/1759 ( 49%)]  Loss: 3.604 (3.35)  LR: 2.448e-03  Grad: 2.2310  max=0.3212(module.vfe.pfn_layers.0.linear.weight)  min: -0.6349(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6152, loss_cls=0.1109, loss_bbox=0.7078, matched_ious=0.5364, loss_iou=0.0917, loss_iou_reg=0.2191, d_time=0.00(0.01), f_time=1.27(1.25), b_time=1.28(1.26)  Time cost: 18:16/18:41 [12:40:47/9:32:55]  Acc_iter 36050       Data time: 0.00(0.01)  Forward time: 1.27(1.25)  Batch time: 1.28(1.26)
2025-09-03 21:35:48,228   INFO  Train:   21/36 ( 58%) [ 919/1759 ( 52%)]  Loss: 3.288 (3.34)  LR: 2.444e-03  Grad: 2.4117  max=0.4715(module.vfe.pfn_layers.0.linear.weight)  min: -0.2922(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6183, loss_cls=0.1148, loss_bbox=0.6670, matched_ious=0.5328, loss_iou=0.0911, loss_iou_reg=0.2210, d_time=0.00(0.01), f_time=1.28(1.25), b_time=1.28(1.26)  Time cost: 19:19/17:39 [12:41:50/9:32:03]  Acc_iter 36100       Data time: 0.00(0.01)  Forward time: 1.28(1.25)  Batch time: 1.28(1.26)
2025-09-03 21:36:51,448   INFO  Train:   21/36 ( 58%) [ 969/1759 ( 55%)]  Loss: 3.286 (3.34)  LR: 2.439e-03  Grad: 2.7035  max=0.5307(module.vfe.pfn_layers.0.linear.weight)  min: -0.5188(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6225, loss_cls=0.1107, loss_bbox=0.7180, matched_ious=0.5366, loss_iou=0.0918, loss_iou_reg=0.2184, d_time=0.00(0.01), f_time=1.19(1.25), b_time=1.20(1.26)  Time cost: 20:23/16:36 [12:42:53/9:31:05]  Acc_iter 36150       Data time: 0.00(0.01)  Forward time: 1.19(1.25)  Batch time: 1.20(1.26)
2025-09-03 21:37:54,023   INFO  Train:   21/36 ( 58%) [1019/1759 ( 58%)]  Loss: 3.767 (3.34)  LR: 2.434e-03  Grad: 3.0011  max=0.6721(module.vfe.pfn_layers.0.linear.weight)  min: -0.3371(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6314, loss_cls=0.1141, loss_bbox=0.7396, matched_ious=0.5320, loss_iou=0.0899, loss_iou_reg=0.2203, d_time=0.00(0.01), f_time=1.23(1.25), b_time=1.24(1.26)  Time cost: 21:25/15:32 [12:43:56/9:29:50]  Acc_iter 36200       Data time: 0.00(0.01)  Forward time: 1.23(1.25)  Batch time: 1.24(1.26)
2025-09-03 21:38:56,863   INFO  Train:   21/36 ( 58%) [1069/1759 ( 61%)]  Loss: 2.927 (3.34)  LR: 2.429e-03  Grad: 3.0237  max=0.1788(module.vfe.pfn_layers.0.linear.weight)  min: -0.6253(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6028, loss_cls=0.1096, loss_bbox=0.6682, matched_ious=0.5326, loss_iou=0.0905, loss_iou_reg=0.2208, d_time=0.00(0.01), f_time=1.24(1.25), b_time=1.24(1.26)  Time cost: 22:28/14:29 [12:44:59/9:28:42]  Acc_iter 36250       Data time: 0.00(0.01)  Forward time: 1.24(1.25)  Batch time: 1.24(1.26)
2025-09-03 21:39:59,995   INFO  Train:   21/36 ( 58%) [1119/1759 ( 64%)]  Loss: 3.343 (3.34)  LR: 2.424e-03  Grad: 3.1872  max=0.6442(module.vfe.pfn_layers.0.linear.weight)  min: -0.4749(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6267, loss_cls=0.1145, loss_bbox=0.6806, matched_ious=0.5251, loss_iou=0.0926, loss_iou_reg=0.2256, d_time=0.00(0.01), f_time=1.27(1.25), b_time=1.28(1.26)  Time cost: 23:31/13:26 [12:46:02/9:27:42]  Acc_iter 36300       Data time: 0.00(0.01)  Forward time: 1.27(1.25)  Batch time: 1.28(1.26)
2025-09-03 21:41:02,075   INFO  Train:   21/36 ( 58%) [1169/1759 ( 66%)]  Loss: 3.515 (3.34)  LR: 2.419e-03  Grad: 3.3862  max=0.4185(module.vfe.pfn_layers.0.linear.weight)  min: -0.1549(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6268, loss_cls=0.1162, loss_bbox=0.6799, matched_ious=0.5318, loss_iou=0.0908, loss_iou_reg=0.2215, d_time=0.01(0.01), f_time=1.24(1.25), b_time=1.25(1.26)  Time cost: 24:33/12:23 [12:47:04/9:26:17]  Acc_iter 36350       Data time: 0.01(0.01)  Forward time: 1.24(1.25)  Batch time: 1.25(1.26)
2025-09-03 21:42:05,624   INFO  Train:   21/36 ( 58%) [1219/1759 ( 69%)]  Loss: 2.914 (3.34)  LR: 2.414e-03  Grad: 3.6735  max=0.6304(module.vfe.pfn_layers.0.linear.weight)  min: -0.6853(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6014, loss_cls=0.1072, loss_bbox=0.7116, matched_ious=0.5356, loss_iou=0.0911, loss_iou_reg=0.2186, d_time=0.00(0.01), f_time=1.21(1.25), b_time=1.22(1.26)  Time cost: 25:37/11:20 [12:48:08/9:25:26]  Acc_iter 36400       Data time: 0.00(0.01)  Forward time: 1.21(1.25)  Batch time: 1.22(1.26)
2025-09-03 21:43:08,777   INFO  Train:   21/36 ( 58%) [1269/1759 ( 72%)]  Loss: 3.385 (3.34)  LR: 2.409e-03  Grad: 2.9629  max=0.4365(module.vfe.pfn_layers.0.linear.weight)  min: -0.5756(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6188, loss_cls=0.1154, loss_bbox=0.6949, matched_ious=0.5348, loss_iou=0.0898, loss_iou_reg=0.2184, d_time=0.00(0.01), f_time=1.22(1.25), b_time=1.22(1.26)  Time cost: 26:40/10:17 [12:49:11/9:24:27]  Acc_iter 36450       Data time: 0.00(0.01)  Forward time: 1.22(1.25)  Batch time: 1.22(1.26)
2025-09-03 21:44:12,205   INFO  Train:   21/36 ( 58%) [1319/1759 ( 75%)]  Loss: 3.992 (3.34)  LR: 2.404e-03  Grad: 3.0654  max=0.3779(module.vfe.pfn_layers.0.linear.weight)  min: -0.2732(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6130, loss_cls=0.1107, loss_bbox=0.7349, matched_ious=0.5319, loss_iou=0.0907, loss_iou_reg=0.2205, d_time=0.00(0.01), f_time=1.22(1.25), b_time=1.22(1.26)  Time cost: 27:43/09:14 [12:50:14/9:23:32]  Acc_iter 36500       Data time: 0.00(0.01)  Forward time: 1.22(1.25)  Batch time: 1.22(1.26)
2025-09-03 21:45:15,553   INFO  Train:   21/36 ( 58%) [1369/1759 ( 78%)]  Loss: 3.933 (3.33)  LR: 2.399e-03  Grad: 3.2240  max=0.4718(module.vfe.pfn_layers.0.linear.weight)  min: -0.3802(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6054, loss_cls=0.1128, loss_bbox=0.6559, matched_ious=0.5338, loss_iou=0.0904, loss_iou_reg=0.2210, d_time=0.00(0.01), f_time=1.19(1.25), b_time=1.20(1.26)  Time cost: 28:47/08:11 [12:51:18/9:22:36]  Acc_iter 36550       Data time: 0.00(0.01)  Forward time: 1.19(1.25)  Batch time: 1.20(1.26)
2025-09-03 21:46:18,929   INFO  Train:   21/36 ( 58%) [1419/1759 ( 81%)]  Loss: 3.364 (3.33)  LR: 2.395e-03  Grad: 3.6167  max=0.5762(module.vfe.pfn_layers.0.linear.weight)  min: -0.6355(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6029, loss_cls=0.1088, loss_bbox=0.7007, matched_ious=0.5342, loss_iou=0.0918, loss_iou_reg=0.2192, d_time=0.01(0.01), f_time=1.20(1.25), b_time=1.21(1.26)  Time cost: 29:50/07:08 [12:52:21/9:21:39]  Acc_iter 36600       Data time: 0.01(0.01)  Forward time: 1.20(1.25)  Batch time: 1.21(1.26)
2025-09-03 21:47:23,328   INFO  Train:   21/36 ( 58%) [1469/1759 ( 84%)]  Loss: 3.781 (3.33)  LR: 2.390e-03  Grad: 3.6405  max=0.3942(module.vfe.pfn_layers.0.linear.weight)  min: -0.4105(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6029, loss_cls=0.1083, loss_bbox=0.6867, matched_ious=0.5373, loss_iou=0.0942, loss_iou_reg=0.2194, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.21(1.26)  Time cost: 30:54/06:05 [12:53:25/9:21:00]  Acc_iter 36650       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.21(1.26)
2025-09-03 21:48:25,942   INFO  Train:   21/36 ( 58%) [1519/1759 ( 86%)]  Loss: 3.884 (3.33)  LR: 2.385e-03  Grad: 7.0975  max=2.8484(module.vfe.pfn_layers.0.linear.weight)  min: -3.7439(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6119, loss_cls=0.1089, loss_bbox=0.6988, matched_ious=0.5358, loss_iou=0.0922, loss_iou_reg=0.2182, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.26)  Time cost: 31:57/05:02 [12:54:28/9:19:49]  Acc_iter 36700       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.26)
2025-09-03 21:49:30,010   INFO  Train:   21/36 ( 58%) [1569/1759 ( 89%)]  Loss: 3.071 (3.33)  LR: 2.380e-03  Grad: 4.0795  max=0.2831(module.vfe.pfn_layers.0.linear.weight)  min: -0.5072(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6279, loss_cls=0.1126, loss_bbox=0.7180, matched_ious=0.5266, loss_iou=0.0923, loss_iou_reg=0.2228, d_time=0.00(0.01), f_time=1.29(1.26), b_time=1.30(1.26)  Time cost: 33:01/03:59 [12:55:32/9:19:03]  Acc_iter 36750       Data time: 0.00(0.01)  Forward time: 1.29(1.26)  Batch time: 1.30(1.26)
2025-09-03 21:50:32,972   INFO  Train:   21/36 ( 58%) [1619/1759 ( 92%)]  Loss: 2.774 (3.33)  LR: 2.374e-03  Grad: 4.2334  max=0.2111(module.dense_head.prediction_head.dim.1.bias)  min: -0.2808(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6008, loss_cls=0.1088, loss_bbox=0.7036, matched_ious=0.5425, loss_iou=0.0915, loss_iou_reg=0.2161, d_time=0.00(0.01), f_time=1.26(1.26), b_time=1.26(1.26)  Time cost: 34:04/02:56 [12:56:35/9:17:57]  Acc_iter 36800       Data time: 0.00(0.01)  Forward time: 1.26(1.26)  Batch time: 1.26(1.26)
2025-09-03 21:51:35,892   INFO  Train:   21/36 ( 58%) [1669/1759 ( 95%)]  Loss: 3.140 (3.33)  LR: 2.369e-03  Grad: 2.0383  max=0.2717(module.vfe.pfn_layers.0.linear.weight)  min: -0.2968(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6254, loss_cls=0.1117, loss_bbox=0.7155, matched_ious=0.5404, loss_iou=0.0915, loss_iou_reg=0.2158, d_time=0.00(0.01), f_time=1.22(1.26), b_time=1.23(1.26)  Time cost: 35:07/01:53 [12:57:38/9:16:51]  Acc_iter 36850       Data time: 0.00(0.01)  Forward time: 1.22(1.26)  Batch time: 1.23(1.26)
2025-09-03 21:52:38,977   INFO  Train:   21/36 ( 58%) [1719/1759 ( 98%)]  Loss: 3.426 (3.33)  LR: 2.364e-03  Grad: 3.3067  max=2.0586(module.vfe.pfn_layers.0.linear.weight)  min: -1.1541(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6171, loss_cls=0.1121, loss_bbox=0.7333, matched_ious=0.5313, loss_iou=0.0915, loss_iou_reg=0.2198, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.26)  Time cost: 36:10/00:50 [12:58:41/9:15:48]  Acc_iter 36900       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.26)
2025-09-03 21:53:27,199   INFO  Train:   21/36 ( 58%) [1758/1759 (100%)]  Loss: 4.136 (3.33)  LR: 2.360e-03  Grad: 2.6856  max=0.6863(module.vfe.pfn_layers.0.linear.weight)  min: -0.2706(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6319, loss_cls=0.1160, loss_bbox=0.6836, matched_ious=0.5355, loss_iou=0.0906, loss_iou_reg=0.2185, d_time=0.01(0.01), f_time=0.73(1.26), b_time=0.73(1.26)  Time cost: 36:58/00:01 [12:59:29/9:14:43]  Acc_iter 36939       Data time: 0.01(0.01)  Forward time: 0.73(1.26)  Batch time: 0.73(1.26)

                                               [Aepochs:  58%|█████▊    | 21/36 [12:59:30<9:15:08, 2220.60s/it]epochs:  58%|█████▊    | 21/36 [12:59:30<9:15:09, 2220.62s/it]epochs:  58%|█████▊    | 21/36 [12:59:30<9:15:09, 2220.61s/it]epochs:  58%|█████▊    | 21/36 [12:59:30<9:15:09, 2220.61s/it]epochs:  58%|█████▊    | 21/36 [12:59:30<9:15:09, 2220.61s/it]epochs:  58%|█████▊    | 21/36 [12:59:30<9:15:09, 2220.61s/it]epochs:  58%|█████▊    | 21/36 [12:59:30<9:15:09, 2220.64s/it]epochs:  58%|█████▊    | 21/36 [12:59:30<9:15:09, 2220.60s/it]
train:   0%|          | 0/1759 [00:00<?, ?it/s][A2025-09-03 21:53:32,652   INFO  Train:   22/36 ( 61%) [   0/1759 (  0%)]  Loss: 3.055 (3.05)  LR: 2.360e-03  Grad: 2.7239  max=0.8960(module.vfe.pfn_layers.0.linear.weight)  min: -0.7935(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5679, loss_cls=0.1090, loss_bbox=0.5580, matched_ious=0.5809, loss_iou=0.0879, loss_iou_reg=0.1962, d_time=1.50(1.50), f_time=2.65(2.65), b_time=4.15(4.15)  Time cost: 00:03/1:51:49 [12:59:35/27:57:20]  Acc_iter 36940       Data time: 1.50(1.50)  Forward time: 2.65(2.65)  Batch time: 4.15(4.15)
2025-09-03 21:53:45,191   INFO  Train:   22/36 ( 61%) [  10/1759 (  1%)]  Loss: 2.158 (3.11)  LR: 2.359e-03  Grad: 3.0111  max=1.5111(module.vfe.pfn_layers.0.linear.weight)  min: -0.4675(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5633, loss_cls=0.1039, loss_bbox=0.6534, matched_ious=0.5327, loss_iou=0.0895, loss_iou_reg=0.2219, d_time=0.00(0.14), f_time=1.27(1.38), b_time=1.27(1.52)  Time cost: 00:16/43:20 [12:59:47/10:53:30]  Acc_iter 36950       Data time: 0.00(0.14)  Forward time: 1.27(1.38)  Batch time: 1.27(1.52)
2025-09-03 21:54:49,367   INFO  Train:   22/36 ( 61%) [  60/1759 (  3%)]  Loss: 3.192 (3.27)  LR: 2.354e-03  Grad: 2.9417  max=0.7763(module.vfe.pfn_layers.0.linear.weight)  min: -0.4820(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6163, loss_cls=0.1112, loss_bbox=0.6886, matched_ious=0.5273, loss_iou=0.0931, loss_iou_reg=0.2229, d_time=0.00(0.04), f_time=1.17(1.28), b_time=1.18(1.33)  Time cost: 01:20/37:22 [13:00:51/9:39:13]  Acc_iter 37000       Data time: 0.00(0.04)  Forward time: 1.17(1.28)  Batch time: 1.18(1.33)
2025-09-03 21:55:52,172   INFO  Train:   22/36 ( 61%) [ 110/1759 (  6%)]  Loss: 3.010 (3.28)  LR: 2.349e-03  Grad: 3.6541  max=1.5637(module.vfe.pfn_layers.0.linear.weight)  min: -1.3285(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6069, loss_cls=0.1121, loss_bbox=0.6734, matched_ious=0.5346, loss_iou=0.0891, loss_iou_reg=0.2196, d_time=0.00(0.03), f_time=1.20(1.27), b_time=1.20(1.29)  Time cost: 02:23/35:29 [13:01:54/9:25:28]  Acc_iter 37050       Data time: 0.00(0.03)  Forward time: 1.20(1.27)  Batch time: 1.20(1.29)
2025-09-03 21:56:55,828   INFO  Train:   22/36 ( 61%) [ 160/1759 (  9%)]  Loss: 3.959 (3.33)  LR: 2.344e-03  Grad: 3.3158  max=0.5018(module.vfe.pfn_layers.0.linear.weight)  min: -0.7765(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6347, loss_cls=0.1123, loss_bbox=0.7442, matched_ious=0.5326, loss_iou=0.0913, loss_iou_reg=0.2178, d_time=0.00(0.02), f_time=2.47(1.27), b_time=2.48(1.29)  Time cost: 03:26/34:15 [13:02:58/9:21:56]  Acc_iter 37100       Data time: 0.00(0.02)  Forward time: 2.47(1.27)  Batch time: 2.48(1.29)
2025-09-03 21:57:58,621   INFO  Train:   22/36 ( 61%) [ 210/1759 ( 12%)]  Loss: 3.008 (3.34)  LR: 2.339e-03  Grad: 3.6515  max=0.4931(module.vfe.pfn_layers.0.linear.weight)  min: -0.9360(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6419, loss_cls=0.1163, loss_bbox=0.7251, matched_ious=0.5283, loss_iou=0.0891, loss_iou_reg=0.2209, d_time=0.00(0.02), f_time=1.26(1.26), b_time=1.26(1.28)  Time cost: 04:29/33:00 [13:04:01/9:17:47]  Acc_iter 37150       Data time: 0.00(0.02)  Forward time: 1.26(1.26)  Batch time: 1.26(1.28)
2025-09-03 21:59:01,254   INFO  Train:   22/36 ( 61%) [ 260/1759 ( 15%)]  Loss: 3.192 (3.35)  LR: 2.334e-03  Grad: 3.6086  max=0.4694(module.vfe.pfn_layers.0.linear.weight)  min: -0.9376(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6317, loss_cls=0.1126, loss_bbox=0.7289, matched_ious=0.5306, loss_iou=0.0895, loss_iou_reg=0.2199, d_time=0.01(0.01), f_time=1.16(1.26), b_time=1.16(1.27)  Time cost: 05:32/31:49 [13:05:03/9:14:33]  Acc_iter 37200       Data time: 0.01(0.01)  Forward time: 1.16(1.26)  Batch time: 1.16(1.27)
2025-09-03 22:00:04,152   INFO  Train:   22/36 ( 61%) [ 310/1759 ( 18%)]  Loss: 2.799 (3.34)  LR: 2.329e-03  Grad: 3.6951  max=0.2035(module.vfe.pfn_layers.0.linear.weight)  min: -0.6100(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6212, loss_cls=0.1140, loss_bbox=0.6683, matched_ious=0.5422, loss_iou=0.0878, loss_iou_reg=0.2143, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.25(1.27)  Time cost: 06:35/30:41 [13:06:06/9:12:24]  Acc_iter 37250       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.25(1.27)
2025-09-03 22:01:07,467   INFO  Train:   22/36 ( 61%) [ 360/1759 ( 20%)]  Loss: 3.050 (3.34)  LR: 2.323e-03  Grad: 2.3643  max=0.3848(module.vfe.pfn_layers.0.linear.weight)  min: -0.1830(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6279, loss_cls=0.1142, loss_bbox=0.6859, matched_ious=0.5264, loss_iou=0.0940, loss_iou_reg=0.2246, d_time=0.00(0.01), f_time=1.21(1.26), b_time=1.22(1.27)  Time cost: 07:38/29:37 [13:07:09/9:11:03]  Acc_iter 37300       Data time: 0.00(0.01)  Forward time: 1.21(1.26)  Batch time: 1.22(1.27)
2025-09-03 22:02:09,916   INFO  Train:   22/36 ( 61%) [ 410/1759 ( 23%)]  Loss: 2.834 (3.34)  LR: 2.318e-03  Grad: 2.7192  max=0.5242(module.vfe.pfn_layers.0.linear.weight)  min: -0.4309(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6219, loss_cls=0.1111, loss_bbox=0.7165, matched_ious=0.5335, loss_iou=0.0914, loss_iou_reg=0.2182, d_time=0.00(0.01), f_time=1.20(1.26), b_time=1.20(1.27)  Time cost: 08:41/28:30 [13:08:12/9:08:51]  Acc_iter 37350       Data time: 0.00(0.01)  Forward time: 1.20(1.26)  Batch time: 1.20(1.27)
2025-09-03 22:03:12,626   INFO  Train:   22/36 ( 61%) [ 460/1759 ( 26%)]  Loss: 3.793 (3.34)  LR: 2.313e-03  Grad: 3.2165  max=0.9639(module.vfe.pfn_layers.0.linear.weight)  min: -0.9827(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6120, loss_cls=0.1099, loss_bbox=0.6989, matched_ious=0.5355, loss_iou=0.0897, loss_iou_reg=0.2187, d_time=0.01(0.01), f_time=1.32(1.26), b_time=1.32(1.27)  Time cost: 09:43/27:24 [13:09:15/9:07:10]  Acc_iter 37400       Data time: 0.01(0.01)  Forward time: 1.32(1.26)  Batch time: 1.32(1.27)
2025-09-03 22:04:15,134   INFO  Train:   22/36 ( 61%) [ 510/1759 ( 29%)]  Loss: 3.412 (3.34)  LR: 2.308e-03  Grad: 3.0448  max=0.1974(module.vfe.pfn_layers.0.linear.weight)  min: -0.7069(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5951, loss_cls=0.1111, loss_bbox=0.6913, matched_ious=0.5341, loss_iou=0.0913, loss_iou_reg=0.2205, d_time=0.01(0.01), f_time=1.32(1.26), b_time=1.33(1.27)  Time cost: 10:46/26:19 [13:10:17/9:05:25]  Acc_iter 37450       Data time: 0.01(0.01)  Forward time: 1.32(1.26)  Batch time: 1.33(1.27)
2025-09-03 22:05:18,550   INFO  Train:   22/36 ( 61%) [ 560/1759 ( 32%)]  Loss: 3.295 (3.33)  LR: 2.303e-03  Grad: 3.4715  max=0.3691(module.vfe.pfn_layers.0.linear.weight)  min: -0.9546(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5881, loss_cls=0.1090, loss_bbox=0.6746, matched_ious=0.5373, loss_iou=0.0900, loss_iou_reg=0.2183, d_time=0.00(0.01), f_time=1.28(1.26), b_time=1.28(1.27)  Time cost: 11:49/25:16 [13:11:21/9:04:30]  Acc_iter 37500       Data time: 0.00(0.01)  Forward time: 1.28(1.26)  Batch time: 1.28(1.27)
2025-09-03 22:06:21,319   INFO  Train:   22/36 ( 61%) [ 610/1759 ( 35%)]  Loss: 3.015 (3.32)  LR: 2.297e-03  Grad: 3.4054  max=0.3449(module.vfe.pfn_layers.0.linear.weight)  min: -0.2402(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5807, loss_cls=0.1083, loss_bbox=0.6191, matched_ious=0.5382, loss_iou=0.0931, loss_iou_reg=0.2182, d_time=0.01(0.01), f_time=1.27(1.26), b_time=1.27(1.26)  Time cost: 12:52/24:12 [13:12:23/9:03:07]  Acc_iter 37550       Data time: 0.01(0.01)  Forward time: 1.27(1.26)  Batch time: 1.27(1.26)
2025-09-03 22:07:24,195   INFO  Train:   22/36 ( 61%) [ 660/1759 ( 38%)]  Loss: 3.748 (3.32)  LR: 2.292e-03  Grad: 3.6100  max=0.6154(module.vfe.pfn_layers.0.linear.weight)  min: -0.2385(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6048, loss_cls=0.1086, loss_bbox=0.6976, matched_ious=0.5326, loss_iou=0.0909, loss_iou_reg=0.2209, d_time=0.00(0.01), f_time=1.24(1.26), b_time=1.24(1.26)  Time cost: 13:55/23:08 [13:13:26/9:01:50]  Acc_iter 37600       Data time: 0.00(0.01)  Forward time: 1.24(1.26)  Batch time: 1.24(1.26)
2025-09-03 22:08:26,786   INFO  Train:   22/36 ( 61%) [ 710/1759 ( 40%)]  Loss: 3.374 (3.31)  LR: 2.287e-03  Grad: 3.8082  max=0.6027(module.vfe.pfn_layers.0.linear.weight)  min: -0.5472(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6123, loss_cls=0.1109, loss_bbox=0.6389, matched_ious=0.5381, loss_iou=0.0898, loss_iou_reg=0.2169, d_time=0.00(0.01), f_time=1.34(1.25), b_time=1.34(1.26)  Time cost: 14:57/22:04 [13:14:29/9:00:25]  Acc_iter 37650       Data time: 0.00(0.01)  Forward time: 1.34(1.25)  Batch time: 1.34(1.26)
2025-09-03 22:09:29,812   INFO  Train:   22/36 ( 61%) [ 760/1759 ( 43%)]  Loss: 3.541 (3.31)  LR: 2.281e-03  Grad: 4.0506  max=1.2255(module.vfe.pfn_layers.0.linear.weight)  min: -0.3219(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5976, loss_cls=0.1091, loss_bbox=0.6675, matched_ious=0.5401, loss_iou=0.0887, loss_iou_reg=0.2167, d_time=0.00(0.01), f_time=1.32(1.25), b_time=1.33(1.26)  Time cost: 16:00/21:01 [13:15:32/8:59:18]  Acc_iter 37700       Data time: 0.00(0.01)  Forward time: 1.32(1.25)  Batch time: 1.33(1.26)
2025-09-03 22:10:32,195   INFO  Train:   22/36 ( 61%) [ 810/1759 ( 46%)]  Loss: 3.898 (3.32)  LR: 2.276e-03  Grad: 4.4070  max=1.2148(module.vfe.pfn_layers.0.linear.weight)  min: -0.3167(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6350, loss_cls=0.1158, loss_bbox=0.7291, matched_ious=0.5395, loss_iou=0.0920, loss_iou_reg=0.2149, d_time=0.01(0.01), f_time=1.24(1.25), b_time=1.25(1.26)  Time cost: 17:03/19:57 [13:16:34/8:57:51]  Acc_iter 37750       Data time: 0.01(0.01)  Forward time: 1.24(1.25)  Batch time: 1.25(1.26)
2025-09-03 22:11:34,980   INFO  Train:   22/36 ( 61%) [ 860/1759 ( 49%)]  Loss: 2.818 (3.32)  LR: 2.271e-03  Grad: 4.3521  max=0.1943(module.dense_head.prediction_head.height.1.bias)  min: -0.6581(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6029, loss_cls=0.1110, loss_bbox=0.6601, matched_ious=0.5346, loss_iou=0.0920, loss_iou_reg=0.2224, d_time=0.00(0.01), f_time=1.22(1.25), b_time=1.23(1.26)  Time cost: 18:06/18:54 [13:17:37/8:56:39]  Acc_iter 37800       Data time: 0.00(0.01)  Forward time: 1.22(1.25)  Batch time: 1.23(1.26)
2025-09-03 22:12:37,819   INFO  Train:   22/36 ( 61%) [ 910/1759 ( 52%)]  Loss: 2.687 (3.31)  LR: 2.266e-03  Grad: 4.4681  max=0.5802(module.vfe.pfn_layers.0.linear.weight)  min: -0.2131(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5699, loss_cls=0.1084, loss_bbox=0.6407, matched_ious=0.5451, loss_iou=0.0898, loss_iou_reg=0.2152, d_time=0.00(0.01), f_time=1.30(1.25), b_time=1.31(1.26)  Time cost: 19:08/17:50 [13:18:40/8:55:29]  Acc_iter 37850       Data time: 0.00(0.01)  Forward time: 1.30(1.25)  Batch time: 1.31(1.26)
2025-09-03 22:13:40,772   INFO  Train:   22/36 ( 61%) [ 960/1759 ( 55%)]  Loss: 3.049 (3.31)  LR: 2.260e-03  Grad: 4.2511  max=0.3908(module.vfe.pfn_layers.0.linear.weight)  min: -0.3872(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6068, loss_cls=0.1085, loss_bbox=0.6947, matched_ious=0.5281, loss_iou=0.0912, loss_iou_reg=0.2215, d_time=0.01(0.01), f_time=1.27(1.25), b_time=1.27(1.26)  Time cost: 20:11/16:47 [13:19:43/8:54:23]  Acc_iter 37900       Data time: 0.01(0.01)  Forward time: 1.27(1.25)  Batch time: 1.27(1.26)
2025-09-03 22:14:43,679   INFO  Train:   22/36 ( 61%) [1010/1759 ( 57%)]  Loss: 2.758 (3.31)  LR: 2.255e-03  Grad: 4.4396  max=0.6415(module.vfe.pfn_layers.0.linear.weight)  min: -0.3079(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5886, loss_cls=0.1070, loss_bbox=0.6467, matched_ious=0.5437, loss_iou=0.0889, loss_iou_reg=0.2158, d_time=0.00(0.01), f_time=1.18(1.25), b_time=1.18(1.26)  Time cost: 21:14/15:44 [13:20:46/8:53:17]  Acc_iter 37950       Data time: 0.00(0.01)  Forward time: 1.18(1.25)  Batch time: 1.18(1.26)
2025-09-03 22:15:46,173   INFO  Train:   22/36 ( 61%) [1060/1759 ( 60%)]  Loss: 3.509 (3.31)  LR: 2.249e-03  Grad: 4.6786  max=0.6576(module.vfe.pfn_layers.0.linear.weight)  min: -0.3853(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5940, loss_cls=0.1096, loss_bbox=0.7054, matched_ious=0.5401, loss_iou=0.0899, loss_iou_reg=0.2164, d_time=0.00(0.01), f_time=1.21(1.25), b_time=1.21(1.26)  Time cost: 22:17/14:41 [13:21:48/8:52:00]  Acc_iter 38000       Data time: 0.00(0.01)  Forward time: 1.21(1.25)  Batch time: 1.21(1.26)
2025-09-03 22:16:48,483   INFO  Train:   22/36 ( 61%) [1110/1759 ( 63%)]  Loss: 2.934 (3.31)  LR: 2.244e-03  Grad: 4.4615  max=1.8577(module.vfe.pfn_layers.0.linear.weight)  min: -0.2753(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5961, loss_cls=0.1061, loss_bbox=0.7008, matched_ious=0.5340, loss_iou=0.0916, loss_iou_reg=0.2195, d_time=0.00(0.01), f_time=1.27(1.25), b_time=1.28(1.26)  Time cost: 23:19/13:37 [13:22:51/8:50:41]  Acc_iter 38050       Data time: 0.00(0.01)  Forward time: 1.27(1.25)  Batch time: 1.28(1.26)
2025-09-03 22:17:52,074   INFO  Train:   22/36 ( 61%) [1160/1759 ( 66%)]  Loss: 2.848 (3.30)  LR: 2.239e-03  Grad: 4.3075  max=0.3856(module.vfe.pfn_layers.0.linear.weight)  min: -0.7741(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5874, loss_cls=0.1059, loss_bbox=0.6652, matched_ious=0.5358, loss_iou=0.0906, loss_iou_reg=0.2209, d_time=0.00(0.01), f_time=1.31(1.25), b_time=1.31(1.26)  Time cost: 24:23/12:34 [13:23:54/8:49:51]  Acc_iter 38100       Data time: 0.00(0.01)  Forward time: 1.31(1.25)  Batch time: 1.31(1.26)
2025-09-03 22:18:55,283   INFO  Train:   22/36 ( 61%) [1210/1759 ( 69%)]  Loss: 2.640 (3.31)  LR: 2.233e-03  Grad: 4.5176  max=0.3678(module.vfe.pfn_layers.0.linear.weight)  min: -0.4604(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.6159, loss_cls=0.1129, loss_bbox=0.7417, matched_ious=0.5247, loss_iou=0.0919, loss_iou_reg=0.2251, d_time=0.00(0.01), f_time=1.29(1.25), b_time=1.30(1.26)  Time cost: 25:26/11:32 [13:24:57/8:48:52]  Acc_iter 38150       Data time: 0.00(0.01)  Forward time: 1.29(1.25)  Batch time: 1.30(1.26)
2025-09-03 22:19:58,730   INFO  Train:   22/36 ( 61%) [1260/1759 ( 72%)]  Loss: 2.863 (3.31)  LR: 2.228e-03  Grad: 2.6231  max=0.4620(module.vfe.pfn_layers.0.linear.weight)  min: -0.5007(module.vfe.pfn_layers.0.linear.weight)  NaN: False  loss_hm=0.5821, loss_cls=0.1086, loss_bbox=0.6719, matched_ious=0.5408, loss_iou=0.0900, loss_iou_reg=0.2158, d_time=0.00(0.01), f_time=1.22(1.25), b_time=1.22(1.26)  Time cost: 26:29/10:29 [13:26:01/8:47:58]  Acc_iter 38200       Data time: 0.00(0.01)  Forward time: 1.22(1.25)  Batch time: 1.22(1.26)
                                                              Traceback (most recent call last):
  File "/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/train.py", line 246, in <module>
    main()
  File "/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/train.py", line 188, in main
    train_model(
  File "/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/train_utils/train_utils.py", line 272, in train_model
    accumulated_iter = train_one_epoch(
  File "/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/train_utils/train_utils.py", line 71, in train_one_epoch
    loss, tb_dict, disp_dict = model_func(model, batch)
  File "/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/models/__init__.py", line 44, in model_func
    ret_dict, tb_dict, disp_dict = model(batch_dict)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/models/detectors/transfusion.py", line 12, in forward
    batch_dict = cur_module(batch_dict)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/models/dense_heads/transfusion_head.py", line 290, in forward
    res = self.predict(feats)
  File "/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/models/dense_heads/transfusion_head.py", line 267, in predict
    query_feat_T = self.decoder(
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/../pcdet/models/model_utils/transfusion_utils.py", line 91, in forward
    query2 = self.multihead_attn(query=self.with_pos_embed(query, query_pos_embed),
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/functional.py", line 5300, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/nn/functional.py", line 4846, in _in_projection_packed
    return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 986.00 MiB. GPU 3 has a total capacty of 23.78 GiB of which 822.56 MiB is free. Process 2654396 has 22.97 GiB memory in use. Of the allocated memory 21.04 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2025-09-03 22:20:37,823] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 23 closing signal SIGTERM
[2025-09-03 22:20:37,825] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 24 closing signal SIGTERM
[2025-09-03 22:20:37,825] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 25 closing signal SIGTERM
[2025-09-03 22:20:37,826] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 27 closing signal SIGTERM
[2025-09-03 22:20:37,826] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 28 closing signal SIGTERM
[2025-09-03 22:20:37,827] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 29 closing signal SIGTERM
[2025-09-03 22:20:37,827] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 30 closing signal SIGTERM
[2025-09-03 22:20:38,044] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 3 (pid: 26) of binary: /root/miniconda3/envs/sparseformerv2/bin/python
Traceback (most recent call last):
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launch.py", line 196, in <module>
    main()
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launch.py", line 192, in main
    launch(args)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launch.py", line 177, in launch
    run(args)
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/miniconda3/envs/sparseformerv2/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/mnt/csi-data-aly/user/hongfeizhang/mypaper/SparseFormerV2/tools/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-03_22:20:37
  host      : eflops102.aliyun.com
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 26)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
